{
  "name" : "1703.02196.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cooperative Epistemic Multi-Agent Planning for Implicit Coordination",
    "authors" : [ "Sujata Ghosh", "Thorsten Engesser", "Thomas Bolander", "Robert Mattmüller", "Bernhard Nebel" ],
    "emails" : [ "engesset@cs.uni-freiburg.de", "tobo@dtu.dk", "mattmuel@cs.uni-freiburg.de", "nebel@cs.uni-freiburg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Sujata Ghosh and R. Ramanujam: M4M9 EPTCS 243, 2017, pp. 75–90, doi:10.4204/EPTCS.243.6\nc© T. Engesser, T. Bolander, R. Mattmüller & B. Nebel This work is licensed under the Creative Commons Attribution License.\nCooperative Epistemic Multi-Agent Planning for Implicit Coordination\nThorsten Engesser Institut für Informatik\nAlbert-Ludwigs-Universität Freiburg, Germany\nengesset@cs.uni-freiburg.de\nThomas Bolander DTU Compute\nTechnical University of Denmark Copenhagen, Denmark\ntobo@dtu.dk\nRobert Mattmüller Institut für Informatik\nAlbert-Ludwigs-Universität Freiburg, Germany\nmattmuel@cs.uni-freiburg.de\nBernhard Nebel Institut für Informatik\nAlbert-Ludwigs-Universität Freiburg, Germany\nnebel@cs.uni-freiburg.de\nEpistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. We extend the DEL-based epistemic planning framework to include perspective shifts, allowing us to define new notions of sequential and conditional planning with implicit coordination. With these, it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time. First we define the central planning notions and sketch the implementation of a planning system built on those notions. Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice."
    }, {
      "heading" : "1 Introduction",
      "text" : "One important task in Multi-Agent Systems is to collaboratively reach a joint goal with multiple autonomous agents. The problem is particularly challenging in situations where the knowledge and capabilities required to reach the goal are distributed among the agents. Most existing approaches therefore apply some centralized coordinating instance from the outside, strictly separating the stages of communication and negotiation from the agents’ internal planning and reasoning processes. In contrast, building upon the epistemic planning framework by Bolander and Andersen [8], we propose a decentralized planning notion in which each agent has to individually reason about the entire problem and autonomously decide when and how to (inter-)act. For this, both reasoning about the other agents’ possible contributions and reasoning about their capabilities of performing the same reasoning is needed. We achieve our notion of implicitly coordinated plans by requiring all desired communicative abilities to be modeled as epistemic actions which then can be planned alongside their ontic counterparts, thus enabling the agents to perform observations and coordinate at run time. It captures the intuition that communication clearly constitutes an action by itself and, more subtly, that even a purely ontic action can play a communicative role (e.g. indirectly suggesting follow-up actions to another agent). Thus, for many problems our approach appears quite natural. On the practical side, the epistemic planning framework allows a very expressive way of defining both the agents’ physical and communicative abilities.\nConsider the following example scenario. Bob would like to borrow the apartment of his friend Anne while she is away on vacation. Anne would be very happy to do him this favor. So they now have the\njoint goal of making sure that Bob can enter the apartment when he arrives. Anne will think about how to achieve the goal, and might come up with the following plan: Anne puts the key under the door mat; when Bob arrives, Bob takes the key from under the door mat; Bob opens the door with the key. Note that the plan does not only contain the actions required by Anne herself, but also the actions of Bob. These are the kind of multi-agent plans that this paper is about.\nHowever, the plan just presented does not count as an implicitly coordinated plan. When Bob arrives at the apartment, he will clearly not know that the key is under the door mat, unless Anne has told him, and this announcement was not part of the plan just presented. If Anne has the ability to take Bob’s perspective (she has a Theory of Mind concerning Bob [29]), Anne should of course be able to foresee this problem, and realize that her plan can not be expected to be successful. An improved plan would then be: Anne puts the key under the door mat; Anne calls Bob to let him know where the key is; when Bob arrives, Bob takes the key from under the door mat; Bob opens the door with the key. This does qualify as an implicitly coordinated plan. Anne now knows that Bob will know that he can find the key under the door mat and hence will be able to reach the goal. Anne does not have to request or even coordinate the sub-plan for Bob (which is: take key under door mat; open door with key), as she knows he will himself be able to determine this sub-plan given the information she provides. This is an important aspect of implicit coordination: coordination happens implicitly as a consequence of observing the actions of others (including announcements), never explicitly through agreeing or committing to a specific plan. The essential contributions of this paper are to formally define this notion of implicitly coordinated plans as well as to document and benchmark an implemented epistemic planner that produces such plans.\nOur work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and Löwe, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13]. Andersen, Bolander, and Jensen [4] extended the approach to allow strong and weak conditional planning in the single-agent case. Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of “nested” [8] or “shallow” knowledge states [26, 27, 28]. Since compilation approaches to classical planning can only deal with bounded nesting of knowledge (or belief), similar to Bolander and Andersen [8], we use search in the space of epistemic states to find a solution. One of the important features that distinguishes our work from more traditional multi-agent planning [9] is the explicit notion of perspective shifts needed for agents to reason about the possible plan contributions of other agents—and hence needed to achieve implicit coordination.\nOur concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information. These approaches are based on concurrent epistemic game structures. Our approach is different in a number of ways, including: 1) As in classical planning, our actions and their effects are explicitly and compactly represented in an action description language (using the event models of DEL); 2) Instead of joint actions we have sequential action execution, where the order in which the agents act is not predefined; 3) None of the existing solution concepts considered in temporal epistemic logics capture the stepwise shifting perspective underlying our notion of implicitly coordinated plans.\nThe present paper is an extended and revised version of a paper presented at the Workshop on Distributed and Multi-Agent Planning (DMAP) 2015 (without archival proceedings). The present paper primarily offers an improved presentation: extended and improved introduction, improved motivation, better examples, improved formulation of definitions and theorems, simplified notation, and more discussions of related work. The technical content is essentially as in the original DMAP paper, except we now compare implicitly coordinated plans to standard sequential plans, we now formally derive the correct notion of an implicitly coordinated plan from the natural conditions it should satisfy, and we have added a proposition that gives a semantic characterisation of implicitly coordinated policies."
    }, {
      "heading" : "2 Theoretical Background",
      "text" : "To represent planning tasks as the ‘apartment borrowing’ example of the introduction, we need a formal framework where: (1) agents can reason about the knowledge and ignorance of other agents; (2) both fully and partially observable actions can be described in a compact way (Bob doesn’t see Anne placing the key under the mat). Dynamic Epistemic Logic (DEL) satisfies these conditions. We first briefly recapitulate the foundations of DEL, following the conventions of Bolander and Andersen [8]. What is new in this exposition is mainly the parts on perspective shifts in Section 2.2.\nWe now define epistemic languages, epistemic states and epistemic actions. All of these are defined relative to a given finite set of agents A and a given finite set of atomic propositions P. To keep the exposition simple, we will not mention the dependency on A and P in the following."
    }, {
      "heading" : "2.1 Epistemic Language and Epistemic States",
      "text" : "Definition 1. The epistemic language LKC is ϕ ::= > | ⊥ | p | ¬ϕ | ϕ ∧ϕ | Kiϕ |Cϕ where p ∈ P and i ∈ A.\nWe read Kiϕ as “agent i knows ϕ” and Cϕ as “it is common knowledge that ϕ”.\nDefinition 2. An epistemic model isM= 〈W,(∼i)i∈A,V 〉 where\n• The domain W is a non-empty finite set of worlds.\n• ∼i ⊆W ×W is an equivalence relation called the indistinguishability relation for agent i.\n• V : P→P(W ) assigns a valuation to each atomic proposition.\nFor Wd ⊆W , the pair (M,Wd) is called an epistemic state (or simply a state), and the worlds of Wd are called the designated worlds. A state is called global if Wd = {w} for some world w (called the actual world), and we then often write (M,w) instead of (M,{w}). We use Sgl to denote the set of global states. For any state s = (M,Wd), we let Globals(s) = {(M,w) | w ∈Wd}. A state (M,Wd) is called a local state for agent i if Wd is closed under ∼i. A local state for i is minimal if Wd is a minimal set closed under∼i. We use Smini to denote the set of minimal local states of i. Given a state s = (M,Wd), the associated local state of agent i, denoted si, is (M,{v | v∼i w and w ∈Wd}).\nDefinition 3. Let (M,Wd) be a state withM= 〈W,(∼i)i∈A,V 〉. For i ∈ A, p ∈ P and ϕ,ψ ∈ LKC, we define truth as follows (with the propositional cases being standard and hence left out):\n(M,Wd) |= ϕ iff (M,w) |= ϕ for all w ∈Wd (M,w) |= Kiϕ iff (M,w′) |= ϕ for all w′ ∼i w (M,w) |=Cϕ iffM,w′ |= ϕ for all w′ ∼∗ w\nwhere ∼∗ is the transitive closure of ⋃\ni∈A ∼i.\nExample 1. LetA= {Anne,Bob} and P = {m}, where m is intended to express that the key is under the door mat. Consider the following global state s = (M,w), where the nodes represent worlds, the edges represent the indistinguishability relations (reflexive edges left out), and is used for designated worlds:\ns = (M,w) = w : m v :\nBob\nEach node is labeled by the name of the world, and the list of atomic propositions true at the world. The state represents a situation where the key is under the door mat (m is true at the actual world w), but Bob considers it possible that it isn’t (m is not true at the world v indistinguishable from w by Bob). We can verify that in this state Anne knows that the key is under the mat, Bob doesn’t, and Anne knows that he doesn’t: s |= KAnnem∧¬KBobm∧KAnne¬KBobm. The fact that Bob does not know the key to be under the mat can also be expressed in terms of local states. Bob’s local perspective on the state s is his associated local state sBob = (M,{w,v}). We have sBob 6|= m, signifying that from Bob’s perspective, m can not be verified."
    }, {
      "heading" : "2.2 Perspective Shifts",
      "text" : "In general, given a global state s, the associated local state si will represent agent i’s internal perspective on that state. Going from si to (si) j amounts to a perspective shift, where agent i’s perspective on the perspective of agent j is taken. In Example 1, Anne’s perspective on the state s is sAnne, which is s itself. Bob’s perspective is sBob = (M,{w,v}). When Anne wants to reason about the available actions to Bob, e.g. whether he will be able to take the key from under the door mat or not, she will have to shift to his perspective, i.e. reason about what holds true in (sAnne)Bob, which is the same as sBob in this case. This type of perspective shift is going to be central in our notion of implicitly coordinated plans, since it is essential to the ability of an agent to reason about other agents’ possible contributions to a plan from their own perspective. As the introductory example shows, this ability is essential: If Anne can not reason about Bob’s contribution to the overall plan from his own perspective, she will not realize that she needs to call him to let him know where the key is.\nNote that any local state s induces a unique set of global states, Globals(s), and that we can hence choose to think of s as a compact representation of the “belief state” Globals(s) over global states.\nWe have the following basic properties concerning perspective shifts (associated local states), where the third follows directly from the two first:\nProposition 1. Let s be a state, i ∈ A and ϕ ∈ LKC.\n1. si |= ϕ iff s |= Kiϕ .\n2. If s is local for agent i then si = s.\n3. If s is local for agent i then s |= ϕ iff s |= Kiϕ ."
    }, {
      "heading" : "2.3 Dynamic Language and Epistemic Actions",
      "text" : "To model actions, like announcing locations of keys and picking them up, we use the event models of DEL.\nDefinition 4. An event model is E = 〈E,(∼i)i∈A,pre,post〉 where\n• The domain E is a non-empty finite set of events.\n• ∼i ⊆ E×E is an equivalence relation called the indistinguishability relation for agent i.\n• pre : E→LKC assigns a precondition to each event.\n• post : E →LKC assigns a postcondition to each event. For all e ∈ E, post(e) is a conjunction of literals (atomic propositions and their negations, including >).\nFor Ed ⊆ E, the pair (E ,Ed) is called an epistemic action (or simply an action), and the events in Ed are called the designated events. An action is called global if Ed = {e} for some event e, and we then often write (E ,e) instead of (E ,{e}). Similar to states, (E ,Ed) is called a local action for agent i when Ed is closed under ∼i.\nEach event of an epistemic action represents a different possible outcome. By using multiple events e,e′ ∈ E that are indistinguishable (i.e. e ∼i e′), it is possible to obfuscate the outcomes for some agent i ∈A, i.e. modeling partially observable actions. Using event models with |Ed |> 1, it is also possible to model sensing actions (where a priori, multiple outcomes are considered possible), and nondeterministic actions [8].\nThe product update is used to specify the successor state resulting from the application of an action in a state.\nDefinition 5. Let a state s = (M,Wd) and an action a = (E ,Ed) be given withM= 〈W,(∼i)i∈A,V 〉 and E = 〈E,(∼i)i∈A,pre,post〉. Then the product update of s with a is s⊗a = (〈W ′,(∼′i)i∈A,V ′〉 ,W ′d) where\n• W ′ = {(w,e) ∈W ×E | M,w |= pre(e)}\n• ∼′i = {((w,e),(w′,e′)) ∈W ′×W ′ | w∼i w′ and e∼i e′} • V ′(p) = {(w,e) ∈W ′ | post(e) |= p or (M,w |= p and post(e) 6|= ¬p)}\n• W ′d = {(w,e) ∈W ′ | w ∈Wd and e ∈ Ed}.\nExample 2. Consider the following epistemic action try-take = (E ,{e, f}), using the same conventions as for epistemic models, except each event e is labeled by 〈pre(e),post(e)〉:\ntry-take = (E ,{e, f}) = e : 〈m,h∧¬m〉 f : 〈¬m,>〉\nAnne\nIt represents the action of Bob attempting to take the key from under the mat. The event e = 〈m,h∧¬m〉 represents that if the key is indeed under the mat (the precondition m is true), then the result will be that Bob holds the key and it is no longer under the mat (the postcondition h∧¬m becomes true). The event f = 〈¬m,>〉 represents that if the key is not under the mat, nothing will happen (the postcondition is the trivial one, >). Note the indistinguishability edge for Anne: She is not there to see whether the action is successful or not. Note however that she is still is aware that either e or f happens, so the action represents the situation where she knows that he is attempting to take the key, but not necessarily whether he is successful (except if she already either knows m or knows ¬m).\nLetting s denote the state from Example 1, we can calculate the result of executing try-take in s:\ns⊗ try-take = (w,e) : h (v, f ) :\nNote that the result is for Bob to have the key and for this to be common knowledge among Anne and Bob (s⊗ try-take |= Ch). So it seems that if we assume initially to be in the state s, and want to find a plan to achieve h, then the simple plan consisting only of the action try-take should work. It is, however, a bit more complicated than that. Let us assume that Bob does strong planning, that is, only looks for plans that are guaranteed to reach the goal. The problem is then that, from Bob’s perspective, try-take can not be guaranteed to reach the goal. This is formally because we have:\nsBob⊗ try-take = (w,e) : h (v, f ) :\nBoth worlds being designated, but distinguishable, means that Bob at plan time (before executing the action) considers them both as possible outcomes of try-take, but is aware that he will at run time (after having executed the action) know which one it is (see [8] for a more full discussion of the plan time/run time distinction). Since the world (v, f ) is designated, we have sBob⊗ try-take 6|= h. So from Bob’s perspective, try-take might fail to produce h and is hence not a strong plan to achieve h. Intuitively, it is of course simply because he does not, at plan time, know whether the key is under the mat or not.\nSince sAnne = s and s⊗ try-take |= h, it might seem that try-take is still a strong plan to achieve h from the perspective of Anne. But in fact, it is not, at least not of the implicitly coordinated type we will define below. The issue is, try-take is still an action that Bob has to execute, but Anne knows that Bob does not know it to be succesful, and she can therefore not expect him to execute it. The idea is that when Anne comes up with a plan that involves actions of Bob, she should change her perspective to his, in order to find out what he can be expected to do. Technically speaking, it is because (sAnne)Bob⊗ try-take 6|= h that the plan is not implicitly coordinated from the perspective of Anne.\nWe extend the language LKC into the dynamic language LDEL by adding a modality [(E ,e)] for each global action (E ,e). The truth conditions are extended with the following standard clause from DEL: (M,w) |= [(E ,e)]ϕ iff (M,w) |= pre(e) implies (M,w)⊗ (E ,e) |= ϕ .\nWe define the following abbreviations:\n[(E ,Ed)]ϕ := ∧ e∈Ed [(E ,e)]ϕ and 〈(E ,Ed)〉ϕ := ¬[(E ,Ed)]¬ϕ.\nWe say that an action (E ,Ed) is applicable in a state (M,Wd) if for all w∈Wd there is an event e∈ Ed s.t. (M,w) |= pre(e). Intuitively, an action is applicable in a state if for each possible situation (designated world), at least one possible outcome (designated event) is specified.\nExample 3. Consider again the state s from Example 1 and the action try-take from Example 2. The action try-take is trivially seen to be applicable in the state s, since the designated event e has its precondition satisfied in the designated world w. The action try-take is also applicable in sBob, since (M,w) |= pre(e) and (M,v) |= pre( f ). This shows that try-take is applicable from the perspective of Bob. Intuitively, it is so because it is only an action for attempting to take the key. Even if the key is not under the mat, the action will specify an outcome (having the trivial postcondition >).\nLet s = (M,Wd) denote an epistemic state and a = (E ,Ed) an action. Andersen [3] shows that a is applicable in s iff s |= 〈a〉>, and that s |= [a]ϕ iff s⊗ a |= ϕ . We now define a further abbreviation: ((a))ϕ := 〈a〉>∧ [a]ϕ . Hence:\ns |= ((a))ϕ iff a is applicable in s and s⊗a |= ϕ (1)\nThus ((a))ϕ means that the application of a is possible and will (necessarily) lead to a state fulfilling ϕ ."
    }, {
      "heading" : "3 Cooperative Planning",
      "text" : "As mentioned in the introduction, in this paper we assume each action to be executable by a single agent, that is, we are not considering joint actions. In our ‘apartment borrowing’ example there are two agents, Anne and Bob. They are supposed to execute different parts of the plan to reach the goal of Bob getting access to the apartment. For instance, Anne is the one to put the key under the mat, and Bob is the one to take it from under the mat. To represent who performs which actions of a plan, we will introduce what we call an owner function (inspired by the approach of Löwe, Pacuit, and Witzel [22]). An owner function is defined to be a mapping ω : A→A, mapping each action to the agent who can execute it. For\nany action a ∈ A, we call ω(a) the owner of a. Note that by defining the owner function this way, every action has a unique owner. This can be done without loss of generality, since we can always include any number of semantically equivalent actions in A, if we wish some action to be executable by several agents (e.g., if we want both Anne and Bob to be able to open and close the door). We can now define epistemic planning tasks.\nDefinition 6. A cooperative planning task (or simply a planning task) Π = 〈s0,A,ω,γ〉 consists of an initial epistemic state s0, a finite set of epistemic actions A, an owner function ω : A→A, and a goal formula γ of LKC. Each a ∈ A has to be local for ω(a). When s0 is a global state, we call it a global planning task. When s0 is local for agent i, we call it a planning task for agent i. Given a planning task Π = 〈s0,A,ω,γ〉 and a state s, we define Π(s) := 〈s,A,ω,γ〉. Given a planning task Π = 〈s0,A,ω,γ〉, the associated planning task for agent i is Πi = Π(si).\nGiven a multi-agent system facing a global planning task Π, each individual agent i is facing the planning task Πi (agent i cannot observe the global initial state s0 directly, only the associated local state s0i).\nIn the following, we will investigate various possible solution concepts for cooperative planning tasks. The solution to a planning task is called a plan. A plan can either be sequential (a sequence of actions) or conditional (a policy). We will first, in Section 3.1, consider the simplest possible type of solution, a standard sequential plan. Then, in Section 3.2, we are going to introduce the more complex notion of a sequential implicitly coordinated plan, and in Section 3.3 this will be generalized to implicitly coordinated policies."
    }, {
      "heading" : "3.1 Standard Sequential Plans",
      "text" : "The standard notion of a sequential plan in automated planning is as follows (see, e.g., [17]). An action sequence (a1, . . . ,an) is called a (standard sequential) plan if for every i≤ n, ai is applicable in the result of executing the action sequence (a1, . . . ,ai−1) in the initial state, and when executing the entire sequence (a1, . . . ,an) in the initial state, a state satisfying the goal formula is reached. Let us transfer this notion into the DEL-based setting of this paper. In our setting, the result of executing an action a in a state s is given as s⊗ a. Hence, the above conditions for (a1, . . . ,an) being a plan can be expressed in the following way in our setting, where s0 denotes the initial state, and γ the goal formula: for every i ≤ n, ai is applicable in s0⊗a1⊗·· ·⊗ai−1, and s0⊗a1⊗·· ·⊗an |= γ . Note that by equation (1) above, these conditions are equivalent to simply requiring s0 |= ((a1))((a2)) · · ·((an))γ . Hence we get the following definition.\nDefinition 7. Let Π = 〈s0,A,ω,γ〉 be a planning task. A standard sequential plan for Π is a sequence (a1, . . . ,an) of actions from A satisfying s0 |= ((a1))((a2)) · · ·((an))γ .\nThis solution concept is equivalent to the one considered in [8]. As the following example shows, it is not sufficiently strong to capture the notion of an ‘implicitly coordinated plan’ that we are after.\nExample 4. Consider again the scenario of Example 2 where the key is initially under the mat, Bob does not know this, and the goal is for Bob to have the key. The only action available in the scenario is for Bob to attempt to take the key from under the mat. Using the states and actions defined in Examples 1 and 2, we can express this scenario as a cooperative planning task Π= 〈s0,A,ω,γ〉where s0 = s, A= {try-take}, ω(try-take) = Bob, γ = h. In Example 2, we informally concluded that the plan only consisting of trytake is a strong plan, since it is guaranteed to reach the goal, but that it is not a strong plan from the perspective of Bob. Given our formal definitions, we can now make this precise as follows:\n1. The sequence (try-take) is a standard sequential plan for Π.\n2. The sequence (try-take) is not a standard sequential plan for ΠBob.\nThe first item follows from the fact that try-take is applicable in s (Example 3), and that s⊗ try-take |= h (Example 2). The second item follows from sBob⊗ try-take 6|= h (Example 2).\nWe also have that (try-take) is a standard sequential plan for ΠAnne, since sAnne = s. This proves that the notion of a standard sequential plan is insufficient for our purposes. If Anne is faced with the planning task ΠAnne, she should not be allowed to consider (try-take) as a sufficient solution to the problem. She should be aware that the action try-take is to be executed by Bob, and from Bob’s perspective, (try-take) is not a (strong) solution to the planning task (Item 2 above). So we need a way of explicitly integrating perspective-shifts into our notion of a solution to a planning task, and this is what we will do next."
    }, {
      "heading" : "3.2 Implicitly Coordinated Sequential Plans",
      "text" : "It follows from Definition 7 that (a1, . . . ,an) is a standard sequential plan for some planning task Π = 〈s0,A,ω,γ〉 iff a1, . . . ,an ∈ A and the formula ((a1)) · · ·((an))γ is true in s0. More generally, we can think of a planning notion as being defined via a mapping X that takes a plan π and a planning task Π as parameters, and returns a logical formula X(π,Π) such that, for all states s, s |= X(π,Π) iff π is a plan for Π(s). In the case of standard sequential plans, it follows directly from Definition 7 that X would be defined like this:\nX((a1, . . . ,an),〈s0,A,ω,γ〉) = ((a1)) · · ·((an))γ (2)\nfor all planning tasks 〈s0,A,ω,γ〉 and all a1, . . . ,an ∈ A. We now wish to define a similar mapping Y , so that s |= Y (π,Π) iff π is an implicitly coordinated plan for Π(s). Our strategy is to list the natural conditions that Y should satisfy, and then derive the exact definition of Y (and hence implicitly coordinated plans) directly from those. First of all, the empty action sequence, denoted by ε , should be an implicitly coordinated plan iff it satisfies the goal formula, which is expressed by the following simple condition on Y :\nY (ε,〈s0,A,ω,γ〉) = γ. (3)\nFor non-empty action sequences, the ‘apartment borrowing’ example studied above gives us the following insights. If Anne is trying to come up with a plan where one of the steps is to be executed by Bob, then Anne has to make sure that Bob can himself verify his action to be applicable, and that he can himself verify that executing the action will lead to a state where the rest of the plan will succeed. More generally, for an action sequence (a1, . . . ,an) to be considered implicitly coordinated, the owner of the first action a1 has to know that a1 is applicable and will lead to a situation where (a2, . . . ,an) is again an implicitly coordinated plan. This leads us directly to the following condition on Y , for all planning tasks 〈s0,A,ω,γ〉 and all a1, . . . ,an ∈ A with n≥ 1:\nY ((a1, . . . ,an),〈s0,A,ω,γ〉) = Kω(a1)((a1))Y ((a2, . . . ,an),〈s0,A,ω,γ〉) (4)\nIt is now easy to see that any mapping Y satisfying (3) and (4) must necessarily be defined as follows, for all planning tasks 〈s0,A,ω,γ〉 and all action sequences (a1, . . . ,an) ∈ A:\nY ((a1, . . . ,an),〈s0,A,ω,γ〉) = Kω(a1)((a1))Kω(a2)((a2)) · · ·Kω(an)((an))γ\nThis leads us directly to the following definition.\nDefinition 8. Let Π = 〈s0,A,ω,γ〉 be a cooperative planning task. An implicitly coordinated plan for Π is a sequence (a1, . . . ,an) of actions from A such that:\ns0 |= Kω(a1)((a1))Kω(a2)((a2)) · · ·Kω(an)((an))γ (5)\nIf (a1, . . . ,an) is an implicitly coordinated plan for Πi, then it is said to be an implicitly coordinated plan for agent i to the planning task Π.\nNote that the formula used to define implicitly coordinated plans above is uniquely determined by the natural conditions (3) and (4).\nThe following proposition gives a more structural, and semantic, characterization of implicitly coordinated plans. It becomes clear that such plans can be found by performing a breadth-first search over the set of successively applicable actions, shifting the perspective for each state transition to the owner of the respective action. Proposition 2. For a cooperative planning task Π = 〈s0,A,ω,γ〉, a non-empty sequence (a1, . . . ,an) of actions from A is an implicitly coordinated plan for Π iff a1 is applicable in s ω(a1) 0 and (a2, . . . ,an) is an implicitly coordinated plan for Π(sω(a1)0 ⊗a1). The proposition can be seen as a semantic counterpart of (4), and is easily proven using (1), Proposition 1 and (5). Example 5. Consider again the planning task Π = 〈s0,A,ω,γ〉 of Example 4 with s0 = s, A = {try-take}, ω(try-take) = Bob, γ = h. In Example 4 we concluded that (try-take) is a standard sequential plan for Π. From Example 2, we have that sBob⊗ try-take 6|= h, and hence s 6|= Kω(try-take)((try-take))h (using (1) and Proposition 1). This shows that, as expected, (try-take) is not an implicitly coordinated plan for Π.\nIn the introduction, we noted that the solution to this problem would be for Anne to make sure to announce the location of the key to Bob. Let us now treat this formally within the developed framework. We need to give Anne the possibility of announcing the location of the key, so we define a new planning task Π′ = 〈s0,A′,ω,γ〉 with A′ = {try-take,announce}. Here announce is the action e : 〈m,>〉 with ω(announce) = Anne. In DEL, this action is known as a public announcement of m. It can now easily be formally verified that s |= KAnne((announce))KBob((try-take))h. In words: Anne knows that she can announce the location of the key, and that this will lead to a situation where Bob knows he can attempt to take the key, and he knows that he will be successful in this attempt. In other words, (announce, try-take) is indeed an implicitly coordinated plan to achieve that Bob has the key, consistent with our informal analysis in the introduction of the paper. Example 6. Consider a situation with agents A= {1,2,3} where a letter is to be passed from agent 1 to one of the other two agents, possibly via the third agent. Mutually exclusive propositions at-1,at-2,at-3∈ P are used to denote the current carrier of the letter, while for-1, for-2, for-3 ∈ P denote the addressee. In our example, agent 1 has a letter for agent 3, so at-1 and for-3 are initially true.\ns0 = at-1, for-2 at-1, for-3\n2,3\nIn s0, all agents know that agent 1 has the letter (at-1), but agents 2 and 3 do not know who of them is the addressee (for-2 or for-3). We assume that agent 1 can only exchange letters with agent 2 and agent 2 can only exchange letters with agent 3. We thus define the four possible actions a12,a21,a23,a32, with ai j being the composite action of agent i publicly passing the letter to agent j and privately informing him about the correct addressee (the name of the addressee is on the envelope, but only visible to the receiver). I.e.\nai j = 〈at-i∧ for-2,¬at-i∧at- j〉 〈at-i∧ for-3,¬at-i∧at- j〉\nA\\{ j}\nGiven that the joint goal is to pass a letter to its addressee, the global planning task then is Π= 〈s0,A,ω,γ〉 with A = {a12,a21,a23,a32}, ω(ai j) = i for all i, j, and γ = ∧ i∈{1,2,3} (for-i→ at-i). Consider the action sequence (a12,a23): Agent 1 passes the letter to agent 2, and agent 2 passes it on to agent 3. It can now be verified that s01 |= K1((a12))K2((a23))γ and s0i 6|= K1((a12))K2((a23))γ for i = 2,3. Hence (a12,a23) is an implicitly coordinated plan for agent 1, but not for agents 2 and 3. This is because in the beginning agents 2 and 3 do not know to whom of them the letter is intended and hence cannot verify that (a12,a23) will lead to a goal state. However, after agent 1’s execution of a12, agent 2 can distinguish between the possible addressees at run time, and find his subplan (a23), as contemplated by agent 1."
    }, {
      "heading" : "3.3 Conditional Plans",
      "text" : "Sequential plans are often not sufficient to solve a given epistemic planning task. In particular, as soon as branching on nondeterministic action outcomes or obtained observations becomes necessary, we need conditional plans to solve such a task. Unlike Andersen, Bolander, and Jensen [4], who represent conditional plans as action trees with branches depending on knowledge formula conditions, we represent them as policy functions (πi)i∈A, where each πi maps minimal local states of agent i into actions of agent i. We now define two different types of policies, joint policies and global policies, and later show them to be equivalent.\nDefinition 9 (Joint policy). Let Π = 〈s0,A,ω,γ〉 be a cooperative planning task. Then a joint policy (πi)i∈A consists of partial functions πi : Smini → A satisfying for all states s and actions a: if πi(s) = a then ω(a) = i and a is applicable in s. Definition 10 (Global policy). Let Π = 〈s0,A,ω,γ〉 be a cooperative planning task. Then a global policy π is a mapping π : Sgl→P(A) satisfying the requirements knowledge of preconditions (KOP), per-agent determinism (DET), and uniformity (UNIF):\n(KOP) For all s ∈ Sgl, a ∈ π(s): a is applicable in sω(a). (DET) For all s ∈ Sgl,a,a′ ∈ π(s) with ω(a) = ω(a′): a = a′. (UNIF) For all s, t ∈ Sgl,a ∈ π(s) with sω(a) = tω(a): a ∈ π(t). Proposition 3. Any joint policy (πi)i∈A induces a global policy π given by\nπ(s) = { πi(si) | i ∈ A and πi(si) is defined } .\nConversely, any global policy π induces a joint policy (πi)i∈A given by\nπi(si) = a for all (s,A′) ∈ π, a ∈ A′ with ω(a) = i.\nFurthermore, the two mappings (πi)i∈A 7→ π (mapping joint policies to induced global policies) and π 7→ (πi)i∈A (mapping global policies to induced joint policies) are each other’s inverse.\nProof. First we prove that the induced mapping π as defined above is a global policy. Condition (KOP): If a ∈ π(s) then πi(si) = a for some i, and by definition of joint policy this implies a is applicable in si. Condition (DET): Assume a,a′ ∈ π(s) with ω(a) = ω(a′). By definition of π we have πi(si) = a and π j(s j) = a′ for some i, j. By definition of joint policy, ω(a) = i and ω(a′) = j. Since ω(a) = ω(a′) we get i = j and hence πi(si) = π j(s j). This implies a = a′. Condition (UNIF): Assume a ∈ π(s) and sω(a) = tω(a). By definition of π and joint policy, we get πi(si) = a for i = ω(a). Thus si = t i, and\nsince πi(si) = a, we immediately get πi(t i) = a and hence a ∈ π(t). We now prove that the induced mappings (πi)i∈A defined above form a joint policy. Constraint (KOP) ensures the applicability property as required by Definition 9, while the constraints (DET) and (UNIF) ensure the right-uniqueness of each partial function πi. It is easy to show that the two mappings (πi)i∈A 7→ π and π 7→ (πi)i∈A are each other’s inverse, using their definition.\nBy Proposition 3, we can identify joint and global policies, and will in the following move back and forth between the two. Notice that Definitions 9 and 10 allow a policy to distinguish between modally equivalent states. A more sophisticated definition avoiding this is possible, but is beyond the scope of this paper. Usually, a policy π is only considered to be a solution to a planning task if it is closed in the sense that π is defined for all non-goal states reachable following π . Here, we want to distinguish between two different notions of closedness: one that refers to all states reachable from a centralized perspective, and one that refers to all states considered reachable when tracking perspective shifts. To that end, we distinguish between centralized and perspective-sensitive successor functions.\nWe take a successor function to be any function σ : Sgl×A→P(Sgl). Successor functions are intended to map pairs of states s and actions a into the states σ(s,a) that can result from executing a in s. Which states can result from executing a in s depend on whether we take the objective, centralized view, or whether we take the subjective view of an agent. An agent might subjectively consider more outcomes possible than are objectively possible. We define the centralized successor function as σcen(s,a) = Globals(s⊗ a). It specifies the global states that are possible after the application of a in s. If closedness of a global policy π based on the centralized successor function is required, then no execution of π will ever lead to a non-goal state where π is undefined. Like for sequential planning, we are again interested in the decentralized scenario where each agent has to plan and decide when and how to act by himself under incomplete knowledge. We achieve this by encoding the perspective shifts to the next agent to act in the perspective-sensitive successor function σps(s,a) = Globals(sω(a)⊗ a). Unlike σcen(s,a), σps(s,a) considers a state s′ to be a successor of s after application of a if agent ω(a) considers s′ possible after the application of a, not only if s′ is actually possible from a global perspective. Thus, σcen(s,a) is always a (possibly strict) subset of σps(s,a), and a policy πps that is closed wrt. σps(s,a) must be defined for at least the states for which a policy πcen that is closed wrt. σcen(s,a) must be defined. This corresponds to the intuition that solution existence for decentralized planning with implicit coordination is a stronger property than solution existence for centralized planning. For both successor functions, we can now formalize what a strong solution is that can be executed by the agents. Our notion satisfies the usual properties of strong plans [11], namely closedness, properness and acyclicity.\nDefinition 11 (Strong Policy). Let Π = 〈s0,A,ω,γ〉 be a cooperative planning task and σ a successor function. A global policy π is called a strong policy for Π with respect to σ if\n(i) Finiteness: π is finite. (ii) Foundedness: for all s ∈ Globals(s0), (1) s |= γ , or (2) π(s) 6= /0.\n(iii) Closedness: for all (s,A′) ∈ π , a ∈ A′,s′ ∈ σ(s,a), (1) s′ |= γ , or (2) π(s′) 6= /0.\nNote that we do not explicitly require acyclicity, since this is already implied by a literal interpretation of the product update semantic that ensures unique new world names after each update. It then follows from (i) and (iii) that π is proper. We call strong plans with respect to σcen centralized policies and strong plans with respect to σps implicitly coordinated policies.\nAnalogous to Proposition 2, we want to give a semantic characterization of implicitly coordinated policies. For this, we first define a successor of a state s0 by following a policy π to be a state s ∈ Globals(s′0⊗a) for arbitrary states s′0 ∈ Globals(s0) and arbitrary actions a ∈ π(s′0). We can then show that if π is an implicitly coordinated policy for a planning task Π, each successor state s of the initial\nstate s0 either will already be a goal state, or there will be some agent i ∈ A who can find an implicitly coordinated policy for his own associated planning task prescribing an action for himself.\nProposition 4. Let π be an implicitly coordinated policy for a planning task Π = 〈s0,A,ω,γ〉 and let s be a non-goal successor state of s0 by following π . Then there is an agent i ∈ A such that π(s) contains at least one of agent i’s actions and π is an implicitly coordinated policy of Π(si).\nProof. The existence of an action a ∈ π(s) with some owner i follows directly from the closedness of implicitly coordinated policies. We need to show that π is also implicitly coordinated for Π(si). Finiteness and closedness of π still hold for Π(si), since π was already finite and closed for Π, and this does not change when replacing s0 with si. For foundedness of π for Π(si), we have to show that π is defined and returns a nonempty set of actions for all global states s′ ∈ Globals(si). For s itself, we already known that a ∈ π(s). By uniformity, since all such s′ are indistinguishable from s for agent i, π must assign the same action a to all states s′ ∈ Globals(si).\nExample 7. Consider again the letter passing problem introduced in Example 6. Let s0,2 and s0,3 denote the global states that are initially considered possible by agent 2.\ns0,2 = at-1, for-2 at-1, for-3\n2,3 s0,3 =\nat-1, for-2 at-1, for-3\n2,3\nWith s1,3 = s0,3⊗a12, a policy for agent 2 is given by π1 = {s0,3 7→ a12,s0,2 7→ a12} ,π2 = {s1,3 7→ a23} . After the contemplated application of a12 by agent 1 (in both cases), agent 2 can distinguish between s1,2 = s0,2⊗ a12, where the goal is already reached and nothing has to be done, and s1,3, where agent 2 can apply a23, leading directly to the goal state s1,3⊗a23. Thus, π is an implicitly coordinated policy for Π2. While in the sequential case, agent 2 has to wait for the first action a12 of agent 1 to be able to find its subplan, it can find the policy (πi)i∈A in advance by explicitly planning for a run-time distinction.\nIn general, strong policies can be found by performing an AND-OR search, where AND branching corresponds to branching over different epistemic worlds and OR branching corresponds to branching over different actions. By considering modally equivalent states as duplicates and thereby transforming the procedure into a graph search, space and time requirements can be reduced, although great care has to be taken to deal with cycles correctly."
    }, {
      "heading" : "4 Experiments",
      "text" : "We implemented a planner that is capable of finding implicitly coordinated plans and policies1, and conducted two experiments: one small case study of the Russian card problem [12] intended to show how this problem can be modeled and solved from an individual perspective, and one experiment investigating the scaling behavior of our approach on private transportation problems in the style of Examples 6 and 7, using instances of increasing size."
    }, {
      "heading" : "4.1 Russian Card Problem",
      "text" : "In the Russian card problem, seven cards numbered 0, . . . ,6 are randomly dealt to three agents. Alice and Bob get three cards each, while Eve gets the single remaining card. Initially, each agent only knows its own cards. The task is now for Alice and Bob to inform each other about their respective cards using only public announcements, without revealing the holder of any single card to Eve. The problem\n1Our code can be downloaded at https://gkigit.informatik.uni-freiburg.de/tengesser/planner\nwas analyzed and solved from the global perspective by van Ditmarsch et al. [14], and a given protocol was verified from an individual perspective by Ågotnes et al. [1]. We want to solve the problem from the individual perspective of agent Alice and find an implicitly coordinated policy for her. To keep the problem computationally feasible, we impose some restrictions on the resulting policy, namely that the first action has to be Alice truthfully announcing five possible alternatives for her own hand, and that the second one has to be Bob announcing the card Eve is holding. Without loss of generality, we fix one specific initial hand for Alice, namely 012. From a plan for this initial hand, plans for all other initial hands can be obtained by renaming. For simplicity, we only generate applicable actions for Alice, i.e. announcements that include her true hand 012. This results in the planning task having a total of 46376 options for the first action, and 7 for the second action. Still, the initial state s0 consists of 140 worlds, one for each possible deal of cards. Agents can only distinguish worlds where their own hands differ. Alice’s designated worlds in her associated local state of s0 are those four worlds in which she holds hand 012.\nOur planner needs approximately two hours and 600MB of memory to come up with a solution policy. In the solution, Alice first announces her hand to be one of 012, 034, 156, 236, and 245. It can be seen that each of the five hands other than the true hand 012 contains at least one of Alice’s and one of Bob’s cards, meaning that Bob will immediately be able to identify the complete deal. Also, Eve stays unaware of the individual cards of Alice and Bob since she will be able to rule out exactly two of the hands, with each of Alice and Bob’s cards being present and absent in at least one of the remaining hands. Afterwards, Alice can wait for Bob to announce that Eve has either card 3, 4, 5 or 6."
    }, {
      "heading" : "4.2 Mail Instances",
      "text" : "Our second experiment concerns the letter passing problem from Examples 6 and 7. We generalized the scenario to allow an arbitrary number of agents with an arbitrary undirected neighborhood graph, indicating which agents are allowed to directly pass letters to each other. As neighborhood graphs, we used randomly generated Watts-Strogatz small-world networks [30], exhibiting characteristics that can also be found in social networks. Watts-Strogatz networks have three parameters: The number N of nodes (determining the number of agents in our setting), the average number K of neighbors per node (roughly determining the average branching factor of a search for a plan), and the probability β of an edge being a “random shortcut” instead of a “local connection” (thereby influencing the shortest path lengths between agents). We only generate connected networks in order to guarantee plan existence.\nWe distinguish between the example domains MAILTELL and MAILCHECK. To guarantee plan existence, in both scenarios the actions are modeled such as to ensure that the letter position remains common knowledge among the agents in all reachable states. The mechanics of MAILTELL directly correspond to those given in Example 6. There is only one type of action, publicly passing the letter to a neighboring agent while privately informing him about the final addressee. This allows for sequential implicitly coordinated plans. In the resulting plans, letters are simply moved along a shortest path to the addressee. In contrast, in MAILCHECK, an agent that has the letter can only check if he himself is the addressee or not using a separate action (without learning the actual addressee if it is not him). To ensure plan existence in this scenario, we allow an agent to pass on the letter only if it is destined for someone else. Unlike in MAILTELL, conditional plans are required here. In a solution policy, the worst-case sequence of successively applied actions contains an action passing the letter to each agent at least once. As soon as the addressee has been reached, execution is stopped.\nExperiments were conducted for both scenarios with different parameters (Table 1). For finding sequential as well as conditional plans, our implementation uses breadth-first search over a regular graph\nand over an AND-OR graph, respectively. For each set of parameters, 100 trials were performed. For MAILTELL, direct path denotes the average shortest path length between sender and addressee, while for MAILCHECK, full path denotes the average length of a shortest path passing through all agents starting from the sender.\nWhile the shortest path length between sender and addressee grows very slowly with the number of agents (due to the shortcut connections in the network), the shortest path passing through all agents roughly corresponds to the number of agents. Since these measures directly correspond to the minimal plan lengths, the observed exponential growth of space and time requirements with respect to them (and to the base K) is unsurprising.\nNote also that in both scenarios, the number of agents determines the number of worlds (one for each possible addressee) in the initial state. Since the preconditions of the available actions are mutually exclusive, this constitutes an upper bound on the number of worlds per state throughout the search. Thus we get only a linear overhead in comparison to directly searching the networks for the relevant paths."
    }, {
      "heading" : "5 Conclusion and Future Work",
      "text" : "We introduced an interesting new cooperative, decentralized planning concept without the necessity of explicit coordination or negotiation. Instead, by modeling all possible communication directly as plannable actions and relying on the ability of the autonomous agents to put themselves into each other’s shoes (using perspective shifts), some problems can be elegantly solved achieving implicit coordination between the agents. We briefly demonstrated an implementation of both the sequential and conditional solution algorithms and its performance on the Russian card problem and two letter passing problems.\nBased on the foundation this paper provides, a number of challenging problems needs to be addressed. First of all, concrete problems (e.g. epistemic versions of established multi-agent planning tasks) need to be formalized with a particular focus on the question of which kinds of communicative actions the agents would need to solve these problems in an implicitly coordinated way. As seen in the MAILTELL benchmark, the dynamic epistemic treatment of a problem does not necessarily lead to more than linear overhead. It will be interesting to identify classes of tractable problems and see how agents cope in a simulated environment. Another issue that is relevant in practice concerns the interplay of the single agents’ individual plans. In our setting, the agents have to plan individually and decide autonomously when and how to act. Also, when it comes to action application, there is no predefined notion of agent precedence. This leads to the possibility of incompatible plans, and in consequence to the necessity for agents having to replan in some cases. While our notion of implicitly coordinated planning explicitly forbids the execution of actions leading to dead-end situations (i.e. non-goal states where there is no implicitly coordinated plan for any of the agents), replanning can still lead to livelocks. Both the conditions leading to livelocks and individually applicable strategies to avoid them need to be investigated."
    } ],
    "references" : [ {
      "title" : "A Translation-Based Approach to Contingent Planning",
      "author" : [ "Alexandre Albore", "Hector Palacios", "Hector Geffner" ],
      "venue" : "Proc. IJCAI",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Towards Theory-of-Mind agents using Automated Planning and Dynamic Epistemic Logic",
      "author" : [ "Mikkel Birkegaard Andersen" ],
      "venue" : "Ph.D. thesis,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2015
    }, {
      "title" : "Undecidability in Epistemic Planning",
      "author" : [ "Guillaume Aucher", "Thomas Bolander" ],
      "venue" : "Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Symbolic Model Checking for Dynamic Epistemic Logic",
      "author" : [ "Johan van Benthem", "Jan van Eijck", "Malvin Gattinger", "Kaile Su" ],
      "venue" : "Proceedings of the 5th International Workshop on Logic, Rationality, and Interaction (LORI",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2015
    }, {
      "title" : "Epistemic planning for single and multi-agent systems",
      "author" : [ "Thomas Bolander", "Mikkel Birkegaard Andersen" ],
      "venue" : "Journal of Applied Non-Classical Logics",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Continual planning and acting in dynamic multiagent environments",
      "author" : [ "Michael Brenner", "Bernhard Nebel" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Comparing variants of strategic ability: how uncertainty and memory influence general properties of games",
      "author" : [ "Nils Bulling", "Wojciech Jamroga" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Weak, Strong, and Strong Cyclic Planning via Symbolic Model Checking",
      "author" : [ "Alessandro Cimatti", "Marco Pistore", "Marco Roveri", "Paolo Traverso" ],
      "venue" : "Artificial Intelligence",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2003
    }, {
      "title" : "The Russian Cards Problem",
      "author" : [ "Hans P. van Ditmarsch" ],
      "venue" : "Studia Logica 75(1),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2003
    }, {
      "title" : "Dynamic Epistemic Logic",
      "author" : [ "Hans P. van Ditmarsch", "Wiebe van der Hoek", "Barteld Kooi" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Distributed Problem Solving and Planning",
      "author" : [ "Edmund H. Durfee" ],
      "venue" : "ACAI",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2001
    }, {
      "title" : "Dynamic epistemic modelling. CWI. Software Engineering [SEN",
      "author" : [ "Jan van Eijck" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2004
    }, {
      "title" : "Automated Planning: Theory and Practice",
      "author" : [ "Malik Ghallab", "Dana S. Nau", "Paolo Traverso" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2004
    }, {
      "title" : "Strategic Planning through Model Checking of ATL Formulae",
      "author" : [ "Wojciech Jamroga" ],
      "venue" : "ICAISC, pp. 879–884,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Constructive knowledge: what agents can achieve under imperfect information",
      "author" : [ "Wojciech Jamroga", "Thomas Ågotnes" ],
      "venue" : "Journal of Applied Non-Classical Logics",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Agents that Know How to Play",
      "author" : [ "Wojciech Jamroga", "Wiebe van der Hoek" ],
      "venue" : "Fundam. Inform",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Beliefs in Multiagent Planning: From One Agent to Many",
      "author" : [ "Filippos Kominis", "Hector Geffner" ],
      "venue" : "Proc. ICAPS",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "DEL planning and some tractable cases",
      "author" : [ "Benedikt Löwe", "Eric Pacuit", "Andreas Witzel" ],
      "venue" : "LORI",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Planning over Multi-Agent Epistemic States: A Classical Planning Approach",
      "author" : [ "Christian Muise", "Vaishak Belle", "Paolo Felli", "Sheila McIlraith", "Tim Miller", "Adrian R. Pearce", "Liz Sonenberg" ],
      "venue" : "Proc. AAAI",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Distributed Heuristic Forward Search for Multi-agent Planning",
      "author" : [ "Raz Nissim", "Ronen I. Brafman" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Compiling Uncertainty Away in Conformant Planning Problems with Bounded Width",
      "author" : [ "Héctor Palacios", "Hector Geffner" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "A Knowledge-Based Approach to Planning with Incomplete Information and Sensing",
      "author" : [ "Ronald P.A. Petrick", "Fahiem Bacchus" ],
      "venue" : "Proc. AIPS",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2002
    }, {
      "title" : "Extending the Knowledge-Based Approach to Planning with Incomplete Information and Sensing",
      "author" : [ "Ronald P.A. Petrick", "Fahiem Bacchus" ],
      "venue" : "Proc. ICAPS",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2004
    }, {
      "title" : "Does the chimpanzee have a theory of mind",
      "author" : [ "David Premack", "Guy Woodruff" ],
      "venue" : "Behavioral and Brain Sciences",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1978
    }, {
      "title" : "Collective dynamics of ’small-world",
      "author" : [ "Duncan J. Watts", "Steven H. Strogatz" ],
      "venue" : "networks. Nature 393(6684),",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In contrast, building upon the epistemic planning framework by Bolander and Andersen [8], we propose a decentralized planning notion in which each agent has to individually reason about the entire problem and autonomously decide when and how to (inter-)act.",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 23,
      "context" : "If Anne has the ability to take Bob’s perspective (she has a Theory of Mind concerning Bob [29]), Anne should of course be able to foresee this problem, and realize that her plan can not be expected to be successful.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and Löwe, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and Löwe, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and Löwe, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 9,
      "context" : "Our work is situated in the area of distributed problem solving and planning [15] and directly builds upon the framework introduced by Bolander and Andersen [8] and Löwe, Pacuit, and Witzel [22], who formulated the planning problem in the context of Dynamic Epistemic Logic (DEL) [13].",
      "startOffset" : 280,
      "endOffset" : 284
    }, {
      "referenceID" : 0,
      "context" : "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of “nested” [8] or “shallow” knowledge states [26, 27, 28].",
      "startOffset" : 112,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of “nested” [8] or “shallow” knowledge states [26, 27, 28].",
      "startOffset" : 112,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of “nested” [8] or “shallow” knowledge states [26, 27, 28].",
      "startOffset" : 112,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of “nested” [8] or “shallow” knowledge states [26, 27, 28].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 21,
      "context" : "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of “nested” [8] or “shallow” knowledge states [26, 27, 28].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 22,
      "context" : "Algorithmically, (multi-agent) epistemic planning can be approached either by compilation to classical planning [2, 21, 23] or by search in the space of “nested” [8] or “shallow” knowledge states [26, 27, 28].",
      "startOffset" : 196,
      "endOffset" : 208
    }, {
      "referenceID" : 4,
      "context" : "Since compilation approaches to classical planning can only deal with bounded nesting of knowledge (or belief), similar to Bolander and Andersen [8], we use search in the space of epistemic states to find a solution.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 5,
      "context" : "One of the important features that distinguishes our work from more traditional multi-agent planning [9] is the explicit notion of perspective shifts needed for agents to reason about the possible plan contributions of other agents—and hence needed to achieve implicit coordination.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "Our concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information.",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Our concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information.",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "Our concepts can be considered related to recent work in temporal epistemic logics [10, 19, 20], which addresses a question similar to ours, namely what groups of agents can jointly achieve under imperfect information.",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 4,
      "context" : "We first briefly recapitulate the foundations of DEL, following the conventions of Bolander and Andersen [8].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "Using event models with |Ed |> 1, it is also possible to model sensing actions (where a priori, multiple outcomes are considered possible), and nondeterministic actions [8].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 4,
      "context" : "Both worlds being designated, but distinguishable, means that Bob at plan time (before executing the action) considers them both as possible outcomes of try-take, but is aware that he will at run time (after having executed the action) know which one it is (see [8] for a more full discussion of the plan time/run time distinction).",
      "startOffset" : 262,
      "endOffset" : 265
    }, {
      "referenceID" : 1,
      "context" : "Andersen [3] shows that a is applicable in s iff s |= 〈a〉>, and that s |= [a]φ iff s⊗ a |= φ .",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "To represent who performs which actions of a plan, we will introduce what we call an owner function (inspired by the approach of Löwe, Pacuit, and Witzel [22]).",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 12,
      "context" : ", [17]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "This solution concept is equivalent to the one considered in [8].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "Our notion satisfies the usual properties of strong plans [11], namely closedness, properness and acyclicity.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "We implemented a planner that is capable of finding implicitly coordinated plans and policies1, and conducted two experiments: one small case study of the Russian card problem [12] intended to show how this problem can be modeled and solved from an individual perspective, and one experiment investigating the scaling behavior of our approach on private transportation problems in the style of Examples 6 and 7, using instances of increasing size.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 24,
      "context" : "As neighborhood graphs, we used randomly generated Watts-Strogatz small-world networks [30], exhibiting characteristics that can also be found in social networks.",
      "startOffset" : 87,
      "endOffset" : 91
    } ],
    "year" : 2017,
    "abstractText" : "Epistemic planning can be used for decision making in multi-agent situations with distributed knowledge and capabilities. Recently, Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. We extend the DEL-based epistemic planning framework to include perspective shifts, allowing us to define new notions of sequential and conditional planning with implicit coordination. With these, it is possible to solve planning tasks with joint goals in a decentralized manner without the agents having to negotiate about and commit to a joint policy at plan time. First we define the central planning notions and sketch the implementation of a planning system built on those notions. Afterwards we provide some case studies in order to evaluate the planner empirically and to show that the concept is useful for multi-agent systems in practice.",
    "creator" : "LaTeX with hyperref package"
  }
}