{
  "name" : "1505.04497.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Definition of Happiness for Reinforcement Learning Agents∗",
    "authors" : [ "Mayank Daswani", "Jan Leike" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords. Temporal difference error, reward prediction error, pleasure, wellbeing, optimism, machine ethics"
    }, {
      "heading" : "1 Introduction",
      "text" : "People are constantly in search of better ways to be happy. However, philosophers and psychologists have not yet agreed on a notion of human happiness. In this paper, we pursue the more general goal of defining happiness for intelligent agents. We focus on the reinforcement learning (RL) setting [SB98] because it is an intensively studied formal framework which makes it easier to make precise statements. Moreover, reinforcement learning has been used to model behaviour in both human and non-human animals [Niv09].\nHere, we decouple the discussion of happiness from the discussion of consciousness, experience, or qualia. We completely disregard whether happiness is actually consciously experienced or what this means. The problem of consciousness has to be solved separately; but its answer might matter insofar that it could tell us which agents’ happiness we should care about.\nDesiderata. We can simply ask a human how happy they are. But artificial reinforcement learning agents cannot yet speak. Therefore we use our human “common sense” intuitions about happiness to come up with a definition. We arrive at the following desired properties.\n∗Research supported by the People for the Ethical Treatment of Reinforcement Learners http://petrl.org. This is the extended technical report. The final publication is available at http://link.springer.com. †Both authors contributed equally.\nar X\niv :1\n50 5.\n04 49\n7v 1\n[ cs\n.A I]\n1 8\nM ay\n2 01\n• Scaling. Happiness should be invariant under scaling of the rewards. Replacing every reward rt by crt+d for some c, d ∈ R with c > 0 (independent of t) does not change the reinforcement learning problem in any relevant way. Therefore we desire a happiness measure to be independent under rescaling of the rewards.\n• Subjectivity. Happiness is a subjective property of the agent depending only on information available to the agent. For example, it cannot depend on the true environment.\n• Commensurability. The happiness of different agents should be comparable. If at some time step an agent A has happiness x, and another agent B has happiness y, then it should be possible to tell whether A is happier than B by computing x− y. This could be relaxed by instead asking that A can calculate the happiness of B according to A’s subjective beliefs.\n• Agreement. The happiness function should match experimental data about human happiness.\nIt has to be emphasised that in humans, happiness cannot be equated with pleasure [RSDD14]. In the reinforcement learning setting, pleasure corresponds to the reward. Therefore happiness and reward have to be distinguished. We crudely summarise this as follows; for a more detailed discussion see Section 3.\npleasure = reward 6= happiness\nThe happiness measure that we propose is the following. An agent’s happiness in a time step t is the difference between the value of the obtained reward and observation and the agent’s expectation of this value at time step t. In the Markov setting, this is also known as the temporal difference error (TD error) [SB90]. However, we do not limit ourselves to the Markov setting in this paper. In parts of the mammalian brain, the neuromodulator dopamine has a strong connection to the TD error [Niv09]. Note that while our definition of happiness is not equal to reward it remains highly correlated to the reward, especially if the expectation of the reward is close to 0.\nOur definition of happiness coincides with the definition for joy given by Jacobs et al. [JBJ14], except that the latter is weighted by 1 minus the (objective) probability of taking the transition which violates subjectivity. Schmidhuber’s work on ‘intrinsic motivation’ adds a related component to the reward in order to motivate the agent to explore in interesting directions [Sch10].\nOur definition of happiness can be split into two parts. (1) The difference between the instantaneous reward and its expectation, which we call payout, and (2) how the latest observation and reward changes the agent’s estimate of future rewards, which we call good news. Moreover, we identify two sources of happiness: luck, favourable chance outcomes (e.g. rolling a six on a fair die), and pessimism, having low expectations of the environment (e.g. expecting a fair die to be biased against you). We show that agents that know the world perfectly have zero expected happiness. Proofs can be found in Appendix A.\nIn the rest of the paper, we use our definition as a starting point to investigate the following questions. Is an off-policy agent happier than an on-policy one? Do monotonically increasing rewards necessarily imply a happy agent? How does value function initialisation affect the happiness of an agent? Can we construct an agent that maximises its own happiness?"
    }, {
      "heading" : "2 Reinforcement Learning",
      "text" : "In reinforcement learning (RL) an agent interacts with an environment in cycles: at time step t the agent chooses an action at ∈ A and receives an observation ot ∈ O and a real-valued reward rt ∈ R; the cycle then repeats for time step t + 1 [SB98]. The list of interactions a1o1r1a2o2r2 . . . is called a history. We use ht to denote a history of length t, and we use the shorthand notation h := ht−1 and h\n′ := ht−1atotrt. The agent’s goal is to choose actions to maximise cumulative rewards. To avoid infinite sums, we use a discount factor γ with 0 < γ < 1 and maximise the discounted sum ∑∞ t=1 γ\ntrt. A policy is a function π mapping every history to the action taken after seeing this history, and an environment µ is a stochastic mapping from histories to observation-rewardtuples.\nA policy π together with an environment µ yields a probability distribution over histories. Given a random variable X over histories, we write the π-µexpectation of X conditional on the history h as Eπµ[X | h].\nThe (true) value function V πµ of a policy π in environment µ maps a history ht to the expected total future reward when interacting with environment µ and taking actions according to the policy π:\nV πµ (ht) := Eπµ [∑∞ k=t+1 γ k−t−1rk | ht ] . (1)\nIt is important to emphasise that Eπµ denotes the objective expectation that can be calculated only by knowing the environment µ. The optimal value function V ∗µ is defined as the value function of the optimal policy, V ∗ µ (h) := supπ V π µ (h).\nTypically, reinforcement learners do not know the environment and are trying to learn it. We model this by assuming that at every time step the agent has (explicitly or implicitly) an estimate V̂ of the value function V πµ . Formally, a value function estimator maps a history h to a value function estimate V̂ . Finally, we define an agent to be a policy together with a value function estimator. If the history is clear from context, we refer to the output of the value function estimator as the agent’s estimated value.\nIf µ only depends on the last observation and action, µ is called Markov decision process (MDP). In this case, µ(otrt | ht−1at) = µ(otrt | ot−1at) and the observations are called states (st = ot). In MDPs we use the Q-value function, the value of a state-action pair, defined asQπµ(st, at) := Eπµ [∑∞ k=t+1 γ k−t−1rk | stat ] . Assuming that the environment is an MDP is very common in the RL literature, but here we will not make this assumption."
    }, {
      "heading" : "3 A Formal Definition of Happiness",
      "text" : "The goal of a reinforcement learning agent is to maximise rewards, so it seems natural to suppose an agent is happier the more rewards it gets. But this does not conform to our intuition: sometimes enjoying pleasures just fails to provide happiness, and reversely, enduring suffering does not necessarily entail unhappiness (see Example 3 and Example 7). In fact, it has been shown empirically that rewards and happiness cannot be equated [RSDD14] (p-value < 0.0001).\nThere is also a formal problem with defining happiness in terms of reward: we can add a constant c ∈ R to every reward. No matter how the agent-environment interaction plays out, the agent will have received additional cumulative rewards\nC := ∑t i=1 c. However, this did not change the structure of the reinforcement learning problem in any way. Actions that were optimal before are still optimal and actions that are slightly suboptimal are still slightly suboptimal to the same degree. For the agent, no essential difference between the original reinforcement learning problem and the new problem can be detected: in a sense the two problems are isomorphic. If we were to define an agent’s happiness as received reward, then an agent’s happiness would vary wildly when we add a constant to the reward while the problem stays structurally exactly the same.\nWe propose the following definition of happiness.\nDefinition 1 (Happiness). The happiness of a reinforcement learning agent with estimated value V̂ at time step t with history hat while receiving observation ot and reward rt is\n,(hatotrt, V̂ ) := rt + γV̂ (hatotrt)− V̂ (h). (2)\nIf ,(h′, V̂ ) is positive, we say the agent is happy, and if ,(h′, V̂ ) is negative, we say the agent is unhappy.\nIt is important to emphasise that V̂ represents the agent’s subjective estimate of the value function. If the agent is good at learning, this might converge to something close to the true value function V πµ . In an MDP (2) is also known as the temporal difference error [SB90]. This number is used used to update the value function, and thus plays an integral part in learning.\nIf there exists a probability distribution ρ on histories such that the value function estimate V̂ is given by the expected future discounted rewards according to the probability distribution ρ,\nV̂ (h) = Eπρ [∑∞ k=t+1 γ k−t−1rk | h ] , (3)\nthen we call E := Eπρ the agent’s subjective expectation. Note that we can always find such a probability distribution, but this notion only really makes sense for model-based agents (agents that learn a model of their environment). Using the agent’s subjective expectation, we can rewrite Definition 1 as follows.\nProposition 2 (Happiness as Subjective Expectation). Let E denote an agent’s subjective expectation. Then\n,(h′, V̂ ) = rt − E[rt | h] + γ ( V̂ (h′)− E[V̂ (haor) | h] ) . (4)\nProposition 2 states that happiness is given by the difference of how good the agent thought it was doing and what it learns about how well it actually does. We distinguish the following two components in (4):\n• Payout: the difference of the obtained reward rt and the agent’s expectation of that reward E[rt | h].\n• Good News: the change in opinion of the expected future rewards after receiving the new information otrt.\n,(h′, V̂ ) = rt − E[rt | h]︸ ︷︷ ︸ payout\n+γ ( V̂ (h′)− E[V̂ (haor) | h]︸ ︷︷ ︸\ngood news\n)\nExample 3. Mary is travelling on an air plane. She knows that air planes crash very rarely, and so is completely at ease. Unfortunately she is flying on a budget airline, so she has to pay for her food and drink. A flight attendant comes to her seat and gives her a free beverage. Just as she starts drinking it, the intercom informs everyone that the engines have failed. Mary feels some happiness from the free drink (payout), but her expected future reward is much lower than in the state before learning the bad news. Thus overall, Mary is unhappy.\nFor each of the two components, payout and good news, we distinguish the following two sources of happiness.\n• Pessimism:1 the agent expects the environment to contain less rewards than it actually does.\n• Luck: the outcome of rt is unusually high due to randomness.\nrt − E[rt | h] = rt − Eπµ[rt | h]︸ ︷︷ ︸ luck +Eπµ[rt | h]− E[rt | h]︸ ︷︷ ︸ pessimism\nV̂ (h′)− E[V̂ (haor) | h] = V̂ (h′)− Eπµ[V̂ (haor) | h]︸ ︷︷ ︸ luck\n+ Eπµ[V̂ (haor) | h]− E[V̂ (haor) | h]︸ ︷︷ ︸ pessimism\nExample 4. Suppose Mary fears flying and expected the plane to crash (pessimism). On hearing that the engines failed (bad luck), Mary does not experience very much change in her future expected reward. Thus she is happy that she (at least) got a free drink.\nThe following proposition states that once an agent has learned the environment, its expected happiness is zero. In this case, underestimation cannot contribute to happiness and thus the only source of happiness is luck, which cancels out in expectation.\nProposition 5 (Happiness of Informed Agents). An agent that knows the world has an expected happiness of zero: for every policy π and every history h,\nEπµ[,(h′, V πµ ) | h] = 0.\nAnalogously, if the environment is deterministic, then luck cannot be a source of happiness. In this case, happiness reduces to how much the agent underestimates the environment. By Proposition 5, having learned a deterministic environment perfectly, the agent’s happiness is equal to zero."
    }, {
      "heading" : "4 Matching the Desiderata",
      "text" : "Here we discuss in which sense our definition of happiness satisfies the desiderata from Section 1.\n1Optimism is a standard term in the RL literature to denote the opposite phenomenon. However, this notion is somewhat in discord with optimism in humans.\nScaling. If we transform the rewards to r′t = crt + d with c > 0, d ∈ R for each time step t without changing the value function, the value of , will be completely different. However, a sensible learning algorithm should be able to adapt to the new reinforcement learning problem with the scaled rewards without too much problem. At that point, the value function gets scaled as well, Vnew(h) = cV (h) + d/(1− γ). In this case we get\n,(hatotr′, Vnew) = r′t + γVnew(hatotr ′ t)− Vnew(h)\n= crt + d+ γcV (hatotr ′ t) + γ\nd 1− γ − cV (h)− d\n1− γ = c ( rt + γV (hatotr ′ t)− V (h) ) ,\nhence happiness gets scaled by a positive factor and thus its sign remains the same, which would not hold if we defined happiness just in terms of rewards.\nSubjectivity. The definition (4) of , depends only on the current reward and the agent’s current estimation of the value function, both of which are available to the agent.\nCommensurability. The scaling property as described above means that the exact value of the happiness is not useful in comparing two agents, but the sign of the total happiness can at least tell us whether a given agent is happy or unhappy. Arguably, failing this desideratum is not surprising; in utility theory the utilities/rewards of different agents are typically not commensurable either.\nHowever, given two agents A and B, A can still calculate the A-subjective happiness of a history experienced by B as ,(haorB , V̂ A). This corresponds to the human intuition of “putting yourself in someone else’s shoes”. If both agents are acting in the same environment, the resulting numbers should be commensurable, since the calculation is done using the same value function. It is entirely possible that A believes B to be happier, i.e. ,(haorB , V̂ A) > ,(haorA, V̂ A), but also that B believes A to be happier ,(haorA, V̂ B) > ,(haorB , V̂ B), because they have different expectations of the environment.\nAgreement. Rutledge et al. measure subjective well-being on a smartphonebased experiment with 18,420 participants [RSDD14]. In the experiment, a subject goes through 30 trials in each of which they can choose between a sure reward and a gamble that is resolved within a short delay. Every two to three trials the subjects are asked to indicate their momentary happiness.\nOur model based on Proposition 2 with a very simple learning algorithm and no loss aversion correlates fairly well with reported happiness (mean r = 0.56, median r2 = 0.41, median R2 = 0.27) while fitting individual discount factors, comparative to Rutledge et al.’s model (mean r = 0.60, median r2 = 0.47, median R2 = 0.36) and a happiness=cumulative reward model (mean r = 0.59, median r2 = 0.46, median R2 = 0.35). This analysis is inconclusive, but unsurprisingly so: the expected reward is close to 0 and thus our happiness model correlates well with rewards. See Appendix B for the details of our data analysis.\nThe hedonic treadmill [BC71] refers to the idea that humans return to a baseline level of happiness after significant negative or positive events. Stud-\nies have looked at lottery winners and accident victims [BCJB78], and people dealing with paralysis, marriage, divorce, having children and other life changes [DLS06]. In most cases these studies have observed a return to baseline happiness after some period of time has passed; people learn to make correct reward predictions again. Hence their expected happiness returns to zero (Proposition 5). Our definition unfortunately does not explain why people have different baseline levels of happiness (or hedonic set points), but these may be perhaps explained by biological means (different humans have different levels of neuromodulators, neurotransmitters, hormones, etc.) which may move their baseline happiness. Alternatively, people might simply learn to associate different levels of happiness with “feeling happy” according to their environment."
    }, {
      "heading" : "5 Discussion and Examples",
      "text" : ""
    }, {
      "heading" : "5.1 Off-policy Agents",
      "text" : "In reinforcement learning, we are mostly interested in learning the value function of the optimal policy. A common difference between RL algorithms is whether they learn off-policy or on-policy. An on-policy agent evaluates the value of the policy it is currently following. For example, the policy that the agent is made to follow could be an ε-greedy policy, where the agent picks arg maxaQ\nπ(h, a) a fraction (1− ε) of the time, and a random action otherwise. If ε is decreased to zero over time, then the agent’s learned policy tends to the optimal policy in MDPs. Alternatively, an agent can learn off-policy, that is it can learn about one policy (say, the optimal one) while following a different behaviour policy.\nThe behaviour policy (πb) determines how the agent acts while it is learning the optimal policy. Once an off-policy learning agent has learned the optimal value function V ∗µ , then it is not happy if it still acts according to some other (possibly suboptimal) policy.\nProposition 6 (Happiness of Off-Policy Learning). Let π be some policy and µ be some environment. Then for any history h\nEπµ[,(h′, V ∗µ ) | h] ≤ 0.\nQ-learning is an example of an off-policy algorithm in the MDP setting. If Q-learning converges, and the agent is still following the sub-optimal behaviour policy then Proposition 6 tells us that the agent will be unhappy. Moreover, this means that SARSA (an on-policy RL algorithm) will be happier than Q-learning on average and in expectation."
    }, {
      "heading" : "5.2 Increasing and Decreasing Rewards",
      "text" : "Intuitively, it seems that if things are constantly getting better, this should increase happiness. However, this is not generally the case: even an agent that obtains monotonically increasing rewards can be unhappy if it thinks that these rewards mean even higher negative rewards in the future.\nExample 7. Alice has signed up for a questionable drug trial which examines the effects of a potentially harmful drug. This drug causes temporary pleasure to the user every time it is used, and increased usage results in increased pleasure.\nHowever, the drug reduces quality of life in the long term. Alice has been informed of the potential side-effects of the drug. She can be either part of a placebo group or the group given the drug. Every morning Alice is given an injection of an unknown liquid. She finds herself feeling temporary but intense feelings of pleasure. This is evidence that she is in the non-placebo group, and thus has a potentially reduced quality of life in the long term. Even though she experiences pleasure (increasing rewards) it is evidence of very bad news and thus she is unhappy.\nAnalogously, decreasing rewards do not generally imply unhappiness. For example, the pains of hard labour can mean happiness if one expects to harvest the fruits of this labour in the future."
    }, {
      "heading" : "5.3 Value Function Initialisation",
      "text" : "Example 8 (Increasing Pessimism Does Not Increase Happiness). Consider the deterministic MDP example in Figure 1. Assume that the agent has an initial value function Q̂0(s0, α) = 0, Q̂0(s0, β) = −ε, Q̂0(s1, α) = ε and Q̂0(s1, β) = 0. If no forced exploration is carried out by the agent, it has no incentive to visit s1. The happiness achieved by such an agent for some time step t is ,(s0αs00, V̂0) = 0 where V̂0(s0) := Q̂0(s0, α) = 0. However, suppose the agent is (more optimistically) initialised with Q̂0(s0, α) = 0, Q̂0(s0, β) = ε. In this case, the agent would take action β and arrive in state s1. This transition would have happiness ,(s0βs1−1, V̂0) = −1 + γQ̂0(s1, α) − Q̂0(s0, β) = −1 − 0.5ε. However, the next transition is s1αs12 which has happiness ,(s1αs12, V̂0) = 2 + γQ̂0(s1, α) − Q̂0(s1, α) = 2 − 0.5ε. If Q̂0 is not updated by some learning mechanism the agent will continue to accrue this positive happiness for all future time steps. If the agent does learn, it will still be some time steps before Q̂ converges to Q∗ and the positive happiness becomes zero (see Figure 2). It\nis arguable whether this agent which suffered one time step of unhappiness but potentially many time steps of happiness is overall a happier agent, but it is some evidence that absolute pessimism does not necessarily lead to the happiest agents."
    }, {
      "heading" : "5.4 Maximising Happiness",
      "text" : "How can an agent increase their own happiness? The first source of happiness, luck, depends entirely on the outcome of a random event that the agent has no control over. However, the agent could modify its learning algorithm to be systematically pessimistic about the environment. For example, when fixing the value function estimation below rmin/(1− γ) for all histories, happiness is positive at every time step. But this agent would not actually take any sensible actions. Just as optimism is commonly used to artificially increase exploration, pessimism discourages exploration which leads to poor performance. As demonstrated in Example 8, a pessimistic agent may be less happy than a more optimistic one.\nAdditionally, an agent that explicitly tries to maximise its own happiness is no longer a reinforcement learner. So instead of asking how an agent can increase its own happiness, we should fix a reinforcement learning algorithm and ask for the environment that would make this algorithm happy."
    }, {
      "heading" : "6 Conclusion",
      "text" : "An artificial superintelligence might contain subroutines that are capable of suffering, a phenomenon that Bostrom calls mind crime [Bos14, Ch. 8]. More generally, Tomasik argues that even current reinforcement learning agents could have moral weight [Tom14]. If this is the case, then a general theory of happiness for reinforcement learners is essential; it would enable us to derive ethical standards in the treatment of algorithms. Our theory is very preliminary and should be thought of as a small step in this direction. Many questions are left unanswered, and we hope to see more research on the suffering of AI agents in the future.\nAcknowledgements. We thank Marcus Hutter and Brian Tomasik for careful reading and detailed feedback. The data from the smartphone experiment was kindly provided by Robb Rutledge. We are also grateful to many of our friends for encouragement and interesting discussions."
    }, {
      "heading" : "A Omitted Proofs",
      "text" : "Proof of Proposition 2.\n,(h′, V̂ ) = rt + γV̂ (h′)− V̂ (h)\n= rt + γV̂ (h ′)− E [ ∞∑ k=t γk−trk | h ]\n= rt − E[rt | h] + γV̂ (h′)− E [ ∞∑ k=t+1 γk−trk | h ]\n= rt − E[rt | h] + γV̂ (h′)− γE [ E [ ∞∑ k=t+1 γk−t−1rk | haor ] | h ] = rt − E[rt | h] + γV̂ (h′)− γE [ V̂ (haor) | h ] ,\nwhere the second to last equality uses the tower property for conditional expectations.\nProof of Proposition 5. For the true value function V πµ , a subjective expectation exists by definition since (3) is the same as (1). Hence we can apply Proposition 2:\nEπµ[,(h′, V πµ ) | h] = Eπµ[rt − Eπµ[rt | h] + γV πµ (h′)− γEπµ[V πµ (haor) | h] | h] = Eπµ[rt | h]− Eπµ[Eπµ[rt | h] | h] + γEπµ[V πµ (h′) | h]− γEπµ[Eπµ[V πµ (haor) | h] | h] = Eπµ[rt | h]− Eπµ[rt | h] + γEπµ[V πµ (h′) | h]− γEπµ[V πµ (haor) | h] = 0\nProof of Proposition 6. Let h′ be any history of length t, and let π∗ denote an optimal policy for environment µ, i.e., V ∗µ = V π∗\nµ . In this case, we have V ∗µ (h) ≥ V ∗µ (hπ(h)), and hence\nEπ ∗\nµ [ ∞∑ k=t γk−trk | h ] ≥ Eπ ∗ µ [ ∞∑ k=t γk−trk | hπ(h) ] . (5)\nWe use this in the following:\nEπµ[,(h′, V ∗µ ) | h] = Eπµ[rt + γV ∗µ (h′)− V ∗µ (h) | h]\n= Eπµ [ rt + γEπ ∗ µ [ ∞∑ k=t+1 γk−t−1rk | h′ ] − Eπ ∗ µ [ ∞∑ k=t γk−trk | h ] | h ] (5) ≤ Eπµ [ rt + Eπ ∗ µ [ ∞∑ k=t+1 γk−trk | h′ ] − Eπ ∗ µ [ ∞∑ k=t γk−trk | hat ] | h\n] = Eπµ [ rt − Eπ ∗\nµ [rt | hat]︸ ︷︷ ︸ =0\n+ Eπ ∗\nµ [ ∞∑ k=t+1 γk−trk | h′ ] − Eπ ∗ µ [ ∞∑ k=t+1 γk−trk | hat ] ︸ ︷︷ ︸\n=0\n| h ]\nsince Eπµ[Eπ ∗ µ [X | hatotrt] | h] = Eπµ[Eπ ∗\nµ [X | hat] | h] because conditional on hat, the distribution of ot and rt is independent of the policy."
    }, {
      "heading" : "B Data Analysis",
      "text" : "In this section we describe the details of the data analysis on the Great Brain Experiment2 conducted by Rutledge et al. [RSDD14]. This experiment measures subjective well-being on a smartphone app and had 18,420 subjects. Each subject goes through 30 trials and starts with 500 points. At the start of a trial, the subject is given an option to choose between a certain reward (CR) and a 50–50 gamble between two outcomes. The gamble is pictorially shown to have an equal probability between the outcomes, thus making it easy for the subject to choose between the certain reward and the gamble.\nBefore the trials start the agent is asked to rate their current happiness on a scale of 0-100. The slider for the scale is initialised randomly. Every two to three trials and 12 times in each play the subject is asked the question “How happy are you at this moment?”\nWe model this experiment as a reinforcement learning problem with 150 different states representing the different trials that the subject can see (the possible combinations of certain rewards and gamble outcomes), and two actions certain and gamble. For example, in a trial the subject could be presented with the choice between a certain reward of 30 and a 50–50 gamble between 0 and 72.\nThe expected reward after each state is uniquely determined by the state’s description, but the agent has subjective uncertainty about the value of the state, since it does not know which states (types of trials) will follow the current one or how these states are distributed. (In the experiment, the a priori expected outcome of a trial (the average value of the states) was 5.5 points, and the maximum gain or loss in a single trial is 220 points.) Furthermore, the subject might incorrectly estimate the value of the gamble: although the visualisation correctly suggests that each of the two outcomes is equally likely, the subject might be uncertain about this, or simply compute the average incorrectly.\nRutledge et al. model happiness as an affine-linear combination of the certain reward (CR), the expected value of the gamble (EV) and the reward prediction error at the outcome of the gamble (RPE). The weights for this affine-linear combination were learned through linear regression on fMRI data from another similar experiment on humans (wCR = 0.52, wEV = 0.35, and wRPE = 0.8 for z-scored happiness ratings).\nThe data was kindly provided to us by Rutledge. We disregard the first happiness rating that occurs before the first trial. Moreover, we removed all 762 subjects whose happiness ratings (other than the first) had a standard deviation of 0.\n2http://www.thegreatbrainexperiment.com/\nTo test our happiness definition on the data, we have to model how humans estimate the value of a history. We chose a very simple model where a subject’s expectation before each trial is the average of the previous outcomes. We use the empirical distribution function, that estimates all future rewards as the average outcome so far:\nρ(rn | h) := #{k ≤ t | rk = rn}\nt for n > t.\nWith Equation 3, this gives a value function estimate of V̂ (a1o1r1 . . . atotrt) = 1 t(1−γ) ∑t i=1 ri.\nWe assume that the subject typically computes the expected value of the trial correctly, i.e. E[rt | h] := max{CR,EV}. We calculate happiness at time t with the formula from Proposition 2.\nSince subjects’ happiness ratings are not given on every trial, we use geometric discounting with discount factor γ to aggregate happiness from previous trials:\npredicted happiness := t∑ k=1 γt−k,(hk, V̂k),\nwhere hk is the history at time step k and V̂k is the value function estimate at time step k. For each subject, we optimise γ such that the Pearson correlation r is maximised. The result is that the majority of subjects use a γ of either very close to 0 or very close to 1 (see Figure 3). A possible explanation of this phenomenon could be that it is unclear to the human subjects whether they are to indicate their instantaneous or their cumulative happiness.\nWe use the sample Pearson correlation r, its square r2 and the coefficient of determination R2 to evaluate each model. For n data points (in our case n = 11,\nthe number of happiness ratings per subject), the sample Pearson correlation r is defined as\nr = rxy = ∑n i=1(xi − x̄)(yi − ȳ)√∑n\ni=1(xi − x̄)2 ∑n i=1(yi − ȳ)2\nwhere x̄ = 1n ∑n i=1 xi is the sample mean. The coefficient of determination R 2 on n data points with (training) data x ∈ X and predictions y ∈ Y is\nR2 = 1− ∑n i=1(yi − xi)2∑n i=1(xi − x̄)2\nWe calculate R2 only on z-scored terms; z-scoring does not affect r. Our model’s predicted happiness correlates fairly well with reported happiness (mean r = 0.56, median r2 = 0.41, median R2 = 0.27) while fitting individual discount factors, comparative to Rutledge et al.’s model (mean r = 0.60, median r2 = 0.47, median R2 = 0.36) and a happiness=cumulative reward model (mean r = 0.59, median r2 = 0.46, median R2 = 0.35). So this analysis is inconclusive. Figure 4 and Figure 5 show the distribution of the correlation coefficients. We emphasise that our model was derived a priori and then tested on the data, while their model has three parameters (weights for CR, EV, and RPE) that were fitted on data from humans using linear regression.\nIt is well known that humans do not value rewards linearly. In particular, people appear to weigh losses more heavily than gains, an effect known as loss aversion. To model this, we set\nreward :=\n{ outcomeα if outcome > 0\n−λ · (−outcome)β otherwise.\nUsing parameters found by Rutledge et al. (mean λ = 1.7, mean α = 1.05, mean β = 1.01) we also get a slightly improved agreement (mean r = 0.60, median r2 = 0.48, median R2 = 0.39). Table 1 lists these results."
    } ],
    "references" : [ {
      "title" : "Hedonic relativism and planning the good society",
      "author" : [ "Philip Brickman", "Donald T Campbell" ],
      "venue" : "Adaptation-Level Theory, pages 287–305,",
      "citeRegEx" : "Brickman and Campbell.,? \\Q1971\\E",
      "shortCiteRegEx" : "Brickman and Campbell.",
      "year" : 1971
    }, {
      "title" : "Lottery winners and accident victims: Is happiness relative",
      "author" : [ "Philip Brickman", "Dan Coates", "Ronnie Janoff-Bulman" ],
      "venue" : "Journal of Personality and Social Psychology,",
      "citeRegEx" : "Brickman et al\\.,? \\Q1978\\E",
      "shortCiteRegEx" : "Brickman et al\\.",
      "year" : 1978
    }, {
      "title" : "Superintelligence: Paths, Dangers, Strategies",
      "author" : [ "Nick Bostrom" ],
      "venue" : null,
      "citeRegEx" : "Bostrom.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bostrom.",
      "year" : 2014
    }, {
      "title" : "Beyond the hedonic treadmill: Revising the adaptation theory of well-being",
      "author" : [ "Ed Diener", "Richard E Lucas", "Christie Napa Scollon" ],
      "venue" : "American Psychologist,",
      "citeRegEx" : "Diener et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Diener et al\\.",
      "year" : 2006
    }, {
      "title" : "distress, hope, and fear in reinforcement learning",
      "author" : [ "Elmer Jacobs", "Joost Broekens", "Catholijn Jonker. Joy" ],
      "venue" : "In Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Jacobs et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jacobs et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning in the brain",
      "author" : [ "Yael Niv" ],
      "venue" : "Journal of Mathematical Psychology,",
      "citeRegEx" : "Niv.,? \\Q2009\\E",
      "shortCiteRegEx" : "Niv.",
      "year" : 2009
    }, {
      "title" : "A computational and neural model of momentary subjective well-being",
      "author" : [ "Robb B Rutledge", "Nikolina Skandali", "Peter Dayan", "Raymond J Dolan" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Rutledge et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rutledge et al\\.",
      "year" : 2014
    }, {
      "title" : "Time-derivative models of Pavlovian reinforcement",
      "author" : [ "Richard Sutton", "Andrew Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1990\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1990
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Formal theory of creativity, fun, and intrinsic motivation (1990–2010)",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "IEEE Transactions on Autonomous Mental Development,",
      "citeRegEx" : "Schmidhuber.,? \\Q2010\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2010
    }, {
      "title" : "Do artificial reinforcement-learning agents matter morally",
      "author" : [ "Brian Tomasik" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Tomasik.,? \\Q2014\\E",
      "shortCiteRegEx" : "Tomasik.",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "What is happiness for reinforcement learning agents? We seek a formal definition satisfying a list of desiderata. Our proposed definition of happiness is the temporal difference error, i.e. the difference between the value of the obtained reward and observation and the agent’s expectation of this value. This definition satisfies most of our desiderata and is compatible with empirical research on humans. We state several implications and discuss examples.",
    "creator" : "LaTeX with hyperref package"
  }
}