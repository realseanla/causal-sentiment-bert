{
  "name" : "1610.02847.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Situational Awareness by Risk-Conscious Skills",
    "authors" : [ "Daniel J. Mankowitz" ],
    "emails" : [ "danielm@tx.technion.ac.il", "avivt@berkeley.edu", "shie@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Hierarchical-Reinforcement Learning (H-RL) is an RL paradigm that utilizes hierarchical abstractions to solve tasks. This enables an agent to abstract away from the lower-level details and focus more on solving the task at hand. Hierarchical abstractions have been utilized to naturally model many real-world problems in machine learning and, more specifically, in RL. This includes high-level controllers in robotics Peters & Schaal (2008); Hagras et al. (2004); da Silva et al. (2012), strategies (such as attack and defend) in soccer Bai et al. (2015) and video games Mann (2015), as well as highlevel sub-tasks in search and rescue missions Liu & Nejat (2015). In RL, hierarchical abstractions are typically referred to as skills, (da Silva et al. (2012)), Temporally Extended Actions (TEAs), options (Sutton et al. (1999)) or macro-actions, (Hauskrecht (1998)). We will use the term skill to refer to hierarchical abstractions from here on in.\nH-RL is important as it utilizes skills to both speed up the convergence rate in RL planning algorithms Mann & Mannor (2013); Precup & Sutton (1997); Mankowitz et al. (2014) as well as mitigating model misspecification. Model misspecification in RL can be sub-divided into (1) feature-based model misspecification - where a limited, sub-optimal feature set is provided (e.g., due to limited memory resources or sub-optimal feature selection) leading to sub-optimal performance; and (2) reward-based model misspecification whereby the reward shaping function is incorrectly designed\nar X\niv :1\n61 0.\n02 84\n7v 1\n[ cs\n.A I]\n(e.g., due to an incorrect understanding of the target problem). Previous work has focused on utilizing skills to mitigate feature-based model misspecification Mankowitz et al. (2014, 2016a,b), but have not attempted to mitigate reward-based model misspecification. Risk sensitivity can be utilized to mitigate this form of misspecification.\nAn important factor missing in H-RL is risk sensitivity. A risk-sensitive H-RL framework would enable us to generate skills with different Risk Attitudes, also known as Situational Awareness (SA) Endsley (1995); Smith & Hancock (1995), which, as we will show in our paper, allows us to mitigate reward-based model misspecification. As seen in Table 1, previous work in H-RL has focused on skill learning Mankowitz et al. (2014, 2016a,b), but has not incorporated risk-sensitivity into the H-RL objective, nor learned risk aware skills to mitigate reward-based model misspecification. From here on in, the terms risk sensitivity, risk attitude and SA will be used interchangeably.\nSituational Awareness (SA): SA can be dependent on both time and space, although the focus of this paper is on time-based SA. We provide both definitions below.\nTime-based SA: Consider a soccer game composed of complicated strategies (skills), such as attack and defend, based on the status of the game. Consider a team losing by one goal to zero with ten minutes remaining. Here, the team needs to play attacking, risky soccer such as making long, risky passes as well as shooting from distance to try and score goals and win the game (Figure 1a). On the other hand, if the team is winning by one goal to zero with ten minutes remaining, the team needs to ‘waste time’ by maintaining possession and playing risk-averse, defensive football to prevent the opponent from gaining the ball and scoring goals (Figure 1b). In both scenarios the team has the same objective which is to score more goals than their opponent once time runs out (I.e. win the game). Time-based SA enables an agent to act in a risk-aware manner based on the amount of time remaining in the task.\nSpatial SA: As mentioned previously, SA can also be defined in terms of space. Consider an autonomous vehicle (the agent) driving in a narrow/wide lane or on dry/wet roads as shown in Figure 2. The proximity of the agent to the other vehicles in the lane example (Figure 2a), or the distance of the agent to other vehicles as well as puddles in the dry/wet road example (Figure 2b) determines the SA and therefore the risk attitude of the agent.\nOur main idea in this paper, is that a simple way to add risk-sensitivity to H-RL is by maximizing a risk-sensitive objective rather than the regular expected return formulation. One example that we focus on in this work is that of a Probabilistic Goal Markov Decision Process (PG-MDP) Xu & Mannor (2011). Previous works that incorporate risk into RL have mainly been focused on learning a single risk aware policy in a non-hierarchical setting Avila-Godoy & Fernández-Gaucherand (1998); Tamar et al. (2015a,b) by maximizing the Conditional Value-at-Risk or the Value-at-Risk objectives. We provide a framework that enables an agent, for the first time to solve a task by maximizing a risksensitive objective in hierarchical RL. We define a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP) which naturally models this setting. By solving the PG-SMDP using our novel SARiCoS algorithm, the agent learns Risk-Aware Skills (RASs) that have a particular Risk Attitude/SA. We show that the learned risk-aware skills exhibit complex human behaviours such as time-wasting in a soccer game. We then show in our experiments that these skills can be used to overcome reward-based model misspecification, in contrast to the regular expected return formulation.\nMain Contributions: (1) Extending hierarchical RL to incorporate SA by defining a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP) (2) The development of the Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which optimizes a hierarchical risk-aware RL objective and learns Risk-Aware Skills (RASs) that incorporate SA. (3) Theorem 1 which derives a policy gradient update rule for learning Risk Aware Skills and inter-skill policy parameters in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). (4) Theorem 2 which proves that SARiCoS converges to a locally optimal solution. (5) Experiments in the RoboCup domain that exhibit an agent’s ability to learn skills possessing SA (e.g., time wasting in a soccer game). In addition, we show the agent utilizing these skills to overcome reward-based model misspecification."
    }, {
      "heading" : "2 Background",
      "text" : "Semi-Markov Decision Process (SMDP) Sutton et al. (1999) A Semi-Markov Decision Process can be defined by the 5-tuple 〈X,Σ, P,R, γ〉, where X is a set of states, Σ is a set of skills, P is a transition probability function and R is a bounded reward function. We assume that the rewards we receive at each timestep are bounded between [0, Rmax]. Therefore R forms a mapping from X × Σ to [0, Rmax1−γ ] and represents the expected discounted sum of rewards that are received from executing skill σ ∈ Σ from state x ∈ X . The discount factor is defined as γ ∈ [0, 1]. The inter-skill policy µ : X → ∆Σ maps states to a probability distribution over skills. The goal in an SMDP is to find\nthe optimal inter-skill policy µ∗ that maximizes the value function V µ(x) = E [∑∞\nt=0 γ tRt|x, µ\n] .\nThis represents the expected return of following the inter-skill policy µ from state x. The optimal policy µ∗ determines the best action to take for a given state and generates the optimal value function V µ ∗ (s).\nSkill, Option and Macro-Action Sutton et al. (1999); da Silva et al. (2012): An RL skill, option or macro action σ is defined as the 3-tuple σ = 〈I, πθ, p(x)〉 where I is a set of initiation states from which a skill can be initialized or executed; πθ is the intra-skill policy which selects the lower-level (or primitive) actions to perform whilst the skill is executing and is parameterized by θ ∈ Rn; The termination probability p(x) which determines the probability of the skill terminating when in state x.\nProbabilistic Goal MDP (PG-MDP) Xu & Mannor (2011): While the standard MDP objective presented above considered the expected reward, in some situations different objectives may be more appropriate. In particular, risk-sensitive criteria that maximize the probabilty of success, and not just the expected outcome, are natural objectives in domains such as finance and operations research, but also in game-playing, such as soccer. The PG-MDP is an extension of the MDP that accounts for such an objective. In a PG-MDP, the goal is to learn a policy π that maximizes the probability that some performance threshold will be attained. That is, it aims to maximize:\nP(Wπ ≥ β) , (1)\nwhere Wπ is a random variable representing the total reward of the MDP under the policy π. The parameter β ∈ R is a performance threshold. The PG-MDP formulation is key for our risk shaping method, and will be further discussed when defining the PG-SMDP.\nPolicy Gradient Peters & Schaal (2006): In continuous as well as high-dimensional MDPs, it is computationally inefficient to learn a policy that determines an action to perform for any given state. Policies therefore need to be generalizable, where the policy will choose the same or similar action to perform when in nearby states. In order to achieve this generalization, a policy is parameterized using techniques such as Linear Function Approximation (LFA) (which we use in this work) Sutton\n& Barto (1998). A popular technique to learning the parameters for these parameterized policies is the policy gradient method. Let Jπ(θ) denote the expected return of the policy parametrized by θ as Jπ(θ) = ∫ τ P (τ)R(τ)dτ where τ is a trajectory of T timesteps 〈x1, a1, r1, x2 · · · , xT 〉; P (τ) is the probability of a trajectory and R(τ) is defined as the total reward of the trajectory. Policy gradient uses sampling to estimate the gradient ∇θJπ(θ) and then updates the parameters using a gradient ascent update rule θt+1 = θt + ∇θJπ(θ) where denotes a positive step size."
    }, {
      "heading" : "3 Probabilistic Goal SMDP (PG-SMDP)",
      "text" : "In this work we focus on solving problems in which the agent must maximize its probability of success for solving a given task in a limited amount of time. A natural model for such problems is the the PG-MDP framework described above. However, we are interested in complex problems that require some hierarchical reasoning, and therefore propose to extend PG-MDPs to incorporate skills, leading to a PG Semi-MDP (PG-SMDP) model. We now derive an equivalent PG-SMDP with an augmented state space and skill set Σ that can easily be utilized with policy gradient algorithms.\nWe assume that we are given a set of skills Σ = {σi|i = 1, 2, · · ·n, σj = 〈Ij , πj , pj(x)〉} and inter-skill policy µ(σ|x)→ ∆Σ which chooses a skill to execute given the current state x ∈ X . We wish to maximize the probability that the total accumulated reward, ∑T t=0 rt, attained during the execution of the inter-skill policy µ, passes the pre-defined performance objective threshold β ∈ R within T timesteps. This takes the form of a Probabilistic Goal SMDP (PG-SMDP) (since we are incorporating skills) defined in Equation 2.\nmax µ P( T∑ t=0 rt ≥ β|µ) . (2)\nIn order to solve this PG-SMDP using traditional RL techniques, we augment the state space with the total accumulated reward Xu & Mannor (2011) to create an equivalent augmented PG-SMDP. We will show the important developments of this formulation for reader clarity. This will enable us to utilize traditional RL techniques in order to maximize the probability of surpassing the performance threshold β, given a set of skills Σ, within T timesteps. First note that maximizing the probability can be formulated as an expectation as shown in Equation 3.\nmaxµ P (∑T t=0 r(xt, σt) ≥ β ∣∣∣µ)\n= maxµ Eµ [ I (∑T t=0 r(xt, σt) ≥ β )]\n(3)\nThis expectation still contains a constraint. We now formulate an equivalent augmented PG-SMDP that removes the β constraint and incorporates the constraint into the reward function. Define an augmented state z = {x,w} where x ∈ X is the original state space and w = ∑T t=0 r(xt, σt) is the accumulated reward up until time T . We can then define the transition probabilities in terms of the augmented state z according to Equation 4.\nP (z′|z, σ) = {{x′, w + r(x, σ)}w.p P (x′|x, σ)} . (4)\nThe reward function for this augmented state is then defined according to Equation 5.\nr̃t(z, σ) =  0, t < T 0, t = T,w < β\n1, t = T,w ≥ β (5)\nTogether, the transition probabilities and the reward function forms an equivalent PG-SMDP with an augmented state space z ∈ Z as shown in Equation 6. This formulation learns an inter-skill policy\nµ that maximizes the probability that the total accumulated reward will surpass the performance threshold β within T timesteps.\nmax µ\nE [ T∑ t=0 r̃(zt, σt) ] (6)\nIn the next Section, we show that risk can be incorporated into the PG-SMDP by incorporating a Risk Awareness Parameter (RAP) into the typical definition of a skill to form a Risk Aware Skill (RAS). We derive a policy gradient algorithm to learn both the inter-skill policy and the RAPs such that the agent is able to successfully solve the PG-SMDP."
    }, {
      "heading" : "4 Risk-Aware Skill",
      "text" : "We modify the typical definition of a skill to include a parameter, called the Risk-Awareness Parameter (RAP) yw ∈ R. This is the parameter that controls the risk-attitude of the Risk-Aware Skill (RAS). Definition 1. A Risk Aware Skill (RAS) σ is a temporally extended action that consists of the 4-tuple σ = 〈I, πθ, p(z), yw〉, where I are the set of states from where the RAS can be initialized; πθ is the parameterized intra skill policy; p(z) is the probability of terminating in state z ∈ Z; and yw ∈ R is the Risk-Awareness Parameter (RAP) governed by the Risk-Aware Distribution (RAD) yw ∼ Pw(·) with parameters w ∈ Rm.\nIn practice, the RAP can parameterize the intra-skill policy, or act as a meta-parameter for the RAS (E.g. Dribble power in the RoboCup experiment (See Experiments Section))."
    }, {
      "heading" : "5 SARiCoS Algorithm",
      "text" : "The Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm learns the parameters of a two-tiered skill selection policy defined as:\nµα,Ωi(σ, y|z) = µα(σ|z)µ σi Ωi (y|z) , (7)\nwhere µα : Z → ∆Σ is the inter-skill policy, parameterized by α ∈ Rd, that selects which RAS σ needs to be executed from a set Σ of N RASs, given the current state z ∈ Z.; µσiΩi(·|z) is the RAD for RAS σi with RAD parameters Ωi = wi ∈ Rm. The RAD parameters for all RASs are stored in a vector Ω = [ω1, ω2, · · · , ωN ] ∈ R|N ||m|×1 for algorithmic purposes. The two-tiered skill selection policy is executed by first sampling a Risk-Aware Skill σi to execute from µα(σ|z). The risk attitude of the skill is then determined by sampling the RAP from the RAD µσiΩi(y|z). SARiCoS learns (1) the inter-skill policy parameters α ∈ R\nd and (2) the RAD parameters Ω to produce Situationally Aware RASs. In order to derive gradient update rules for these parameters in a policy gradient setting, we define the notion of a risk-aware trajectory.\nRisk-Aware Trajectory: In the standard policy gradient framework, we define a typical trajectory as τ = (zt, σt, rt, zt+1) T t=0 where T is the length of the trajectory. To incorporate the two-tiered policy into this trajectory, we define a risk-aware trajectory τr = (zt, σt, ywσt rt, zt+1) T t=0 where at each timestep, we draw a RAP corresponding to the RAS σt that was selected. We can therefore define the probability of a trajectory as Pα,Ω(τr) = P(z0) ∏T−1 t=0 P(zt+1|zt, σt)µα,Ω(σt, yt|zt), where P (z0) is the initial state distribution; P (zt+1|zt, σt) is the transition probability of moving from state zt to state zt+1 given that a RAS σt was executed; and µα,Ω(σt, yt|zt) is the two-tiered selection policy. Using this notion, it is now possible to derive the gradient update rules for each set of parameters as shown in Theorem 1."
    }, {
      "heading" : "5.1 Inter-skill policy and RAP Update Rules",
      "text" : "We define the expected reward for following a policy µα,Ω:\nJ(µα,Ω) = ∫ τ P (τ |α,Ω)R(τ)dτ . (8)\nLet us group the parameters for the inter-skill policy and the continuous RAD Parameters into a single vector χ = [α,Ω] ∈ Rd+m·N . Taking the derivative of this objective and using the well-known likelihood trick Peters & Schaal (2008) yields:\n∇χJ(µχ) = ∫ τ P (τ |χ)∇α logP (τ |χ)R(τ)dτ , (9)\nwhere P (τ |χ) = P (z0) ∏T k=1 P (zk+1|zk, σk)P (σk|zk, χ); zk ∈ Z is the state at timestep k; σk is the RAS selected at timestep k and T is the length of the trajectory. Since only P (σk|zk, χ) is parameterized, the gradient∇χJ(µχ) can be simplified to:\n∇χJ(µχ) = ∫ τ P (τ |χ)∇χ logP (σk|zk, χ)R(τ)dτ , (10)\nwhere P (σk|zk, χ) = µα(σt|zt)µσtΩ (yt|zt). Therefore, substituting the two-tiered policy into Equation 20 and deriving with respect to α leads to the gradient update rule:\n∇αJ(µχ) = ∫ τ P (τ |χ)∇α logµα(σt|zt)R(τ)dτ .\nIf we represent µα(σt|zt) as a Gibb’s distribution which is a common policy choice in many MDPs Sutton & Barto (1998), then we can easily derive the gradient and estimate it by samples using the following gradient update rule:\n∇αJ(µχ) = 〈 H∑ h=0 ∇α logµα(σh|zh) H∑ j=0 γjrj 〉 (11)\nIf we substitute the two-tiered policy into Equation 20 and deriving with respect to Ω for the RAD Parameters, then we get the following gradient update rule:\n∇ΩJ(µχ) = ∫ τ P (τ |χ)∇Ω logµσtΩ (yt|zt)R(τ)dτ .\nIf we represent µσtΩ (yt|zt) as any distribution from the natural exponential family, then we can easily derive the gradient and estimate it by samples using the following gradient update rule:\n∇ΩJ(µχ) = 〈 H∑ h=0 ∇Ω logµσtΩ (yt|zt) H∑ j=0 γjrj 〉 (12)\nThese derivations are summarized in Theorem 1. A full proof can be found in the supplementary material.\nTheorem 1 (Gradient Update Derivation). Suppose that we are maximizing the Policy Gradient (PG) objective J(µα,Ω) = ∫ τ Pα,Ω(τ)R(τ)dτ using risk-aware trajectories, generated by the twotiered skill selection policy µα(σt|xt)µσtΩ (yt|zt), then the expectation of the gradient update rules for the inter-skill policy parameters α ∈ Rd and the RAD parameters Ω ∈ R|N ||m| are the true gradients and are defined as (1) ∇αJ(µα,Ω) = 〈∑H h=0∇α logµα(σh|zh) ∑H j=0 γ jrj 〉 and (2)\n∇ΩJ(µα,Ω) = 〈∑H h=0∇Ω logµ σt Ω (yt|zt) ∑H j=0 γ jrj 〉 respectively. H is the trajectory length and < · > is an average over trajectories as in standard PG.\nGiven the gradient update rules, we can derive an algorithm for learning both the inter-skill parameters α ∈ Rd and the continuous RAD parameters Ω = [ω1, ω2, · · · , ωN ] ∈ R|N ||m|×1 for the N RAS. SARiCoS learns these parameters by two timescale stochastic approximation, as shown in Algorithm 1, and converges to a locally optimal solution as is proven in Theorem 2. The convergence proof is based on standard two-timescale stochastic approximation convergence arguments Borkar (1997) and is found in the supplementary material.\nTheorem 2 (SARiCoS Convergence). Suppose we are optimizing the expected return J(µΩ,α) =∫ R(τ)P (τ)dτ for any arbitrary SARiCoS policy µΩ,α where Ω ∈ R|N ||m| and α ∈ Rd are the inter-skill and Risk Aware Distribution parameters respectively. Then, for step sizes sequences {ak}∞k=0, {bk}∞k=0 that satisfy ∑ k ak = ∞, ∑ k bk = ∞, ∑ k a 2 k < ∞, ∑ k b 2 k < ∞ and bk > ak, the SARiCoS iterates converge a.s αk → α∗,Ωk → λ̄(α∗) as k →∞ to the countable set of locally optimal points of J(µΩ,α).\nAlgorithm 1 SARiCoS Algorithm\nRequire: α ∈ Rd. {Inter-skill policy parameterization}, Ω ∈ R|N ||m|×1 {Set of RAD parameters for each skill}\n1: repeat: 2: αk+1 → αk + ak∇αJα,Ω 3: Ωk+1 → Ωk + bk∇ΩJα,Ω {stepsize bk > ak} 4: until convergence"
    }, {
      "heading" : "6 Experiments",
      "text" : "The experiments were performed in the RoboCup 2D soccer simulation domain Akiyama & Nakashima (2014); a well-known benchmark for many AI challenges. In the experiments, we demonstrate the ability of the agent to learn risk-aware skills (such as ‘time-wasting’ in a soccer game), and therefore exhibit SA, by maximizing the PG-SMDP objective. In the RoboCup domain, we also show the agent’s ability to exit local optima due to reward shaping and therefore overcome reward-based model misspecification.\nRoboCup Offense (RO) Domain: This domain 1 consists of two teams on a soccer field where the striker (the yellow agent) needs to score against a goalkeeper (purple circle) as shown in Figure 3a. The striker has T = 150 timesteps (length of the episode) to try and score a goal. State space - The state space in RO consists of the continuous 〈x, y〉 field locations of the striker, ball, goalposts and goalkeeper as well as the cumulative sum of rewards w. Skills - The Risk-Aware Skill (RAS) set Σ in each of the experiments consists of three RAS: (1) Move to the ball (M), (2) Move to the ball and shoot towards the goal (S) and (3) Move to the ball and dribble in the direction of the goal (D). Each RAS i is parameterized with a Risk Aware Parameter ywi . We focus on learning the dribbling power RAP yw,D that controls how hard the agent kicks the ball when performing the skill Dribble. Data: SARiCoS is trained over 3 independent trials with 20, 000 episodes per trial. Learning Algorithm and features - The learning algorithm for both the inter-skill policy parameters α and the RAPs for the RASs is Actor Critic Policy Gradient (AC-PG) 2. The inter-skill policy µ that chooses which RAS to execute is represented by a Gibb’s distribution with Fourier Features. The Risk Aware Distribution (RAD) is represented as a normal distribution y ∼ N (φ(s)Tω, V ) with a fixed variance V . Here, φ(s) are state dependent features [1, xagent, yagent, w, distGoal] representing the agent’s x, y location, the cumulative reward and the distance of the agent to the goal. Rewards - Engineering of the reward in RL is common practice for the RoboCup domain Hausknecht & Stone (2015); Bai et al. (2015). The rewards for both of the RoboCup scenarios have been engineered based on logical soccer strategies. The striker gets small positive rewards for dribbling outside the box rD,far and shooting when inside or near the box rS,near. Negative rewards come about when the striker dribbles inside the box, rD,near, or shoots from far, rS,far, as the striker has a smaller probability of scoring Yiannakos & Armatas (2006). The striker also gets a small positive reward for moving towards the ball rmove. There is also a game score reward rscore, which is positive if winning and negative if losing or drawing. In the PG-SMDP setting, the rewards r̃ = 1 if w >= β at the end of each episode, otherwise the reward is 0 at each timestep. In the Expected Return setting (see Reward-Based Model Misspecification), the regular rewards are utilized at each timestep."
    }, {
      "heading" : "6.1 Situational Awareness by Risk-Conscious Skills",
      "text" : "In this section we show that learning the inter-skill policy and RAD parameters using SARiCoS so as to maximize a PG-SMDP can bring about risk-aware skills that exhibit time-based SA. We provide\n1 https://github.com/mhauskn/HFO\n2AC-PG has lower variance compared to regular PG and the convergence guarantees are trivial extensions of the current proof.\nthe agent with two different soccer situations: (1) The agent is losing the game 0− 1; (2) The agent is winning the game 1 − 0. Similar results are obtained for different scores (e.g., 2 − 0 and 0 − 2 etc. and have therefore been omitted). For all of the scenarios, the performance threshold β for the PG-SMDP is set to a constant value (β = 1.0) a-priori.\nSA in a Losing Scenario: In a scenario where a team is losing and time is running out, the team needs to play risky, attacking soccer to try and score goals. The agent is placed in a losing scenario where the score is 0− 1 to the opposition with 150 timesteps remaining. Using SARiCoS, the agent learns to perform a fast Dribble by kicking the ball with significant power to make quick progress along the pitch and get in a position to shoot for goal as seen in Figure 3b(i). The average RAP value for the Dribble RAS is approximately 100 (max value 150, min value 0) prompting the agent to kick the ball with significant power and quickly advance up the pitch. The RAP is state dependent, enabling the agent to learn to initially kick the ball with a large amount of power when near the half-way line and then decrease the dribble power when approaching the goal so as to prevent losing possession to the goalkeeper. This is seen from the dribble power color gradient superimposed onto the RO domain in Figure 3c. The color gradient varies from powerful kicks (in red) to soft kicks (in blue). Once the agent is near the goal, it executes the skill Shoot as seen in the figure 3b(i). The average episode length is 70.0± 1.0 (mean±std) as seen in Table 2 and the average number of goals scored over 100 evaluation episodes is 74.3± 6.5. In addition, the keeper captures the ball on average 21± 5.29 times indicating that the striker is playing risky football with aggressive dribbling and, as a result, scores a high number of goals. In addition, the average reward is consistently higher than the β threshold.\nSA in a Winning Scenario: When winning a game with little time remaining, a natural strategy is to hold onto the ball and run out the clock 3 (‘time-wasting’) so as to prevent the opposing team from gaining possession and possibly scoring a goal. SARiCoS learns ‘time-wasting’ since when the agent is winning the game 1− 0, the agent slowly dribbles his way up the pitch, collecting the dribble from far rewards rD,far in the process as seen in Figure 3b(ii). Once the agent crosses the performance threshold, it stands on the ball, and wastes time by executing the M skill, whilst continuing to collect the positive score rewards rscore and the small positive rmove rewards. This strategy causes the agent to take the largest amount of time on average (142.3 ± 1.5 steps) to complete each episode (time wasting) and as a result only scores 1.3± 0.6 goals. However, the ball is almost never captured by the opponent 7.3± 0.6 times on average per 100 evaluation episodes. See a Video4 of the agent’s behavior in each of these scenarios."
    }, {
      "heading" : "6.2 Mitigating Reward-based Model Misspecified",
      "text" : "The learned risk-aware skills can be utilized to overcome reward-based model misspecification. We focus on the losing scenario in RoboCup soccer. We compared SARiCoS to the regular Expected Return (ER) formulation, i.e., an implementation of Actor-Critic Policy Gradient that utilizes regular rewards at each timestep to learn a game-winning policy.\nAs seen in Figure 3d, the ER striker (light blue circle) does not learn to score goals as the algorithm settles quickly on collecting positive dribble from far rewards rD,far and moving to the ball rewards rmove. The ER agent therefore gets stuck in a local optima causing the agent to execute D until it\n3 http://www.collinsdictionary.com/dictionary/american/run-out-the-clock 4 https://youtu.be/xA-8rWJ4a7I\nsettles on the M skill and stands on the ball, receiving small positive rewards. As seen in Table 2, the ER agent only manages to score 1.7± 1.2 goals on average and has a low average reward −0.3± 0.1, well below the β threshold.\nThese rewards are therefore not enough to enable the SARiCoS agent (yellow circle in Figure 3d) to pass its performance threshold β, especially since, in the losing scenario, the agents are also receiving a negative game score reward rscore at each timestep. This forces the SARiCoS agent to search for additional rewards such as a goal-scoring reward. As seen in Table 2, the SARiCoS agent learns to score goals (74± 6.5), and achieves average reward well above the β performance threshold. As a result it mitigates the reward shaping-based model misspecification."
    }, {
      "heading" : "7 Discussion",
      "text" : "We have defined a PG-SMDP which provides a natural risk-sensitive objective for learning SA in hierarchical RL. We find it interesting that an agent can learn a complex human behavior by simply maximizing a risk-sensitive objective. To do so, we have introduced Risk-Aware Skills (RASs) — a type of parameterized option Sutton et al. (1999) with an additional Risk-Aware Parameter (RAP). We have developed the Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which learns both the inter-skill policy that chooses RASs to execute, as well as learning the RAPs for each RAS. We have shown that this algorithm converges to a locally optimal solution. We also show that SARiCoS can induce situational awareness (E.g. ‘time-wasting’) in Risk-Aware Skills in a time dependent RoboCup soccer scenario. In principle, any other risk criteria can be incorporated into this work such as exponential risk, CVaR and VaR Avila-Godoy & Fernández-Gaucherand (1998); Tamar et al. (2015a,b). Extensions of this work include optimizing a PG-MDP performance threshold β for each RAS as well as utilizing SA in lifelong learning problems Thrun & Mitchell (1995); Pickett & Barto (2002); Brunskill & Li (2014). The SARiCoS policy could also be implemented as a Deep Network Mnih et al. (2015), leading to more complex policies on higher dimensional problems."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research leading to these results has received funding from the European Research Council under the European Union’s Seventh Framework Program (FP/2007-2013) / ERC Grant Agreement n. 306638."
    }, {
      "heading" : "A SARiCoS Supplementary Material",
      "text" : "A.1 Full Derivation of Theorem 1\nWe define the expected reward for following a policy µα,Ω as:\nJ(µα,Ω) = ∫ τ P (τ |α,Ω)R(τ)dτ . (13)\nLet us group the parameters for the inter-RAS policy and the continuous RADPs into a single vector χ = [α,Ω] ∈ Rd+m·N . Taking the derivative of this objective and using the well-known likelihood trick Peters & Schaal (2008) yields:\n∇χJ(µχ) = ∇χ ∫ τ P (τ |χ)R(τ)dτ (14)\n= ∫ τ ∇χP (τ |χ)R(τ)dτ (15)\n= ∫ τ P (τ |χ)∇α logP (τ |χ)R(τ)dτ , (16)\nwhere P (τ |χ) = P (z0) ∏T k=1 P (zk+1|zk, σk)P (σk|zk, χ) where zk ∈ Z is the state at timestep k; σk is the RAS selected at timestep k and T is the length of the trajectory. Since only P (σk|zk, χ) is parameterized, the gradient∇χJ(µχ) can be simplified as follows: ∇χJ(µχ) = ∫ τ P (τ |χ)∇χ logP (τ |χ)R(τ)dτ (17)\n= ∫ τ P (τ |χ)∇χ log [ P (z0) T∏ k=1 P (zk+1|zk, σk)P (σk|zk, χ) ] R(τ)dτ (18)\n= ∫ τ P (τ |χ)∇χ [ logP (z0) + Σ T k=1 logP (zk+1|zk, σk) + logP (σk|zk, χ) ] R(τ)dτ(19)\n= ∫ τ P (τ |χ)∇χ logP (σk|zk, χ)R(τ)dτ (20)\nwhere P (σk|zk, χ) = µα(σt|zt)µσtΩ (yt|zt). Therefore, substituting the two-tiered policy into Equation 20 and deriving with respect to α leads to the gradient update rule:\n∇αJ(µχ) = ∫ τ P (τ |χ)∇α logµα(σt|zt)R(τ)dτ .\nIf we represent µα(σt|zt) as a Gibb’s distribution which is a common policy choice in many MDPs Sutton & Barto (1998), then we can easily estimate the gradient by sampling:\n∇αJ(µχ) = 〈 H∑ h=0 ∇α logµα(σh|zh) H∑ j=0 γjrj 〉 , (21)\nwhere H is the length of a trajectory; and 〈 · 〉 represents an average over trajectories. If we derive\nEquation 20 with respect to Ω for the RADPs, then we get the following gradient update rule:\n∇ΩJ(µχ) = ∫ τ P (τ |χ)∇Ω logµσtΩ (yt|zt)R(τ)dτ .\nIf we represent µσtΩ (yt|zt) as any distribution from the natural exponential family, then we can easily estimate the gradient by samples using the following gradient update rule:\n∇ΩJ(µχ) = 〈 H∑ h=0 ∇Ω logµσtΩ (yt|zt) H∑ j=0 γjrj 〉 . (22)\nThese derivations are summarized in Theorem 1. Theorem 1 (Gradient Update Derivation). Suppose that we are maximizing the Policy Gradient (PG) objective J(µα,Ω) = ∫ τ Pα,Ω(τ)R(τ)dτ using risk-aware trajectories, generated by the twotiered skill selection policy µα(σt|xt)µσtΩ (yt|zt), then the expectation of the gradient update rules for the inter-skill policy parameters α ∈ Rd and the RAD parameters Ω ∈ R|N ||m| are the true gradients and are defined as (1) ∇αJ(µα,Ω) = 〈∑H h=0∇α logµα(σh|zh) ∑H j=0 γ jrj 〉 and (2)\n∇ΩJ(µα,Ω) = 〈∑H h=0∇Ω logµ σt Ω (yt|zt) ∑H j=0 γ jrj 〉 respectively. H is the trajectory length and < · > is an average over trajectories as in standard PG.\nA.2 Proof of Theorem 2: SARiCoS Convergence\nTheorem 2 (SARiCoS Convergence). Suppose we are optimizing the expected return J(µΩ,α) =∫ R(τ)P (τ)dτ for any arbitrary SARiCoS policy µΩ,α where Ω ∈ R|N ||m| and α ∈ Rd are the inter-skill and Risk Aware Distribution parameters respectively. Then, for step sizes sequences {ak}∞k=0, {bk}∞k=0 that satisfy ∑ k ak = ∞, ∑ k bk = ∞, ∑ k a 2 k < ∞, ∑ k b 2 k < ∞ and bk > ak, the SARiCoS iterates converge a.s αk → α∗,Ωk → λ̄(α∗) as k →∞ to the countable set of locally optimal points of J(µΩ,α).\nThe true gradient of the two-tiered policy µΩ,α is:\n∇Ω,αJ(µΩ,α) = E[Rτ∇ logP (τ)]\nwhere Rτ = ∑h−1 t=0 γ\ntrt is the discounted cumulative reward for a trajectory τ of length h; the term P (τ) = P (x0)Πh−1i=0 P (xi+1|xi, σi)µΩ,α(σi, yi|xi) is the probability of a trajectory for a given policy µΩ,α(σi, yi|xi). The estimated gradient is:\n∇̂J(x) = Rτ∇ logµΩ,α(σt, yt|xt) = Rτ∇ log(µα(σt|xt)µσtΩ (yt|xt))\nWe need to prove that the parameters α ∈ Rd of the inter-skill policy and the risk-aware parameters Ω ∈ R|N ||m| converge to a locally optimal solution. Here, N is the number of skills and m is the number of risk-aware distribution parameters for each skill. In order to do so, we first derive the gradient with respect to α to yield the following recursive update equations:\nαk+1 = Γα(αk + ak∇̂αJ(x)) = Γα(αk + ak(Rτ∇α logµα(σt|xt))) (1) = Γα(αk + ak(Rτz α k ))\n= Γα(αk + ak(Rτz α k − E[Rτzαk ] + E[Rτzαk ]))\n= Γα(αk + ak(f(α(k),Ω) +Nk+1)) (23)\nwhere (1) zαk = ∇α logµα(σt|xt) and Nk+1 = Rτzαk − E[Rτzαk ] is a zero-mean martingale difference sequence; f(α(k),Ω) = E[Rτzαk ] and Γα : Rd → Rd is a projection operator that projects any αk to a compact region C = {α|gi(α) ≤ 0, i = 1, · · · l} ∈ Rn where gi(·), i = 1, · · · l represent\nthe continuously differentiable constraints that project the iterates to a compact region defined by a ball with a smooth boundary. This operator ensures that the iterates remain bounded. It can be seen by inspection that this recursion represents a noisy discretization of the Ordinary Differential Equation (ODE) Borkar (1997):\nα̇ = Γα(E[Rτzαk ])\nWe also derive the recursive update for the risk-aware parameters Ω as follows:\nΩk+1 = ΓΩ(Ωk + bk∇̂ΩJ(x)) = ΓΩ(Ωk + bk(Rτ∇ logµσtΩ (yt|xt)) = ΓΩ(Ωk + bk(Rτz Ω k ))\n= ΓΩ(Ωk + bk(Rτz Ω k − E[RτzΩk ] + E[RτzΩk ]))\n= ΓΩ(Ωk + bk(g(Ω(k), α) +Mk+1)), (24)\nwhere Mk+1 = RτzΩk − E[RτzΩk ] is a zero mean martingale difference sequence with respect to the σ−fields Ft = σ(Ωn, αn, Nn,Mn, n ≤ t; t ≥ 0); g(Ω(k), α) = E[RτzΩk ] and ΓΩ : R|N ||m| → R|N ||m| is the corresponding projection operator for Ω which ensures that these iterates are projected to a compact region W as in the previous iterate update equation. We can thus represent the Ω update with the following ODE:\nΩ̇ = ΓΩ(E[RτzΩk ]) Define the continuous time projection operators Γ̂Ω(v) = limδ→∞ ΓΩ(Ω+δv)−Ω δ and Γ̂α(p) = limδ→∞ Γα(α+δv)−α\nδ that, given directions v and p to modify the paramaters Ω and α respectively ensures that the iterates are projected into their compact sets C and W respectively. We can thus define the ODEs using this continuous operator as:\nΩ̇ = Γ̂Ω(E[RτzΩk ]) . = ḡ(Ω(k), α) α̇ = Γ̂α(E[Rτzαk ]) . = f̄(α(k),Ω)\nAssumption (A1): For each α ∈ Rd, the ODE:\nΩ̇(t) = ḡ(Ω(k), α)\nhas a globally asymptotically stable equilibrium λ̄(α) such that λ̄ : Rd → R|N ||m| is Lipschitz. Assumption (A2): The ODE:\nα̇ = f̄(α(k), λ̄(α(k)))\nhas a unique global asymptotically stable equilibrium α∗.\nIn order to prove that these ODEs collectively converge, we need to make the following assumptions.\nAssumption (A3): The functions f, g are Lipschitz continuous functions\nAssumption (A4): supk ‖Ωk‖, supk ‖αk‖ <∞ Assumption (A5): ∑ n a(n) = ∑ n b(n) =∞, ∑ n a(n) 2 = ∑ n b(n)\n2 <∞ Assumption (A6): For increasing σ-algebras, the martingale sequences ∑ akNk, ∑ bkMk <∞ a.s\nAssumption (A7) For all α,Ω, the objective function J(µα,Ω) has bounded second derivatives and the set Z of local optima of J(µα,Ω) are countable.\nGiven the above assumptions, the parameter Ωk → λ̄(α∗) and αk → α∗ as k →∞ a.s by standard two-timescale stochastic approximation arguments Borkar (1997). That is, the iterates converge to {λ̄(α∗), α∗|α∗ ∈ Z} .\nA.3 SARiCoS Video\nA video is attached with the supplementary material showing an agent (the striker) applying the learned risk-aware skills in a one-on-one scenario with a goalkeeper. The videos exhibit the Situational Awareness (SA) of the agent in both a losing scenario and a winning scenario."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification Mankowitz et al. (2016a,b); Bacon & Precup (2015). To do so, it utilizes hierarchical abstractions, also known as skills – a type of temporally extended action Sutton et al. (1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA) , into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as ‘time-wasting’ in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification.",
    "creator" : "LaTeX with hyperref package"
  }
}