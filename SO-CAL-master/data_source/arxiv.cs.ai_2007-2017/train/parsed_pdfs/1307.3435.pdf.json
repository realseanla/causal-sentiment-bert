{
  "name" : "1307.3435.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Nicod’s Condition, Rules of Induction and the Raven Paradox",
    "authors" : [ "Hadi Mohasel Afshar", "Peter Sunehag" ],
    "emails" : [ "hadi.afshar@anu.edu.au", "peter.sunehag@anu.edu.au" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n34 35\nv2 [\ncs .A\nI] 1\n6 Ju\nKeywords\nNicod’s condition (NC), Raven paradox, weak projectability (PJ), reasoning by analogy (RA), inductive inference."
    }, {
      "heading" : "1 Introduction",
      "text" : "In this article, we study induction and in particular Nicod’s Condition (NC) from a Bayesian (in the sense of subjective probability) point of view. Rules of induction can be thought of as such restrictions on the class of probability measures (or equivalently, on the class of rational agents1 ).\nThe question is: “How can we agree that any particular rule of induction is plausible and generally entails sensible consequences and therefore should be accepted a priori?”. We are interested in a specific rule, namely NC which informally speaking states that “A proposition of the form All F are G is supported by the observation that a particular object is both F and G” [Hem45]. Does the fact that NC, does not seem to be counterintuitive suffice to persuade us that it is a plausible rule of induction? How can we be sure that it does not violate other intuitively acceptable rules and principles? As the notorious raven paradox [Hem45] shows, NC actually does entail counterintuitive consequences, and more than seven decades of discussion about this paradox shows that assessment of rules of induction can be extremely problematic.\nA summary of the raven paradox is as follows: The hypothesis H := “All ravens are black” is logically equivalent to Ĥ := “Every thing that is not black is not a raven”. A green apple is neither black nor a raven therefore according to NC its observation should confirm Ĥ. But H is logically equivalent to Ĥ , so we end up with a Paradoxical Conclusion (PC), that an observation of a green apple confirms that all ravens are black, which is counterintuitive. In order to resolve the paradox, either it should be shown that NC is not a plausible rule of induction or it should be claimed that PC holds and should not be considered as being counterintuitive.2\nIn order to study the paradox from a Bayesian perspective, first we make a distinction between (objective) background knowledge by which we exclusively refer to the knowledge that can be represented (and consequently can be thought of) as previously observed events, and any other kinds of information which we consider as being subjective and a property of the a priori chosen probability measure (i.e. the initial degrees of beliefs). The cogency of inductive rules is significantly affected by the given background knowledge and the chosen measures. For example, it is already known that relative to some background knowledge, NC violates intuition (e.g. see [Goo67]). In Section 2.1, we argue that relative to unrestricted background knowledge, not only NC but any rule of induction can be refuted. Hempel himself believed that NC and the raven paradox should be considered in the context of absolutely no background information [Hem67]. From a Bayesian perspective,\n1 The term rational agent (that is, the performer of induction), as used in fields such as decision theory and artificial intelligence [RN03], by definition, refers to an agent that satisfies certain consistency axioms (see [Sav54]). Representation theorems show that this implies that the agent has a probability distribution representing a priori beliefs for what will happen and utilities for the various possible outcomes, such that decisions can be explained as maximizing expected utility. Given utilities we can from simpler axioms [SH11] infer the existence of a probability distribution.\n2Some authors have even denied the equivalence of H and Ĥ [SG72].\nhowever, this does not solve the problem. The reason is that background knowledge and priors are convertible (in the sense that they can produce the same effects). For example if we are not allowed to consider NC in the context that we possess the background knowledge that “an object a has a property F” (denoted as: Fa), we can (approximately) produce the same situation by subjectively believing that the probability that that object has the property F , is sufficiently close to 1 (denoted as: pr(Fa) ≈ 1).\n3,4 Therefore, if we want to restrict ourselves to the context of perfect ignorance, not only should we possess no objective knowledge, we should also only be permitted to reason based on an absolutely unbiased probability measure. This raises an important question: “What is an unbiased measure?” Due to its subjective nature, this question does not have a definitive answer but by far, the most widely considered choice is the uniform measure.5,6 On the other hand, it is well known that using the uniform measure, inductive learning is not possible [Car50]. This shows that from the subjective probabilistic perspective, choosing a probability measure that satisfies the condition of perfect ignorance and allows inductive learning, is arguably impossible.\nIt is also notable that demonstrating that a specific probability measure does (or does not) comply with a rule of induction (or a statement such as PC), does not illuminate the reason why a typical human observer believes that such a rule (or statement) is implausible (or plausible). Conversely, one might argue that compliance of a probability measure with a counterintuitive statement such as PC, may\n3 According to Cournot’s principle [Cou43], we do not allow 1 (or 0) priors for events that may or may not happen. If it was allowed then assuming pr(Fa) = 1, would exactly produce the same effect that possessing the background knowledge “a is F” would. This would be problematic, because assigning probability 1 to the events that are not determined (by background knowledge) may lead to undefined conditional probabilities (in case the complement events occur). However, assigning a probability that is arbitrarily close to one (i.e. pr(Fa) ≈ 1) does not cause such a problem while it approximately produces the effect of the same background knowledge to arbitrary precision.\n4 Although for Hempel and his contemporaries, the confirmation theory was (more or less) a logical relation, akin to deductive entailment, rather than a probabilistic relation (in the formal sense) [FH10], what we mentioned about the convertibility of objective information and subjective beliefs, was in a way reflected in their discussions: Good’s Red Herring [Goo67] provided a hypothetical objective background setting with respect to which, NC does not hold. Hempel’s assertion that “NC should be considered in the context of no (objective) background information” was in fact an attempt to address such an issue. However, nothing could prevent Good from producing the same effect by simply replacing the objective information with subjective a priori beliefs of a new born baby (Good’s baby [Goo68]).\n5 The main justification is due to the principle of maximum entropy [Jay03]. This principle recommends that one choose, among all the probability measures satisfying a constraint, the measure which maximizes the Shannon entropy. In the absence of any background knowledge (i.e. no constraint), the uniform probability measure maximizes entropy.\n6 An alternative to the uniform measure is Solomonoff’s theory of universal inductive inference [Sol64] which mathematically formalizes and puts together Occam’s razor (the principle that the simplest model consistent with the background knowledge should be chosen) and Epicurus’ principle of multiple explanations (that all explanations consistent with background knowledge should be kept) (see [Hut07] or [RH11]).\nsuggest that this measure does not provide a suitable model for inductive reasoning. As an example, consider the following two works: [Mah99] and [Mah04]. They are among the most famous answers to the raven paradox. Using Carnap’s measure [Car80], in 1999 Maher argued that both NC and PC hold. In 2004 he suggested a more complex measure that led to opposite results: Maher showed that for this latter measure, neither NC nor PC holds in all settings. Although Maher’s works successfully show that at least for one probability measures NC holds and for another probability measures it does not, they do not show whether NC and PC are generally plausible or not.\nThe mainstream contemporary Bayesian solutions are not restricted to a particular measure and in this sense, are more general. According to [FH06] almost all of them accept PC and argue that observation of a non-black non-raven does provide evidence in support of H ; however, in comparison with the observation of a black raven, the amount of confirmation is very small. This difference, they argue, is due to the fact that the number of ravens is much less than the number of non-black objects. However as [Vra04] explains, these arguments have only been able to reach their intended conclusion by adding some extra assumptions about the characteristics of the chosen probability measure. He shows that the standard Bayesian solution relies on the almost never explicitly defended assumption that “the probability of H should not be affected by evidence that an object is non-black.” – a supposition that he believes, is implausible, i.e. may hold or not.\nTo summarize the above discussion: the general plausibility of a rule of induction cannot be determined if we restrict our study to particular (objective) background knowledge or a particular probability measure. On the other hand, no rule of induction holds in the presence of an unrestricted choice of background knowledge and probability measure. We conclude that rules of induction should be studied for different classes of background knowledge and priors. If a rule of induction holds relative to a large class of reasonable background knowledge (i.e. information similar to our actual configuration of knowledge obtained from observations that often take place in real life) and relative to reasonable probability measures (i.e. measures that have intuitively reasonable characteristics e.g. comply with other rules of induction which are more directly justified by our intuitive notion of induction), then we can claim that the studied rule is plausible, otherwise we cannot.\nIn this paper, we study NC with such an approach. In Section 2, we present a formal representation for three rules of induction, namely, projectability (PJ), reasoning by analogy (RA) and Nicod’s condition (NC). We also define the form of background knowledge that is studied throughout the paper. Informally speaking, we only study pieces of knowledge that do not link the properties of one object to another object. They can be though of as knowledge that can be gained directly by observing the properties of some distinct objects. For example, the background knowledge: “if object a is a raven, then object b is not a raven”, is not of this form. While one can easily constitute pieces of information that do not have such a form and violate the aforementioned rules of induction, we have not found any\nilluminating counterexample to the assumption that relative to a piece of information that does not link properties of distinct objects together, PJ and RA comply with intuition. In the case of NC, we are more inquiring. In the next two sections, we study the restrictions of the probability measures that guarantee the validity of NC relative to two more specific background configurations (that can be expressed in the mentioned form). In Section 3, we find some sufficient conditions for the validity of NC and some sufficient conditions for its invalidity, relative to information about the kind (i.e. being raven or not) and color (i.e. being black or not) of some objects. The sufficient condition that we present for the validity of NC is less restrictive. However, this is insufficient for claiming that in this setting NC is generally plausible. Section 4 deals with the setting where the exact number of objects having one property is known. For example we know how many ravens (or how many non-black objects) exist. We show that in this setting, measures that comply with Nicod’s condition, do not always comply with PJ which seems to be the simplest formalization of inductive inference. It is also shown that in the case of contradiction, intuition (arguably) follows PJ rather than NC. We think that this result is both interesting and somewhat surprising and should be considered as a main contribution of this paper.\nOne limitation of our basic setup is that it limits us to a universe with an arbitrary but known size. However, in Section 5, this strong assumption is replaced by the weaker assumption that there is a probability distribution over the possible sizes of the universe and this distribution is not affected by an observation of a single object. We show that under this weaker assumption, the results from the former sections remain valid.\nIn Section 6, we summarize the paper and conclude that there is a tension between NC and our intuitive notion of inductive reasoning. We also suggest that reasoning by analogy provides a viable alternative to formalize the seemingly intuitive statement that “the observation that a particular object is both F and G confirms the hypothesis that any object that is F is also G” without suffering form the shortcomings of NC. All theorems are proven in Section 7."
    }, {
      "heading" : "1.1 Notation, Basic Definitions and Assumptions",
      "text" : "Throughout sections 2 to 4 we work with a first-order language L whose only nonlogical symbols are a pair of monadic predicates F and G and a set of constants U (officially shown as) {u1, u2, . . . , uN} where N is a known positive integer. However, for simplicity we drop “u” and refer to each constant by its index. We rely on the domain closure axiom [Rei80], that is:\n∀x (x = 1) ∨ (x = 2) ∨ . . . ∨ (x = N) (1.1)\nwhere 1 to N are distinct constants i.e. (1 6= 2) ∧ (1 6= 3) ∧ . . .. Clearly, models of this axiom are restricted to interpretations with domains containing exactly N distinct individuals (objects) each of which is denoted by a constant in U . Using\nthis bijection between the elements of the domain and constants, we refer to U as the universe.\nNegation, conjunction, disjunction and material implication are respectively represented by “¬”, “.” (or “∧”), “∨” and “→”. If b is an individual and ψ is a 1-place predicate (either atomic or a sentential combination of atomic predicates), ψb is defined as a proposition that involves predicate ψ and indicates: “b has (or satisfies or is described by) ψ”. Conjunction of several propositions ψm, ψm+1, . . . , ψn is abbreviated by ψm:n. By definition, for n < m, ψm:n := ⊤ (i.e. tautology) and ψm:m := ψm. More generally, if bm to bn are some objects (not necessarily consecutive), ψbm:bn := ψbm∧ . . . ∧ ψbn . The general hypothesis H := (∀x F→Gx) (which is equivalent to F→G1:N) where F→Gb := Fb → Gb. We also let:\nF̄Ḡb := ¬Fb.¬Gb; F̄Gb := ¬Fb.Gb; FḠb := ¬(F→Gb) = Fb.¬Gb; FGb := Fb.Gb (1.2) Any of F̄Ḡb to FGb defined by relation (1.2) is referred to as a complete description of an object b.7 For example if F and G represent “ravenhood” and “blackness” properties respectively, then F̄Ḡb means “b is not a raven and is not black” and so on. F→Gb means “if b is a raven then it is black” and H is the general hypothesis that “for all b, if b is a raven then it is black”. Clearly all complete descriptions which provide counterexample to H are in the form FḠb.\nWe define ∆ as the set of all propositions ρ which are in the following form (or by simplification can be converted to it):\nρ := ψ1b1 .ψ 2 b2 . . . . ψkbk =\nk ∧\nx=1\nψxbx (1.3)\nwhere ψ1 to ψk are some predicates in {F , ¬F , G, ¬G, F̄Ḡ, ¬F̄Ḡ, F̄G, ¬F̄G, FḠ, F→G, FG, ¬FG} and b1 to bk are some mutually distinct objects: {b1, b2, . . . bk} ⊆ U . Note that ∆ is in fact the set of all propositions that do not link the properties of different objects together. We define the set of all individuals described by ρ as: Iρ := {b1, b2, . . . bk}. We refer to the set of (simple) predicates involved in ρ by: Aρ := {ψ\n1, . . . , ψk}. For example, the proposition ρ′ := FG1 ∨ FḠ3 is not in ∆ but ρ′′ := FG1.FG3.F→G4 ∈ ∆ (assuming N ≥ 4). Iρ′′ = {1, 3, 4} and Aρ′′ = {FG, F→G}. By definition, empty (or tautologous) proposition ⊤ is a member of ∆ with I⊤ = ∅. Two subsets of ∆ are defined as follows:\nδ := { d ∈ ∆ : Ad = {F̄Ḡ, F̄G, FG} }\nΩ := { d ∈ ∆ : Id = U,Ad = {F̄Ḡ, F̄G, FḠ, FG} }\nInformally speaking, δ is the set of propositions that completely describe some individuals and do not falsify H . Likewise, Ω is the set of propositions that completely\n7 F̄Ḡ, F̄G, FḠ and FG are what Maher calls Q4, Q3, Q2 and Q1 respectively. What we call complete description, he calls sample proposition [Mah99].\ndescribe all objects of the universe. We refer to any member of Ω as a Complete Description Vector (CDV). Note that each CDV corresponds to a unique model or world (up to isomorphism). In other words, every interpretation that makes a CDV (and aforementioned axiom (1.1)) true, uniquely determines the value of any sentence in L,8 therefore we can consider them as (representatives of) different worlds. The probability measures which we are concerned with, are defined over the sample space Ω with the power set as σ-algebra. No other restriction is imposed on the choice of measure unless it is mentioned explicitly. For each proposition ρ, let ωΩρ := {o ∈ Ω : o |= ρ} be the set of all CDVs that entail ρ. We say that the event ωΩρ corresponds to the proposition ρ (and vice versa). For convenience’ sake, except in Section 5, we represent the probability of events by the probability of their corresponding propositions9; formally, for propositions ρ, we let pr(ρ) := pr(ωΩρ ). According to Cournot’s principle [Cou43], we do not allow 1 (resp. 0) priors to the sentences that are not valid (resp. unsatisfiable).\nObjective background knowledge (or simply background knowledge) is what we are certain about and can be represented by a subset of Ω. The more formal definition of the background setting studied throughout this paper and its corresponding restrictions are given in Section 2.1.\nWe equate “inductive support” with “probability increment”: It is said that in the presence of background knowledge D, evidence E confirms hypothesis H iff:\npr(H|E.D) > pr(H|D)\nWe are only interested in the case where E and D are consistent, pr(E) and pr(D) are positive and E is not determined by D, i.e. 0 < pr(E|D) < 1."
    }, {
      "heading" : "2 Inductive Reasoning and Nicod’s Condition",
      "text" : "The fundamental assumption behind inductive inference is the so-called principle of the uniformity of nature [Hum88] (or the immutability of natural processes [Pop59]) based on which, uniformity and trend are more probable than diversity and anomaly a priori. Let us assume that ∆ is a set of background knowledge configurations for which we “intuitively” expect that inductive inference holds (for more discussion refer to Section 2.1). Relative to pieces of information in ∆, we present the following varieties of inductive inference (i.e. inductive rules):\n8 The proof is straightforward. With respect to the domain closure axiom, all quantifiers are bounded. Therefore all sentences are convertible to quantifier-free forms and consequently convertible to full disjunctive normal form (DNF) which is in fact a disjunction of some CDVs. In any world, only one CDV is true, therefore only sentences containing that CDV (when expressed in full DNF) are true.\n9 In Section 5, we simultaneously deal with more than one sample space. While w.r.t. different sample spaces, propositions may correspond to different events, in that section we directly represent probability events by their relevant sample space subsets.\nProjectability. For all objects a and b and background knowledge D ∈ ∆ that does not determine ψa or ψb, based on [Mah04], one (and apparently the simplest) kind of inductive inference, namely projectability10,11, is defined as follows:\nStrong projectability: ∀a, b ∈ U pr(ψb|ψa.D) > pr(ψb|D)\nWeak projectability (PJ): ∀a, b ∈ U pr(ψb|ψa.D) ≥ pr(ψb|D) (2.1)\nProjectability (relative to predicate ψ) can be justified as follows: The evidence ψa increases the proportion of the observed individuals that have the predicate ψ. Thus, according to the principle of the uniformity of nature, the estimated frequency of the predicate ψ in the total population should also be increased because the uniformity between the characteristics of the sample and the total population is considered to be likely.\nReasoning by Analogy (RA). The observation that two individuals have some common properties, increases the probability that their unobserved properties are also alike, because it is likely that there is a uniformity between the characteristics of unobserved properties and the observed ones. Maher has formalized one variation of reasoning by analogy (or inference by analogy [Car50]) as: ∀a, b ∈ U pr(Gb|Fb.FGa) > pr(Gb|Fb) [Mah04]. We generalize the relation to cover the case where background knowledge D ∈ ∆ (that does not determine the value of Fa, Ga and Gb) is also present:\nReasoning by analogy (RA): ∀a, b ∈ U pr(Gb|Fb.FGa.D) > pr(Gb|Fb.D) (2.2)\nNicod’s Condition (NC). For U := {1, . . . , N}, H := F→G1:N , all a ∈ U and D ∈ ∆ that does not determine the value of FGa or H , we say NC holds for D iff:\nNicod’s Condition (NC): pr(H|FGa.D) > pr(H|D) (2.3)\nNC is stronger than PJ or RA in the sense that it deals with the confirmation of a generalization rather than a singular prediction. In other words, NC is a form of enumerative induction but PJ and RA are forms of singular predictive inference."
    }, {
      "heading" : "2.1 Restrictions on the Background Knowledge",
      "text" : "Obviously, relative to unconstrained background knowledge, no rule of induction holds in general. For example, in the presence of background knowledge D′ := F1 → (¬F )2:N , at least relative to evidence F1, PJ does not hold. Similarly, (as\n10 According to [Car50] predictive inference (i.e. inference from a sample to another sample) is the most important kind of inference and the most important special kind of it, singular predictive inference, is inference from a sample to an individual object. Maher’s projectability is in fact a special kind of singular predictive inference: inference from one individual to another individual.\n11Maher’s original relation does not mention background knowledge, and only deals with strong projectability (which he calls absolute projectability).\n[Mah04], Theorem 12 formally shows), in the presence of background knowledge D′′ := FG1 → ¬H , NC does not hold (for evidence FG1).\nTo prevent such problems, the biggest set of background configurations studied through out this paper is ∆12, which according to its definition in Section 1.1, is the set of all consistent propositions that can be expressed in the form of a conjunction of some propositions that involve F , G, F̄Ḡ, F̄G, FḠ, FG or their negations.\nObviously, each member of ∆ can be expressed in the form of a conjunction of some propositions each of which describes only one individual. Consequently, problematic statements that interlink properties of different individuals are not expressible. As an example, the mentioned pathological examples D′ and D′′ are not in ∆.\nIn the case of PJ and RA, we did not find a pathological example in ∆, relative to which, the rule of induction contradicts intuition. However, in the case of NC it is already claimed that relative to background knowledge D′′′ := ¬Fa ∈ ∆, it is not intuitively sound to expect that the evidence F̄Ḡa confirms H [FH10]. In Sections 3 and 4, we will investigate the validity of NC relative to two interesting subsets of ∆."
    }, {
      "heading" : "2.2 Restrictions on the probability measure.",
      "text" : "In Section 3 (Setting 1), we impose no restriction on the choice of the probability measure but in Section 4 (Setting 2), we assume that the probability measure is exchangeable [Car80] in a sense that probabilities are not changed by permuting individuals (i.e. swapping the name of objects). To introduce this restriction formally, we need the following definitions:\nDefinition. By the term permutation, we always refer to a bijection from a set of all objects U to itself. Throughout this paper, we denote any arbitrary permutation by π (or π′ and π′′ when we deal with more than one permutation). Having a proposition ρ, the proposition ρπ is obtained from ρ by replacing any occurrence of any individual b with π(b).\nExample 1. If U := {1, 2, 3}, the function π : U → U defined by π(1) = 1; π(2) = 3 and π(3) = 2, is a permutation with a fixed point 1. For short we write π := {2/3; 3/2}. If we define ρ := F1 ∨G3, then ρ π = F1 ∨G2. ♦\nExchangeability. The probability measure pr is exchangeable if for all propositions A and B and all permutations π, pr(A|B) = pr(Aπ|Bπ).\n12 Note that we do not claim that no background knowledge that is not a member of ∆ is not plausible. Investigation of rule of inductions relative to such knowledge, is simply beyond the scope of this paper."
    }, {
      "heading" : "3 Validity of NC when Background Knowledge",
      "text" : "Consists of Complete Descriptions of Some In-\ndividuals (Setting 1)\nIn Section 1.1, δ was defined as the set of all background knowledge that do not refute H and describe some individuals completely (e.g. in the case of the raven paradox, members of δ represent the knowledge that we are already aware of the color and kind (i.e. the state of being raven) of some individuals and none of these known objects have been a non-black raven). Clearly, δ ⊂ ∆. Theorem 3.1 shows that if the chosen probability measure satisfies some conditions, then for any background knowledge D ∈ δ, NC holds (for predicates F and G). On the other hand, Theorem 3.2 shows that under alternative conditions, for some D ∈ δ, NC does not hold.\nTheorem 3.1. If a probability measure complies with the following relation:\n∀B ∈ ∆, ∀a, b /∈ IB pr(FḠb|FGa.B) ≤ pr(FḠb|B) (3.1)\nthen, for this measure and any D ∈ δ that does not determine FGa or H , NC holds, i.e. relation (3.1) entails: pr(H|FGa.D) > pr(H|D).\nExample 2. If background knowledge consists of complete descriptions of some individuals, by Theorem 3.1 for all pairs of predicates F and G, the uniform measure complies with NC since regardless of the interpretation of F and G, for this measure, ∀B ∈ ∆ & ∀a, b /∈ IB pr(FḠb|FGa.B) = pr(FḠb|B). This is not surprising since using this measure, learning is impossible (see [Car50]). This means that no observation changes the probability of being FḠ for an unobserved object. Nonetheless, for this measure NC is valid because any evidence in the form of F̄Ḡa, F̄Ga or FGa confirms H for the simple reason that it removes the possibility that the observed object (i.e. a) is a counterexample to H . ♦\nExample 3. In Carnap’s theory of inductive probability [Car80]:\npr(ψb|E) = nψ + λ · pr(ψb)\nn + λ\nIn the above relations, n is the number of objects mentioned by evidence E; nψ is the number of mentioned objects which satisfy predicate ψ, and λ is a constant measuring the resistance to generalization. Note that b should not be mentioned by E, i.e. b 6∈ IE . Using this measure and choosing ψ ∈ {F̄Ḡ, F̄G, FḠ, FG}, in the presence of background knowledge D ∈ δ such that a, b /∈ ID: pr(FḠb|FGa.D) = λ.pr(FḠb|D)\n1+λ < pr(FḠb|D). Thus by Theorem 3.1, for the class of background knowledge in the form of conjunction of some F̄Ḡ, F̄G and/or FG for distinct individuals, this measure complies with NC. This is equivalent to the setting chosen by [Mah99] and its corresponding results. ♦\nTheorem 3.2. If a probability measure complies with restrictions (3.2) and (3.3), then for this measure (and predicates F and G) and background knowledge D ∈ δ, NC does not hold.\n∀B∈∆, ∀a, b /∈ IB pr(FḠb|FGa.B) > pr(FḠb|B) (3.2)\n∀a /∈ID pr(¬F→Ga|F→Gb1: bn .D) < pr(FḠb1 |FGa.D)− pr(FḠb1 |D) (3.3)\nIn the above relations a 6= b and b1 to bn represent an arbitrary enumeration of all individuals (except a) that are not mentioned by D (that is, ID = U\\{b1 . . . bn, a}).\nAccording to restriction (3.3), the probability that a is not F→G given that all other objects in the universe are F→G should be less than the degree of confirmation by evidence FGa of a hypothesis that an unobserved object b1 is FḠ. Note that b1 can be the index of any unobserved object.\nExample 4. [Mah04] proposes a measure based on the formula:\npr(FḠb|E) = pr(I) · nF + λ · pr(Fb) n+ λ · nG + λ · pr(¬Gb) n+ λ + pr(¬I) · nFḠ + λ · pr(FḠb) n+ λ\nIn the above relation, n is the number of objects mentioned by E; nF and nG denote the number of mentioned objects which are F and ¬G respectively. In this expression, the prior probability of FḠ i.e. pr(FḠb) has to be equal to pr(Fb) · pr(¬Gb) and pr(I) and λ are parameters. Maher proposes a counterexample for NC where N = 2 (Let U := {a, b}), λ = 2, pr(I) = 0.5 and prior probabilities are pr(Fb) = 0.001 and pr(Gb) = 0.1. This conclusion can be confirmed independently by Theorem 3.2 as follows: 1. For these parameters, the only member of ∆ that does not contain a and b, is B = ∅ for which relation (3.2) holds if pr(Fb) < 0.25. 2. By assuming: ( pr(FḠb|FGa)−pr(FḠb) )\n≥ 0.06 and pr(Gb) = 0.1, a cumbersome calculation shows that (for empty background knowledge) relation (3.3) holds if: pr(Fb) < 0.0983 which covers Maher’s proposed configuration. ♦\nComparing Theorems 3.1 and 3.2 shows that creating a probability measure that contradicts NC (w.r.t. D ∈ δ) is harder than making a measure that complies with it (for the same background setting) because the former measure has to satisfy more constraints. The reason is that even if in a measure, evidence E := FGa does not affect the probability of FḠ for unobserved objects (as in the case of the uniform distribution), every hypothesis that is not refuted by E (including H) is confirmed by it since the observation has reduced the number of possible counterexamples by one. On the other hand, in the case of a measure that does not comply with NC, not only should E confirm FḠ for unobserved objects, but the effect of this confirmation should be so substantial that it overwhelms the effect of the elimination of one counterexample to H .13 However, in the case where the size of the universe\n13 To see how the effect of elimination of one possible counterexample leads to relation (3.3), refer to the proof of Theorem 3.2 in Section 7.\nis large, the latter effect should be minute. This is reflected in relation (3.3) as follows: If N is large, then at least for measures that comply with projectability, pr(¬F→Ga|F→Gb1: bn .D) ≈ 0, because if it is known that all objects in the universe except a are F→G, then it should be quite probable that a is F→G too. Therefore, even if the degree of confirmation of FḠb1 by evidence FGa (i.e. pr(FḠb1|FGa.D)− pr(FḠb1 |D)) is very small 14 , relation (3.3) holds.15 To summarize:\n• If regardless of the choice of background knowledge, an observation Fa.Ga does not confirm that any unobserved individual is an F that is not G, then relative to any background knowledge in δ, NC holds.\n• If regardless of the choice of background knowledge, an observation Fa.Ga confirms that any unobserved individual is an F that is not G, and on the other hand, the effect of elimination of one counterexample via an observation is\nnegligible, then relative to any background knowledge in δ, NC does not hold.\nThe above statements delegate the assessment of NC (a form of enumerative induction) to the assessment of expressions which deal with singular predictions. Hence, a new perspective on the nature of NC is provided: Should regardless of the interpretation of F and G, (the observation of) an F that is G disconfirm that any unobserved object is F but not G?\nFor example, relative to background knowledge and a probability measure that reflect our actual configuration of knowledge, should the observation of an F=“walnut” that is G=“round” decrease the probability that any unobserved object is a walnut but not round? Indeed yes; therefore by Theorem 3.1, in this case and for these predicates, NC holds. Should the observation of an F=“round”, G=“walnut” decrease the probability that any unobserved object is “round” but not a “walnut”? Arguably not. Should the observation of an F=“ogre” which is G=“old” decrease the probability that we might encounter an ogre which is not old? Definitely not! 16 Therefore, in\n14 Note that by relation (3.2), this degree of confirmation is positive. 15 Here is another justification for the above argument: By definition, a probability measure defined over a first-order language with an infinite domain is Gaifman iff the probability of the generalization of any predicate (in our case, F→G) is equal to the probability of the conjunction of some positive instances when their number tends to infinity [GS82] or alternatively, pr(∀x ψx|ψ1:n) n→∞ −−−−→ 1 (see [HLNU13] thm. 27) and consequently pr(¬ψa|ψ1:n) n→∞ −−−−→ 0. Since the Gaifman condition is what we intuitively expect from generalization over an infinite universe, it can be considered as a very simple and intuitive rule of induction. In our case, if the universe was infinite and the measure was assumed to be Gaifman, inequality (3.3) would always hold. However we have assumed that the universe is finite therefore we cannot remove this inequality. What we can say is that for very large domains, relation (3.3) is a very weak condition.\n16 This confirmation asymmetry may be due to possible asymmetry in background knowledge and/or prior possibilities of different predicates. For example according to our actual configuration of knowledge, the prior probability of “being an ogre” (for any individual) is quite low. This is a key point in the existing arguments: Good’s baby [Goo68] (that assigns low probability to ravenhood) and Maher’s unicorn [Mah04]. But unlike our discussion, these arguments do not reduce the assessment of NC to a singular prediction.\nthis case, we are intuitively using a probability measure that satisfies the condition (3.2). Now assume that we have seen all objects of the world except one. It has happened that any observed object that has been an ogre has been old as well. Is it reasonable to believe that it is improbable that the last unobserved object is a young ogre? If yes, then our intuitive measure also complies with restriction (3.3), hence by Theorem 3.2, by this denotation for F and G, plausible probability measures do not comply with Nicod’s condition."
    }, {
      "heading" : "4 NC vs. PJ when the Number of Objects having",
      "text" : "One Predicate is Known (Setting 2)\nThis section studies NC in the presence of a completely different background setting where we know that exactly k individuals are F (e.g. ravens) and the rest are not F , but we do not know anything about the other property (e.g. their color).\nFirst we focus on a simpler setting where we know exactly which objects are F and which objects are not F (e.g. we know that objects 1 to k are F and the rest of the universe i.e. objects k + 1 to N are not F ).\nTheorem 4.1. For U := {1, 2, . . .N} and D := F1:k.(¬F )k+1:N , weak projectability (PJ) entails:\npr(H |Gk . D) > pr(H|D) (4.1)\npr(H |GN . D) ≥ pr(H|D) (4.2)\npr(H | ¬GN . D) ≤ pr(H|D) (4.3)\nand reasoning by analogy (RA) entails:17\npr(H |Gk . D) > pr(H|D) (4.4)\nNext, we show that these results are valid in the general setting where the background knowledge is such that we only know the exact number of objects being F but we do not know their names. In other words, we know that exactly one combination of k out of N objects of the universe are F but we do not know which combination. But before that, we should formalize such knowledge in the form of an event (i.e. a subset of the sample space).\nDefinition. CU,k := {C : C ⊆ U, |C| = k} is defined as the set of all (distinct) subsets of U which contain exactly k individuals. Obviously, the cardinality of CU,k is ( N\nk\n)\n.\n17 Therefore, in this setting both PJ and RA suggest thatH := ∀b F→Gb is confirmed by evidence Gk but RA provides no answer whether or not evidence GN should confirm (or disconfirm) H . The reason is that (as the proof of the theorem which is provided in Section 7 shows) in the presence of background knowledge F1:k.(¬F )k+1:N , validity of H only depends on property G of objects 1 to k that do not have a common property with object N .\nExample 5. Given U := {1, 2, 3, 4}, CU,2 = {\n{1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4} } . ♦\nDefinition. For 1 ≤ k ≤ N , “Exactly k objects of the universe U are F” is formally defined as follows:\nExact(k, U, F ) := ∨\nC∈CU,k\n(\n∧\nb′∈C\nFb′ . ∧\nb′′ 6∈C\n¬Fb′′ )\n(4.5)\nExample 6. In the previous example, Exact(2, U, F ) = (F1.F2.¬F3.¬F4) ∨ (F1.F3.¬F2.¬F4) ∨ (F1.F4.¬F2.¬F3) ∨ (F2.F3.¬F1.¬F4) ∨ (F2.F4.¬F1.¬F3) ∨ (F3.F4.¬F3.¬F4). ♦\nBy comparing definition (4.5) with the definition of ∆, it becomes clear that for 1 < k < N , Exact(k, U, F ) 6∈ ∆, therefore we do not expect that in the presence of such background knowledge, rules of induction hold in general and they actually don’t. For instance, knowing that exactly k objects are F , the evidence that a particular object is F , confirms that any other object is not F ,18 which contradicts PJ:\n(intuitively): ∀b 6= a ∈ U pr ( Fb|Fa.Exact(k, U, F ) ) < pr ( Fb|Exact(k, U, F ) )\nHowever, the following theorem shows that for the hypothesis that we are interested in i.e. H := ∀b F→Gb, the background knowledge Exact(k, U, F ) is equivalent to F1:k.¬Fk+1:N which is a member of ∆. Therefore, in the case of the raven paradox and background knowledge Exact(k, U, F ), the rules of induction (that are assumed to hold relative to background knowledge in ∆) should still hold.\nTheorem 4.2. If U = {1, . . . , N} and a is an arbitrary member of U and assuming that a probability measure pr is exchangeable:\npr ( H|Exact(k, U, F ) ) = pr ( H|F1:k.¬Fk+1:N )\n(4.6)\npr ( H|Exact(k, U, F ).Fa.Ga ) = pr ( H|F1:k.¬Fk+1:N .Gk )\n(4.7)\npr ( H|Exact(k, U, F ).¬Fa.Ga ) = pr ( H|F1:k.¬Fk+1:N .GN )\n(4.8)\npr ( H|Exact(k, U, F ).¬Fa.¬Ga ) = pr ( H|F1:k.¬Fk+1:N .¬GN )\n(4.9)\nThe formal proof of this theorem is presented in Section 7.3, but the following simple example shows the main idea behind the general proof.\nExample 7. Having U := {1, 2, 3}, we show that: pr ( H|Exact(2, U, F ).F3.G3 ) = pr ( H|F1.F2.¬F3.G2 ) (that is relation (4.7) for a :=\n18 Suppose that you are in a camp populated by 100 captives, and it is known that 10 of them will be chosen randomly to be executed; Whenever someone except you is chosen, it is reasonable to be more optimist about your fate, for the simple reason that 9\n99 < 10 100 .\n3 and k := 2) as follows:\npr ( Exact(2, U, F ).F3.G3|H )\n= pr ( (F1.F2.¬F3 ∨ F1.F3.¬F2 ∨ F2.F3.¬F1).(F3.G3)|F→G1.F→G2.F→G3 ) , by def.\n= pr(F1.F3.¬F2.G3 ∨ F2.F3.¬F1.G3|F→G1.F→G2.F→G3), by simplification\n= pr(F1.F3.¬F2.G3|F→G1.F→G2.F→G3) + pr(F2.F3.¬F1.G3|F→G1.F→G2.F→G3),\nby σ-additivity of disjoint events (3rd Kolmogorov probability axiom)\n= pr(F1.F2.¬F3.G2|F→G1.F→G3.F→G2) + pr(F1.F2.¬F3.G2|F→G3.F→G1.F→G2),\nby exchangeability assumption, using premutation π′ := {3/2; 2/3} on the first term and π′′ := {3/2; 1/3; 2/1} on the second term\n= 2 · pr(F1.F2.¬F3.G2|H)\nSimilarly it can easily be shown that: pr ( Exact(2, U, F ).F3.G3 ) = 2 · pr ( F1.F2.¬F3.G2 ) . Therefore by Bayes rule:\npr ( H|Exact(2, U, F ).F3.G3 ) = pr ( H ) · pr ( Exact(2, U, F )|H )\npr ( Exact(2, U, F ) )\n= 2 · pr(H) · pr(F1.F2.¬F3.G2|H)\n2 · pr(F1.F2.¬F3.G2) = pr(H|F1.F2.¬F3.G2)\nwhich is what we wanted to show by this example. ♦\nTheorems 4.1 and 4.2 directly entail the main theorem of this section:\nTheorem 4.3. If Exact(k, U, F ) := “exactly k objects (of the universe U) are F” and a ∈ U is an object, and the probability measure pr is exchangeable, weak projectability (PJ) entails:\npr ( H |Exact(k, U, F ).Fa.Ga ) > pr ( H |Exact(k, U, F ) )\n(4.10)\npr ( H |Exact(k, U, F ).¬Fa.Ga ) ≥ pr ( H |Exact(k, U, F ) )\n(4.11)\npr ( H |Exact(k, U, F ).¬Fa.¬Ga ) ≤ pr ( H |Exact(k, U, F ) )\n(4.12)\nand reasoning by analogy (RA) assumption entails:\npr ( H |Exact(k, U, F ).Fa.Ga ) > pr ( H |Exact(k, U, F ) )\n(4.13)\nThe above relations seem to be compatible with intuition. While the total number of objects that satisfy F is known in advance, the consideration of Fa or ¬Fa should not affect our estimation of the frequency of the objects being F . On the other hand, the probability of G can still be affected by observations. Therefore, assuming PJ, consideration of Ga increases the probability of G and consequently decreases the probability of FḠ. As a result it seems reasonable that the evidence Ga confirms H = (¬FḠ)1:N , and the evidence ¬Ga disconfirms it.\nMoreover, an observation Fa.Ga has an extra effect: While it is known that only k objects can be counterexamples to H (because in order to be FḠ, one should be F ), the observation Fa.Ga decreases the number of possible counterexamples by one. This holds even in the case where the chosen measure is such that inductive reasoning is not possible (e.g. the uniform measure is used). Consequently, in (4.10) inequality is strict, but in (4.11) and (4.12) it is not. Theorem (4.3) implies the following results:\nCorollary 4.4. If F := raven and G := black, according to relation (4.10) (or 4.13), PJ (or RA) leads to: pr(H| (exactly k objects are ravens).(a specific object is raven and black)) > pr(H|exactly k objects are ravens)\nCorollary 4.5. If F := nonBlack & G := nonRaven w.r.t. relation (4.10) (or 4.13), PJ (or RA) leads to: pr(H|(exactly k objects are not black).(a specific object is nonBlack and nonRaven)) > pr(H| exactly k objects are not black)\nCorollary 4.6. If F := raven and G := black, w.r.t. (4.11), PJ leads to: pr(H|(exactly k objects are ravens).(a specific object is nonRaven and black)) ≥ pr(H|exactly k individuals are ravens)\nCorollary 4.7. If F :=nonBlack & G:=nonRaven w.r.t. (4.11), PJ leads to: pr(H|(exactly k individuals are not black).(a specific object is black and not raven)) ≥ pr(H|exactly k individuals are not black)\nCorollary 4.8. If F := raven and G := black, w.r.t. (4.12), PJ leads to: pr(H|(exactly k individuals are ravens).(a specific object is not raven and not black)) ≤ pr(H|exactly k individuals are ravens)\nCorollary 4.9. If F := nonBlack & G := nonRaven, w.r.t. (4.12), PJ leads to: pr(H|(exactly k individuals are not black).(a specific object is black and raven)) ≤ pr(H|exactly k individuals are not black)\nCorollaries (4.4) to (4.9) are summarized in (Table 1). NC, if assumed to hold in this setting, suggests that the observation of a non-black non-raven and the observation of a black raven (i.e. entries in the first and fourth rows of the table) should confirm H which clearly contradicts what PJ suggests, therefore, there is a tension between these two rules. When background knowledge is neglected, intuition goes with PJ in the first column of the table. On the other hand, it does not completely match the suggestions of either NC or PC in the second column. This may indicate that intuition is more inclined to the case where “the number of ravens” and not “the number of non-blacks” is known in advance. In real life, none of these numbers is known but the total number of ravens can be estimated much easier than the\nnumber of non-black objects. On the other hand, if we are explicitly informed of the total number of non-black objects, at least in cases similar to the following example, intuition seems to follow PJ’s suggestions in the second column:\nImagine that you are only concerned about objects which are placed inside a bag (i.e. U := set of objects inside a bag). Also imagine that you are told that only 4 objects are not black. In this case, there are just four possible counterexamples to H . Now suppose that a green apple comes out of the bag. Since it is green, it is one of those 4 non-blacks. Therefore, one possible counterexample is removed. Meanwhile, the fact that it is a non-raven may increase the probability of non-ravenhood (w.r.t. PJ). Therefore it is more probable that the 3 remaining non-blacks are also nonraven. Thus, this observation should confirm H . Now suppose that a black raven comes out. Its color informs us that it is not among the possible counterexamples but its kind increases the probability of ravenhood which is not in favor of H . So, this observation cannot confirm H . Therefore, this example suggests that in Setting 2, given the proper background knowledge, intuition does not follow NC. It follows PJ even if it advises that the observation of a green apple confirms that “all ravens are black” and the observation of a black raven does not!\nClearly, we can never “prove” that a particular measure or a particular proposition is (or is not) “intuitively plausible”, due to the subjective nature of the problem. The former example presented a particular method of reasoning that relative to a given configuration supports PJ more than NC. This has convinced us that generally in setting 2, PJ is more plausible than NC but as we mentioned, some people might not be convinced. For example, one might argue that if we are told that the number of non-black objects is precisely 7 million, we can still believe that black ravens confirm that all ravens are black. Such reasoning might be on grounds of\nsome “hidden” background information such as knowing that ravens are animals and animals of the same kind often have similar colors. This particular background knowledge is not in ∆ (and therefore not in Setting 2) however as it was mentioned in the introduction, this knowledge is convertible to the subjectively chosen a priori probability measure. Of course given such knowledge, there will be no surprise if NC holds for F :=raven and G:=black but not for F := non-black and G:= non-raven. However, it is up to the readers to judge about what is intuitive for them and what is not. What was formally provable (and is proved formally) is that in setting 2, no probability measure can simultaneously satisfy PJ and NC for a couple of predicates F and G. Reasoning by analogy vs. Nicod’s condition: Table 1 clearly shows that the only cases where PJ and NC do not contradict is when according to RA, the general hypothesis H should be confirmed. In other cases, RA do not impose a restriction; therefore, it never contradicts either PJ or NC.\nA little thought reveals that RA and NC have many commonalities. We go a step further and propose a conjecture that NC may seem intuitively valid since it can easily be conflated with RA as follows:\nAccording to the informal definition of NC: “The observation of an F that is G confirms that all F are G (or any F is G).” Although this informal statement seems to be plausible a priori, it is vague and NC is not necessarily its only possible formalization. To begin with, it should be noticed that in informal language, the scopes of quantifiers are often ambiguous; For example the informal expression “for all b, the probability of ψb . . . ” can easily be mistaken for “the probability that for all b, ψb”. However the most suitable formalization of the former (i.e. ∀b pr(ψb)) differs from that of the latter (i.e. pr(∀b ψb)). On the other hand, the informal “if” does not exclusively stand for material implication; in a proper context it can also mean conditional probability. Putting these together, it can be seen that:\n(Informal NC): “The observation of an object a which is both F and G confirms that all (or any) object b that is F is also G.”\ncan alternatively be formalized as: ∀b pr(Gb |Fb.Fa.Ga) > pr(Gb |Fb). This relation is the definition of RA (see relation 2.2) – a rule of induction which is used in many fields (e.g. in case-based reasoning [AP94]), is directly justified by the principle of the uniformity of nature and does not suffer from the shortcomings of NC such as contradicting PJ or producing counterintuitive conclusions such as PC in the raven paradox."
    }, {
      "heading" : "5 When the Size of the Universe is Unknown",
      "text" : "In Section 1.1, we defined our probability space using the sample space Ω, the set of all complete description vectors (CDVs), all involving N objects. Thus, from the beginning, we had to assume that the cardinality of the universe is known. In this section, we instead assume that:\n1. The size of the universe (i.e. N) is unknown; however it is known that it is fixed and bounded by some known constants α and β. E.g. assume it is known that the number of the objects of the universe is larger than α = 1010 and less than β = 101000.\n2. The new evidence E := Fa.Ga, does not affect the way the rational agent estimates the size of the universe. E.g., the observation of a black raven does not change the probability distribution over the possible sizes of the universe.\nWe show that in this setting, our previous conclusions are still valid. Informally speaking, the reason is that all (in)equalities of the previous sections hold for any arbitrary (but fixed) size of the universe, therefore this number does not play a role, and consequently, even if it is unknown, (as long as new evidence does not affect the agent’s beliefs about it) all (qualitative) relations should still hold.\nTo justify this claim formally, we need some new notation: If it is known that the size of universe is υ, we let the enumeration of its objects be Uυ := {1, 2, . . . υ} (created recursively by Uυ := Uυ−1 ⊔ {υ}).\n19 Instead of ∆, we write ∆υ to indicate that the members of this set describe individuals which belong to the universe Uυ. Similarly, instead of Ω, we write Ωυ to emphasize that the sample space corresponds to a universe of size υ (i.e. Uυ).\n20 Similarly, instead of pr(·), we write prυ(·) to indicate that by definition, prυ (defined on Ωυ) is a measure that provides a probabilistic model for the rational agent (who performs induction), only if he/she/it knows the cardinality of the universe is υ.\nTo prevent ambiguity, instead of representing the events by propositions, we directly use subsets of the sample spaces: The event that corresponds to an arbitrary proposition ρ relative to a sample space Ωυ is: ω Ωυ ρ := {o ∈ Ωυ : o |= ρ}. For example, if ρ := F1, then ω Ω1 ρ represents the event {FḠ1,FG1}, while ω Ω2 ρ stands for the event {FḠ1F̄Ḡ2, FḠ1F̄G2, FḠ1FḠ2, . . ., FG1FG2}. The Complete Description Vectors (CDVs) are here bold-faced and conjunction symbols “.” are dropped to emphasize that they are not ordinary propositions. For example FG1FG2 is a CDV that not only entails the ordinary proposition FG1.FG2, but also indicates that the universe is U2 = {1, 2}. The reason is that by definition, each CDV describes all objects of the universe (see Section 1.1).\nLet α and β be some known lower and upper bound for the size of the universe.21 We define a new sample space Ωα:β as a set that contains all members of all sample spaces that correspond to universes with sizes at least equal to α and at most equal to β:\n∀α, β ∈ N s.t. α ≤ β Ωα:β :=\nβ ⊔\nυ=α\nΩυ (5.1)\n19 Note that ‘⊔’ denotes the disjoint union operation. 20 While Ωυ contains CDVs that exactly describe υ objects, for all distinct υ\n′ and υ′′, Ωυ′∩Ωυ′′=∅. 21 α is at least equal to the number objects mentioned by background knowledge or evidence. β can be arbitrarily large but for simplicity, we assume that it is finite. To see what would be needed if we wanted to allow β → ∞, refer to Footnote 23.\nRelative to the sample space Ωα:β and for all υ ∈ [α, β], the subset Ωυ represents the event that “the size of the universe is υ”. We let:\nωρ := {o ∈ Ωα:β : o |= ρ} =\nβ ⊔\nυ=α\n{o ∈ Ωυ : o |= ρ} =\nβ ⊔\nυ=α\nωΩυρ =\nβ ⊔\nυ=α\n(ωΩυρ ∩ Ωυ)\nωρ corresponds to the event that regardless of the size of the universe, proposition ρ holds.22 We refer to ωρ as the generalized correspondent of ρ. While ω Ωυ ρ ⊆ Ωυ and for all υ′ 6= υ′′, Ωυ′ ∩ Ωυ′′ = ∅, the above relation entails:\nωΩυρ = ωρ ∩ Ωυ (5.2)\nLikewise, the event that represents “exactly k objects of a universe of unknown size (but bounded by α and β) are F” is defined as follows:\nωExact(k,F ) :=\nβ ⊔\nυ=α\nωΩυ Exact(k,Uυ,F ) =\nβ ⊔\nυ=α\n(ωΩυ Exact(k,Uυ,F ) ∩ Ωυ)\nWe refer to ωExact(k,F ) as the generalized correspondent of Exact(k, Uα, F ) to Exact(k, Uβ, F ).\nOver the sample space Ωα:β, we let Pr denote the probability measure that explains the rational agent ’s a priori degrees of beliefs in the events. As mentioned in footnote (1), the existence of such a measure is deduced from the rationality axioms23 [Sav54]. Note that:\n∀ω ⊆ Ωα:β , ∀υ ∈ N s.t. α ≤ υ ≤ β Pr(ω |Ωυ) = prυ(ω) (5.3)\nThe reason is that by definition, both sides of the above equation represent the probability measure chosen by the rational agent, when the size of the universe is known to be i.24\nLet E := Fa.Ga where a ∈ Uα (consequently for all υ ≥ α, a ∈ Uυ), and ωD be either the generalized correspondent of Exact(k, Uα, F ) to Exact(k, Uβ, F ) or the generalized correspondent of any D ∈ ∆α. Roughly speaking, this means that ωD is the generalized correspondent of any piece of background knowledge which is discussed in the previous sections. Based on these notations, relation (5.4) represents a formal version of Assumption (2) (i.e. the assumption that the estimated size of the universe is not affected by evidence E):\n(Assumption) ∀υ ∈ [α, β] s.t. Pr(Ωυ) > 0 Pr(Ωυ |ωE ∩ ωD) = Pr(Ωυ |ωD) (5.4)\n22 More specifically, ωρ represents: “N=α and ρ w.r.t. Uα” or “N=α+1 and ρ w.r.t. Uα+1” or . . . or “N=β and ρ w.r.t. Uβ”.\n23 For the case where Ωα:β is finite, Savages axioms are sufficient. If we wanted to study the case where it is infinite (i.e. no upper bound exists: β → ∞), we needed to add the monotone continuity assumption [Arr70] to the rationality axioms to guarantee countable additivity.\n24 Evidently, if Pr(Ωυ) = 0, both Pr(ω |Ωυ) and prυ(ω) are undefined.\nHaving this setting, it is easy to show that all (in)equalities of the previous sections hold for the case where the exact number of objects in the universe is unknown. As an example, consider the relation (5.5). This is a typical inequality that informally speaking states that “for any possible universe-size υ, if it is known that the size of the universe is equal to υ, relative to background knowledge D, the evidence E confirms the hypothesis H .” (Note that by the notation that we were using in the previous sections, this equation would be represented by pr(H|E.D) > pr(H|D) where the size of the universe was not mentioned explicitly.)\n∀υ ∈ N s.t. α ≤ υ ≤ β, Pr(Ωυ) > 0 prυ(ω Ωυ H |ω Ωυ E ∩ ω Ωυ D ) > prυ(ω Ωυ H |ω Ωυ D ) (5.5)\nProposition 5.1 proves that the mentioned typical inequality entails that: Pr(ωH |ωE ∩ ωD) > Pr(ωH |ωD), which informally speaking asserts that relative to a universe of an unknown size (that is bounded by α and β), in the presence of background knowledge D, the evidence E confirms H .\nProposition 5.1. Let the size of the universe be unknown but known to be bounded by α and β. Let Pr be a probability measure that corresponds to the degrees of beliefs of a rational agent in the events of the sample space Ωα:β , defined by relation (5.1). If Pr complies with relation (5.4), then relation (5.5) entails:\nPr(ωH |ωE ∩ ωD) > Pr(ωH |ωD)\nProof. Let S := {s ∈ N : α ≤ s ≤ β, Pr(Ωs) > 0} be the set of all possible sizes of the universe.\n∀υ ∈ S prυ(ω Ωυ H |ω Ωυ E ∩ ω Ωυ D ) > prυ(ω Ωυ H |ω Ωυ D ), by (5.5) and definition of S ⇒ ∀υ ∈ S Pr(ωΩυH |ω Ωυ E ∩ ω Ωυ D ∩ Ωυ) > Pr(ω Ωυ H |ω Ωυ D ∩ Ωυ), by relation (5.3) ⇒ ∀υ ∈ S Pr ( ωH∩Ωυ | (ωE∩Ωυ)∩(ωD∩Ωυ)∩Ωυ ) > Pr ( ωH∩Ωυ | (ωD∩Ωυ)∩Ωυ )\n, by relation (5.2)\n⇒ ∀υ ∈ S Pr(ωH |ωE ∩ ωD ∩ Ωυ) > Pr(ωH |ωD ∩ Ωυ), by simplification\n⇒ ∀υ ∈ S Pr(Ωυ |ωE∩ωD)·Pr(ωH |ωE∩ωD∩Ωυ) > Pr(Ωυ |ωD)·Pr(ωH |ωD∩Ωυ), by multiplying the r.h.s. and l.h.s. by the r.h.s. and l.h.s. of relation (5.4)\n⇒ ∑\nυ∈S\nPr(Ωυ |ωE∩ωD) ·Pr(ωH |ωE∩ωD∩Ωυ) > ∑\nυ∈S\nPr(Ωυ |ωD) ·Pr(ωH |ωD∩Ωυ)\nThe above inequality is equivalent to Pr(ωH |ωE ∩ ωD) > Pr(ωH |ωD) (where Ωυ is marginalized out), which is the intended result."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We argued that from a Bayesian perspective, (a) objective background information (from previous observations) and subjective prior information (i.e. prior degrees of\nbeliefs) can produce the same effects and in this sense, are convertible; therefore, in this context, “the state of perfect ignorance” should be interpreted as “the state of possessing no objective information and no subjective biased beliefs”. On the other hand, if we assume that the uniform probability measure corresponds to unbiased subjective degrees of beliefs (as it is often assumed), then we should conclude that: (b) with unbiased subjective beliefs, inductive reasoning is impossible. Therefore, based on (a) and (b), induction in a condition of perfect ignorance is impossible.\nIn addition, by examples we have shown that relative to unrestricted objective/subjective prior information, common rules of induction do not always hold. We concluded that rules of induction should be considered plausible, if they hold relative to a large class of plausible (objective) background knowledge (i.e. knowledge similar to our actual background knowledge) and plausible probability measures (i.e. measures with reasonable characteristics such as complying with more intuitive rules of induction). Subsequently, we scrutinized the plausibility of NC by fixing the background knowledge and studying the characteristics of measures that do or do not comply with it.\nIn the first setting, the background knowledge is composed of complete descriptions of several objects. It is shown that in this setting, validity of NC is implied by the answer to a simpler question that does not seem to have a general intuitive answer i.e. for distinct objects a and b, whether E = Fa.Ga confirms Fb.¬Gb or not. While due to the chosen probability measure and characteristics of F and G, the latter condition does not hold in general, we concluded that in this setting NC is not significantly more reasonable than ¬NC.\nIn the second setting, the number of objects satisfying a particular predicate is known by background knowledge. It is shown that in this case, NC may contradict PJ while seemingly, intuition follows the latter. In summary:\n1. There are reasonable (i.e. not implausible a priori) probability measures for which NC does not hold;\n2. There are reasonable probability measures for which weak projectability, i.e. one of the simplest forms of inductive inference, opposes NC;\n3. In the case of contradiction, intuition “seems to” follow projectability rather than NC;\nHence, we conclude that we have gathered some evidence against the assumption that NC is a generally reliable rule of induction. If NC is not considered plausible then the raven paradox is also dispelled since even if NC holds for two predicates F and G, it does not mean that it should hold for ¬G and ¬F . This asymmetry may be due to possible asymmetry in background knowledge and/or priors.\nOf the three mentioned points, point (1) had already been demonstrated [Mah04]. The distinction is that we have dealt with NC without relying on any particular a priori measure. As mentioned throughout the paper, in the Bayesian framework one’s choice of prior probability distribution is subjective. Nevertheless, Bayesian\nagents’ beliefs at any time are (heavily) dependent on their prior distributions. Therefore, different subjective choices may lead to contradictory results. In the case of variations of Carnap’s measure, this freedom of choice also resurfaces in the form of one or several arbitrary parameters. [Mah99] and [Mah04] are two variations of Carnap’s measure but for the former, NC always holds and for the latter, for some parameter configurations, it does not. Although one might claim that the existence of at least one measure that does not comply with NC is sufficient to discredit NC, one should note that any rule of induction can be violated by some measures and remains valid for some others. In fact one might contrarily claim that contradiction of a measure with an “intuitive” rule of induction should discredit that particular measure rather than the rule. Therefore, our sufficient condition for (¬NC) that covers a class of measures rather than a specific one, should be considered a more general and interesting relation.\nThe main contribution of our work is the conjunction of points (2) and (3). We believe that compared to the raven paradox’s PC and other counterintuitive consequences of NC, the conflict between NC and PJ is more important. To our knowledge, other counterintuitive consequences (such as The red herring [Goo67] and Good’s baby [Goo68] problems) are either related to arguably implausible background configurations, or as is the case with PC, some claim that they are not counterintuitive. PJ is a very simple form of inductive inference. It is more directly justified by our intuitive notion of inductive reasoning than NC, and it’s plausibility cannot be challenged as easily.\nWe also proposed a conjecture that NC seems to be plausible because it can be mistaken for reasoning by analogy (RA) which has a closely related informal representation. We proved that in the case where the exact number of objects satisfying one property is known, RA is compatible with both PJ and NC. It is a conservative condition that intuitively seems plausible and does not suffer from the shortcomings of NC (such as contradicting PJ or producing counterintuitive conclusions such as PC) and is directly justified by the principle of the uniformity of nature."
    }, {
      "heading" : "7 Proof of Theorems",
      "text" : ""
    }, {
      "heading" : "7.1 Proof of Theorems 3.1 and 3.2",
      "text" : "Lemma 1. In relation (2.3), if D ∈ δ, NC is equivalent to Ξ1 · Ξ2 > 1 where: U\\ID := {b1, . . . bn, a} is “the set of objects not described by D”, Ξ1 := pr(F→Gb1:bn |FGa.D)\npr(F→Gb1:bn |D) and Ξ2 := 1 pr(F→Ga|F→Gb1:bn .D) .\nProof. Objects are either completely described by D ∈ δ or are not described at all.\nTherefore:\npr(F→G1:N |D) = pr(F→Gb1:bn .F→Ga|D) (7.1)\npr(F→G1:N |FGa.D) = pr(F→Gb1:bn |FGa.D) (7.2)\nCombination of (7.1), (7.2) and (2.3) proves the lemma.25\nProof of Theorem 3.1. Due to pr(FḠb|·) = 1−pr(F→Gb|·), relation (3.1) is equal to:\n∀B ∈ ∆, ∀a, b /∈ IB pr(F→Gb|FGa.B) ≥ pr(F→Gb|B) (7.3)\nLet arbitrary D ∈ δ and ID = U\\{b1, . . . , bn, a} (i.e. b1 to bn are the objects not mentioned by background knowledge or evidence). According to the definitions of δ and ∆, for all i < n: F→Gb1. . . F→Gbi .D ∈ ∆ (because it is consistent) and a, bi+1 /∈ IF→Gb1...F→Gbi .D. Therefore, from (7.3) it follows that:\n∀i < n pr(F→Gbi+1 |F→Gb1:bi.FGa.D) ≥ pr(F→Gbi+1|F→Gb1:bi .D)\n=⇒ n−1 ∏\ni=0\npr(F→Gbi+1 |F→Gb1:bi.FGa.D) ≥ n−1 ∏\ni=0\npr(F→Gbi+1 |F→Gb1:bi.D) (7.4)\nDue to the chain rule, the r.h.s. of the above equation is equal to pr(F→Gb1:bn |D):\nn−1 ∏\ni=0\npr(F→Gbi+1|F→Gb1:bi.D) =\npr(F→Gb1|D)·pr(F→Gb2|F→Gb1.D) . . . pr(F→Gbn|F→Gb1. . . F→Gbn−1.D) = pr(F→Gb1:bn |D)\nSimilarly, the l.h.s. of (7.4) is equal to pr(F→Gb1:bn |FGa.D). Therefore:\npr(F→Gb1:bn|FGa.D) ≥ pr(F→Gb1:bn|D)\nThis entails Ξ1 ≥ 1 and since Ξ2 > 1, according to Lemma 1, NC holds.\nProof of Theorem 3.2. Similar to the method used in the proof of Theorem 3.1 and by using inequality (3.2) instead of inequality (3.1) it can be proved that:\n∀i < n pr(F→Gbi+1|F→Gb1:bi. FGa. D) < pr(F→Gbi+1|F→Gb1:bi. D)\nConsequently:\nΞ1 = ∏n−1 i=1 pr(F→Gbi+1 |F→Gb1:bi.FGa.D) ∏n−1 i=1 pr(F→Gbi+1 |F→Gb1:bi.D) · pr(F→Gb1 |FGa.D) pr(F→Gb1 |D) < pr(F→Gb1 |FGa.D)\npr(F→Gb1 |D) (7.5)\n25Note that Ξ1 indicates the effect of the observation Fa.Ga, on the probability that unobserved individuals satisfy F→G and Ξ2 corresponds to the effect of elimination of the possibility that the observed object is a counterexample to the generalization.\nFor conciseness, let p := pr(F→Gb1 |FGa.D) and q := pr(F→Gb1 |D).\npr(¬F→Ga|F→Gb1: bn .D) < (1− p)− (1− q), since F→G = ¬FḠ and by inequality (3.3)\n=⇒ pr(F→Ga|F→Gb1: bn .D) > 1− q + p (7.6)\nFinally,\n1− p > 1− q =⇒ p < q, since D ∈ ∆ and by (3.2)\n=⇒ p · (1− q) < q · (1− q) =⇒ p < q · (1− q + p), since q < 1 =⇒ p\nq < 1− q + p since q > 0\n=⇒ Ξ1 < pr(F→Ga|F→Gb1:bn .D), by (7.5) and (7.6) =⇒ Ξ1 · Ξ2 < 1 =⇒ NC does not hold. by Lemma 1"
    }, {
      "heading" : "7.2 Proof of Theorem 4.1",
      "text" : "Lemma 2. Under PJ, for any set of objects {1, 2, . . ., n} ⊆ U , a ∈ U and any D ∈ ∆ that does not determine the value of ψa:\n(Group PJ) pr(ψ1:n|ψa.D) ≥ pr(ψ1:n|D) (7.7)\n(Negative Group PJ) pr(ψ1:n| ¬ψa.D) ≤ pr(ψ1:n|D) (7.8)\nProof. The proof is based on mathematical induction. 1. Proof of relation (7.7): (I) For n = 1, relation (7.7) is equivalent to relation (2.1) and therefore valid. (II) Assume for n = k relation (7.7) holds. We prove that for n = k + 1, it holds as well:\npr(ψ1:k+1|ψa.D) = pr(ψ1:k.ψk+1|ψa.D)\n= pr(ψk+1|ψa.D) · pr(ψ1:k|ψa.ψk+1.D)\n≥ pr(ψk+1|D) · pr(ψ1:k|ψa.ψk+1.D), by inequality (2.1)\n≥ pr(ψk+1|D) · pr(ψ1:k|ψk+1.D), (2.1) applied to (ψk+1.D) ∈ ∆\n= pr(ψ1:k+1|D)\nBy (I) & (II), mathematical induction implies (7.7) for all n. 2. Proof of relation (7.8): Under PJ: ∀b pr(¬ψb| ¬ψa.D) ≥ pr(¬ψb|D), therefore:\n1− pr(ψb| ¬ψa.D) ≥ 1− pr(ψb|D) =⇒ pr(ψb| ¬ψa.D) ≤ pr(ψb|D) (7.9)\nSimilar to the previous case and by using (7.9) instead of (2.1), inequality (7.8) can be proved easily.\nLemma 3. For all 1 ≤ k ≤ N , D := F1:k.(¬F )k+1:N and E being an arbitrary proposition: pr(H|E .D) = pr(G1:k|E .D).\nProof. l.h.s. = pr ( F→G1:k.F→Gk+1:N |E . F1:k.(¬F )k+1:N ) = pr ( F→G1:k.F1:k.F→Gk+1:N .(¬F )k+1:N |E . F1:k.(¬F )k+1:N ) = pr ( (F→G.F )1:k.(F→G.¬F )k+1:N |E . F1:k.(¬F )k+1:N ) = pr ( (F.G)1:k.(¬F )k+1:N |E . F1:k.(¬F )k+1:N )\n(since F→Gb.Fb ≡ Fb.Gb and F→Gb.¬Fb ≡ ¬Fb) = pr ( F1:k.G1:k.(¬F )k+1:N |E . F1:k.(¬F )k+1:N ) = pr ( G1:k|E . F1:k.(¬F )k+1:N ) = r.h.s.\nProof of Theorem 4.1. In the following relations, D := F1:k.(¬F )k+1:N . (I) Proof of relation (4.1), i.e. pr(H |Gk . D) > pr(H|D) by PJ:\npr(H|Gk . D) = pr(G1:k|Gk . D), by Lemma 3\n= pr(G1:k−1|Gk . D) ≥ pr(G1:k−1|D), by Lemma 2\n> pr(G1:k|D), adding Gk & Cournot’s pp. (note: D 0 Gk)\n= pr(H|D), by Lemma 3.\n(II) Proof of relation (4.2), i.e. pr(H |GN . D) ≥ pr(H|D) by PJ:\npr(H|GN . D) = pr(G1:k|GN . D), by Lemma 3\n≥ pr(G1:k|D), by relation (7.7) (Group PJ)\n= pr(H|D), by Lemma 3.\n(III) Proof of relation (4.3) i.e. pr(H | ¬GN . D) ≤ pr(H|D) by PJ:\npr(H|¬GN . D) = pr(G1:k|¬GN . D), by Lemma 3\n≤ pr(G1:k|D), by relation (7.8) (Neg.GroupPJ)\n= pr(H|D), by Lemma 3.\n(IV) Proof of relation (4.4) , i.e. pr(H |Gk . D) > pr(H|D) by RA:\npr(H|Gk . D) = pr(G1:k−1|Gk . D), by Lemma 3\n=\nk−1 ∏\ni=1\npr(Gi|G1:i−1.Gk.D), by the chain rule\n=\nk−1 ∏\ni=1\npr(Gi|Fi.Fk.Gk.G1:i−1.D), since D ⊢ Fi.Fk\n>\nk−1 ∏\ni=1\npr(Gi|Fi.G1:i−1.D), by RA (and since G1:i−1.D ∈ ∆)\n= k−1 ∏\ni=1\npr(Gi|G1:i−1.D), since for i ∈ {1, . . . , k} : D ⊢ Fi\n= pr(G1:k−1|D), by the chain rule\n> pr(G1:k|D), by adding Gk & Cournot’s pp. (note: D 0 Gk)\n= pr(H|D), by Lemma 3."
    }, {
      "heading" : "7.3 Proof of Theorem 4.2",
      "text" : "Definition. Having a set C ⊆ U and π being a permutation (of U), Cπ is defined as: Cπ := {π(b) : b ∈ C}. It is obvious that Uπ = U .\nThe following two relations directly follows from the assumption that π is a bijection:\n∀C ⊆ U |C| = |Cπ| (7.10) ∀C,C ′ ⊆ U C 6= C ′ =⇒ Cπ 6= C ′π (7.11)\nLemma 4. H = Hπ where π is an arbitrary permutation (of U).\nProof. Hπ = ∧\nb∈U\nF→Gπ(b) = ∧\nπ(b)∈Uπ\nF→Gπ(b) = ∧\nb′∈Uπ\nF→Gb′ = ∧\nb′∈U\nF→Gb′ = H\nThe first equality holds by definition. The second equality holds because: b ∈ U ⇐⇒ π(b) ∈ Uπ. In the r.h.s. of the third equality, π(b) is renamed to b′. The fourth equality holds because U = Uπ (note that π is a permutation in U). The last equality is the definition of H .\nLemma 5. For all 1 ≤ k ≤ N , all permutations π and CU,k being defined as the set of all subsets of U which have size k:\nC π U,k := {C π : C ∈ CU,k} = CU,k (7.12)\nProof. Equality (7.10) implies, ∀Cπ ∈ CπU,k |C π| = |C| = k, which means: Cπ is a k-combination from U ; So from the definition of CU,k it follows that C π ∈ CU,k. Therefore, CπU,k ⊆ CU,k. Conversely, according to relation (7.11) and the definition of CπU,k in (7.12), there is a one-to-one relation between the members of CU,k and C π U,k. This entails: |C π U,k| = |CU,k|. Thus: C π U,k = CU,k.\nLemma 6. For all permutations π (of the set U), and for all integers k ∈ [1, N ]: Exact(k, U, F ) = Exactπ(k, U, F )\nProof. By definition, Exactπ(k, U, F ) := ∨\nC∈CU,k\n( ∧\na∈C\nFπ(a) . ∧\nb6∈C\n¬Fπ(b))\n= ∨\nC∈CU,k\n( ∧\nπ(a)∈Cπ\nFπ(a) . ∧\nπ(b)6∈Cπ\n¬Fπ(b)), since c ∈ C ⇐⇒ π(c) ∈ C π\n= ∨\nCπ∈Cπ U,k\n( ∧\nπ(a)∈Cπ\nFπ(a) . ∧\nπ(b)6∈Cπ\n¬Fπ(b)), since C ∈ CU,k ⇐⇒ C π ∈ CπU,k\n= ∨\nC′∈Cπ U,k\n( ∧\na′∈C′\nFa′ . ∧\nb′ 6∈C′\n¬Fb′), renaming C π to C ′ and π(c) to c′\n= ∨\nC′∈CU,k\n( ∧\na′∈C′\nFa′ . ∧\nb′ 6∈C′\n¬Fb′), since C π U,k = CU,k, (Lemma 5)\n= Exact(k, U, F ), by definition.\nDefinition. For each C ⊆ U , the proposition Z(C,F ) is defined as:\nZ(C,F ) := ∧\na∈C\nFa . ∧\nb6∈C\n¬Fb (7.13)\nwhich allows to write definition (4.5) as: Exact(k, U, F ) := ∨\nC∈CU,k Z(C,F )\nLemma 7. For arbitrary propositions A and B and 1 ≤ k ≤ N :\npr(Exact(k, U, F ).A|B) = ∑\nC∈CU,k\npr(Z(C,F ).A|B) (7.14)\nProof. ∀C ′ 6= C ′′ ∈ CU,k:\n∃b ∈ U b ∈ C ′ and b 6∈ C ′′, C ′ &C ′′ being distinct with same size\n=⇒ Z(C′,F ) |= Fb and Z(C′′,F ) |= ¬Fb, by definition (7.13)\n=⇒ Z(C′,F ).Z(C′′,F ) |= Fb .¬Fb ≡ ⊥\nTherefore, the sequence {Z(C,F )}C∈CU,k consists of mutually disjoint events. Hence, by the third Kolmogorov probability axiom, for all propositions A and B:\npr(Exact(k, U, F ).A|B) = pr (\n∨\nC∈CU,k\n(Z(C,F ).A)|B ) = ∑\nC∈CU,k\npr(Z(C,F ).A|B) (7.15)\nLemma 8. Let 1 ≤ k ≤ N . For all C ∈ CU,k and for all permutations π:\nZπ(C,F ) = Z(Cπ ,F )\nProof.\nZπ(C,F ) = ∧\na∈C\nFπ(a) . ∧\nb6∈C\n¬Fπ(b) by relation (7.13)\n= ∧\nπ(a)∈Cπ\nFπ(a) . ∧\nπ(b)6∈Cπ\n¬Fπ(b) by def. c ∈ C ⇐⇒ π(c) ∈ C π\n= ∧\na′∈Cπ\nFa′ . ∧\nb′ 6∈Cπ\n¬Fb′ renaming π(a) to a ′ and π(b) to b′\n= Z(Cπ ,F ) by relation (7.13).\nProof of Theorem 4.2. For the sake of conciseness, here we only prove relation (4.7) i.e.:\n∀a ∈ U, 1 ≤ k ≤ N pr ( H|Exact(k, U, F ) .Fa.Ga ) = pr ( H|F1:k.¬Fk+1:N .Gk )\nThe remaining relations (4.6, 4.8 & 4.9) can be proved using the same method and by small (and obvious) appropriate modifications.\nFor the sake of simplicity, we first swap the names of the objects a and 1 as follows: Let π′ := {a/1; 1/a}. By the exchangeability assumption:\npr ( H|Exact(k, U, F ) .Fa.Ga ) = pr ( Hπ ′ |Exactπ ′ (k, U, F ) .Fπ′(a).Gπ′(a) )\n= pr ( H|Exact(k, U, F ) .F1.G1 ) , by Lemmas 4 and 6\n= pr(H) · pr\n( Exact(k, U, F ). F1.G1|H )\npr ( Exact(k, U, F ) .F1.G1 ) , by Bayes rule. (7.16)\nNow:\npr ( Exact(k, U, F ) .F1.G1|H ) = ∑\nC∈CU,k\npr(Z(C,F ).F1.G1|H), by Lemma 7\n= ∑\nC∈CU,k s.t. 1∈C\npr(Z(C,F ).F1.G1|H) + ∑\nC∈CU,k s.t. 16∈C\npr(Z(C,F ).F1.G1|H)\n= ∑\nC∈CU,k s.t. 1∈C\npr(Z(C,F ).F1.G1|H) (7.17)\nThe second summation is eliminated because in the case of any C ∈ CU,k such that 1 6∈ C, Z(C,F ) ¬F1 (see definition (7.13)), therefore in this case, pr(Z(C,F ).F1.G1|H) = pr(Z(C,F ).¬F1.F1.G1|H) = pr(⊥|H) = 0.\nDue to the exchangeability assumption, the members of the first summation are all equal. The reasoning is as follows: In the case of any C ∈ CU,k such that 1 ∈ C, there exists some permutation that map each of the members of C to the set {1, 2, . . . k} with 1 as a fixed point i.e.:\n∀C ∈ CU,k s.t. 1 ∈ C : ∃π C π = {1, 2, . . . , k}, π(1) = 1 (7.18)\nIn fact, for any member of CU,k, exactly (k − 1)! permutations with such properties exist. Using such permutations, ∀C ∈ CU,k s.t. 1 ∈ C:\n∃π pr(Z(C,F ).F1.G1|H) = pr(Z π (C,F ).Fπ(1).Gπ(1)|H π), by exchangeability\n= pr(Z({1,...,k},F ).Fπ(1).Gπ(1)|H π), using (7.18) in Lemma 8 = pr(Z({1,...,k},F ).F1.G1|H π), 1 being a fix point\n= pr(Z({1,...,k},F ).F1.G1|H), by Lemma 4\n= pr(F1:k.¬Fk+1:N .G1|H), expanding Z(.,.) by def. (7.13) (7.19)\nThus, combining (7.17) and (7.19):\npr ( Exact(k, U, F ).F1.G1|H ) = ∑\nC∈CU,k s.t. 1∈C\npr(Z(C,F ).F1.G1|H)\n=\n(\nN − 1\nk − 1\n)\npr(F1:k.¬Fk+1:N .G1|H) (7.20)\nBy a similar justification:\npr ( Exact(k, U, F ).F1.G1 ) =\n(\nN − 1\nk − 1\n)\npr(F1:k.¬Fk+1:N .G1) (7.21)\nCombining equations (7.20), (7.21) and (7.16):\npr ( Exact(k, U, F ) .F1.G1|H ) = pr(H) · pr\n( Exact(k, U, F ) .F1.G1|H )\npr ( Exact(k, U, F ) .F1.G1 )\n=\n(\nN−1 k−1\n)\npr(H) pr(F1:k¬Fk+1:N .G1|H) (\nN−1 k−1\n) pr(F1:k¬Fk+1:N .G1) = pr(H|F1:k¬Fk+1:N .G1)"
    }, {
      "heading" : "A List of Notation",
      "text" : "Symbol Explanation\nU = {1, 2, . . . , υ} universe of arbitrary size N\n1, 2, . . . , N objects of universe U (short form) υ, υ′, υ′′ ∈ N symbols used to denote the size of universe\nUυ = {1, 2, . . . , υ} universe of size υ α ≤ β lower and higher bounds for the size of the universe\na, b, c and b1, b2, . . . typical objects (or individuals) (not necessarily consecutive) ψ typical 1-place predicate\nψb a proposition assigning predicate ψ to object b\nψbi:bj ψbi .ψbi+1 . . . ψbj F,G atomic 1-place predicates\nFb, Gb propositions assigning F and G to object b, respectively F̄Ḡb, F̄Gb, FḠb, FGb complete descriptions (of object b) F→Gb Fb → Gb ≡ ¬Fb ∨Gb ≡ ¬FḠb H General hypothesis: ∀b ∈ U F→Gb E evidence: Fa.Ga B,D typical (objective) background knowledge\nρ a typical proposition\nIρ set of all individuals described by ρ\nAρ set of all (simple) predicates involved in ρ\n⊤ tautologous proposition\n∆ set of all propositions in form of relation (1.3) (on universe U)\n∆υ set of all propositions in form of relation (1.3) (on universe Uυ) δ set of all complete descriptions that do not falsify H\nCDV complete description vector\nΩ set of all CDVs (w.r.t universe U)\nΩυ set of all CDVs (w.r.t. universe Uυ) Ωα:β union of Ωα to Ωβ ωΩυρ ⊆ Ωυ an event that corresponds proposition ρ (w.r.t. sample space Ωυ) ωρ ⊆ Ωα:β an event that corresponds proposition ρ (w.r.t. sample space Ωα:β) pr probability (over sample space Ω)\nprυ probability (over sample space Ωυ) Pr probability (over sample space Ωα:β) π, π′, π′′ typical permutations (i.e. bijections) in U\nπ(b) an object that b ∈ U is mapped to by bijection π ρπ a proposition obtained from ρ by replacing any b ∈ Iρ with π(b) C,C ′, C ′′ ⊆ U typical subsets of U Cπ {π(b) : b ∈ C}\nCU,k set of all subsets of U with cardinality k C π U,k {C π : C ∈ CU,k} Z(C,F ) ∧ a∈CFa . ∧ b6∈C¬Fb Exact(k, U, F ) a proposition representing: “exactly k members of U are F”"
    } ],
    "references" : [ {
      "title" : "Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches",
      "author" : [ "A. Aamodt", "E. Plaza" ],
      "venue" : "Artificial Intelligence Communications",
      "citeRegEx" : "Aamodt and Plaza,? \\Q1994\\E",
      "shortCiteRegEx" : "Aamodt and Plaza",
      "year" : 1994
    }, {
      "title" : "Essays in the Theory of Risk-Bearing, North-Holland",
      "author" : [ "K. Arrow" ],
      "venue" : null,
      "citeRegEx" : "Arrow,? \\Q1970\\E",
      "shortCiteRegEx" : "Arrow",
      "year" : 1970
    }, {
      "title" : "R",
      "author" : [ "Carnap" ],
      "venue" : "(1950). Logical Foundations of Probability, Chicago: Chicago University Press. 2nd Ed.",
      "citeRegEx" : "Car50",
      "shortCiteRegEx" : null,
      "year" : 1962
    }, {
      "title" : "A Basic System of Inductive Logic, Part II",
      "author" : [ "R. Carnap" ],
      "venue" : "Studies in Inductive Logic and Probability,",
      "citeRegEx" : "Carnap,? \\Q1980\\E",
      "shortCiteRegEx" : "Carnap",
      "year" : 1980
    }, {
      "title" : "How Bayesian Confirmation Theory Handles the Paradox of the Ravens",
      "author" : [ "B. Fitelson", "J. Hawthorne" ],
      "venue" : "(In E. Eells & J. Fetzer (Eds.) Probability in Science). http://fitelson.org/research.htm",
      "citeRegEx" : "Fitelson and Hawthorne,? \\Q2006\\E",
      "shortCiteRegEx" : "Fitelson and Hawthorne",
      "year" : 2006
    }, {
      "title" : "The Wason Task(s) and the Paradox of Confirmation",
      "author" : [ "B. Fitelson", "J. Hawthorne" ],
      "venue" : "Philosophical Perspectives",
      "citeRegEx" : "Fitelson and Hawthorne,? \\Q2010\\E",
      "shortCiteRegEx" : "Fitelson and Hawthorne",
      "year" : 2010
    }, {
      "title" : "The White Shoe is a Red Herring",
      "author" : [ "J. Good I" ],
      "venue" : "British Journal for the Philosophy of Science,",
      "citeRegEx" : "I.,? \\Q1967\\E",
      "shortCiteRegEx" : "I.",
      "year" : 1967
    }, {
      "title" : "The White Shoe qua Red Herring is Pink",
      "author" : [ "I.J. Good" ],
      "venue" : "British Journal for the Philosophy of Science",
      "citeRegEx" : "Good,? \\Q1968\\E",
      "shortCiteRegEx" : "Good",
      "year" : 1968
    }, {
      "title" : "Probabilities over rich languages, testing and randomness",
      "author" : [ "H. Gaifman", "M. Snir" ],
      "venue" : "Journal of Symbolic Logic,",
      "citeRegEx" : "Gaifman and Snir,? \\Q1982\\E",
      "shortCiteRegEx" : "Gaifman and Snir",
      "year" : 1982
    }, {
      "title" : "Studies in the Logic of Confirmation",
      "author" : [ "C.G. Hempel" ],
      "venue" : "Mind 54:",
      "citeRegEx" : "Hempel,? \\Q1945\\E",
      "shortCiteRegEx" : "Hempel",
      "year" : 1945
    }, {
      "title" : "The White Shoe - No Red Herring",
      "author" : [ "C.G. Hempel" ],
      "venue" : "British Journal for the Philosophy of Science,",
      "citeRegEx" : "Hempel,? \\Q1967\\E",
      "shortCiteRegEx" : "Hempel",
      "year" : 1967
    }, {
      "title" : "Probabilities on Sentences in an Expressive Logic",
      "author" : [ "Hutter M", "J.W. Lloyd", "K.S. Ng", "W.T.B. Uther" ],
      "venue" : "Journal of Applied Logic",
      "citeRegEx" : "M. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "M. et al\\.",
      "year" : 2013
    }, {
      "title" : "On universal prediction and Bayesian confirmation",
      "author" : [ "M. Hutter" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Hutter,? \\Q2007\\E",
      "shortCiteRegEx" : "Hutter",
      "year" : 2007
    }, {
      "title" : "Probability theory: The logic of science",
      "author" : [ "E.T. Jaynes" ],
      "venue" : null,
      "citeRegEx" : "Jaynes,? \\Q2003\\E",
      "shortCiteRegEx" : "Jaynes",
      "year" : 2003
    }, {
      "title" : "Inductive Logic and the Ravens Paradox",
      "author" : [ "P. Maher" ],
      "venue" : "Philosophy of Science",
      "citeRegEx" : "Maher,? \\Q1999\\E",
      "shortCiteRegEx" : "Maher",
      "year" : 1999
    }, {
      "title" : "Probability Captures the Logic of Scientific Confirmation",
      "author" : [ "P. Maher" ],
      "venue" : "(In C. Hitchcock (Ed.) Contemporary Debates in the Philosophy of Science,",
      "citeRegEx" : "Maher,? \\Q2004\\E",
      "shortCiteRegEx" : "Maher",
      "year" : 2004
    }, {
      "title" : "K",
      "author" : [ "Popper" ],
      "venue" : "(1959). The Logic of Scientific Discovery. London: Hutchinson. (1st German Ed., Logik der Forschung,",
      "citeRegEx" : "Pop59",
      "shortCiteRegEx" : null,
      "year" : 1935
    }, {
      "title" : "Equality and domain closure in first-order databases",
      "author" : [ "R. Reiter" ],
      "venue" : null,
      "citeRegEx" : "Reiter,? \\Q1980\\E",
      "shortCiteRegEx" : "Reiter",
      "year" : 1980
    }, {
      "title" : "A Philosophical Treatise of Universal Induction, Enthropy",
      "author" : [ "S. Rathmanner", "Hutter M" ],
      "venue" : null,
      "citeRegEx" : "Rathmanner and M.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rathmanner and M.",
      "year" : 2011
    }, {
      "title" : "Artificial Intelligence: A Modern Approach, (2nd ed.)",
      "author" : [ "S.J. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "Russell and Norvig,? \\Q2003\\E",
      "shortCiteRegEx" : "Russell and Norvig",
      "year" : 2003
    }, {
      "title" : "Selective Confirmation and the Ravens",
      "author" : [ "I. Scheffler", "N.J. Goodman" ],
      "venue" : "Journal of Philosophy",
      "citeRegEx" : "Scheffler and Goodman,? \\Q1972\\E",
      "shortCiteRegEx" : "Scheffler and Goodman",
      "year" : 1972
    }, {
      "title" : "Axioms for rational reinforcement learning Proceedings of 22:nd international conference on algorithmic learning theory, Springer Lecture Notes in Computer Science 6925:338–352",
      "author" : [ "P. Sunehag", "M. Hutter" ],
      "venue" : null,
      "citeRegEx" : "Sunehag and Hutter,? \\Q2011\\E",
      "shortCiteRegEx" : "Sunehag and Hutter",
      "year" : 2011
    }, {
      "title" : "A formal theory of inductive inference: Parts 1 and 2, Information and Control, 7:1–22 and 224–254",
      "author" : [ "R.J. Solomonoff" ],
      "venue" : null,
      "citeRegEx" : "Solomonoff,? \\Q1964\\E",
      "shortCiteRegEx" : "Solomonoff",
      "year" : 1964
    }, {
      "title" : "Hempel’s Raven Paradox: A Lacuna in the Standard Bayesian Solution",
      "author" : [ "P. Vranas" ],
      "venue" : "British Journal for the Philosophy of Science",
      "citeRegEx" : "Vranas,? \\Q2004\\E",
      "shortCiteRegEx" : "Vranas",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "On the other hand, it is well known that using the uniform measure, inductive learning is not possible [Car50].",
      "startOffset" : 103,
      "endOffset" : 110
    }, {
      "referenceID" : 16,
      "context" : "The fundamental assumption behind inductive inference is the so-called principle of the uniformity of nature [Hum88] (or the immutability of natural processes [Pop59]) based on which, uniformity and trend are more probable than diversity and anomaly a priori.",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "Maher has formalized one variation of reasoning by analogy (or inference by analogy [Car50]) as: ∀a, b ∈ U pr(Gb|Fb.",
      "startOffset" : 84,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "Similarly, (as 10 According to [Car50] predictive inference (i.",
      "startOffset" : 31,
      "endOffset" : 38
    }, {
      "referenceID" : 2,
      "context" : "This is not surprising since using this measure, learning is impossible (see [Car50]).",
      "startOffset" : 77,
      "endOffset" : 84
    } ],
    "year" : 2013,
    "abstractText" : "Philosophers writing about the ravens paradox often note that Nicod’s Condition (NC) holds given some set of background information, and fails to hold against others, but rarely go any further. That is, it is usually not explored which background information makes NC true or false. The present paper aims to fill this gap. For us, “(objective) background knowledge” is restricted to information that can be expressed as probability events. Any other configuration is regarded as being subjective and a property of the a priori probability distribution. We study NC in two specific settings. In the first case, a complete description of some individuals is known, e.g. one knows of each of a group of individuals whether they are black and whether they are ravens. In the second case, the number of individuals having a particular property is given, e.g. one knows how many ravens or how many black things there are (in the relevant population). While some of the most famous answers to the paradox are measure-dependent, our discussion is not restricted to any particular probability measure. Our most interesting result is that in the second setting, NC violates a simple kind of inductive inference (namely projectability). Since relative to NC, this latter rule is more closely related to, and more directly justified by our intuitive notion of inductive reasoning, this tension makes a case against the plausibility of NC. In the end, we suggest that the informal representation of NC may seem to be intuitively plausible because it can easily be mistaken for reasoning by analogy.",
    "creator" : "LaTeX with hyperref package"
  }
}