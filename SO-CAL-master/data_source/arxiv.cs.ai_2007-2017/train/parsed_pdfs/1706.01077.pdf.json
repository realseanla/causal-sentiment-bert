{
  "name" : "1706.01077.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Actor-Critic for Linearly-Solvable Continuous MDP with Partially Known Dynamics",
    "authors" : [ "Tomoki Nishi", "Prashant Doshi" ],
    "emails" : [ "nishi@mosk.tytlabs.co.jp", "pdoshi@cs.uga.edu", "michael.james@tri.global", "danil.prokhorov@toyota.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning (RL) offers a way of learning high-quality policies (control) for an agent by exploring its environment. Methods for RL have predominantly focused on domains with discrete state and actions. Those that operate on continuous states or actions resort to sampling or other approximations because of the difficulty in analytically solving the continuous Bellman equation [20]. In this regard, Todorov [16] introduced the linearly-solvable Markov decision process (L-MDP), a subclass of general MDPs, which allows us to quickly solve the continuous Bellman equation exactly under a class of structured dynamics and rewards. Specifically, the Bellman equation in L-MDPs is recast as a linearized differential, and its solution is efficiently obtained as a linear eigenfunction when the whole dynamics model is available [18]. As such, L-MDPs are particularly well suited for modeling robotic learning and planning, where the state and actions spaces are usually continuous.\nIn addition to continuous spaces, high-impact robotic applications such as autonomous vehicles impose an additional constraint on RL. They preclude an exhaustive exploration of the state and action space because it would be unacceptable for an autonomous vehicle to optimistically try maneuvers that would lead to a crash or even a near-miss, leaving much of the state and action spaces unexplored. However, at the same time, guaranteeing safe exploration has a high computational cost that is shown to be NP-hard [9].\nL-MDPs decompose the dynamics model into passive and active (control) dynamics with added actuator noise. In this paper, we present a new method for semi model-free RL for L-MDPs,\nar X\niv :1\n70 6.\n01 07\n7v 1\n[ cs\n.A I]\nwhich uses a partially-known system dynamics model. Specifically, the method requires the control dynamics model, which represents the effect of actions to be specified, but not the passive dynamics model nor the noise in the transitions. Knowing control dynamics is feasible in our motivating context of autonomous driving because advanced driver assistance systems such as adaptive cruise control are already available in most new vehicles, and these utilize a model of the control dynamics. More importantly, knowing such a model makes safe active exploration unnecessary. Furthermore, the correct actuator noise is often difficult to prespecify, which adds to this method’s appeal.\nOur method called passive actor-critic (pAC) finds the policy within the standard two-step architecture of actor-critic methods comprising a policy improvement step (actor) and state evaluation (critic). Passive actor-critic combines the data collected on passive state transitions with the known control dynamics. In the context of L-MDPs, the critic estimates the expected value function using the linearized Bellman equation from the data and the actor improves the policy using the standard Bellman equation on data and the active dynamics model. In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements. The method is evaluated on two known synthetic domains and on our motivating domain of freeway merge by an autonomous car both in simulation and using real-world traffic data. Interestingly, pAC improves on previous model-based methods despite requiring reduced model specifications. Importantly, pAC finds policies that succeed in freeway merge on real data at rates exceeding 90%, motivating transition to real-world testing as future work."
    }, {
      "heading" : "2 Related Work",
      "text" : "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21]. These efficiently optimize control policies by solving the linearized Bellman in discrete- or continuous-state L-MDPs when the system dynamics is fully known. Our method relaxes this requirement using samples of passive dynamics, while knowing the control dynamics. We also introduce multi-layer neural networks for approximating the value functions in L-MDPs in addition to the previously used radial basis functions.\nUchibe and Doya [19] formulate Z-learning based on least-square TD learning for continuous LMDPs. The method optimizes the policy while requiring knowledge of the control dynamics and transition noise. The motion noise inherent in robotic platforms is often unknown due to which this method may not apply to robot learning. pAC learns the noise levels during optimization given sampled data and knowledge of control dynamics by minimizing the error between the value and action-value functions. This makes pAC well positioned for application to robot-based RL.\nAs pAC can learn from data containing samples of passive dynamics, it bears resemblance to batch RL methods [8]. A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics. It searches for actions that minimize the Q-value, which requires that either the action space be discrete or the Q-function has structure such as being quadratic due to computational cost. In contrast, pAC uses policy that is analytically derived from the estimated Z-value, parameter for transition noise, and known control dynamics.\nPath integral control also learns a policy based on linearized Bellman equation [15, 3]. Unlike approaches for L-MDPs, path integral control can directly optimize the policy. However, the approach has to sample sample many trajectories under a training policy from a certain initial state. As we mentioned previously, we seek to avoid such active and potentially unsafe explorations in the real world.\nFinally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14]. These methods optimize a policy with active exploration while our method seeks to find a policy from data on passive state transitions (of the underlying Markov chain) with known control dynamics. On the other hand, the utility of networks to approximate the value function remains the same."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "We briefly review L-MDPs for understanding our method. We focus on a discrete-time system with a real-valued state x ∈ Rn and control input u ∈ Rm, whose stochastic dynamics is defined as follows:\nxk+1 = xk +A(xk)∆t+Buk∆t+ diag (σ) ∆ω, (1) where ∆ω is differential Brownian motion simulated by a Gaussian N (0, I∆t), where I is the identity matrix. A(xk), Buk and σ ∈ Rn denote the passive dynamics, control dynamics due to action, and the transition noise level, respectively (B is an input-gain matrix). ∆t is a step size of time and k denotes a time index. System dynamics structured in this way are quite general: for example, models of many mechanical systems conform to these dynamics.\nL-MDP [17] is a subclass of MDPs [13] defined by a tuple, 〈X ,U ,P,R〉, where X ⊆ Rn and U ⊆ Rm are continuous state and action spaces. P := {p(y|x,u) | x,y ∈ X ,u ∈ U} is a state transition model due to action, which is structured as in Eq. 1, andR := {r(x,u) | x ∈ X ,u ∈ U} is an immediate cost function with respect to state x and action u. A control policy u = π(x) is a function that maps a state x to an action u. The goal is to find a policy that minimizes the following average expected cost: Vavg := limn→∞ 1nE [∑n−1 k=0 r(xk, π(xk)) ] .\nGrondman et al. [4] notes that the Bellman equation for MDPs can be rewritten using the value function V (x) called V-value, state-action value function Q(x,u) called Q-value, and average value Vavg under an policy.\nVavg +Qk = rk + Ep(xk+1|xk,uk)[Vk+1]. (2)\nAs we may expect, Vk = minu∈U Qk. Ep(xk+1|xk)[·] is expectation over a probability distribution of state transition under the passive dynamics. Here and elsewhere, subscript k is values at time step k.\nAn L-MDP defines the cost of an action (control cost) to be the amount of stochastic effect it has on the system, adding it to the state cost:\nr(xk,uk) := q(xk)∆t+KL(p(xk+1|xk)||p(xk+1|xk,uk)). (3)\nHere, q(x) ≥ 0 is the state-cost function; KL(·||·) is the Kullback-Leibler (KL) divergence; p(xk+1|xk) models the passive dynamics while p(xk+1|xk,uk) represents the active or control dynamics of the system. L-MDPs further add a condition on the dynamics as shown below.\np(xk+1|xk) = 0⇒ ∀uk p(xk+1|xk,uk) = 0.\nThis condition ensures that no action introduces new transitions that are not achievable under passive dynamics. The stochastic dynamical system represented by Eq. 1 satisfies this assumption naturally because the dynamic is Gaussian. However, systems that are deterministic under passive dynamics remain so under active dynamics. This condition is easily met in robotic systems where noise is prevalent.\nThe standard Bellman equation for MDPs can then be recast in L-MDPs to be a linearized differential equation for exponentially transformed value function of Eq. 2 (hereafter referred to as the linearized Bellman equation) [18]:\nZavgZk = e −qk∆t Ep(xk+1|xk)[Zk+1], (4)\nwhere Zk := e−Vk and Zavg := e−Vavg . Here, Zk and Zavg are an exponentially transformed value function called Z-value and the average cost under an optimal policy, respectively. Because the passive and control dynamics with the Brownian noise are Gaussian, the KL divergence between these dynamics becomes,\nKL(p(xk+1|xk)||p(xk+1|xk,uk)) = 0.5u>k S−1uk∆t, (5)\nwhere S−1 := B>(diag(σ2i )) −1B and σi denotes i-th element of σ. Then, the optimal control policy for L-MDPs can we derived as,\nπ(xk) = −SB> ∂Vk ∂xk . (6)"
    }, {
      "heading" : "4 Passive Actor-Critic for L-MDP",
      "text" : "We present a novel actor-critic method for continuous L-MDP, which we label as passive actorcritic (pAC). While the actor-critic method usually operates using samples collected actively in the environment [6], pAC finds a converged policy without exploration. Instead, it uses samples of passive state transitions and a known control dynamics model. pAC follows the usual two-step schema of actor-critic: a state evaluation step (critic), and a policy improvement step (actor).\n1. Critic: Estimate the Z-value and the average cost from the linearized Bellman equation using samples under passive dynamics;\n2. Actor: Improve a control policy by optimizing the Bellman equation given the known control dynamics model, and the Z-value and cost from the critic.\nWe provide details on these two components below."
    }, {
      "heading" : "4.1 Estimation by Critic using Linearized Bellman",
      "text" : "The critic step of pAC estimates Z-value and the average cost by minimizing the least-square error between the true Z-value and estimated one denoted by Ẑ.\nmin ν,Ẑavg\n1\n2 ∫ x ( ẐavgẐ(x;ν)−ZavgZ(x) )2 dx, (7)\ns.t. ∫ x Ẑ(x;ν)dx = C, ∀x 0 < Ẑ(x;ν) ≤ 1 Ẑavg ,\nwhere ν is a parameter vector of the approximation and C is a constant value used to avoid convergence to the trivial solution Ẑ(x;ν) = 0 for all x. The second constraint comes from ∀x, Z(x) := e−V (x) > 0 and ∀x, q(x) ≥ 0. The latter implies that V + Vavg > 0, and note that ZavgZ(x) := e −(V+Vavg), which is less than 1.\nWe minimize the least-square error in Eq. 7, ẐavgẐk − ZavgZk, with TD-learning. The latter minimizes TD error instead of the least-square error that requires the true Z(x) and Zavg , which are not available. The TD error denoted as eik for linearized Bellman equation is defined using a sample (xk,xk+1) of passive dynamics as, eik := Ẑ i avgẐ i k − e−qk Ẑik+1, where the superscript i denotes the\niteration. Ẑavg here is updated using the gradient as follows:\nẐi+1avg = Ẑ i avg − αi1\n∂ ( eik )2\n∂Ẑavg = Ẑiavg − 2αi1eikẐik, (8)\nwhere αi1 is the learning rate, which may adapt with iterations.\nIn this work, we approximate the Z-value function in two ways: (i) using a linear combination of weighted RBFs, and (ii) using a neural network (NN). When a NN with an exponentiated activation function of output layer is used, the parameters ν are updated with the following gradient based on backpropagation. 1\n∂\n∂νi\n( ẐavgẐ i k − ZavgZk )2 ≈ 2eikẐiavg\n∂Ẑik ∂νi , (9)\nwhere eik is the TD error as defined previously.\nOn the other hand, when weighted RBFs are used, Ẑ(x;ν) := ν>f(x), and a Lagrangian relaxation of the objective function is useful as it includes the three constraints weighted using Lagrangian parameters λ1, λ2 and λ3. For convenience, denote ν̃i as,\nν̃i := νi − 2αi2eikẐiavg ∂Ẑik ∂νi ,\n1e−tanh(x) or e−softplus(x) is used as an activation function of the output layer to satisfy the constraint Ẑ ≥ 0. The constraint ∫ x Ẑ(x;ν)dx = C is ignored in practice because convergence to ∀x, Ẑ(x;ν) = 0 is rare. min ([1, e−qk Ẑik+1]) is used instead of e −qk Ẑik+1 to satisfy Ẑ ≤ 1/Zavg in Eq. 7.\nwhere αi2 is the learning rate. The update of ν cognizant of the constraints is then,\nνi+1 = ν̃i + ∂\n∂ν̃i\n( λi1 (∫ x ν̃i>f(x)dx− C ) + λi>2 ( ν̃i − 0 ) + λi3 ( Ẑk − 1/Ẑavg )) ,\n= ν̃i + λi11 + λ i 2 + λ i 3fk, (10)\nwhere we utilize ∫ x\nf(x)dx = 1 and replace the constraint ∀x Ẑ(x;ν) > 0 by ν > 0 because the constraint on ν always satisfies the former. 1 is a vector of all ones.\nLagrangian parameters λ1, λ2 and λ3 in each iteration are obtained by ensuring that the updated parameter vector νi+1 satisfies the three constraints in Eq. 7 for xk. Formally,\nẐ(xk;ν i+1) = (ν̃i + λi11 + λ i 2 + λ i 3fk) >fk ≤ 1/Ẑavg, νi+1 = ν̃i + λi11 + λ i 2 + λ\ni 3fk > 0 and,∫\nx\nẐ(x;νi+1)dx = ( ν̃i + λi11 + λ i 2 + λ i 3fk )> 1 = C."
    }, {
      "heading" : "4.2 Actor Improvement using Standard Bellman",
      "text" : "The actor component improves a policy by computing S (Eq. 5) using the estimated Z-values from the critic because we do not assume knowledge of noise level σ. It is estimated by minimizing the least-square error between the V-value and the state-action Q-value:\nmin S\n1\n2 ∫ x ( Q̂(x, û(x))− V (x) )2 dx,\nwhere V is the true V-value and Q̂ is the estimated Q-value under the estimated action û(x). Notice from Eq. 6 that a value for S results in a policy as B is known. Thus, we seek the S that yields the optimal policy by minimizing the error because the Q-value equals V-value iff û is maximizing.\nAnalogously to the critic, we minimize the least-square error given above, Q̂ik−Vk, with TD-learning. To formulate the TD error for the standard Bellman update, let xk+1 be a sample at the next time step given state xk under passive dynamics, and let x̃k+1 := xk+1 +Bûk∆t be the next state using control dynamics. Rearranging terms of the Bellman update given in Eq. 2, the TD error dik is,\ndik := r(xk, ûk) + V̂ i(x̃k+1)− V̂avg − V̂ ik .\nWe may use Eqs. 3 and 5 to replace the reward function,\nr(xk, ûk) = (qk + 0.5û > k (Ŝ i)−1ûk)∆t = qk∆t+ 0.5 ∂V̂ ik ∂xk\n>\nBŜiB> ∂V̂ ik ∂xk ∆t.\nThe last step is obtained by noting that ûik = −ŜiB> ∂V̂ ik ∂xk\n, where Ŝ denotes estimated S in Eq. 6. The estimated V-value and its derivative is calculated by utilizing the approximate Z-value function from the critic. Ŝ is updated based on standard stochastic gradient descent using the TD error,\nŜi+1 = Ŝi − βi ∂ ∂Ŝi\n( Q̂ik − Vk )2 ≈ Ŝi − 2βidik\n∂dik ∂Ŝi ,\nwhere β is the learning rate. The actor mitigates the impact of error from estimated Z-value by minimizing the approximated least-square error between V- and Q-values under the learned policy."
    }, {
      "heading" : "4.3 Algorithm",
      "text" : "We show a pseudo code of pAC in Algorithm 1. Z(x) and Zavg are estimated in the critic with samples, and S is done in the critic with samples, estimated Ẑ(x) and Ẑavg . In the critic, feedback from the actor is not needed (unlike actor critic methods for MDPs) because the Z-value is approximated with samples from passive dynamics only. We emphasize that the actor and critic steps do not use the functions A and σ but does indeed rely on B, of Eq. 1. As such, the updates use a sample (xk,xk+1) of the passive dynamics, and the state cost qk. Consequently, pAC achieves semi model-free learning for L-MDPs.\nAlgorithm 1 passive Actor Critic 1: Initialize parameters Ẑ0avg,ν0, Ŝ0, α01, β0"
    }, {
      "heading" : "5 Numerical Experiments",
      "text" : "We evaluate pAC for L-MDPs on two synthetic domains, Car-on-a-Hill and Pendulum, also used previously on L-MDPs by Todorov [18]; and on our motivating domain of autonomous merging in a congested freeway."
    }, {
      "heading" : "5.1 Problem settings",
      "text" : "Car-on-a-Hill Car-on-a-Hill has a two-dimensional state space, x = [xp, xv]> where xp and xv denote position and velocity, respectively, and a one-dimensional action space. Table 1 shows the dynamics in detail. The state cost is given by,\nq(x) =4.0 ( exp ( −0.5(xp − 1)2 − (xv + 1)2 ) + exp ( −0.5(xp + 1)2 − (xv − 1)2 ) − 2 ) .\nInitial states are randomly set in −2π ≤ xp ≤ 2π and −π ≤ xv ≤ π.\nPendulum The Pendulum problem also has a two-dimensional state space similar to the Car-on-aHill and a one-dimensional action space. Table 1 shows the dynamics in detail. State cost is given by\nq(x) = 4.0 ( exp ( −(xv − 3)2 ) + exp ( −(xv + 3)2 ) − 2) ) .\nInitial states are randomly set in −2π ≤ xp ≤ 2π and −π ≤ xv ≤ π.\nSimulated freeway merging This new contemporary domain simulates freeway merges by an automated vehicle. We refer the reader to Fig. 1 for establishing the four-dimensional state space. Here, x = [dx12, dv12, dx02, dv02]> where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j ∈ [0, 1, 2]. The action space is one-dimensional (acceleration). Table 1 shows the dynamics in detail. The dynamics presume that the leading vehicle is driven with a constant speed v2 = 30[m/sec], and the following vehicle is driven by a known car-following model [12]. The acceleration of the vehicle a0(x) is calculated with −αvβ2 dv02/(−dx) γ 02, where if the following vehicle is slower than the leading vehicle (dv02 < 0), α = 1.55, β = 1.08, γ = 1.65, otherwise α = 2.15, β = −1.65, γ = −0.89. The assumptions are used to simulate maneuvers of ambient vehicles in only the simulated freeway merge domain.\nThe state cost designed to motivate Car-1 to merge midway between Cars 0 and 2 with the same velocity as Car-0, is:\nq(x) = k1 − k1 exp ( −k2 ( 1− 2dx12\ndx02\n)2 − k3dv210 )\nwhere k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise). Initial states are randomly picked in −100 < dx12 < 100 [m], −10 < dv12 < 10 [m/sec], −100 < dx02 < −5 [m], and −10 < dx02 < 10 [m/sec]."
    }, {
      "heading" : "5.2 Performance Evaluation",
      "text" : "We compared pAC with two other methods: model-based learning based on quadratic programming (QP) and Z-learning. QP requires all system dynamics and approximates the Z-value with quadratic programming [17]. Z-learning assumes that B and σ are available to approximate Z-value using the critic. QP and Z-learning calculate the policy with Eq. 6. We are unaware of any fully model-free method for L-MDPs. model-free PI control (e.g. [15]) assume action cost is available and it is equivalent to the assumption of the known transition noise level in L-MDPs. Table 2 gives the prior knowledge requirement on components of the system dynamics in Eq. 1.\nGaussian RBFs are used to approximate the Z-value function in all methods and NNs are additionally used in Z-learning and pAC. RBFs were spaced uniformly in the range of sampled data: 400 RBFs were used for Car-on-a-Hill and Pendulum domains, and 4,096 RBFs were used for the Merging task. The standard deviations of the bases were 0.7 of the distance between the closest two bases in each dimension.\nA three-hidden layer perceptron with 200, 200 and 50 units in first, second and third hidden layers is used as the NN. The number of nodes in the input is same as the dimensions of state space and one output, respectively. The rectified linear function [10] is used as the hidden layers’ activation function. The activation function in the output layer for Car-on-a-Hill is exp(−tanh(x)) and exp(−softplus(x)) for other domains. We estimated S as constant in pAC. Inputs of the perceptron were normalized to the range [0, 1].\nIn Fig. 2, we compared average cost calculated over periods of 10 seconds for Car-on-a-Hill and Pendulum, and 30 seconds for Merging, under learned policies in each domain. Observe that pAC finds a similar or better policy in comparison to other learning methods in all domains. QP could not learn any reasonable policy in Merging domains because of an issue referred to in [18]: QP might not converge to a principal Eigen pair, instead of converging to a 2nd or higher-order pair.\nThis improvement over Z-learning may come as a surprise because Z-learning makes greater use of model knowledge – it calculates S using the true B and σ. Nevertheless, the presence of the additional actor step in pAC makes the difference. The actor additionally minimizes the error in Q-value to obtain S. This error is not minimized if S is obtained as in Eq. 5 due to the approximation error of Z-value. Subsequently, pAC estimates S differently, in a more targeted way to obtain a better policy.\nWe evaluated the rate of merging successfully in a time limit of 30 seconds starting from 125 different states in Freeway-Merge. We defined success as being between the leading and following vehicles after 30 seconds. Figure 3 (a) shows the success rate: pAC with RBFs and NN achieved 93% and 97% success rate respectively, which is comparable to Z-learning. All results shows pAC is comparable or better than Z-learning despite less prior knowledge instead of using actor step."
    }, {
      "heading" : "5.3 Experiment on real-world traffic",
      "text" : "The NGSIM data set contains vehicle trajectory data recorded by cameras mounted on top of a building for 45 minutes around the evening rush hour[11]. Vehicle trajectories were extracted using a vehicle tracking method from collected videos [7]. We extracted three-vehicle systems (Fig. 1) representing 637 freeway merge events.\nWe compared pAC and Z-learning based on RBFs and NN on the extracted data from NGSIM. Z-learning calculated a policy with transition noise σ estimated with a Gaussian process due to unknown true dynamics. We used the same state variables, action variable and reward function as used in the simulated Freeway-Merge domain. We calculated next states xk+1 under passive dynamics by subtracting state change caused by actions:xk+1 = xDk+1 −B>uDk , where xDk+1 and uDk are the next state and the action recorded in the data set respectively. We resampled data to mitigate any imbalance and sparseness by sampling randomly a state and choosing a nearest data from the state using the approximate nearest neighbor method.\nSuccess was defined as being between Car-0 and Car-2 at the merging point, for those instances in the data set where Car-1 on entry ramp completed its merge. Trajectories of Car-0 and Car-2 were played back from recorded logs, and trajectories of Car-1 were simulated with the control dynamics and learned policies. The dataset was randomly partitioned into five sub-datasets where an almost equal number of trajectories were included. Four sub-dataset were used as training data and a remaining sub-dataset were used as the test data for testing the policy. Each method was evaluated five times for each different test data. Figure 3(b) shows the success rate for each method. Firstly, we note the significant performance improvement when NN is used as the function approximator. Both pAC and Z-learning with RBFs simply do not perform well. Specifically, pAC with NN achieved a 93% success rate significantly outperforming Z-learning with NN. The result shows that the actor, which\nminimizes the error of Q-value, improves performance more than the noise level estimation from the dataset. The model-free RL approaches cannot be applied here due to the need for active exploration."
    }, {
      "heading" : "6 Concluding Remarks",
      "text" : "We presented a novel method for semi model-free RL for L-MDPs – an important subclass of MDPs. The passive actor-critic optimizes a policy without active exploration, instead of using samples of the passive dynamics and knowledge of the control dynamics. This is a first formulation of the actor-critic schema in the context of L-MDPs. We evaluated the method using three domains. Results show that pAC achieves comparable or better performances than benchmarked methods despite less prior knowledge requirements. As such, pAC represents a significant step toward more efficient RL in continuous domains.\nData under passive dynamics and accurate models of control dynamics are needed for pAC. This may seem to limit applicability apparently but, as mentioned, it is well suited for contemporary robotic applications such as automated driving in real-world traffic. In this case, passive and control dynamics correspond to models of ambient and autonomous vehicles, respectively. pAC obtains a better policy with the car’s own dynamics model, which is now commonly available, and uses data collected on maneuvers of the ambient vehicles whose models are usually not known. An evaluation of the learned policies toward freeway merging using a real traffic data set illustrates its usefulness for practical applications. We are interested in exploring additional challenges, e.g., entering roundabouts."
    } ],
    "references" : [ {
      "title" : "Fitted q-iteration in continuous action-space mdps. In Advances in neural information processing systems",
      "author" : [ "András Antos", "Csaba Szepesvári", "Rémi Munos" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "Fitted q-iteration by functional networks for control problems",
      "author" : [ "Matteo Gaeta", "Vincenzo Loia", "Sergio Miranda", "Stefania Tomasiello" ],
      "venue" : "Applied Mathematical Modelling,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Policy search for path integral control",
      "author" : [ "Vicenç Gómez", "Hilbert J Kappen", "Jan Peters", "Gerhard Neumann" ],
      "venue" : "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "A survey of actorcritic reinforcement learning: Standard and natural policy gradients",
      "author" : [ "Ivo Grondman", "Lucian Busoniu", "Gabriel AD Lopes", "Robert Babuska" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "Learning continuous control policies by stochastic value gradients",
      "author" : [ "Nicolas Heess", "Gregory Wayne", "David Silver", "Tim Lillicrap", "Tom Erez", "Yuval Tassa" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Actor-critic algorithms. In Advances in neural information processing systems",
      "author" : [ "Vijay R Konda", "John N Tsitsiklis" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1999
    }, {
      "title" : "Video-based vehicle trajectory data collection",
      "author" : [ "Vijay Gopal Kovvali", "Vassili Alexiadis", "PE Zhang" ],
      "venue" : "In Transportation Research Board 86th Annual Meeting,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Batch reinforcement learning",
      "author" : [ "Sascha Lange", "Thomas Gabel", "Martin Riedmiller" ],
      "venue" : "In Reinforcement learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Safe exploration in markov decision processes",
      "author" : [ "Teodor M Moldovan", "Pieter Abbeel" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E Hinton" ],
      "venue" : "In Proceedings of the 27th International Conference on Machine Learning",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Comparison of car-following models",
      "author" : [ "Johan Janson Olstam", "Andreas Tapani" ],
      "venue" : "Swedish National Road and Transport Research Institute, Project VTI meddelande,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Markov Decision Processes",
      "author" : [ "Martin L. Puterman" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1994
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel" ],
      "venue" : "CoRR, abs/1502.05477,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2015
    }, {
      "title" : "A Generalized Path Integral Control Approach to Reinforcement Learning",
      "author" : [ "E. Theodorou", "Jonas Buchli", "Stefan Schaal" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "Linearly-solvable markov decision problems",
      "author" : [ "Emanuel Todorov" ],
      "venue" : "In Advances in neural information processing systems",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2006
    }, {
      "title" : "Efficient computation of optimal actions",
      "author" : [ "Emanuel Todorov" ],
      "venue" : "Proceedings of the national academy of sciences,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2009
    }, {
      "title" : "Eigenfunction approximation methods for linearly-solvable optimal control problems",
      "author" : [ "Emanuel Todorov" ],
      "venue" : "In Adaptive Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Combining learned controllers to achieve new goals based on linearly solvable mdps",
      "author" : [ "Eiji Uchibe", "Kenji Doya" ],
      "venue" : "In Robotics and Automation (ICRA),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Aggregation methods for lineary-solvable markov decision process",
      "author" : [ "Mingyuan Zhong", "Emanuel Todorov" ],
      "venue" : "In Proceedings of the World Congress of the International Federation of Automatic Control,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "In this regard, Todorov [16] introduced the linearly-solvable Markov decision process (L-MDP), a subclass of general MDPs, which allows us to quickly solve the continuous Bellman equation exactly under a class of structured dynamics and rewards.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 16,
      "context" : "Specifically, the Bellman equation in L-MDPs is recast as a linearized differential, and its solution is efficiently obtained as a linear eigenfunction when the whole dynamics model is available [18].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "However, at the same time, guaranteeing safe exploration has a high computational cost that is shown to be NP-hard [9].",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 17,
      "context" : "In addition to the well-known radial basis function [18, 19], multi-layer neural networks are also introduced for approximating the value function with demonstrated performance improvements.",
      "startOffset" : 52,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 16,
      "context" : "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "Previous approaches for solving L-MDPs are predominantly model based [17, 18, 21].",
      "startOffset" : 69,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "Uchibe and Doya [19] formulate Z-learning based on least-square TD learning for continuous LMDPs.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "As pAC can learn from data containing samples of passive dynamics, it bears resemblance to batch RL methods [8].",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 1,
      "context" : "A popular and model-free batch RL method is fitted Q-iteration [1, 2], which finds policy from collected data without a model of system dynamics.",
      "startOffset" : 63,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Path integral control also learns a policy based on linearized Bellman equation [15, 3].",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Path integral control also learns a policy based on linearized Bellman equation [15, 3].",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Finally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 12,
      "context" : "Finally, many RL methods that use neural networks for continuous MDPs have been recently proposed [5, 14].",
      "startOffset" : 98,
      "endOffset" : 105
    }, {
      "referenceID" : 15,
      "context" : "L-MDP [17] is a subclass of MDPs [13] defined by a tuple, 〈X ,U ,P,R〉, where X ⊆ R and U ⊆ R are continuous state and action spaces.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 11,
      "context" : "L-MDP [17] is a subclass of MDPs [13] defined by a tuple, 〈X ,U ,P,R〉, where X ⊆ R and U ⊆ R are continuous state and action spaces.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "[4] notes that the Bellman equation for MDPs can be rewritten using the value function V (x) called V-value, state-action value function Q(x,u) called Q-value, and average value Vavg under an policy.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "2 (hereafter referred to as the linearized Bellman equation) [18]: ZavgZk = e −qk∆t Ep(xk+1|xk)[Zk+1], (4)",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 5,
      "context" : "While the actor-critic method usually operates using samples collected actively in the environment [6], pAC finds a converged policy without exploration.",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 0,
      "context" : "B [ 0, 1 ]> [ 0, 1 ]> [ 0.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "B [ 0, 1 ]> [ 0, 1 ]> [ 0.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "σ [ 0, 1 ]> [ 0, 2 ]> [ 0, 2.",
      "startOffset" : 2,
      "endOffset" : 10
    }, {
      "referenceID" : 1,
      "context" : "σ [ 0, 1 ]> [ 0, 2 ]> [ 0, 2.",
      "startOffset" : 12,
      "endOffset" : 20
    }, {
      "referenceID" : 16,
      "context" : "We evaluate pAC for L-MDPs on two synthetic domains, Car-on-a-Hill and Pendulum, also used previously on L-MDPs by Todorov [18]; and on our motivating domain of autonomous merging in a congested freeway.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "Here, x = [dx12, dv12, dx02, dv02] where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j ∈ [0, 1, 2].",
      "startOffset" : 138,
      "endOffset" : 147
    }, {
      "referenceID" : 1,
      "context" : "Here, x = [dx12, dv12, dx02, dv02] where dxij and dvij denote the horizontal signed distance and relative velocity between cars i and j ∈ [0, 1, 2].",
      "startOffset" : 138,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : "The dynamics presume that the leading vehicle is driven with a constant speed v2 = 30[m/sec], and the following vehicle is driven by a known car-following model [12].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).",
      "startOffset" : 67,
      "endOffset" : 78
    }, {
      "referenceID" : 9,
      "context" : "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 9,
      "context" : "where k1, k2 and k3 are weights for the state cost ([k1, k2, k3] = [1, 10, 10] if dx02 < dx12 < 0, [10, 10, 0] otherwise).",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 15,
      "context" : "QP requires all system dynamics and approximates the Z-value with quadratic programming [17].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "[15]) assume action cost is available and it is equivalent to the assumption of the known transition noise level in L-MDPs.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "The rectified linear function [10] is used as the hidden layers’ activation function.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "Inputs of the perceptron were normalized to the range [0, 1].",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 16,
      "context" : "QP could not learn any reasonable policy in Merging domains because of an issue referred to in [18]: QP might not converge to a principal Eigen pair, instead of converging to a 2nd or higher-order pair.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "Vehicle trajectories were extracted using a vehicle tracking method from collected videos [7].",
      "startOffset" : 90,
      "endOffset" : 93
    } ],
    "year" : 2017,
    "abstractText" : "In many robotic applications, some aspects of the system dynamics can be modeled accurately while others are difficult to obtain or model. We present a novel reinforcement learning (RL) method for continuous state and action spaces that learns with partial knowledge of the system and without active exploration. It solves linearly-solvable Markov decision processes (L-MDPs), which are well suited for continuous state and action spaces, based on an actor-critic architecture. Compared to previous RL methods for L-MDPs and path integral methods which are model based, the actor-critic learning does not need a model of the uncontrolled dynamics and, importantly, transition noise levels; however, it requires knowing the control dynamics for the problem. We evaluate our method on two synthetic test problems, and one real-world problem in simulation and using real traffic data. Our experiments demonstrate improved learning and policy performance.",
    "creator" : "LaTeX with hyperref package"
  }
}