{
  "name" : "1307.7793.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-dimensional Parametric Mincuts for Constrained MAP Inference",
    "authors" : [ "Yongsub Lim", "Kyomin Jung", "Pushmeet Kohli" ],
    "emails" : [ "yongsub@kaist.ac.kr", "kyomin@kaist.edu", "pkohli@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Markov Random Fields (MRF) is an undirected graphical model, which has been extensively studied and used in various fields, including statistical physics [11], and computer vision [19]. It represents interdependency of discrete random variables as a graph over which a probabilistic space is defined. Computing the solution which has the maximum probability under the random field, or Maximum a Posteriori (MAP) inference is NP-hard in general. However, a number of subclasses of MRFs have been isolated for which the problem can be solved in polynomial time [2]. Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem. Such algorithms are widely used for various problems in machine learning and computer vision [38, 17]. Since MAP inference in an MRF is equivalent to minimizing the corresponding energy function1, in what follows, we will explain these problems in terms of energy minimization.\nIn many real world problems, the values of certain statistics of the desired solution may be available as prior knowledge. For instance, in the case of foreground-background image segmentation, we may know the approximate shape and/or size of the object being segmented, and thus might want to find the most probable segmentation that has a particular area (number of foreground pixels) and boundary length (number of discontinuities). Another example is community detection in a network [8] where we may know the number of nodes belonging to each community. Such scenarios result in constraints in the solution space, and MAP inference becomes a constrained energy minimization problem, which is generally NP-hard even if the unconstrained version is polynomial time solvable.\n∗yongsub@kaist.ac.kr †kyomin@kaist.edu ‡pkohli@microsoft.com 1Energy of a labelling is the negative logarithm of its posterior probability.\nar X\niv :1\n30 7.\n77 93\nv1 [\ncs .L\nG ]\n3 0\nJu l 2\nEnergy minimization under the above-mentioned statistics constraints results in a challenging optimization problem. However, recent work in computer vision has shown that this problem can be handled efficiently using the parametric mincuts [14] which allow simultaneous computation of exact solutions for some constraint instances. Although the parametric mincuts provide a general framework to deal with constrained energy minimization, they can only handle one linear equality constraint.\nFor minimizing energy functions under multiple constraints, a number of continuous relaxation based methods have been proposed in the literature. For instance, linear relaxation approaches were adopted to handle bounding-box and connectivity constraints defined on the labelling [18, 24]. Further, Klodt and Cremers [12] proposed a convex relaxation framework to deal with moment constraints. Continuous relaxation based methods have also been used for constrained discrete optimization, and can handle multiple inequality constraints. All the above-mentioned methods suffer from following basic limitations: they only handle linear constraints, and the solution involves rounding of the solution of the relaxed problem which may introduce large errors."
    }, {
      "heading" : "1.1 Our contribution",
      "text" : "In this paper we show how the constrained discrete optimization problem associated with constrained MAP inference can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and propose an algorithm that isolates all constraint instances for which the problem can be solved exactly. This leads to densely many minimizers, each of which is, optimal under distinct constraint instance. These minimizers can be used to compute good approximate solutions of problems with soft constraints (enforced with a higher order term in the energy).\nOur algorithm works by exploiting the Lagrangian dual of the minimization problem, and requires an oracle which can compute values of the Lagrangian dual efficiently. A graph-cut algorithm [3] is a popular example of such an oracle. In fact, our algorithm generalizes the (one-dimensional) parametric mincuts [5, 14] to multiple-dimensions. In contrast to the parametric mincuts [5], our algorithm can deal with multiple constraints simultaneously, including some non-linear constraints (as we show in the paper). This extension allows our algorithm to be used as a technique for multi-dimensional sampling e.g. to obtain different segmentation results for image segmentation as done in [4].\nWe propose two variants of our algorithm to efficiently deal with the problem of performing MAP Inference under hard constraints. The first variant computes the maximum of the dual and outputs its corresponding primal solution as an approximation of the constrained minimization. The primal is computed using selective oracle calls, leading to fast computation time. The other variant combines the first variant with our multidimensional parametric mincuts algorithm to deal with problems with soft-constraints, which allows to find a solution closer to a desired one via additional search.\nOur method is quite general and can be applied to any constrained discrete optimization problems whose Lagrangian dual value is efficiently computable. Examples include submodular function minimization with constraints such as the balanced minimum cut problem, and constrained shortest path problems. Further, in contrast to traditional continuous relaxation based methods, our technique can easily handle complicated soft constraints.\nIn Section 5, we demonstrate that our algorithms compute solutions very close to the ground truth compared with these continuous relaxation based methods on the foreground-background image segmentation problem."
    }, {
      "heading" : "1.2 Related work",
      "text" : "A number of methods have been proposed to obtain better labelling solutions by inferring the MAP solution from a restricted domain of solutions which satisfy some constraints. Among them, solutions to image labelling problems which have a particular distribution of labels [36] or satisfy a topological property like connectivity [32] have been widely studied.\nMore specifically, for the problem of foreground-background image segmentation, most probable segmentations under the label count constraint have been shown to be closer to the ground truth [14, 20]. Another\nexample is the silhouette constraint which has been used for the problem of 3D reconstruction [13, 29]. This constraint ensured that a ray emanating from any silhouette pixel must pass through at least one voxel which belongs to the ‘object’.\nRecently, dual decomposition has been proposed for constrained MAP inference [7, 37]. Gupta et al. [7] dealt with cardinality-based clique potentials and developed both exact and approximate algorithms. Also Woodford et al. [37] studied a problem involving marginal statistics such as the area constraint especially with convex penalties, and showed that the proposed method improves quality of solutions for various computer vision problems.\nMAP inference under constraints are also applied to combinatorial optimization such as the balanced metric labelling. For this problem, Naor and Schwartz [23] obtained an O( lnn )-approximate algorithm where each label is assigned to at most min [ O(ln k) 1− , `+ 1 ] (1 + )` variables/nodes."
    }, {
      "heading" : "2 Setup and preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Energy minimization",
      "text" : "Markov Random Fields (MRF) defined on a graph G = (V,E) is a probability distribution where every vertex u ∈ V has a corresponding random variable xu taking a value from the finite label set L. The probability distribution is defined as Pr(x) ∝ exp(−f(x)) where x = (xu), and the corresponding energy function f is in the following form:\nf(x) = ∑ c∈CG φc(xc), (1)\nwhere CG is the set of cliques in G and φc is a potential defined over the clique c. The MAP problem is to find an assignment x∗ ∈ L|V | which has the maximum probability, and is equivalent to minimizing the corresponding energy function f . In general it is NP-hard to minimize f , but it is known that if f is submodular, it can be minimized in polynomial time. Especially, if f is a pairwise submodular energy function defined on binary variables, which considers only cliques of size up to 2, i.e.\nf(x) = ∑ u∈V φu(xu) + ∑ (u,v)∈E φuv(xu, xv), (2)\nit can be efficiently minimized by solving a equivalent st-mincut problem [15]. Such f is widely used in machine learning and computer vision [9, 20]."
    }, {
      "heading" : "2.2 Energy minimization with constraints",
      "text" : "Energy minimization with constraints is to compute the solution x∗ minimizing an energy function among x’s satisfying given constraints. A typical example of constraints is the label count constraint ∑ i xi = b where xi ∈ {0, 1}. In this paper, we consider the following energy minimization with multiple constraints.\nmin x∈{0,1}n\n{f(x) : hi(x) = bi, 1 ≤ i ≤ m} , (3)\nwhere x ∈ {0, 1}n, m is a constant, and for 1 ≤ i ≤ m, hi : {0, 1}n → R and bi ∈ R. In (3), each constraint hi(x) = bi encodes distinct prior knowledge on a desired solution. For convenience, we denote (h1(x), . . . , hm(x)) by H(x).\nLet us consider the following Lagrangian dual g : Rm → R of f , which is widely used for discrete optimization [16, 30].\ng(λ) = min x∈{0,1}n L(x, λ), (4)\nwhere L(x, λ) = f(x) + λT (H(x)− b). (5)\nNote that g is defined over a continuous space while f is defined over a discrete space. As in the continuous minimization, maximizing g over λ ∈ Rm provides a lower bound for (3). Now we define the characteristic set, which is the collection of minimizers of (5) over all λ ∈ Rm.\nDefinition 1 (Characteristic Set). The Characteristic Set is defined by χg = ⋃\nλ∈Rm argmin x∈{0,1}n L(x, λ). (6)\nLemma 1. Let x∗ ∈ χg and b∗ = H(x∗). Then f(x∗) = minx∈{0,1}n {f(x) : H(x) = b∗} [6].\nProof. Suppose that x̂ satisfies that H(x̂) = b∗. It implies λT (H(x̂)− b∗) = λT (H(x∗)− b∗) for any λ ∈ Rm. Since x∗ ∈ χg, L(x∗, λ) ≤ L(x̂, λ) for some λ ∈ Rm. Thus, from (5), f(x∗) ≤ f(x̂).\nIn this paper, we develop a novel algorithm to compute the characteristic set χg. We will show that if the dual g(λ) is efficiently computable for any fixed λ ∈ Rm, for example, when L(x, λ) is submodular on x, our algorithm computes χg by evaluating g(λ) for poly(|χg|) number of λ ∈ Rm. One implication of χg is\ng(λ) = min x∈{0,1}n L(x, λ) = min x∈χg L(x, λ), (7)\nmeaning that minx∈{0,1}n L(x, λ) indeed depends on a much smaller set χg. Note that χg does not depend on the constraint instance b, thus, in the remaining of the paper, we regard b = 0 unless there is explicit specification. In Section 5, we will show that |χg| is polynomially bounded in n for many constraints corresponding to useful statistics of the solution. Through experiments, we will show that |χg| is densely many among all possible constraint instances by an example of image segmentation.\nNote that if we can compute minimizers of (3) for densely many constraint instances b, we can obtain a good approximate solution for the following soft-constrained problem with any global penalty function ρ.\nmin x∈{0,1}n\n{ f(x) + ρ(H(x)− b̂) } . (8)\nIn (8), b̂ encodes our prior knowledge on a solution, and examples of ρ include ‖ · ‖`p and sigmoid functions. This soft-constrained optimization has been widely used in terms of lasso regularization and ridge regression, and also in computer vision [17, 31]."
    }, {
      "heading" : "2.3 Generalization",
      "text" : "Although we describe our method for problems involving pseudo-Boolean2 objective functions, there is a class of multi-label functions to which our method can be applied. For instance, the results of [26] show transformation of any multi-label submodular functions of order up to 3 to a pairwise submodular one, meaning that it can be solved by the graph-cut algorithm. This enables us to handle the following type of constraints, which is analogue of linear constraints in binary cases: for each j ∈ L,\nhj(x) = ∑ i∈V aijδxi;j = bj , (9)\nwhere δxi,j is Kronecker delta function. Our method is also applicable to any constrained combinatorial optimization problems whose g(λ) is efficiently computable. We will discuss it more in detail in Section 5.2.\n2Real-valued functions defined over boolean vectors {0, 1}n."
    }, {
      "heading" : "3 Computing the Characteristic Set",
      "text" : ""
    }, {
      "heading" : "3.1 Algorithm description",
      "text" : "In this section, we describe our algorithm that computes the characteristic set χg. We assume that for a given set S = ∏m i=1[Ni,Mi] where Ni,Mi ∈ R for all i, there is an oracle to compute the Lagrangian dual g efficiently for any λ ∈ S. For simplicity of explanation, we assume S = [−M,M ]m for some M > 0. We denote the oracle call by\nO(λ) = argmin x∈{0,1}n L(x, λ). (10)\nEssentially, our algorithm iteratively decides the λ’s in S for which the oracle will be called. Later we prove that the number of oracle calls in our algorithm to compute χg is polynomial in |χg|.\nWe first define the following, which has a central role in our algorithm.\nDefinition 2 (Induced dual of g on X). Let g : Rm → R be the Lagrangian dual of f , and X ⊆ {0, 1}n. The induced dual gX of g is defined by\ngX(λ) = min x∈X L(x, λ). (11)\nFrom the definition of χg, note that g = g{0,1}n = gχg . For each x ∈ {0, 1}n, we define a hyperplane Px by\nPx = {(λ, z) ∈ Rm+1 : λ ∈ Rm, z = L(x, λ)}. (12)\nFor (λ, z) ∈ Px, we use the notation so that Px(λ) = z. For convenience, we will denote any v ∈ Rm+1 by (λv, zv), where λv ∈ Rm is the first m coordinates of v and zv ∈ R is the (m+ 1)-th coordinate of v. Since {0, 1}n is finite and each x ∈ {0, 1}n corresponds to a hyperplane in (m + 1)-dimension, g consists of the boundary of the upper polytope of (4). Then χg corresponds to the collection of m-dimensional facets of this polytope.\nTo compute χg, we will recursively update a structure called the skeleton of gX defined below. Intuitively, the skeleton of gX is the collection of vertices and edges of the polytope corresponding to gX .\nDefinition 3 (Proper convex combination). Given x, x1, . . . , xk ∈ R`, x is a proper convex combination of {xi : 1 ≤ i ≤ k} if x = ∑k i=1 αixi for some α ∈ (0, 1)k with ∑k i=1 αi = 1.\nDefinition 4 (Skeleton of gX over S). For a given induced dual gX : Rm → R, let ΓX(S) = {q ∈ Rm+1 : λq ∈ S, zq ≤ gX(λq)}, and for u, v ∈ ΓX(S), e(u, v) ⊆ ΓX(S) is the line segment connecting u and v. The skeleton of gX is GgX = (VgX , EgX ) satisfying the followings.\n• VgX = {v ∈ ΓX(S) : if v is a proper convex combination of U ⊆ ΓX(S), then U = {v}}.\n• EgX = {e(u, v) : u, v ∈ VgX , and if y ∈ e(u, v) is a proper convex combination of W ⊆ ΓX(S), then W ⊆ e(u, v)} ∪ {e(u, v) : u ∈ VgX , λu ∈ {−M,M}m, v = (λu,−∞)}.\nOur algorithm runs by updating X ⊆ χg and GgX iteratively. If a new minimizer x ∈ {0, 1}n is computed by the oracle call, it is inserted to X and the algorithm computes GgX = (VgX , EgX ). Then, the algorithm determines new λ’s for which the oracle will be called from the new vertices added to VgX . We prove in Theorem 1 that at the end of the algorithm, X = χg.\nInitially, the algorithm begins with X = {x0} where x0 is the output of the oracle call for any arbitrary λ0 ∈ {−M,M}m. The inittial skeleton G = (V, E) is given by V = {v1, . . . , v2m} ⊂ Rm+1 where {λvi : 1 ≤ i ≤ 2m} = {−M,M}m and zvi = Px0(λvi) for 1 ≤ i ≤ 2m; and E = EgX . Note that G = GgX , i.e. the skeleton of gX . This initialization is denoted by InitSkeleton() and it returns X and G.\nIn each iteration with the skeleton GgX = (VgX , EgX ), the algorithm chooses any vertex v ∈ VgX , and checks whether zv = gX(λv) using the oracle call for λv. If zv = gX(λv), we confirm that zv = g(λv) and v ∈ Vg. If not, xv /∈ X computed from the oracle satisfies Pxv (λv) = g(λv) < zv. Then, the algorithm computes a new skeleton GgX∪{xv} as explained below.\nAlgorithm 1: DualSearch\nInput: Oracle O Output: X\n1 (X,G)← InitSkeleton() 2 Give V an arbitrary order 3 foreach v ∈ V in the order do 4 xv = O(λv) 5 if Pxv (λv) < zv then 6 X = X ∪ {xv} 7 Append V+ = {u ∈ Pxv ∩ e : e ∈ E , e 6⊆ Pxv} to V in arbitrary order 8 Remove V− = {u ∈ V : zu > Pxv (λu)} from V 9 E− = {e(u1, u2) ∈ E : u1 ∈ V− or u2 ∈ V−}\n10 E+ = {e(u1, u3) : ∃ e(u1, u2) ∈ E−, u3 = e(u1, u2) ∩ Pxv} 11 E = E ∪ ConvEdge(V+) ∪ E+ − E− 12 end\n13 end\nLet X ′ = X ∪{xv}. To compute GgX′ , geometrically we cut GgX by Pxv . This can be done by finding the set V− of skeleton vertices of gX strictly above Pxv , and finding the set V+ of all intersection points between Pxv and EgX . Then, V− is removed from VgX , and V+ is added to VgX . Lastly, the set of edges of the convex hull of V+, which is denoted by ConvEdge(V+), is added to EgX 3. Then, the updated GgX is GgX′ . Due to the concavity of gX , we can compute all the above sets by the depth or breadth first search starting from v. Algorithm 1 describes the whole procedure.\nExample of execution We explain the running process of DualSearch with a toy example. Let us consider an energy function f(x1, x2) = x1 + x2, and two constraints h1 and h2 defined as follows.\nh1(x1, x2) = x1 − x2, (13) h2(x1, x2) = 2|x1 − x2|. (14)\nHere, we set M = 2. Initially, the algorithm computes a minimizer x(0) = (1, 0) for λ(0) = (−2,−2). Then the initial V becomes {(−2,−2,−5), (−2, 2, 3), (2,−2,−1), (2, 2, 7)}, which is shown in Figure 1(a). At this point, X = {x(0)}. Let (−2, 2, 3) ∈ V be chosen in the next iteration, and for that vertex, the new minimizer x(1) ∈ χg is found. This updates both X = {x(0), x(1)} and the skeleton as shown in Figure 1(b). In the following iterations, (2, 2, 7), (2,−2,−1) and (−2, 0.5, 0) are chosen, but for those vertices, there is no new minimizer; that is, for those vertices, a minimizer is either x(1) = (1, 0) or x(2) = (0, 0). The skeleton at this point is shown in Figure 1(c). Next, (2,−1.5, 0) is chosen, and the new minimizer x(2) = (0, 1) is computed so that X is updated by {x(0), x(1), x(2)}. This changes the skeleton as in Figure 1(d)."
    }, {
      "heading" : "3.2 Correctness of the algorithm",
      "text" : "In what follows, we analyze the correctness and query complexity of DualSearch. All proofs are provided in Section A.\nLemma 2. At the end of each iteration of DualSearch, G = GgX .\nLemma 2 states that when DualSearch terminates, G is the skeleton of an induced dual gX where X is the output of the algorithm. It remains to show that the computed X is indeed the characteristic set χg.\n3 For a given V+, ConvEdge(V+) can be computed, for example, by [1]. In general, for given (m + 1)-dimensional points, a convex hull algorithm outputs a set of m dimensional facets of the convex hull. Then, we can obtain the edges of the convex hull by recursively applying the algorithm to every computed facets.\nTheorem 1. When DualSearch terminates with X, X = χg.\nFrom Lemma 2 and Theorem 1, the following holds.\nCorollary 1. When DualSearch terminates, G = Gχg .\nNow we analyze the query complexity. At each iteration, the algorithm uses exactly one oracle call. Then, either one new v ∈ Vg is identified if Line 5 of Algorithm 1 is not satisfied, or one new x ∈ χg is obtained if Line 5 is satisfied. Using these facts, we prove the following theorem.\nTheorem 2. The number of oracle calls in DualSearch is |Vg|+ |χg|.\nRecall that each x ∈ χg corresponds to a facet of the (m + 1)-dimensional convex polytope of g. Since each vertex is determined as the intersection of at least (m + 1) facets, at the end of our algorithm, |Vg| is bounded by O(|χg|m+1). Thus, the query complexity becomes O (poly(|χg|))."
    }, {
      "heading" : "4 Algorithms for a specific constraint instance",
      "text" : "In this section, we propose two variants of DualSearch to compute an approximate solution for a specific constraint instance. The first one is called DualMax, and the second one is AdaptSearch which combines DualMax and DualSearch. While DualSearch essentially does not need prior knowledge, DualMax and AdaptSearch explicitly use a given prior knowledge b̂ for more efficient computation.\n4.1 DualMax\nGiven b̂ ∈ Rm, this algorithm finds the maximum of the dual g, which provides a lower bound of (3). If a corresponding minimizer of (3) is in χg, this algorithm finds that minimizer efficiently. Even though the corresponding minimizer is not in χg, the algorithm finds a lower bound of the minimum, which is a good approximate solution as shown in Section 5.1.2.\nThe main difference of DualMax from DualSearch is the vertex set appended to V in Line 7 of Algorithm 1. At each iteration, DualMax calls the oracle for the maximum of the current induced dual. While DualSearch appends all vertices in V+, DualMax only appends one vertex v ∈ V+ where zv ≥ zu for all u ∈ V+. Then zv becomes the maximum of the induced dual for the next iteration. Since the (induced) dual is concave, such a local search on S enables us to eventually find the maximum of the dual. The following is the modification of DualSearch to obtain DualMax.\n• The initial vertex set is changed to V ′ = {v} where zv ≥ zu for all u ∈ V where V is the ordinary initial skeleton vertex.\n• Line 7 of Algorithm 1 is changed to “append to V the one vertex v ∈ V+ such that zv ≥ zu for all u ∈ V+”.\nThen, the following Lemma holds, and the proof is provided in Appendix.\nLemma 3. When DualMax terminates, for the last v∗ for which the oracle is called, zv∗ = g(λv∗) = maxλ g(λ).\nNote that DualMax uses far fewer oracle calls than DualSearch, which leads to fast computation of the maximum value of g and the corresponding primal solution. The cutting plane method [6] can do the same computation as DualMax, and DualMax can be understood as one implementation of the cutting plane method. While the cutting plane method computes the maximum of the dual by linear programming with computed hyperplanes at each time, DualMax computes it by keeping and updating the skeleton of the dual.\nNow, we suggest a way for DualMax to deal with inequality constraints by inserting a slack variable. For a given problem with inequality constraints, we first transform the problem to one with equality constraints, and apply the algorithm to the transformed problem. Let us consider the following problem.\nmin x\n{ f(x) : b̄− k ≤ H(x) ≤ b̄ } , (15)\nwhere k ∈ Rm, and the inequality is the coordinatewise inequality. The inequality gap contains our prior knowledge, i.e. b̂i ∈ [b̄i−ki, b̄i]. First we transform the problem to a problem with equality constraints using a slack variable y ∈ Rm as follows.\nmin x,y\n{ f̂(x, y) : H(x) + y = b̄ } , (16)\nwhere y ∈ ∏m i=1[0, ki], and f̂(x, y) = f(x). Let us consider the following Lagrangian.\nL̂(x, y, λ) = f̂(x, y) + λT (H(x) + y − b̄). (17)\nFor a minimizer (x∗, y∗) of L̂ for a fixed λ, it always holds that y∗i = 0 for λi > 0, y ∗ i = ki for λi < 0, and y∗ can be any number in [0, ki] for λi = 0. Hence, y ∗ only depends on λ. Then, the dual ĝ(λ) of f̂(x, y) becomes ĝ(λ) = min\nx\n{ f(x) + λT (H(x) + y∗ − b̄) } . (18)\nNote that maxλ ĝ(λ) is a lower bound of (15). Since y ∗ is determined only by λ, ĝ(λ) can be computed by the same oracle for g(λ). Now, we obtain the following lemma.\nLemma 4. Let (x∗, y∗) be such that L̂(x∗, y∗, λ∗) = ĝ(λ∗) for some λ∗ ∈ S, and b∗ = H(x∗) + y∗. Then f(x∗) = minx {f(x) : b∗ − k ≤ H(x) ≤ b∗}.\nProof. Assume (x̂, ŷ) satisfying H(x̂) + ŷ = b∗. It implies that λT (H(x̂) + ŷ− b∗) = λT (H(x∗) + y∗− b∗) for any λ ∈ Rm. Then, ĝ(λ∗) = L̂(x∗, y∗, λ∗) ≤ L(x̂, ŷ, λ∗). Finally, f̂(x∗, y∗) ≤ f̂(x̂, ŷ), and by the definition of f̂ , f(x∗) ≤ f(x̂) holds.\nHence, we can solve (15) by the same manner as in the equality case. Inequality constraints make DualMax more widely applicable because we may not know the exact statistics of a desired solution in practice.\n4.2 AdaptSearch\nDualSearch is a very effective algorithm because it finds minimizers for all λ ∈ S. But in general we do not know where good solutions are found, and thus we should use a large search region S, which leads to slow running time. On the other hand, while DualMax efficiently finds the maximum of the dual for a specific b̂, it may be difficut to determine b̂ for equality constraints in practice. Even if we use inequality constraints to deal with the uncertainty, as inequality gap gets larger, the accuracy of DualMax gets lower. To overcome these drawbacks, we propose a hybrid algorithm, called AdaptSearch, to combine advantages of DualMax and DualSearch, which runs as follows.\nStep 1 Let our prior knowledge b̂ be given, and S ⊂ Rm be a large search region. Step 2 Run DualMax on S with inequality constraint b̂ − k− ≤ H(x) ≤ b̂ + k+ for moderately small\nk−, k+ > 0. Let b∗ be the constraint instance for which the dual maximum is computed.\nStep 3 Run DualMax again on S with equality constraint H(x) = b∗. Then, we obtain λ∗ at which DualMax computes the maximum of the dual.\nStep 4 Run DualSearch for a small search region ∏m i=1[λ ∗ i − αi, λ∗i + αi] where αi ≥ 0, and let X∗ be the\noutput of DualSearch.\nStep 5 Output a solution among X∗ that minimizes the soft-constrained objective.\nNote that in AdaptSearch, we can also use the cutting plane method instead of DualMax. In general, any convex search region is adoptable in Step 4, but we observed from extensive experiments that small constants αi are enough to obtain a good solution. We will show in Section 5 that AdaptSearch computes better solutions than DualMax and runs quite fast."
    }, {
      "heading" : "5 Applications",
      "text" : ""
    }, {
      "heading" : "5.1 Labelling problems in computer vision",
      "text" : "In computer vision, a number of problems can be reduced to labelling problems, including image segmentation, 3D-reconstruction, and stereo. Our constrained energy minimization algorithms can be applied to those problems, for instance, when we may have knowledge on the volume of a reconstructed object for 3D-reconstruction or on the number of pixels belonging to an object for image segmentation. In this section, we show how our algorithms are applied to the foreground-background (fg-bg) image segmentation problem.\nThe fg-bg image segmentation problem is to divide a given image to foreground (object) and background. This can be done by labelling all pixels such that 1 is assigned to foreground pixels and 0 is assigned to background pixels. For this problem, one popular approach is to consider an image as a grid graph in which each node has four neighbours, and minimize an energy function f of the form (2), which is submodular. The unary terms of the function encode how likely each pixel belongs to the foreground or background, while the pairwise terms encode the smoothness of the boundary of the object being segmented. However, in general, a minimizer of (2) is not the ground truth, and it has been shown that imposing statistics on a desired solution can improve segmentation results [12].\nBelow, we describe some linear constraints that have been successfully used in computer vision. • Size: ∑ i∈V xi = b where b ∈ R [20, 35, 36].\n• Mean: ∑ i∈V cixi∑ i∈V xi\n= b where b ∈ R2 and ci = (vi, hi) ∈ R2 denotes the vertical and horizontal coordinates of a pixel i, respectively [12].\n• Cov.: ∑ i∈V (vi−µv)(hi−µh)xi∑ i∈V xi\n= b where b ∈ R and (µv, µh) ∈ R2 denotes the mean center of the object [12].\nWe can define the variance constraints for the vertical and horizontal coordinates in a similar way to the covariance constraint.\nIn many scenarios, researchers are interested in ensuring that the boundary of the object in the segmentation has a particular length. This length can be measured by counting the number of pairs of adjacent variables having different labels and described by ∑ (i,j)∈E |xi − xj | = b where b ∈ R. For this boundary constraint, the search region S may be restricted to a subregion of R× [K,∞] where K ≤ 0 is the smallest real number ensuring L(x, λ) submodular for all λ ∈ S. Figure 2 shows improvement of segmentation results by imposing the above constraints.\n5.1.1 Query complexity of DualSearch\nRecall that the query complexity of DualSearch is polynomial in |χg|. Note that |χg| is upper bounded by the number C of all possible constraint instances. For all the constraints above, we can show C = O(poly(n)). For example, for the size constraint, C = n, and for the boundary length constraint, C ≤ 2n because G is a grid graph. Let us consider the mean constraint, and let b̂ ∈ R2 be obtained from our prior knowledge. Then, the Lagrangian is as follows:\nL(x, λ) = f(x) + λT (∑ i (ci − b̂)xi ) , (19)\nwhere ci ∈ Z2 is bounded by the size of row and column of the image. Hence, the numbers of possible values of ∑ i cixi and b̂ ∑ i xi are O(n\n2) and O(n), respectively, which leads to C = O(n3). By a similar analysis, we can show C = O(n3) for the covariance and variance constraints. If we consider multiple constraints simultaneously, C is bounded by multiplication of the upper bound of each constraint. Hence, |χg| = O(poly(n)) for any combination of the constraints above."
    }, {
      "heading" : "5.1.2 Experiments",
      "text" : "First we did experiments for the size and boundary constraints, and used the following Lagrangian. L(x, λ) = f(x) + λ1 ∑ i∈V xi + λ2 ∑ (i,j)∈E |xi − xj |. (20)\nTable 1 reports the summary of results of DualSearch for 12 images with size 120 × 120. DualSearch produces minimizers for a very large number of constraint instances. One implication is that for any given constraint instance, DualMax and AdaptSearch can compute a minimizer with very close constraint instance to the original one. Figure3 shows an example of a skeleton projected onto two dimensional λ space that is computed with (20) for a 12× 12 image.\nFigure 4 shows experimental results of DualMax and AdaptSearch. For AdaptSearch, we used a soft constraint with a square penalty function for the size and the boundary length constraint, that is, η1( ∑ i xi − b̂1)2 and η2( ∑ (i,j)∈E |xi − xj | − b̂2)2. We chose η1 = 1 and η2 = 100 with which segmentation results generally show less error. Also for the first running of DualMax, we used the inequality gap k+, k− of ±10% of b̂, and b̂1, b̂2 were obtained from the ground truth. The small search region to apply DualSearch is used with α1 = α2 = 1, except for the first image with α1 = α2 = 0.3.\nWe also compared our algorithms with LP [18] and QP [12] relaxation based methods. Table 2 shows that DualMax is faster and more accurate compared with both methods. Since LP and QP cannot handle higher order both-side constrained inequality constraints unlike our algorithms, we used linear constraints introduced previously. Segmentation results are provided in Appendix."
    }, {
      "heading" : "5.2 Combinatorial optimization",
      "text" : "Submodular minimization Our method can also be used for constrained submodular function minimization (SFM). SFM is known to be polynomial time solvable and a number of studies have considered SFM under specific constraints such as vertex cover and size constraints [10, 22]. In contrast to previous work, we provide a framework for dealing with multiple general constraints. Our method can not only deal with any linear constraint, but can also handle some higher order constraints which ensure that the dual is computable. For instance, as shown in the previous section, any submodular constraint hi(x) can be handled with restricted λi ≥ 0.\nShortest path problem The restricted shortest path problem is a widely studied constrained version in which each edge has an associated delay in addition to its length. A path is feasible if its total delay is less than some threshold D [21]. This is a linear constraint ∑ i dixi ≤ D where di is the delay of edge i. Another\nnatural constraint for the shortest path problem is to drop some k nodes among a given set of m nodes. For instance, we may want to design a tour that should contain k cities among m cities. Indeed, this becomes a Hamiltonian path problem when k = m = n. As in the project selection problem, we may partition n cities to r groups, and try to visit ki number of cities from each group where 1 ≤ i ≤ r. Note that all constraints above are linear so that our method can be applied.\nProject selection problem Given a set P of projects, a profit function q : P → R, and a prerequisite relation R ⊆ P×P , this problem is to find projects maximizing the total profit while satisfying a prerequisite relation. This is also known as the maximal closure problem and can be solved in polynomial time by transforming it to a st-mincut problem [25]. In practice, P may be represented by sets P1, . . . , Pm that may overlap, and we may want to select ki projects from Pi for 1 ≤ i ≤ m. This can be formulated using linear constraints ki = u T i x where ui is an indicator which projects belong to Pi. This enables the use of our method to solve the constrained project selection problem."
    }, {
      "heading" : "6 Conclusions",
      "text" : "This paper proposes novel algorithms to deal with the multiple constrained MAP inference problem. Our algorithm AdaptSearch is able to generate high-quality candidate solutions in a short time (see Figure 4) and enables handling of problems with very high order potential functions. We believe it would have a significant impact on the solution of many labelling problems encountered in computer vision and machine learning. As future work, we intend to analyze the use of our algorithms for enforcing statistics in problems encountered in various domains of machine learning."
    }, {
      "heading" : "A Proofs",
      "text" : "In this section, we provide the proofs omitted in the main body. We use notations G(t), V+(t), V−(t) and X(t) to indicate V+,V−,G and X at the end of the t-th iteration in lgorithm 1, respectively. Also we denote a vertex chosen in Line 3 at the t-th iteration by v, and ΓX(S) by ΓX without S. Note that initially G(0) = GgX by the definition of InitSkeleton().\nA.1 Proof of Lemma 2\nLemma 2 is proved by the following Lemma 5 and Lemma 6.\nLemma 5. Assume that G(t− 1) = GgX(t−1) . Then V(t) = VgX(t) .\nProof. (=⇒) Let u ∈ V(t). First assume that u is also in V(t − 1). Suppose that u /∈ VgX(t) . There is Q ⊆ ΓX(t) and Q ∩ {u} = ∅ so that u is a proper convex combination of Q. Note that ΓX(t) ⊆ ΓX(t−1). Thus, u is a proper convex combination of Q over ΓX(t−1). It is a contradiction to u ∈ V(t− 1).\nAssume that u /∈ V(t−1). Then, u ∈ V+(t), implying that u ∈ e(u1, u2) ∈ E(t−1) and u ∈ Pxv . Suppose that there is Q ⊆ ΓX(t) and Q∩{u} = ∅ so that u is a proper convex combination of Q. Since Q * e(u1, u2) and ΓX(t) ⊆ ΓX(t−1), it is a contradiction to the definition of E(t− 1).\n(⇐=) Let u ∈ VgX(t) . Suppose that u /∈ V(t) but u ∈ V(t − 1), which means that u ∈ V−. Then, zu > gX(t)(λu), a contradiction to u ∈ VgX(t) . Suppose that u /∈ V(t) nor u /∈ V(t − 1). Then, there is Q ⊂ ΓX(t−1) and Q ∩ {u} = ∅ so that u is a proper convex combination of Q. Note that Q * Pxv because u ∈ VgX(t) . Since zu ≤ Pxv (λu), at least one of Q is strictly below Pxv , and let Q− be the set of such elements of Q. Since u ∈ VgX(t) , at least one of Q is strictly above Pxv , and let Q+ be the set of such elements of Q. Let P be the set of intersections of e(q−, q+) and Pxv where q\n− ∈ Q− and q+ ∈ Q+. Suppose that u is strictly below Pxv , then u is a proper convex combination of P and Q\n− ⊂ ΓX(t), implying a contradiction. So u ∈ Pxv . Suppose that |P | > 1, then u is a proper convex combination of P ⊂ ΓX(t), which is a contradiction. Thus, |P | = 1 and u ∈ Pxv . Then since u is on some edge e(w1, w2) ∈ E(t − 1) and u ∈ Pxv , u ∈ V+(t) by the algorithm so that u is present in V(t), which is a contradiction.\nLemma 6. Assume that G(t− 1) = GgX(t−1) . Then E(t) = EgX(t) .\nProof. (=⇒) Let e(u,w) ∈ E(t). Assume that e(u,w) is added by E+ so that w is the intersection of e(u, u′) ∈ E(t− 1) and Pxv . Suppose that there is Q ⊂ ΓX(t), and Q * e(u,w) so that for some p ∈ e(u,w), p is a proper convex combination of Q. Since ΓX(t) ⊆ ΓX(t−1), Q ⊆ ΓX(t−1). Also since Q ⊂ ΓX(t) and Q * e(u,w), Q * e(u, u′). It is a contradiction to p ∈ e(u, u′) ∈ E(t− 1).\nAssume that e(u,w) ∈ E(t− 1). Since ΓX(t) ⊆ ΓX(t−1), no p ∈ e(u,w) is a proper convex combination of Q ∈ ΓX(t) and Q * e(u,w). Thus, e(u,w) ∈ EgX(t) due to u,w ∈ V(t) = VgX(t) by Lemma 5.\nAssume that e(u,w) is added by ConvEdge(V+(t)). Suppose that there is Q ⊂ ΓX(t) and Q * e(u,w) so that for some p ∈ e(u,w), p is a proper convex combination of Q. Since p ∈ e(u,w) ∈ ConvEdge(V+(t)) ⊂ Pxv , Q ⊂ ΓX(t) ∩ Pxv . Since e(u,w) is an edge of the convex hull of V+(t), any p ∈ e(u,w) cannot be a proper convex combination of Q, which is a contradiction.\n(⇐=) Let e(u,w) ∈ EgX(t) . Suppose that e(u,w) /∈ E(t). If u ∈ V− or w ∈ V−, it is a contradiction to e(u,w) ∈ EgX(t) ⊂ 2ΓX(t) . If both u,w ∈ V+(t), by the definition of ConvEdge(V+), and the fact that e(u,w) /∈ E(t), e(u,w) /∈ EgX(t) , which is a contradiction. Therefore one of u,w belongs to V(t − 1) ∩ V(t), and the other belongs to V+(t). Without loss of generality, let u ∈ V(t − 1) ∩ V(t) and w ∈ V+(t), then u must be strictly below Pxv . Then, e(u,w) /∈ E(t − 1). There is Q ∈ ΓX(t−1) so that Q * e(u,w) and for some p ∈ e(u,w), p is a proper convex combination of Q. If all q ∈ Q are strictly above Pxv , p is also strictly above Pxv . If for all q ∈ Q, zq ≤ Pxv (λq), Q ⊂ ΓX(t), implying a contradiction to e(u,w) ∈ EgX(t) . Thus, at least one of Q is strictly below Pxv , and let Q − be the set of such elements of Q. Also at least one of Q is\nstrictly above Pxv , and let Q + be the set of such elements of Q. Let P be the set of intersections of e(q−, q+) and Pxv where q − ∈ Q− and q+ ∈ Q+. If |P | > 1 or p /∈ P , p is a proper convex combination of Q− and P . Thus, |Q−| = |Q+| = 1 and p ∈ Pxv . This holds for all p ∈ e(u,w), which means that e(u,w) ∈ Pxv . This is a contradiction to the fact that u is strictly below Pxv .\nA.2 Proof of Theorem 1\nProof. Let G = (V, E) be the skeleton at the end of DualSearch. It holds that X ⊆ χg for each iteration by the algorithm. Suppose that there is x∗ ∈ χg\\X when the algorithm terminates. Then, there is λ∗ ∈ S such that Px∗(λ ∗) < Px(λ ∗) for every x ∈ X. Also there is x̂ ∈ X such that Px̂(λ∗) = gX(λ∗). Then, (λ∗, Px̂(λ ∗)) can be represented as a convex combination of V ′ ⊂ V such that Px̂(λv) = gX(λv) for all v ∈ V ′. By a property of the algorithm, Px∗(λv) ≥ Px̂(λv) = gX(λv) = g(λv) for each v ∈ V ′. Since λ∗ is a convex combination of {λv : v ∈ V ′}, we have Px∗(λ∗) ≥ Px̂(λ∗). This implies a contradiction to Px∗(λ ∗) < Px̂(λ ∗).\nA.3 Proof of Theorem 2\nProof. For each iteration, there is exactly one oracle call. Let C be a set of confirmed vertices u, that is, zu = g(λu). Note that at each iteration, either |C| or |X| increases by one, depending on whether Pxv (λv) < zv, and confirmed vertices are never removed from V. When the algorithm terminates, |X| = |χg| by Theorem 1 and |C| = |Vg| by Corollary 1. Thus, the algorithm uses |Vg|+ |χg| number of oracle calls.\nA.4 Proof of Lemma 3\nProof. First we prove that for every iteration, a chosen v satisfies that zv = maxλ gX(λ). It initially holds by the definition of v. Assume that at the (t− 1)-th iteration, the statement holds. Let v be chosen in the t-th iteration. If Pxv (λv) ≥ zv, there is no change on X and G so that the statement holds, and the algorithm terminates. When Pxv (λv) < zv, let v̂ ∈ V+ be such that zv̂ ≥ zu for all u ∈ V+. Suppose that there is v̄ ∈ V(t) such that zv̄ > zv̂ and zv̄ is the maximum value of gX(t). Note that zv is the maximum value of gX(t−1) by the assumption. Suppose zv̄ = zv, and let P ⊆ V(t − 1) be the set such that for every p ∈ P , zp = zv. Since v̄ ∈ V(t), zv̄ ≤ Pxv (λv̄) and zv > Pxv (λv). Then, some edge between two vertices of P should intersect with Pxv due to the concavity of gX(t), and let v\n′ be the intersection. Then, zv = zv′ = zv̂, which is a contradiction to zv̄ > zv̂.\nIn GX(t−1), since v̄ is not the maximum, and by the concavity of gX(t−1), there is at least one edge e(v̄, v̄′) where v̄′ ∈ V(t−1) such that zv̄′ > zv̄. In order that zv̄ becomes the maximum of gX(t), v̄′ should not belong to V(t), implying that zv̄′ > Pxv (λv̄′). Then, there is intersection q of Pxv and e(v̄, v̄′), implying that zq ≥ zv̄. If zq > zv̄, it is a contradiction to the fact that zv̄ is the maximum value of gX(t) due to q ∈ V(t). If zq = zv̄, it means that zv̄ ∈ V+ and thus zv̄ = zv̂, which is a contradiction to zv̄ > zv̂.\nWe have proved that when the algorithm terminates, zv = maxλ gX(λ) ≥ maxλ g(λ). Let the last v be v∗. In the last iteration, zv∗ = Pxv∗ (λ ∗) = g(λ∗) ≤ maxλ g(λ).\nB Segmentation results for Table 2"
    } ],
    "references" : [ {
      "title" : "The quickhull algorithm for convex hulls",
      "author" : [ "C.B. Barber", "D.P. Dobkin", "H. Huhdanpaa" ],
      "venue" : "ACM Transactions on Mathematical Software (TOMS)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1996
    }, {
      "title" : "Pseudo-boolean optimization",
      "author" : [ "E. Boros", "P. Hammer" ],
      "venue" : "Discrete Applied Mathematics",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "Fast approximate energy minimization via graph cuts",
      "author" : [ "Y. Boykov", "O. Veksler", "R. Zabih" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "CPMC: Automatic object segmentation using constrained parametric min-cuts",
      "author" : [ "J. Carreira", "C. Sminchisescu" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "A fast parametric maximum flow algorithm and applications",
      "author" : [ "G. Gallo", "M. Grigoriadis", "R. Tarjan" ],
      "venue" : "SIAM J. on Comput",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1989
    }, {
      "title" : "Efficient inference with cardinality-based clique potentials",
      "author" : [ "R. Gupta", "A.A. Diwan", "S. Sarawagi" ],
      "venue" : "Z. Ghahramani (ed.) ICML, ACM International Conference Proceeding Series,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "Community detection as an inference problem",
      "author" : [ "M.B. Hastings" ],
      "venue" : "Phys. Rev. E 74,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2006
    }, {
      "title" : "Transformation of general binary MRF minimization to the first-order case",
      "author" : [ "H. Ishikawa" ],
      "venue" : "PAMI 33,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Submodular function minimization under covering constraints",
      "author" : [ "S. Iwata", "K. Nagano" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "A convex framework for image segmentation with moment constraints",
      "author" : [ "A. Klodt", "D. Cremers" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Integration of multiview stereo and silhouettes via convex functionals on convex domains",
      "author" : [ "K. Kolev", "D. Cremers" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "Application of parametric maxflow in computer vision",
      "author" : [ "V. Kolmogorov", "Y. Boykov", "C. Rother" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Minimizing non-submodular functions with graph cuts - a review",
      "author" : [ "V. Kolmogorov", "C. Rother" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "MRF optimization via dual decomposition: message-passing revisited",
      "author" : [ "N. Komodakis", "N. Paragios", "G. Tziritas" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Graph cut based inference with co-occurrence statistics",
      "author" : [ "L. Ladicky", "C. Russell", "P. Kohli", "P. Torr" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Image segmentation with a bounding box prior",
      "author" : [ "V. Lempitsky", "P. Kohli", "C. Rother", "T. Sharp" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Markov random filed models in computer vision",
      "author" : [ "S.Z. Li" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1994
    }, {
      "title" : "Energy minimization under constraints on label counts",
      "author" : [ "Y. Lim", "K. Jung", "P. Kohli" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    }, {
      "title" : "A simple efficient approximation scheme for the restricted shortest path problem",
      "author" : [ "D.H. Lorenz", "D. Raz" ],
      "venue" : "Operations Research Letters 28,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1999
    }, {
      "title" : "Size-constrained submodular minimization through minimum norm base",
      "author" : [ "K. Nagano", "Y. Kawahara", "K. Aihara" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2011
    }, {
      "title" : "Balanced metric labeling",
      "author" : [ "J. Naor", "R. Schwartz" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2005
    }, {
      "title" : "Global connectivity potentials for random field models",
      "author" : [ "S. Nowozin", "C. Lampert" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Maximal closure of a graph and applications to combinatorial problems",
      "author" : [ "J.C. Picard" ],
      "venue" : "Management Science",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1976
    }, {
      "title" : "Exact inference in multi-label CRFs with higher order cliques",
      "author" : [ "S. Ramalingam", "P. Kohli", "K. Alahari", "P. Torr" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    }, {
      "title" : "High resolution matting via interactive trimap segmentation",
      "author" : [ "C. Rhemann", "C. Rother", "A. Rav-Acha", "T. Sharp" ],
      "venue" : null,
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "grabcut”: interactive foreground extraction using iterated graph cuts",
      "author" : [ "C. Rother", "V. Kolmogorov", "A. Blake" ],
      "venue" : "ACM Trans. Graph",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2004
    }, {
      "title" : "Multi-view reconstruction using photo-consistency and exact silhouette constraints: A maximum-flow formulation",
      "author" : [ "S. Sinha", "M. Pollefeys" ],
      "venue" : null,
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2005
    }, {
      "title" : "Parallel and distributed graph cuts by dual decomposition",
      "author" : [ "P. Strandmark", "F. Kahl" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2010
    }, {
      "title" : "Random field model for integration of local information and global information",
      "author" : [ "T. Toyoda", "O. Hasegawa" ],
      "venue" : "PAMI 30,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2008
    }, {
      "title" : "Graph cut based image segmentation with connectivity priors",
      "author" : [ "S. Vicente", "V. Kolmogorov", "C. Rother" ],
      "venue" : null,
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2008
    }, {
      "title" : "MAP estimation via agreement on trees: message-passing and linear programming",
      "author" : [ "M. Wainwright", "T. Jaakkola", "A. Willsky" ],
      "venue" : "IEEE Transactions on Information Theory",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2005
    }, {
      "title" : "MAP estimation, linear programming and belief propagation with convex free energies",
      "author" : [ "Y. Weiss", "C. Yanover", "T. Meltzer" ],
      "venue" : "UAI",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2007
    }, {
      "title" : "High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (MAP-MRF)",
      "author" : [ "T. Werner" ],
      "venue" : null,
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2008
    }, {
      "title" : "A global perspective on MAP inference for low-level vision",
      "author" : [ "O. Woodford", "C. Rother", "V. Kolmogorov" ],
      "venue" : null,
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2009
    }, {
      "title" : "A global perspective on map inference for low-level vision",
      "author" : [ "O.J. Woodford", "C. Rother", "V. Kolmogorov" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "Markov Random Fields (MRF) is an undirected graphical model, which has been extensively studied and used in various fields, including statistical physics [11], and computer vision [19].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 1,
      "context" : "However, a number of subclasses of MRFs have been isolated for which the problem can be solved in polynomial time [2].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 31,
      "context" : "Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 30,
      "context" : "Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem.",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Further, a number of heuristics or approximation algorithms based on belief propagation [34], tree reweighted message passing [33], and graph-cut [3] have also been proposed for the problem.",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 14,
      "context" : "Such algorithms are widely used for various problems in machine learning and computer vision [38, 17].",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 6,
      "context" : "Another example is community detection in a network [8] where we may know the number of nodes belonging to each community.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 11,
      "context" : "However, recent work in computer vision has shown that this problem can be handled efficiently using the parametric mincuts [14] which allow simultaneous computation of exact solutions for some constraint instances.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : "For instance, linear relaxation approaches were adopted to handle bounding-box and connectivity constraints defined on the labelling [18, 24].",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 21,
      "context" : "For instance, linear relaxation approaches were adopted to handle bounding-box and connectivity constraints defined on the labelling [18, 24].",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 9,
      "context" : "Further, Klodt and Cremers [12] proposed a convex relaxation framework to deal with moment constraints.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 2,
      "context" : "A graph-cut algorithm [3] is a popular example of such an oracle.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "In fact, our algorithm generalizes the (one-dimensional) parametric mincuts [5, 14] to multiple-dimensions.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "In fact, our algorithm generalizes the (one-dimensional) parametric mincuts [5, 14] to multiple-dimensions.",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "In contrast to the parametric mincuts [5], our algorithm can deal with multiple constraints simultaneously, including some non-linear constraints (as we show in the paper).",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "to obtain different segmentation results for image segmentation as done in [4].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 33,
      "context" : "Among them, solutions to image labelling problems which have a particular distribution of labels [36] or satisfy a topological property like connectivity [32] have been widely studied.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 29,
      "context" : "Among them, solutions to image labelling problems which have a particular distribution of labels [36] or satisfy a topological property like connectivity [32] have been widely studied.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 11,
      "context" : "More specifically, for the problem of foreground-background image segmentation, most probable segmentations under the label count constraint have been shown to be closer to the ground truth [14, 20].",
      "startOffset" : 190,
      "endOffset" : 198
    }, {
      "referenceID" : 17,
      "context" : "More specifically, for the problem of foreground-background image segmentation, most probable segmentations under the label count constraint have been shown to be closer to the ground truth [14, 20].",
      "startOffset" : 190,
      "endOffset" : 198
    }, {
      "referenceID" : 10,
      "context" : "example is the silhouette constraint which has been used for the problem of 3D reconstruction [13, 29].",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 26,
      "context" : "example is the silhouette constraint which has been used for the problem of 3D reconstruction [13, 29].",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 5,
      "context" : "Recently, dual decomposition has been proposed for constrained MAP inference [7, 37].",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 34,
      "context" : "Recently, dual decomposition has been proposed for constrained MAP inference [7, 37].",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "[7] dealt with cardinality-based clique potentials and developed both exact and approximate algorithms.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 34,
      "context" : "[37] studied a problem involving marginal statistics such as the area constraint especially with convex penalties, and showed that the proposed method improves quality of solutions for various computer vision problems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "For this problem, Naor and Schwartz [23] obtained an O( lnn )-approximate algorithm where each label is assigned to at most min [ O(ln k) 1− , `+ 1 ] (1 + )` variables/nodes.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 12,
      "context" : "it can be efficiently minimized by solving a equivalent st-mincut problem [15].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 7,
      "context" : "Such f is widely used in machine learning and computer vision [9, 20].",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 17,
      "context" : "Such f is widely used in machine learning and computer vision [9, 20].",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "Let us consider the following Lagrangian dual g : R → R of f , which is widely used for discrete optimization [16, 30].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 27,
      "context" : "Let us consider the following Lagrangian dual g : R → R of f , which is widely used for discrete optimization [16, 30].",
      "startOffset" : 110,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "This soft-constrained optimization has been widely used in terms of lasso regularization and ridge regression, and also in computer vision [17, 31].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 28,
      "context" : "This soft-constrained optimization has been widely used in terms of lasso regularization and ridge regression, and also in computer vision [17, 31].",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 23,
      "context" : "For instance, the results of [26] show transformation of any multi-label submodular functions of order up to 3 to a pairwise submodular one, meaning that it can be solved by the graph-cut algorithm.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "3 For a given V+, ConvEdge(V+) can be computed, for example, by [1].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "However, in general, a minimizer of (2) is not the ground truth, and it has been shown that imposing statistics on a desired solution can improve segmentation results [12].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 17,
      "context" : "• Size: ∑ i∈V xi = b where b ∈ R [20, 35, 36].",
      "startOffset" : 33,
      "endOffset" : 45
    }, {
      "referenceID" : 32,
      "context" : "• Size: ∑ i∈V xi = b where b ∈ R [20, 35, 36].",
      "startOffset" : 33,
      "endOffset" : 45
    }, {
      "referenceID" : 33,
      "context" : "• Size: ∑ i∈V xi = b where b ∈ R [20, 35, 36].",
      "startOffset" : 33,
      "endOffset" : 45
    }, {
      "referenceID" : 9,
      "context" : "• Mean: ∑ i∈V cixi ∑ i∈V xi = b where b ∈ R and ci = (vi, hi) ∈ R denotes the vertical and horizontal coordinates of a pixel i, respectively [12].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : ": ∑ i∈V (vi−μv)(hi−μh)xi ∑ i∈V xi = b where b ∈ R and (μv, μh) ∈ R denotes the mean center of the object [12].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "Table 1: Results of DualSearch on 12 images from [27] each of which has the size 120× 120.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 25,
      "context" : "The values are averaged over 6 images from [28] of size 321× 481.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 15,
      "context" : "We also compared our algorithms with LP [18] and QP [12] relaxation based methods.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "We also compared our algorithms with LP [18] and QP [12] relaxation based methods.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "SFM is known to be polynomial time solvable and a number of studies have considered SFM under specific constraints such as vertex cover and size constraints [10, 22].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 19,
      "context" : "SFM is known to be polynomial time solvable and a number of studies have considered SFM under specific constraints such as vertex cover and size constraints [10, 22].",
      "startOffset" : 157,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "A path is feasible if its total delay is less than some threshold D [21].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 25,
      "context" : "The used images are from [28].",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 22,
      "context" : "This is also known as the maximal closure problem and can be solved in polynomial time by transforming it to a st-mincut problem [25].",
      "startOffset" : 129,
      "endOffset" : 133
    } ],
    "year" : 2013,
    "abstractText" : "In this paper, we propose novel algorithms for inferring the Maximum a Posteriori (MAP) solution of discrete pairwise random field models under multiple constraints. We show how this constrained discrete optimization problem can be formulated as a multi-dimensional parametric mincut problem via its Lagrangian dual, and prove that our algorithm isolates all constraint instances for which the problem can be solved exactly. These multiple solutions enable us to even deal with ‘soft constraints’ (higher order penalty functions). Moreover, we propose two practical variants of our algorithm to solve problems with hard constraints. We also show how our method can be applied to solve various constrained discrete optimization problems such as submodular minimization and shortest path computation. Experimental evaluation using the foreground-background image segmentation problem with statistic constraints reveals that our method is faster and its results are closer to the ground truth labellings compared with the popular continuous relaxation based methods.",
    "creator" : "LaTeX with hyperref package"
  }
}