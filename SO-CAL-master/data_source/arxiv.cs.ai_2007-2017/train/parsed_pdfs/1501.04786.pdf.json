{
  "name" : "1501.04786.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Considérant la dépendance dans la théorie des fonctions de croyance",
    "authors" : [ "Mouna Chebbah", "Mouloud Kharoune", "Arnaud Martin", "Boutheina Ben Yaghlane" ],
    "emails" : [ "Mouloud.Kharoune@univ-rennes1.fr,", "Arnaud.Martin@univ-rennes1.fr", "Mouna.Chebbah@univ-rennes1.fr", "boutheina.yaghlane@ihec.rnu.tn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Considérant la dépendance dans la théorie des fonctions de croyance\nMouna Chebbah∗ ∗∗, Mouloud Kharoune∗\nArnaud Martin∗, Boutheina Ben Yaghlane∗∗∗\n∗UMR 6074 IRISA, Université de Rennes1 / IUT de Lannion, Rue Edouard Branly BP 3021, 22302 Lannion cedex\nMouloud.Kharoune@univ-rennes1.fr, Arnaud.Martin@univ-rennes1.fr ∗∗LARODEC, ISG Tunis, 41 Rue de la Liberté, Cité Bouchoucha\n2000 Le Bardo, Tunisie Mouna.Chebbah@univ-rennes1.fr\n∗∗∗LARODEC, IHEC Carthage, Carthage Présidence 2016, Tunisie\nboutheina.yaghlane@ihec.rnu.tn\nRésumé. La fusion d’informations issues de plusieurs sources cherche à améliorer la prise de décision. La théorie des fonctions de croyance, pour réaliser cette fusion, utilise des règles de combinaison faisant bien souvent l’hypothèse forte de l’indépendance des sources. Cette hypothèse d’indépendance n’est cependant pas formalisée ni vérifiée. Nous proposons dans cet article un apprentissage de l’indépendance cognitive de sources d’information permettant de mesurer la dépendance ou l’indépendance. Cette mesure exprimée par une fonction de masse est ensuite intégrée par une approche d’affaiblissement avant de réaliser la combinaison d’informations."
    }, {
      "heading" : "1 Introduction",
      "text" : "La théorie des fonctions de croyance issue des travaux de Dempster (1967) et Shafer (1976) permet une bonne modélisation des données imprécises et/ou incertaines et offre un outil puissant pour fusionner des informations issues de plusieurs sources. Pour ce faire, les données incertaines et imprécises des différentes sources sont modélisées par des fonctions de masse et combinées afin de mettre en évidence les croyances communes et assurer une prise de décision plus fiable.\nLe choix de la règle de combinaison à appliquer repose sur des hypothèses d’indépendance de sources. En effet, certaines règles de combinaison comme celles de Dempster (1967); Smets (1990); Yager (1987); Dubois et Prade (1988) combinent des fonctions de croyance dont les sources sont supposées indépendantes par contre les règles prudente et hardie proposées par Denœux (2006) n’exigent pas d’hypothèse d’indépendance. L’indépendance cognitive est une hypothèse fondamentale pour le choix du type de règles de combinaison à appliquer. Les indépendances évidentielle, cognitive et doxastique ont été définies dans la cadre de la théorie des\nar X\niv :1\n50 1.\n04 78\n6v 1\n[ cs\n.A I]\n2 0\nJa n\n20 15\nfonctions de croyance.\nD’une part, les travaux de Ben Yaghlane et al. (2002a,b) étudient principalement l’indépendance doxastique des variables. D’autre part, les travaux de Smets (1993) et ceux de Shafer (1976) ont défini l’indépendance cognitive des variables par une absence d’implication sur la modification des croyances d’une variable en cas de changement des croyances sur l’autre variable. Autrement dit, deux variables sont cognitivement indépendantes si la connaissance de la croyance de l’une n’affecte pas celle de l’autre. Dans la littérature, l’indépendance cognitive des variables est formalisée mais l’indépendance cognitive des sources n’est pas abordée.\nCe papier est focalisé sur l’indépendance cognitive des sources, les indépendances doxastique, cognitive et évidentielle des variables ne sont pas abordées. Nous proposons une approche statistique pour l’estimation de l’indépendance cognitive de deux sources. Deux sources sont cognitivement indépendantes si elles ne communiquent pas entre elles et si elles n’ont pas le même corpus de croyance 1. La méthode proposée permet d’étudier le comportement général de deux sources et de les comparer pour déceler toute dépendance pouvant exister entre elles. Dans le cas de sources dépendantes, nous proposons d’étudier le type de cette dépendance. C’est-à-dire analyser les données de sorte à voir si les sources sont positivement dépendantes ou si elles sont négativement dépendantes. Cette mesure d’indépendance peut soit guider le choix du type de règles de combinaison, soit être intégrée dans les fonctions de masse afin de justifier l’hypothèse d’indépendance des sources.\nDans la suite de cet article, nous commençons par rappeler quelques notions de base de la théorie des fonctions de croyance. Ensuite, nous présentons dans la troisième section notre approche statistique d’estimation de l’indépendance ou de la dépendance cognitive. Lors de cette méthode, nous proposons d’appliquer l’algorithme de classification non-supervisée sur toutes les informations incertaines de chaque source et de chercher un appariement des clusters. L’indépendance des sources est estimée à partir des poids attribués à chaque couple de clusters liés. Dans la quatrième section, si les sources sont dépendantes, une étude de la nature de la dépendance est faite afin de voir si cette dépendance est positive ou négative. Cette mesure d’indépendance, dépendance positive et dépendance négative peut être prise en compte dans les fonctions de masse des sources afin de justifier l’hypothèse d’indépendance dans la cinquième section. Finalement, avant de conclure dans la septième section, nous présentons dans la sixième section les expérimentations sur des données générées aléatoirement."
    }, {
      "heading" : "2 Théorie des fonctions de croyance",
      "text" : "La théorie des fonctions de croyance initialement introduite par Dempster (1967), formalisée ensuite par Shafer (1976) est employée dans des applications de fusion d’informations. Nous présentons ci-dessous quelques principes de base de cette théorie.\n1. Le corpus de croyance est l’ensemble de connaissances ou d’informations acquises par une source."
    }, {
      "heading" : "2.1 Principes de base",
      "text" : "Soit un cadre de discernement Ω = {ω1, ω2, . . . , ωn} l’ensemble de toutes les hypothèses exclusives et exhaustives. Le cadre de discernement est aussi l’univers de discours d’un problème donné.\nL’ensemble 2Ω = {A|A ⊆ Ω} = {∅, ω1, ω2, . . . , ωn, ω1 ∪ ω2, . . . , Ω}, est l’ensemble de toutes les hypothèses de Ω ainsi que leurs disjonctions.\nUne fonction de masse est une fonction de 2Ω vers l’intervalle [0, 1] qui affecte à chaque sous-ensemble une masse de croyance élémentaire. Cette fonction de masse fournie par une source d’information 2 est une représentation des connaissances incertaines et imprécises. Formellement, une fonction de masse, notée mΩ, est définie comme suit :\nmΩ : 2Ω → [0, 1] (1)\ntel que : ∑ A⊆Ω mΩ(A) = 1 (2)\nUn sous-ensemble ayant une masse de croyance élémentaire non-nulle est un élément focal. La masse d’un élément focal A représente le degré de croyance élémentaire de la source à ce que l’hypothèse vraie soit dans A."
    }, {
      "heading" : "2.2 Combinaison",
      "text" : "Dans le cadre de la théorie des fonctions de croyance, plusieurs règles de combinaison sont proposées pour la fusion d’informations. Les fonctions de masse sont issues de différentes sources et sont définies sur le même ensemble de discernement. La combinaison permet de synthétiser ces différentes informations en vue d’une prise de décision plus fiable. Le choix des règles de combinaison dépend de certaines hypothèses initiales, les opérateurs de type conjonctif peuvent être employés lorsque les sources sont fiables et indépendantes cognitivement que nous définirons précisément à la section 3. La combinaison conjonctive s’écrit pour deux fonctions de masse mΩ1 et m Ω 2 et pour tout A ⊆ Ω par :\nmΩ∩©(A) = m Ω 1 ∩©m Ω 2 (A) = ∑ B∩C=A mΩ1 (B)×mΩ2 (C). (3)\nNotons que l’élément neutre pour cette règle est la masse : mΩ(A) = 1 si A = Ω et 0 sinon. Lorsque l’hypothèse de fiabilité est trop forte et que l’on ne peut supposer que seule une des sources est fiable, la combinaison disjonctive peut alors être employée toujours sous l’hypothèse d’indépendance cognitive :\nmΩ∪©(A) = m Ω 1 ∪©m Ω 2 (A) = ∑ B∪C=A mΩ1 (B)×mΩ2 (C). (4)\nNotons que l’élément neutre pour cette règle est la masse : mΩ(A) = 1 si A = ∅ et 0 sinon. Bien que les règles disjonctive et conjonctive soient associatives et commutatives, elles\n2. La source peut être un expert humain, un classificateur, un capteur, . . .\nne sont pas idempotentes ce qui justifie leur inefficacité à la fusion d’informations issues de sources dépendantes cognitivement. La plupart des règles de combinaison issues des règles conjonctives et disjonctives, en particulier pour répartir le conflit, supposent que les sources sont indépendantes cognitivement. Martin (2010) en rappelle quelques unes.\nDans cet article, nous utilisons la moyenne pour la combinaison. Cette règle est choisie parce qu’elle est idempotente et commutative en plus elle combine tous types de fonctions de masse. Toute autre règle de combinaison vérifiant ces critères peut être utilisée. Pour chaque élément focal A des M fonctions de masse, la masse combinée de A, mΩMoyenne(A), est calculée à partir des M masses de croyance élémentaires m Ω i (A) comme suit :\nmΩMoyenne(A) = 1\nM M∑ i=1 mΩi (A) (5)"
    }, {
      "heading" : "2.3 Conditionnement et déconditionnement",
      "text" : "Après l’acquisition d’une fonction de masse, une information certaine peut apparaître confirmant que l’hypothèse vraie est (ou n’est pas) dans l’un des sous-ensembles de 2Ω. Dans ce cas, la fonction de masse doit être mise à jour afin de prendre en considération cette nouvelle information certaine. Cette mise à jour est réalisée par l’opérateur de conditionnement qui consiste à transférer la masse attribuée à chaque élément focal à son intersection avec l’ensemble certain. Le conditionnement d’une fonction de masse mΩ par l’hypothèse A ⊂ Ω revient à transférer les masses de croyance de tous les éléments focaux de mΩ à leurs intersections avec A. La fonction de masse conditionnée mΩ[A] est donnée par Smets et Kruse (1997) comme suit :\nmΩ[A](C) =  0 for C 6⊆ A∑ B⊆Ac mΩ(C ∪B) for C ⊆ A (6)\navec Ac le complémentaire de A, Ac = Ω \\ {A}. Notons que le conditionnement sur une hypothèse du même cadre de discernement revient à la combinaison conjonctive de la fonction de masse initiale mΩ avec la nouvelle fonction de masse certaine mΩ(A) = 1 sachant que A est une hypothèse certaine, donc mΩA ∩©m\nΩ = mΩ[A]. Le déconditionnement est l’opération inverse du conditionnement qui permet de retrouver une fonction de masse la moins informative à partir d’une fonction de masse conditionnée. Étant donnée mΩ[A], la fonction de masse conditionnellement à A, il est difficile de retrouver la fonction de masse originale mais il est possible de retrouver la fonction de masse qui engage le moins (Hsia (1991) et Smets (1993)) telle que son conditionnement par A donne mΩ[A]. Le déconditionnement de mΩ[A] permet de retrouver mΩ comme suit :\nmΩ(C ∪Ac) = mΩ[A](C) ∀C ⊆ 2Ω, Ac = Ω\\A (7)"
    }, {
      "heading" : "2.4 Affaiblissement",
      "text" : "Shafer (1976) a proposé la procédure d’affaiblissement suivante :\nαmΩ(A) = α×mΩ(A) ∀A ⊂ Ω (8) αmΩ(Ω) = 1− α× (1−mΩ(Ω)) (9)\noù α est un facteur d’affaiblissement de [0, 1]. Cette procédure est généralement employée pour affaiblir les fonctions de masse par la fiabilité α de leurs sources. Cette procédure a pour effet d’augmenter la masse sur l’ignorance Ω. Smets (1993) a justifié cette procédure en considérant que :\nmΩ[F ](A) = mΩ(A) (10) mΩ[F̄ ](A) = mΩ(X) (11)\noùmΩ(X) = 1 siX = Ω et 0 sinon, F et F̄ représentent la fiabilité et la non fiabilité etmΩ[F ] est une fonction de masse conditionnellement à la fiabilité F . Soit F = {F, F̄} le cadre de discernement correspondant, et mF la fonction de masse représentant la connaissance sur la fiabilité de la source : {\nmF (F ) = α mF (F) = 1− α. (12)\nAfin de combiner les deux sources d’informations fournissant les deux fonctions de masse mΩ[F ] et mF , il faut pouvoir les représenter dans le même espace Ω×F . Ainsi, nous devons effectuer une extension à vide sur la fonction de massemF , opération que l’on notemF↑Ω×F :\nmF↑Ω×F (Y ) = { mF (X) si Y = Ω×X, X ⊆ F 0 sinon (13)\nDans le cas de la fonction de masse mΩ[F ], il faut déconditionner :\nmΩ[F ]⇑Ω×F ((A× F ) ∪ (Ω× F )) = mΩ [F ] (A) , A ⊆ Ω (14)\nIl est ainsi possible d’effectuer la combinaison :\nmΩ×F∩© (B) = m F↑Ω×F ∩©mΩ[F ]⇑Ω×F (B), ∀B ⊂ Ω×F (15)\nEnsuite il faut marginaliser la fonction de masse obtenue pour revenir dans l’espace Ω :\nmΩ×F↓Ω (A) = ∑\n{B⊆Ω×F |Proj(B↓Ω)=A}\nmΩ×F∩© (B) (16)\noù Proj (Y ↓ Ω) est la projection de Y sur Ω. Nous retrouvons ainsi :\nαmΩ(A) = mΩ×F↓Ω (A) (17)\nMercier (2006) a proposé une extension de cet affaiblissement en contextualisant le coefficient d’affaiblissement α en fonction des sous-ensembles de Ω. Smets (1993) détaille l’extension à vide, le déconditionnement ainsi que la marginalisation."
    }, {
      "heading" : "2.5 Transformation pignistique",
      "text" : "La prise de décision dans la théorie des fonctions de croyance est fondée sur des probabilités pignistiques issues de la transformation pignistique proposée par Smets (2005). Cette transformation calcule une probabilité pignistique à partir des fonctions de masse en vue de\nprendre une décision. Si un expert fournit une fonction de masse reflétant son avis sur la solution d’un problème précis, la probabilité pignistique reflète la probabilité de chaque hypothèse. La probabilité pignisitique BetP d’un élément A ∈ Ω est calculée comme suit :\nBetP (A) = ∑\nC∈2Ω,C 6=∅\n|A ∩ C| |C| mΩ(C) 1−mΩ(∅) . (18)\nLa décision peut être prise suivant le principe du maximum de la probabilité pignistique."
    }, {
      "heading" : "2.6 Classification non-supervisée",
      "text" : "Nous proposons ici d’utiliser un algorithme de classification non-supervisée de type Cmoyenne, utilisant une distance sur les fonctions de masse définie par Jousselme et al. (2001) comme proposé par Ben Hariz et al. (2006), Chebbah et al. (2012a) et Chebbah et al. (2012b). Soit un ensemble T contenant n objets oi : 1 ≤ i ≤ n à classifier dans C clusters. Les valeurs des oi sont des fonctions de masse mΩi définies sur un cadre de discernement Ω. Le but est de classifier les n fonctions de masse valeurs des objets oi. Les fonctions de masse mΩi sont fournies par la même source, c’est-à-dire qu’une même source a attribué les valeurs des objets oi. Appliquer un algorithme de classification non-supervisée sur ces fonctions de masse revient à regrouper les fonctions de masse ayant des éléments focaux non contradictoires. Une mesure de dissimilarité D(oi, Clk) permet de mesurer la dissimilarité entre un objet oi et un cluster Clk comme suit :\nD(oi, Clk) = 1\nnk nk∑ j=1 d(mΩi ,m Ω j ) (19)\nd(mΩ1 ,m Ω 2 ) =\n√ 1\n2 (mΩ1 −mΩ2 )tD(mΩ1 −mΩ2 ), (20)\nD(A,B) = { 1 si A = B = ∅ |A∩B| |A∪B| ∀A,B ∈ 2 Ω (21)\nLa dissimilarité d’un objet oi et un cluster Clk est définie par la moyenne des distances entre la fonction de masse mΩi valeur de cet objet et toutes les nk fonctions de masse valeurs des oj : 1 ≤ j ≤ nk objets contenus dans le cluster Clk. Chaque objet est affecté au cluster qui lui est le plus similaire (ayant une valeur de dissimilarité minimale) de manière itérative jusqu’à ce qu’une répartition stable soit obtenue.\nÀ la fin de la classification non-supervisée, C clusters contenant chacun un certain nombre d’objets sont obtenus. Dans cet article nous supposons que le nombre de clusters C soit égal à la cardinalité du cadre de discernement (C = |Ω|) puisque Ω représente les classes possibles dans un problème de classification. L’utilisation de cette mesure de dissimilarité pour la classification assure le regroupement des objets dont les valeurs (fonctions de masse) ne sont pas contradictoires, c’est-à-dire les éléments focaux sont compatibles et intersectent.\nNotons que l’algorithme de classification non-supervisée est fondée sur une distance sur les fonctions de masse et non pas sur des centres mobiles comme proposé par Ben Hariz et al. (2006) parce que la distance est fortement sensible au nombre d’éléments focaux des centres. Il suffit d’avoir des fonctions de masse avec différents éléments focaux dans le même cluster pour que la fonction de masse combinée (le centre) ait beaucoup d’éléments focaux.\n3 Indépendance\nL’indépendance a été introduite en premier dans le cadre de la théorie des probabilités pour modéliser l’indépendance statistique des évènements. Avec les probabilités, deux évènements A et B sont indépendants si P (A ∩B) = P (A)× P (B) ou encore si P (A|B) = P (A).\nLes fonctions de masse peuvent être perçues comme des probabilités subjectives fournies par des sources s’exprimant sur un problème étant donné un ensemble de connaissances ou d’informations appelé corpus de croyance. Dans le cas d’une hypothèse d’indépendance cognitive des sources, les corpus de croyance doivent être distincts et aucune communication entre les sources n’est tolérée.\nShafer (1976) définit l’indépendance cognitive des variables comme étant le non changement des croyances de l’une des variables si une nouvelle croyance élémentaire apparaît sur l’autre. Il définit également l’indépendance évidentielle, deux variables sont évidentiellement indépendantes par rapport à une fonction de masse si cette fonction de masse peut être retrouvée en combinant les fonctions de masse de ces variables.\nDéfinition 1. \"Two frames of discernment may be called cognitively independent with respect to the evidence if new evidence that bears on only one of them will not change the degrees of support for propositions discerned by the other\" 3 (Shafer (1976), page 149).\nDéfinition 2. \"Two frames of discernment are evidentially independent with respect to a support function if that support function could be obtained by combining evidence that bears on only one of them with evidence that bears on only the other\" 4 (Shafer (1976), page 149).\nDans ce papier, nous nous intéressons à l’indépendance des sources et non pas celle des variables.\nDéfinition 3. Deux sources sont cognitivement indépendantes si elles ne communiquent pas et si leurs corpus de croyance sont distincts.\nNous définissons la dépendance cognitive comme étant la ressemblance du comportement général de deux sources dû à une dépendance de connaissances ou à une éventuelle communication entre elles. Nous proposons une démarche statistique afin d’étudier l’indépendance cognitive de deux sources. Le but étant soit de tenir compte de cette indépendance dans les fonctions de masse comme détaillé dans la section 5, soit faire les hypothèses adaptées pour le choix du type de règles de combinaison à appliquer. Nous introduisons d’abord la mesure d’indépendance de deux sources S1 et S2, notée Id(S1, S2), comme étant l’indépendance de S1 de S2. Cette mesure vérifie les axiomes suivants :\n1. Non-négative : L’indépendance d’une source S1 de S2, Id(S1, S2) est une valeur qui est, soit nulle si S1 est complètement dépendante de S2, soit strictement positive.\n3. Deux cadres de discernement sont cognitivement indépendants par rapport à une croyance si toute nouvelle croyance apparaissant sur l’un n’affecte pas la croyance sur l’autre.\n4. Deux cadres de discernement sont évidentiellement indépendants par rapport à une fonction de masse si cette fonction de masse est obtenue en combinant les croyances sur ces cadres de discernement.\n2. Normalisée : Id(S1, S2) ∈ [0, 1], si Id est nulle alors S1 est complètement dépendante de S2. Si Id = 1, alors S1 est complètement indépendante de S2 autrement c’est un degré de ]0, 1[.\n3. Non-symétrique : Si S1 est indépendante de S2, cela n’implique pas forcement que S2 soit indépendante de S1. S1 et S2 peuvent être simultanément indépendantes avec des degrés d’indépendance égaux ou différents.\n4. Identité : Id(S1, S1) = 0, toute source est complètement dépendante d’elle même.\nSi deux sources S1 et S2 sont dépendantes, alors des éléments focaux similaires sont choisis pour s’exprimer sur des objets similaires. L’approche proposée dans ce papier est une approche statistique pour mesurer le degré d’indépendance cognitive. Nous proposons ainsi de classifier toutes les fonctions de masse des deux sources et de comparer les clusters obtenus. La classification non supervisée regroupe les objets ayant pour valeurs des fonctions de masse similaires.\nSi les clusters des deux sources sont similaires, c’est que les sources ont tendance à choisir des éléments focaux similaires voire non contradictoires pour les mêmes objets, alors il est fort probable qu’elles soient dépendantes. Si les clusters sont fortement liés c’est-à-dire qu’ils contiennent des fonctions de masse relatives aux mêmes objets, alors les sources sont dépendantes cognitivement."
    }, {
      "heading" : "3.1 Appariement des clusters",
      "text" : "Dans de nombreuses applications, plusieurs sources s’expriment sur la même problématique et fournissent différentes fonctions de masse comme valeurs aux mêmes objets. L’algorithme de classification non-supervisée est appliqué aux fonctions de masse de chaque source séparément et puis ces clusters doivent être comparés dans le but de voir s’il y a un lien entre eux. Plus les liens entre ces clusters sont forts plus les sources ont tendance à être dépendantes.\nSoient deux sources S1 et S2, fournissant chacune n fonctions de masse pour les mêmes objets. Ceci exprime le fait que la fonction de masse mΩi fournie par S1 et celle fournie par S2 se réfèrent au même objet oi. Après avoir classifié les fonctions de masse de S1 et S2, la matrice de correspondance des clusters M est obtenue par :\nM1 =  β11,1 β 1 1,2 . . . β 1 1,C . . . . . . . . . . . . β1k,1 β 1 k,2 . . . β 1 k,C\n. . . . . . . . . . . . β1C,1 β 1 C,2 . . . β 1 C,C\n and M2 =  β21,1 β 2 1,2 . . . β 2 1,C . . . . . . . . . . . . β2k,1 β 2 k,2 . . . β 2 k,C\n. . . . . . . . . . . . β2C,1 β 2 C,2 . . . β 2 C,C  (22) avec\nβiki,kj = |Cliki ∩ Cl j kj |\n|Cliki | (23)\nNotons que βiki,kj est la similarité des clusters Cl i ki de Si et Cl j kj de Sj par rapport à Si avec {i, j} ∈ {1, 2} et i 6= j.\nLa similarité de deux clusters est la proportion des objets classés simultanément dans Cliki et Cljkj par rapport à la cardinalité du cluster de la source référente. Si par exemple, le cluster Cl11 de S1 contient les 5 objets {o1, o5, o9, o12, o15} et le clusterCl25 de S2 contient les 3 objets\n{o1, o2, o9}. β11,5 est la similarité de Cl25 de S2 par rapport à Cl11 de S1 représentant la proportion des objets deCl25 qui sont aussi classés dansCl 1 1 de S1 donc β 1 1,5 = 2 5 et β 2 1,5 = 2 3 . Notons que l’algorithme de classification non-supervisée est appliqué sur les mêmes objets ayant différentes valeurs puisqu’elles sont fournies par deux sources différentes S1 et S2. Les matrices M1 etM2 sont ainsi différentes puisque les clusters de S1 et ceux de S2 sont différents. Notons également que d’autres coefficients de similarité peuvent aussi être utilisés. Une fois les deux matrices de correspondances M1 et M2 calculées, une correspondance entre les clusters est établie suivant l’algorithme 1. Chaque cluster est lié au cluster qui lui est le plus similaire, ayant le β maximal, en vérifiant que deux clusters de la même source ne peuvent pas être liés au même cluster de l’autre source. Par exemple, nous ne pouvons pas avoir les deux clusters Cl11 et Cl 1 3 de S1 liés au même cluster Cl 2 3 de S2. Pour déterminer la correspondance des clusters pour chaque source, les deux clusters ayant la plus grande similarité β sont reliés et écartés de l’ensemble des clusters à apparier. La recherche de correspondances des clusters est faite pour les deux sources. Deux correspondances différentes peuvent être obtenues pour les deux sources.\nAlgorithm 1 Appariement de clusters Require: M matrice de correspondances.\n1: while M est non vide do 2: Rechercher le maximum de M ainsi que les indices l et c du maximum. 3: Apparier les clusters l et c. 4: Enlever la ligne l et la colonne c de M . 5: end while 6: return Un appariement de clusters."
    }, {
      "heading" : "3.2 Indépendance des clusters",
      "text" : "Une fois la correspondance des clusters établie, une fonction de masse définissant l’indépendance de chaque couple de cluster est déduite. Ceci revient à avoir un agent ayant les correspondances des clusters (ki, kj) avec les similarités correspondantes βiki,kj comme corpus de croyance pour s’exprimer sur l’indépendance de ces clusters. Pour résumer, nous supposons que les deux sources S1 et S2 fournissent n fonctions de masse comme valeurs aux n objets oi. Les fonctions de masse des deux sources sont classifiées chacune à part en utilisant l’algorithme de classification détaillé dans la section 3.1. Différents C clusters sont le résultat de classification des fonctions de masse de S1 et ceux de S2. Après appariement de clusters, les clusters de S1 sont liés aux clusters de S2 qui leur sont similaires et ceux de S2 sont également liés aux clusters de S1 les plus similaires. Différents appariements sont obtenus pour S1 et S2. Rappelons que le but est d’estimer l’indépendance cognitive des sources à partir d’un ensemble de fonctions de masse fourni par chacune.\nDans cette section, nous définissons l’indépendance de chaque couple de clusters liés (k1, k2) comme une fonction de masse définie sur le cadre de discernement I = {Ī , I}, où\nĪ représente la dépendance et I l’indépendance : mIki,kj (I) = α i ki,kj (1− βiki,kj ) mIki,kj (Ī) = α i ki,kj\nβiki,kj mIki,kj (I) = 1− α i ki,kj\n(24)\nLe coefficient αiki,kj est un facteur d’affaiblissement utilisé pour tenir compte du nombre d’objets contenus dans les clusters de la source référente. Si deux clusters contenant très peu de fonctions de masse sont reliés et que deux autres clusters contenant beaucoup plus d’objets sont aussi reliés avec un même degré de similarité, les fonctions de masse des deux couples de clusters ne doivent pas avoir un même poids. Bien que ce facteur ne soit pas encore défini, il dépend du nombre d’objets dans le cluster de la source référente ainsi que le nombre total d’objets (fonctions de masse).\nUne fonction de masse est définie pour chaque couple de clusters appariés pour chacune des sources. Pour avoir une fonction de masse sur l’indépendance globale de chaque source, toutes ces fonctions de masse sont combinées avec la moyenne. Pour résumer, les C clusters de S1 sont appariés aux C clusters de S2, une fonction de masse est ainsi obtenue pour chaque couple de clusters reliés afin de refléter leur degré d’indépendance. C fonctions de masse sont alors obtenues pour chaque source. Notons que toute autre règle 5 idempotente et commutative peut être utilisée pour combiner ces fonctions de masse. L’idempotence de la règle est exigée parce que l’indépendance des clusters appariés peut être la même, l’indépendance de la source doit aussi être égale à l’indépendance des clusters dans ce cas. La règle de combinaison doit également être commutative puisqu’elle mixe C fonctions de masse à la fois. La combinaison de ces C fonctions de masse est une fonction de masse mIi décrivant l’indépendance globale de la source Si par rapport à Sj : mIi (I) = 1 C C∑ ki=1 mIki,kj (I) mIi (Ī) = 1 C C∑ ki=1 mIki,kj (Ī) mIi (Ī ∪ I) = 1C C∑\nki=1\nmIki,kj (Ī ∪ I)\n(25)\nou encore :  mIi (I) = 1 C C∑ ki=1 αiki,kj (1− β i ki,kj ) mIi (Ī) = 1 C C∑ ki=1 αiki,kjβ i ki,kj mIi (Ī ∪ I) = 1C C∑\nki=1\n(1− αiki,kj )\n(26)\n5. La règle doit permettre de combiner tous types de fonctions de masse (la règle prudente de Denœux (2008) ne peut pas être utilisée dans ce cas puisqu’elle est limitée à la combinaison des fonctions de masse non dogmatiques).\nLes probabilités pignistiques calculées à partir de la fonction de masse combinée permettent la prise de décision sur l’indépendance des sources. L’indépendance de la source S1 de la source S2, Id(S1, S2) n’est autre que la probabilité pignistique de I , Id(S1, S2) = BetP (I) et Īd(S1, S2) = BetP (Ī) ce qui revient à écrire Id comme suit : Id(Si, Sj) = 1 C C∑ ki=1 [αiki,kj β i ki,kj + 1 2 (1− αiki,kj )] Īd(Si, Sj) = 1 C\nC∑ ki=1 [αiki,kj (1− β i ki,kj ) + 1 2 (1− αiki,kj )]\n(27)\nSi Id(S1, S2) < Īd(S1, S2), alors S1 est dépendante de S2, dans le cas contraire S1 est indépendante de S2. Id n’est pas forcement symétrique, c’est-à-dire que le cas où Id(S1, S2) 6= Id(S2, S1) peut être fréquent puisque la correspondance des clusters est différente pour S1 et S2, ainsi ces fonctions de masse d’indépendance des clusters liés sont aussi différentes. Cette propriété permet par exemple de mettre en évidence une indépendance de S1 par rapport à S2 et une indépendance de S2 par rapport à S1."
    }, {
      "heading" : "4 Dépendance positive ou négative",
      "text" : "La mesure Id(S1, S2) informe sur l’indépendance ou a contrario la dépendance de la source S1 par rapport à la source S2 permettant par exemple de choisir la règle de combinaison à utiliser ou encore intégrer cette information dans ses fonctions de masse. Quant au moins l’une des sources S1 ou S2 est dépendante de l’autre (Id(S1, S2) < Īd(S1, S2) ou Id(S2, S1) < Īd(S2, S1)), il est alors préférable d’utiliser les règles de Denœux (2006); Boubaker et al. (2013); Elouedi et Mellouli (1998) sinon les règles de Dubois et Prade (1988); Martin et Osswald (2007); Murphy (2000); Smets et Kennes (1994); Yager (1987) permettent par exemple de redistribuer la masse de l’ensemble vide. Dans le cas de sources dépendantes Id n’est pas suffisante pour indiquer le type de la dépendance.\nDeux sources dépendantes peuvent être positivement ou négativement dépendantes. Si S1 est dépendante de S2, elle peut soit avoir les mêmes croyances si elle lui est positivement dépendante soit avoir des croyances contradictoires si elle lui est négativement dépendante.\nSi par exemple Id(S1, S2) < Īd(S1, S2), alors S1 est dépendante de S2 ce qui signifie que les clusters de S1 ressemblent aux clusters de S2. Nous définissons une mesure de conflit entre les clusters de S1 et S2 quantifiant cette dépendance que nous qualifions de positive ou négative. Si les clusters liés ne sont pas conflictuels alors S1 est positivement dépendante de S2 sinon elle est négativement dépendante. Nous définissons alors le conflit entre les deux clusters dépendants Cliki et Cl j kj\n({i, j} ∈ {1, 2} et i 6= j) à partir de la moyenne des distances entre les fonctions de masse des objets en commun :\n Conf(Cliki , Cl j kj ) = 1 |Cliki∩Cl j kj | ∑ l∈E(Cliki ,Cl j kj ) d(mΩ,il ,m Ω,j l ) si |Cliki ∩ Cl j kj | 6= 0\n1 sinon (28)\navec\nE(Cliki , Cl j kj ) = {l ∈ [1, n], n = |Cliki ∩ Cl j kj |,mΩ,il ∈ Cl i kiet m Ω,j l ∈ Cl j kj } (29)\nCette mesure de conflit est la moyenne des conflits entre les objets contenus dans les clusters Cliki et Cl j kj\nce qui explique le fait de ne considérer que les objets communs. Il ne peut s’agir de conflit entre sources que lorsqu’elles s’expriment sur les mêmes problèmes c’est à dire les mêmes objets. Le conflit est calculé pour chaque couple de clusters liés. Alors si Cl11 de S1 est lié à Cl25 de S2 et que S1 est dépendante de S2 alors le conflit entre Cl 1 1 et Cl 2 5 est la moyenne des distances des fonctions de masse relatives aux objets qui sont à la fois dans Cl11 et Cl 2 5. Une fonction de masse définie sur le cadre de discernement P = {I, P, P̄} (où P représente la dépendance positive et P̄ la dépendance négative) décrivant cette dépendance est obtenue pour chaque couple de clusters :{\nmPki,kj [Ī](P ) = 1− Conf(Cl i ki , Cljkj ) mPki,kj [Ī](P̄ ) = Conf(Cl i ki , Cljkj )\n(30)\nNotons que le conflit entre les clusters reflète la contradiction entre ces clusters. Puisque les clusters de chaque source groupent des fonctions de masse ayant les éléments focaux non contradictoires, alors le conflit mesuré par l’équation (28) compare les clusters en mesurant la contradiction entre des éléments focaux des fonctions de masse des deux clusters. Plus le conflit est important, plus les sources sont dépendantes négativement mais par contre moins il est important plus les sources sont dépendantes positivement.\nNotons que ces fonctions de masse sont conditionnelles puisque la dépendance positive ou négative des clusters n’est mesurée qu’avec une forte hypothèse de dépendance des clusters liés. L’hypothèse de dépendance ou encore de non indépendance des clusters explique le fait que les fonctions de masse de l’équation (30) soient conditionnées sur Ī ou encore sur {P∪P̄}. Afin de pouvoir combiner les fonctions de masse (24) et (30) pour tenir compte du degré de dépendance des clusters dans la fonction de masse de la dépendance positive ou négative, il faut déconditionner les fonctions de masse conditionnelles et redéfinir les deux fonctions de masse sur un cadre le discernement commun P . Le déconditionnement de la fonction de masse conditionnelle de l’équation (30) utilisant l’équation (7) permet de retrouver la fonction de masse la moins informative en supprimant l’hypothèse forte sur la dépendance de tous les clusters liés. Les fonctions de masse obtenues sont alors :{\nmPki,kj (P ∪ I) = 1− Conf(Cl i ki , Cljkj ) mPki,kj (P̄ ∪ I) = Conf(Cl i ki , Cljkj )\n(31)\nLe cadre de discernement I = {Ī , I} peut être raffiné en raffinant l’hypothèse Ī = {P ∪ P̄}, ceci mènera au cadre de discernement raffiné P . Les fonctions de masse marginales de la dépendance des clusters liés de l’équation (24) deviennent après raffinement : mPki,kj (I) = α i ki,kj (1− βiki,kj ) mPki,kj (P ∪ P̄ ) = α i ki,kj\nβiki,kj mPki,kj (I ∪ P ∪ P̄ ) = 1− α i ki,kj\n(32)\nNous définissons ainsi la fonction de masse de l’indépendance, dépendance positive et dépendance négative de chaque couple de clusters liés de S1 et S2 après combinaison conjonctive des fonctions de masse des équations (31) et (32) définies sur le cadre de discernement P : mPki,kj (I) = α i ki,kj (1− βiki,kj ) mPki,kj (P ) = α i ki,kj βiki,kj (1− Conf(Cl i ki , Cljkj )) mPki,kj (P̄ ) = α i ki,kj βiki,kjConf(Cl i ki , Cljkj ) mPki,kj (I ∪ P ) = (1− α i ki,kj ) (1− Conf(Cliki , Cl j kj ))\nmPki,kj (I ∪ P̄ ) = (1− α i ki,kj )Conf(Cliki , Cl j kj )\n(33)\nLa fonction de masse générale sur la dépendance de la source S1 par rapport à S2 est donnée par :\nmP(A) = 1\nC C∑ ki=1 mPki,kj (A) (34)\navec {i, j} ∈ {1, 2} et i 6= j, où k2 est le cluster de la source S2 associé au cluster k1 de la source S1. Cette fonction de masse représente ainsi l’ensemble des croyances élémentaires sur l’indépendance, la dépendance positive et négative de la source S1 face à la source S2. Cette fonction de masse est la combinaison avec la moyenne de toutes les fonctions de masse sur l’indépendance, dépendance positive et dépendance négative de tous les clusters liés.\nNotons aussi qu’une source est positivement dépendante d’elle-même puisqu’en appliquant l’algorithme de classification nous obtenons exactement la même répartition de classes ce qui impliquera Id(S, S) = 0 et mP(P ) = 1. Le degré d’indépendance est la probabilité pignistique de l’hypothèse I , BetP (I), ceux des dépendances positive et négative sont respectivement BetP (P ) et BetP (P̄ )."
    }, {
      "heading" : "5 Intégration de l’indépendance dans une fonction de masse",
      "text" : "Nous avons vu que l’indépendance est généralement une information supplémentaire nécessaire à la fusion d’informations, mais non prise en compte dans le formalisme choisi. La section 3 propose une modélisation et estimation d’une mesure d’indépendance dans le cadre de la théorie des fonctions de croyance. Nous allons ici nous appuyer sur le principe de l’affaiblissement présenté dans la section 2.4 afin de tenir compte de l’indépendance dans les fonctions de masse en vue de la combinaison.\nEn effet, lors de la combinaison conjonctive par exemple, l’hypothèse d’indépendance cognitive des sources d’informations est nécessaire. Si les sources sont dépendantes on peut penser qu’elles ne devraient pas être combinées par ce biais. Cependant, comme le montre la section 3 les sources peuvent avoir des degrés de dépendance et d’indépendance. L’information fournie sur l’indépendance n’est pas catégorique. Ainsi, combiner deux sources fortement indépendantes devraient revenir à la combinaison de deux sources indépendantes. Si une source est dépendante d’une autre source, nous pouvons considérer que cette première source ne doit pas influer la combinaison avec la seconde. Ainsi cette source doit représenter l’élément neutre de la combinaison.\nDans ce cas, il suffit d’appliquer la procédure d’affaiblissement de la section 2.4 sur la fonction de masse mΩ de la source S1 en considérant l’indépendance donnée par la fonction de masse de l’équation (34) au lieu de celle de l’équation (12) dans le cas de la fiabilité.\nÀ présent, nous distinguons la dépendance positive de la dépendance négative. Si une source est dépendante positivement d’une autre source, il ne faut pas en tenir compte et donc tendre vers un résultat de combinaison qui prendrait cette première source comme un élément neutre. Enfin si une source est dépendante négativement d’une autre source, il peut être intéressant de marquer cette dépendance conflictuelle en augmentant la masse sur l’ensemble vide.\nPour réaliser ce schéma, nous proposons d’affaiblir les fonctions de masse d’une source S1 en fonction de sa mesure d’indépendance à une autre source S2, donnée par la fonction de masse mIi de l’équation (34).\nNous considérons ici une fonction de masse d’une source mΩ en fonction de son indépendance ou dépendance à une autre source. Ainsi nous définissons : m Ω[I](X) = mΩ(X) mΩ[P̄ ](X) = mΩ(X) mΩ(X) = 1 si X = ∅, 0 sinon mΩ[P ](X) = mΩ(X) mΩ(X) = 1 si X = Ω, 0 sinon (35)\nSuivant la procédure d’affaiblissement, nous effectuons une extension à vide sur la fonction de masse mI :\nmI↑Ω×I (Y ) = { mI (X) si Y = Ω×X, X ⊆ I 0 sinon (36)\nLe déconditionnement des fonctions de masse mΩ[I], mΩ[P ] et mΩ[P̄ ] est donné par :\nmΩ⇑Ω×I [I]((A× I) ∪ (Ω× I)) = mΩ[I](A), A ⊆ Ω (37)\noù Ī = {P ∪ P̄} est un raffinement.\nmΩ⇑Ω×I [P̄ ]((A× P̄ ) ∪ (Ω× {I ∪ P})) = mΩ[P̄ ](A), A ⊆ Ω (38)\nmΩ⇑Ω×I [P ]((A× P ) ∪ (Ω× {I ∪ P̄})) = mΩ[P ](A), A ⊆ Ω (39)\nCe dernier déconditionnement mène en fait à la masse de l’ignorance et est l’élément neutre de la combinaison conjonctive.\nNous réalisons ensuite la combinaison conjonctive :\nmΩ×IConj(Y ) = m I↑Ω×I ∩©mΩ⇑Ω×I [I] ∩©mΩ⇑Ω×I\n[ P̄ ] (Y ), ∀Y ⊂ Ω× I (40)\nLa marginalisation de la fonction de masse permet ensuite de revenir dans l’espace Ω :\nmΩ×I↓Ω (X) = ∑\n{Y⊆Ω×I |Proj(Y ↓Ω)=X}\nmΩ×IConj (Y ) (41)\nCette procédure réalisée pour la source S1 en rapport à la source S2 peut être réalisée pour la source S2 au regard de la source S1. Ainsi les deux fonctions de masse obtenues peuvent être combinées par la règle de combinaison conjonctive qui suppose l’indépendance.\n6 Expérimentations Pour tester la méthode précédemment décrite, nous avons généré des fonctions de masse aléatoirement. Tous les tirages aléatoires sont fait suivant la loi uniforme. L’algorithme 2 est utilisé pour générer n fonctions de masse.\nAlgorithm 2 Générer n fonctions de masse Require: |Ω|, n : nombre des fonctions de masse à générer\n1: for i = 1 to n do 2: Tirer aléatoirement | F |, le nombre d’éléments focaux dans l’intervalle [1, |2Ω|]. 3: Tirer aléatoirement | F | éléments focaux notés F . 4: Couper l’intervalle [0, 1] en | F | −1 sous-intervalles aléatoires continus. 5: for j = 1 to | F | do 6: La masse de chaque élément focal est la longueur de l’un des sous-intervalles. 7: end for 8: end for 9: return n fonctions de masse.\nCet algorithme a été utilisé pour générer 100 fonctions de masse, définies sur un cadre de discernement de cardinalité 5, pour deux sources S1 et S2 avec les trois hypothèses sur la dépendance des sources (sources indépendantes, sources dépendantes positivement et sources dépendantes négativement), les résultats des tests sont présentés dans le tableau 1.\n1. Sources indépendantes : Supposons que deux sources S1 et S2 sont complètement indépendantes. Nous avons alors généré 100 fonctions de masse pour chaque source comme décrit dans l’algorithme 2 avec | Ω |= 5.\n2. Sources dépendantes positivement : Lorsque deux sources S1 et S2 sont dépendantes positivement, les classes de décision (en terme de probabilité pignistique), calculées à partir des fonctions de masse qu’elles fournissent, sont les mêmes. Les deux sources S1 et S2 s’expriment de la même manière puisqu’elles sont dépendantes. Nous avons généré aléatoirement 100 fonctions de masse pour chaque source. Ces fonctions de masse sont modifiées par la suite suivant l’algorithme 3.\nAlgorithm 3 Générer des fonctions de masse dépendantes positivement Require: n fonctions de masse générées aléatoirement avec l’algorithme 2, Les classes de\ndécision. 1: for i = 1 to n do 2: Recherche des éléments focaux F de chaque fonction de masse mi 3: for j = 1 to | F | do 4: La masse affectée au jème élément focal est transférée à son union avec la classe de décision. 5: end for 6: end for 7: return n fonctions de masse modifiées.\n3. Sources dépendantes négativement : Lorsque deux sources S1 et S2 sont dépendantes négativement, les classes de décision (en terme de probabilité pignistique), calculées à partir des fonctions de masse qu’elles fournissent, sont contradictoires mais d’une façon ordonnée. Les deux sources S1 et S2 sont dépendantes mais l’une des deux sources a tendance à dire l’opposé de l’autre. Nous avons généré aléatoirement 100 fonctions de masse pour chaque source. Ces fonctions de masse sont modifiées par la suite suivant l’algorithme 4.\nAlgorithm 4 Générer des fonctions de masse dépendantes négativement Require: n fonctions de masse générées aléatoirement avec l’algorithme 2 pour une source,\nLes classes de décision de l’autre source, La correspondance de classes contradictoires. 1: for i = 1 to n do 2: Recherche des éléments focaux F de chaque fonction de masse mi. 3: for j = 1 to | F | do 4: Transférer la masse de l’élément focal j à son union avec la classe contradictoire (la\nclasse contradictoire à la classe de décision de mi) privé de la classe de décision de mi.\n5: end for 6: end for 7: return n fonctions de masse modifiées.\nType de dépendance Degrés d’indépendance, dépendance positive et dépendance négative\nIndépendance Id(S1, S2) = 0.72, Īd(S1, S2) = 0.28 Id(S2, S1) = 0.66, Īd(S2, S1) = 0.34 Dépendance positive mI,1(I) = 0.26, mI,1(P ) = 0.56,mI,1(P̄ ) = 0.18 mI,2(I) = 0.35, mI,2(P ) = 0.5,mI,2(P̄ ) = 0.15 Dépendance négative mI,1(I) = 0.35, mI,1(P ) = 0.25,mI,1(P̄ ) = 0.4\nmI,2(I) = 0.38, mI,2(P ) = 0.18,mI,2(P̄ ) = 0.44\nTAB. 1 – Résultats des tests sur 100 fonctions de masse générées\nLa complexité temporelle 6 de l’algorithme proposé dépend fortement de la cardinalité du cadre de discernement. Dans le cas illustré, | Ω |= 5, la complexité est de quelques secondes mais elle peut être beaucoup plus importante pour les plus grands cadres de discernement. Notons que la complexité temporelle de l’algorithme de classification est optimisée puisque les distances entre les fonctions de masse ne sont calculées qu’une seule fois.\n6. Les expérimentations ainsi que la complexité temporelle ont été testé sous Matlab R2010a."
    }, {
      "heading" : "6.1 Fonctionnement de l’affaiblissement par la mesure d’indépendance",
      "text" : "Nous allons, dans un premier temps, illustrer le fonctionnement de l’affaiblissement par la mesure d’indépendance. Nous considérons ici un cadre de discernement Ω = {ω1, ω2, ω3}. Supposons que nous ayons deux sources S1 et S2 donnant deux fonctions de masse :\nmΩ1 (ω1) = 0.2, m Ω 1 (ω1 ∪ ω2) = 0.5, mΩ1 (Ω) = 0.3, (42)\nmΩ2 (ω2) = 0.1, m Ω 2 (ω1 ∪ ω2) = 0.6, mΩ2 (Ω) = 0.3 (43)\nLa combinaison conjonctive donne :\nmΩ1 ∩© 2(∅) = 0.02, mΩ1 ∩© 2(ω1) = 0.18, mΩ1 ∩© 2(ω2) = 0.08,\nmΩ1 ∩© 2(ω1 ∪ ω2) = 0.63, mΩ1 ∩© 2(Ω) = 0.09\nCette combinaison conjonctive est effectuée avec l’hypothèse d’indépendance cognitive des deux sources. Si une connaissance externe permet de mesurer la dépendance positive et négative de la source S1 par rapport à la source S2 telle que fournie par l’équation (24), nous devons en tenir compte avant la combinaison conjonctive. Supposons une fonction de masse traduisant donc une forte dépendance positive de S1 par rapport à S2. Nous avons ainsi la fonction de masse suivante : m P(I) = 0.26 mP(P ) = 0.56 mP(P̄ ) = 0.18 (44)\nNotons que mP(I ∪ P ) = 0 et mP(I ∪ P̄ ) = 0 puisque les facteurs d’affaiblissement αi ne sont pas définis donc fixés à 1. Le tableau 2 présente les différentes étapes d’extension à vide, de déconditionnement et de combinaison dans l’espace Ω × P . L’extension à vide et le déconditionnement transfèrent les masses sur les éléments focaux correspondant de l’espace Ω × P . La combinaison des trois fonctions de masse dans cet espace fait apparaître la masse sur l’ensemble vide qui correspond à la part de dépendance négative.\nLe tableau 3 présente ensuite la marginalisation et le résultat de combinaison avec la fonction de masse mΩ2 non modifiée (i.e. que l’hypothèse d’indépendance totale de S2 par rapport à S1 est faite). Nous constatons que la masse transférée sur l’ignorance ne devient plus importante que lors de la combinaison conjonctive sans hypothèse sur la dépendance positive.\nAfin de bien illustrer le transfert de masse sur l’ensemble vide et sur l’ignorance, les figures 1 et 2 représentent les masses en fonction des variations de αi (représentant un facteur d’affaiblissement de la source Si) , βi (représentant le taux de dépendance de Si face à Sj) et γi (représentant le taux de dépendance négative de Si face à Sj) d’une fonction de masse décrite par (35) pour une fonction de masse dogmatique quelconque de la source Si. Ainsi sur la figure 1, représentant les variations de masse sur l’ensemble vide, αi est fixé à 1, βi et γi varient. Sur la figure 2, représentant les variations de masse sur l’ignorance, γi est fixé à 1, αi et βi varient.\nLa figure 1 montre ainsi que, pour la source Si, plus βi et γi sont grands plus on obtient une masse importante sur l’ensemble vide et donc une dépendance négative. La quantité βi\nfocal mP↑Ω×P mΩ[I]⇑Ω×P mΩ[P̄ ]⇑Ω×P mΩ×PConj ∅ 0.18 ω1 × I 0.052 (ω1 ∪ ω2)× I 0.13\nΩ× I 0.26 0.078 Ω× P 0.56 0.56\n(ω1 × I) ∪ (Ω× P ) ((ω1 ∪ ω2)× I) ∪ (Ω× P )\nΩ× P̄ 0.18 Ω× (I ∪ P ) 1\n(ω1 × I) ∪ (Ω× (P ∪ P̄ )) 0.2 ((ω1 ∪ ω2)× I) ∪ (Ω× (P ∪ P̄ )) 0.5\nΩ× P 0.3\nTAB. 2 – Détails de l’affaiblissement de la mesure d’indépendance : fonctions de masse dans Ω× P .\nfocal mΩ×P↓Ω1 m Ω 2 m Ω×P↓Ω 1 ∩©m Ω 2\n∅ 0.18 0.25432 ω1 0.052 0.0468 ω2 0.1 0.00768 ω1 ∪ ω2 0.13 0.6 0.15528 Ω 0.638 0.3 0.53592\nTAB. 3 – Détails de l’affaiblissement de la mesure d’indépendance : marginalisation et combinaison\nreprésente la part de dépendance de la source et la quantité γi représente la part de dépendance négative.\nLa figure 2 présente quand à elle, la variation de la masse sur Ω, l’ignorance. Cette masse est donnée directement par αi(1− βi) qui contient donc la part d’indépendance (1− βi) et la fiabilité αi de la source Si.\nNous illustrons ainsi le résultat escompté de l’affaiblissement par la mesure d’indépendance, c’est-à-dire que nous retrouvons sur la masse de l’ensemble vide la quantité de dépendance négative (associé à la part de dépendance de la source) et sur l’ignorance la quantité de fiabilité et d’indépendance. Nous remarquons aussi que lorsque la source est fiable (αi = 1) et indépendante (βi = 0), la fonction de masse de la source n’est pas modifiée."
    }, {
      "heading" : "6.2 Influence sur le résultat de combinaison",
      "text" : "Afin d’illustrer l’influence de la prise en compte de la mesure d’indépendance sur les fonctions de masse, nous allons considérer ici les deux sources précédentes Si et Sj qui fournissent\nles fonctions de masse données par les équations (42) et (43). Nous allons considérer trois cas pour chaque source avec un cas où la source Si est plutôt indépendante de Sj (αi = 0.95, βi = 0.95, γi = 0.05), un cas où elle est plutôt dépendante positivement (αi = 0.95, βi = 0.05, γi = 0.95) et un cas où elle est plutôt dépendante négativement (αi = 0.95, βi = 0.05, γi = 0.05). Pour la source Sj , nous considérons trois cas moins catégoriques en fixant la fiabilité αj = 0.9 : le cas plutôt indépendant (βj = 0.9, γj = 0.1), le cas plutôt dépendant positivement (βj = 0.1, γj = 0.9) et le cas plutôt dépendant négativement (βj = 0.1, γj = 0.1).\nAinsi, le tableau 4 présente les résultats de la combinaison des deux sources en fonction des hypothèses d’indépendance et de dépendance positive ou négative des deux sources S1 et S2. Nous constatons que lorsque les deux sources sont plutôt indépendantes, les résultats obtenus sont proches de la combinaison conjonctive sous l’hypothèse d’indépendance. Lorsqu’une des deux sources est dépendante négativement de l’autre, la masse transférée sur l’ensemble vide est importante. Lorsque l’une des deux sources est dépendante positivement de l’autre, la masse est transférée sur l’ignorance mais de façon moins importante que pour la dépendance négative. En effet, l’ensemble vide est un élément absorbant pour la combinaison conjonctive. Cette masse sur l’ensemble vide, à l’issue de la combinaison conjonctive, peut ainsi jouer un rôle d’alerte sur la dépendance négative. Une autre alternative serait d’envisager une autre règle de combinaison lorsque la masse issue de la dépendance négative est très importante."
    }, {
      "heading" : "7 Conclusion et perspectives",
      "text" : "Dans cet article, nous avons proposé une méthode d’apprentissage de l’indépendance de sources afin de justifier l’hypothèse sur l’indépendance lors du choix des règles de combinaison à utiliser pour la fusion. Nous avons également proposé d’estimer la dépendance positive et négative afin de pouvoir tenir compte de cette information dans les fonctions de masse avant de les combiner. Cette nouvelle information sur la dépendance des sources peut être intégrée dans les fonctions de masse fournies par ces sources avant de les combiner. Une autre solution à la dépendance des sources est de proposer une règle de combinaison tenant compte de\nl’indépendance, dépendance positive et négative des sources. Dans les prochains travaux, nous proposerons une règle de combinaison mixant la combinaison disjonctive et la combinaison prudente en fonction des degrés de dépendance. Nous définirons également le facteur d’affaiblissement αiki,kj permettant de tenir compte du nombre de fonctions de masse dans les clusters appariés lors de l’apprentissage de leurs indépendances.\nRéférences Ben Hariz, S., Z. Elouedi, et K. Mellouli (2006). Clustering approach using belief function\ntheory. In AIMSA, pp. 162–171. Ben Yaghlane, B., P. Smets, et K. Mellouli (2002a). Belief function independence : I. the\nmarginal case. Int. J. Approx. Reasoning 29(1), 47–70. Ben Yaghlane, B., P. Smets, et K. Mellouli (2002b). Belief function independence : Ii. the\nconditional case. Int. J. Approx. Reasoning 31(1-2), 31–75. Boubaker, J., Z. Elouedi, et E. Lefevre (2013). Conflict management with dependent informa-\ntion sources in the belief function framework. In The 14th IEEE International Symposium on Computational Intelligence and Informatics, CINTI 2013, Budapest, Hongrie.\nChebbah, M., A. Martin, et B. Ben Yaghlane (2012a). About sources dependence in the theory of belief functions. In Belief Functions, pp. 239–246.\nChebbah, M., A. Martin, et B. Ben Yaghlane (2012b). Positive and negative dependence for evidential database enrichment. In IPMU (3), pp. 575–584.\nDempster, A. P. (1967). Upper and Lower probabilities induced by a multivalued mapping. Annals of Mathematical Statistics 38, 325–339.\nDenœux, T. (2006). The cautious rule of combination for belief functions and some extensions. In International Conference on Information Fusion, Florence, Italy.\nSj : αj = 0.9 cas élément βj = 0.9, γj = 0.1 βj = 0.1, γj = 0.9 βj = 0.1, γj = 0.1\nfocal mΩ×P↓Ω1 m Ω×P↓Ω 2 m Ω×P↓Ω 1∩2 m Ω×P↓Ω 2 m Ω×P↓Ω 1∩2 m Ω×P↓Ω 2 m Ω×P↓Ω 1∩2\nSi ∅ 0.045125 0.081 0.12257 0.081 0.12337 0.009 0.0545389 αi = 0.95 ω1 0.01 0.00909 0.00829 0.00909 βi = 0.95 ω2 0.01 0.0779175 0.09 0.0850388 0.082 0.0774798 γi = 0.05 ω1 ∪ ω2 0.025 0.06 0.07138 0.54 0.517457 0.492 0.475303\nΩ 0.919875 0.849 0.780974 0.289 0.265844 0.417 0.383588 Si ∅ 0.045125 0.081 0.12437 0.081 0.13957 0.009 0.0692989 αi = 0.95 ω1 0.19 0.17271 0.15751 0.17271 βi = 0.05 ω2 0.01 0.00764875 0.09 0.0688388 0.082 0.0627198 γi = 0.95 ω1 ∪ ω2 0.475 0.06 0.449167 0.54 0.550307 0.492 0.574394\nΩ 0.289875 0.849 0.246104 0.289 0.0837739 0.417 0.120878 Si ∅ 0.002375 0.081 0.0849926 0.081 0.0994726 0.009 0.0261956 αi = 0.95 ω1 0.181 0.164529 0.150049 0.164529 βi = 0.05 ω2 0.01 0.00816625 0.09 0.0734962 0.082 0.0669633 γi = 0.05 ω1 ∪ ω2 0.4525 0.06 0.43317 0.54 0.57175 0.492 0.590472\nΩ 0.364125 0.849 0.309142 0.289 0.105232 0.417 0.15184\nTAB. 4 – Résultats de combinaison selon les hypothèses de dépendance et d’indépendance des deux sources Si et Sj .\nDenœux, T. (2008). Conjunctive and disjunctive combination of belief functions induced by nondistinct bodies of evidence. Artificial Intelligence 172, 234–264.\nDubois, D. et H. Prade (1988). Representation and combination of uncertainty with belief functions and possibility measures. Computational Intelligence 4, 244–264.\nElouedi, Z. et K. Mellouli (1998). Pooling dependent expert opinions using the theory of evidence. In Seventh Information Processing and Management of Uncertainty in KnowledgeBased System (IPMU’98), Volume I, pp. 32–39.\nHsia, Y.-T. (1991). Characterizing belief with minimum commitment. In IJCAI, pp. 1184– 1189.\nJousselme, A.-L., D. Grenier, et E. Bossé (2001). A new distance between two bodies of evidence. Information Fusion 2, 91–101.\nMartin, A. (2010). Le conflit dans la théorie des fonctions de croyance. In Actes Extraction et gestion des connaissances (EGC’2010), Hammamet, Tunisia, pp. 655–666.\nMartin, A. et C. Osswald (2007). Une nouvelle règle de combinaison répartissant le conflit - applications en imagerie sonar et classification de cibles radar. Traitement du Signal 24(2), 71–82.\nMercier, D. (2006). Fusion d’informations pour la reconnaissance automatique d’adresses postales dans le cadre de la théorie des fonctions de croyance. Ph. D. thesis, Université de Technologie de Compiègne.\nMurphy, C. (2000). Combining belief functions when evidence conflicts. Decision Support Systems 29, 1–9.\nShafer, G. (1976). A mathematical theory of evidence. Princeton University Press. Smets, P. (1990). The Combination of Evidence in the Transferable Belief Model. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 12(5), 447–458. Smets, P. (1993). Belief Functions : the Disjunctive Rule of Combination and the Generalized\nBayesian Theorem. International Journal of Approximate Reasoning 9, 1–35. Smets, P. (2005). Decision making in the TBM : the necessity of the pignistic transformation.\nInternational Journal of Approximate Reasonning 38, 133–147. Smets, P. et R. Kennes (1994). The Transferable Belief Model. Artificial Intelligent 66, 191–\n234. Smets, P. et R. Kruse (1997). The transferable belief model for belief representation, pp. 343–\n368. Boston : Kluwer Academic Publishers. Yager, R. R. (1987). On the Dempster-Shafer Framework and New Combination Rules. Infor-\nmation Sciences 41, 93–137.\nSummary In this paper, we propose to learn sources independence in order to choose the appropriate type of combination rules when aggregating their beliefs. Some combination rules are used with the assumption of their sources independence whereas others combine beliefs of dependent sources. Therefore, the choice of the combination rule depends on the independence of sources involved in the combination. In this paper, we propose also a measure of independence, positive and negative dependence to integrate in mass functions before the combinaision with the independence assumption."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Résumé. La fusion d’informations issues de plusieurs sources cherche à améliorer la prise de décision. La théorie des fonctions de croyance, pour réaliser cette fusion, utilise des règles de combinaison faisant bien souvent l’hypothèse forte de l’indépendance des sources. Cette hypothèse d’indépendance n’est cependant pas formalisée ni vérifiée. Nous proposons dans cet article un apprentissage de l’indépendance cognitive de sources d’information permettant de mesurer la dépendance ou l’indépendance. Cette mesure exprimée par une fonction de masse est ensuite intégrée par une approche d’affaiblissement avant de réaliser la combinaison d’informations.",
    "creator" : "LaTeX with hyperref package"
  }
}