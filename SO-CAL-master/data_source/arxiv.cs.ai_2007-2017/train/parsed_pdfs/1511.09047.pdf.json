{
  "name" : "1511.09047.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Solving Transition-Independent Multi-agent MDPs with Sparse Interactions (Extended version)∗",
    "authors" : [ "Joris Scharpff", "Diederik M. Roijers", "Frans A. Oliehoek", "Matthijs T. J. Spaan", "Mathijs M. de Weerdt" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "When cooperative teams of agents are planning in uncertain domains, they must coordinate to maximise their (joint) team value. In several problem domains, such as traffic light control [Bakker et al., 2010], system monitoring [Guestrin et al., 2002a], multi-robot planning [Messias et al., 2013] or maintenance planning [Scharpff et al., 2013], the full state of the environment is assumed to be known to each agent. Such centralised planning problems can be formalised as multi-agent Markov decision processes (MMDPs) [Boutilier, 1996], in which the availability of complete and perfect information leads to highly-coordinated policies. However, these models suffer from exponential joint action spaces as well as a state that is typically exponential in the number of agents.\nIn problem domains with local observations, sub-classes of decentralised models exist that admit a value function that is exactly factored into additive components [Becker et al., 2003, Nair et al., 2005, Witwicki and Durfee, 2010] and more general classes admit upper bounds on the value function that are factored [Oliehoek et al., 2015]. In centralised models however, the possibility of a factored value function can be ruled out in general: by observing the full state, agents can predict the actions of others ∗This article is an extended version of the paper that was published under the same title in the Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI16), held in Phoenix, Arizona USA on February 12-17, 2016. The most significant difference is that here a more strict definition of dependent actions, transition influence and, consequentially, the conditional return graphs is given. Furthermore, this version contains additional details and explanations that did not make it into the conference paper due to the page limit.\nar X\niv :1\n51 1.\n09 04\n7v 2\n[ cs\n.A I]\n1 1\nFe b\nbetter than when only observing a local state. This in turn means that each agent’s action should be conditioned on the full state and that the value function therefore also depends on the full state.\nA class of problems that exhibits particular structure is that of task-based planning problems, such as the maintenance planning problem (MPP) from [Scharpff et al., 2013]. In the MPP every agent needs to plan and complete its own set of road maintenance tasks at minimal (private) maintenance cost. Each task is performed only once and may delay with a known probability. As maintenance causes disruption to traffic, agents are collectively fined relative to the (super-additive) hindrance from their joint actions. Although agents plan autonomously, they depend on others via these fines and must therefore coordinate. Still, such reward interactions are typically sparse: they apply only to certain combinations of maintenance tasks, e.g. in the same area, and often involve only a few agents. Moreover, when an agent has performed its maintenance tasks that potentially interfere with others, it will no longer interact with any of the other agents.\nOur main goal is to identify and exploit such structure in centralised models, for which we consider transition independent MMDPs (TI-MMDPs). In TI-MMDPS, agent rewards depend on joint states and actions, but transition probabilities are individual. Our key insight is that we can exploit the reward structure of TI-MMDPs by decomposing the returns of all execution histories (i.e., all possible state/action sequences from the initial time step to the planning horizon) into components that depend on local states and actions.\nWe build on three key observations. 1) Contrary to the optimal value function, returns can be decomposed without loss of optimality, as they depend only on local states and actions of execution sequences. This allows a compact representation of rewards and efficiently computable bounds on the optimal policy value via a data structure we call the conditional return graph (CRG). 2) In TI-MMDPs agent interactions are often sparse and/or local, for instance in the domains mentioned initially, typically resulting in very compact CRGs. 3) In many (e.g. task-modelling) problems the state space is transient, i.e., states can only be visited once, leading to a directed, acyclic transition graph. With our first two key observations this often gives rise to conditional reward independence, i.e. the absence of further reward interactions, and enables agent decoupling during policy search.\nHere we propose conditional return policy search (CoRe), a branch-and-bound policy search algorithm for TI-MMDPs employing CRGs, and show that it is effective when reward interactions between agents are sparse. We evaluate CoRe on instances of the aforementioned MPP with uncertain outcomes and very large state spaces. We demonstrate that CoRe evaluates only a fraction of the policy search space and thus finds optimal policies for previously unsolvable instances and commonly requires less runtime than its alternatives."
    }, {
      "heading" : "2 Related work",
      "text" : "Scalability is a major issue in multi-agent planning under uncertainty. In response to this challenge, two important lines of work have been developed. One line of work proposed approximate solutions by imposing and exploiting an additive structure in the value function [Guestrin et al., 2002a]. This approach has been applied in a range of stochastic planning settings, fully and partially observable alike, both from a single-agent perspective [Koller and Parr, 1999, Parr, 1998] and multi-agent [Guestrin et al., 2002b, Meuleau et al., 1998, Kok and Vlassis, 2004, Oliehoek et al., 2013b]. The drawback of such methods is that typically no bounds on the efficiency loss can be given. We focus on optimal solutions, required to deal with strategic behaviour in a mechanism [Cavallo et al., 2006, Scharpff et al., 2013].\nThis is part of another line of work that has not sacrificed optimality, but instead targets sub-classes of problems with properties that can be exploited [Becker et al., 2003, Becker et al., 2004, Mostafa and Lesser, 2009, Witwicki and Durfee, 2010]. In particular, several methods that exploit the same type of additive structure in the value function have been shown exact, simply because value functions of the sub-class of problems they address are guaranteed to have such shape [Nair et al., 2005, Oliehoek\net al., 2008, Varakantham et al., 2007]. However, all these approaches are for decentralised models in which actions are conditioned only on local observations. Consequentially, optimal policies for decentralised models typically yield lower value than the optimal policies for their fully-observable counterparts (shown in our experiments).\nOur focus is on transition-independent problems, suitable for multi-agent problems in which the effects of activities of agents are (assumed) independent. In domains where agents directly influence each other, e.g., by manipulating shared state variables, this assumption is violated. Still, transition independence allows agent coordination at a task level, as in the MPP, and is both practically relevant and not uncommon in literature [Becker et al., 2003, Spaan et al., 2006, Melo and Veloso, 2011, Dibangoye et al., 2013].\nAnother type of interaction between agents is through limited (global) resources required for certain actions. While this introduces a global coupling, some scalability is achievable [Meuleau et al., 1998]. Whether context-specific and conditional agent independence remains exploitable in the presence of such resources in TI-MMDPs is yet unclear. Additionally, there exist also methods that exploit reward sparsity and independence but through a reinforcement learning approach identifying ‘interaction states’ [De Hauwere et al., 2012, Melo and Veloso, 2009]. Although these target a similar structure, learning implies that there are no guarantees on the solution quality (until all states have been recognised as interaction states, which implies a brute-force solve of the MMDP)."
    }, {
      "heading" : "3 Model",
      "text" : "We consider a (fully-observable) transition-independent, multi-agent Markov decision process, or TIMMDP, with a finite horizon of length h, and no discounting of rewards.\nDefinition 1. A TI-MMDP is a tuple 〈N,S,A, T,R〉:\nN = {1, ..., n} is a set of n enumerated agents;\nS = S1 × ... × Sn is the agent-factored state space, which is the Cartesian product of n factored states spaces Si (composed of features f ∈ F , i.e. si = {f ix, f iy, . . .});\nA = A1 × ... × An is the joint action space, which is the Cartesian product of the n local action spaces Ai;\nT (s,~a, ŝ) = ∏ i∈N T\ni(si, ai, ŝi) defines a transition probability, which is the product of the local transition probabilities due to transition independence; and\nR is the set of reward functions over transitions that we assume without loss of generality is structured as {Re|e ⊆ N}. When e = {i}, Ri is the local reward function for agent i, and when |e| > 1, Re is called an interaction reward. The total team reward per time step, given a joint state s, joint action ~a and new joint state ŝ, is the sum of all rewards:\nR(s,~a, ŝ) = ∑ Re∈R Re({sj}j∈e, {~a j}j∈e, {ŝj}j∈e). (1)\nTwo agents i and j are called dependent when there exists a reward function with both agents in its scope, e.g., a two-agent reward Ri,j({si, sj}, {ai, aj}, {ŝi, ŝj}) could describe the super-additive hindrance that results when agents in the MPP do concurrent maintenance on two nearby roads. We focus on problems with sparse interaction rewards, i.e., reward functions Re with non-zero rewards for a small subset of the local joint actions (e.g.,Aij ⊂ Ai×Aj) or only a few agents in its scope. Of course, sparseness is not a binary property: the maximal number of actions with non-zero interaction rewards\nand participating agents (respectively α and w in Theorem 1) determine the level of sparsity. Note that this is not a restriction but rather a classification of problems that benefit most from our approach.\nThe goal in a TI-MMDP is to find the optimal joint policy π∗ of which the actions ~a maximise the expected sum of rewards, expressed by the Bellman equation:\nV ∗(st) = max ~at ∑ st+1∈S T (st,~at, st+1) ( ∑ Re∈R Re(set ,~a e t , s e t+1) + V ∗(st+1) ) . (2)\nAt the last timestep there are no future rewards, so V ∗(sh) = 0 for every sh ∈ S. Although V ∗(st) can be computed through a series of maximisations over the planning period, e.g. via dynamic programming [Puterman, 2014], it cannot be written as a sum of independent local value functions without losing optimality [Koller and Parr, 1999], i.e. V ∗(s) 6= ∑ e V\ne,∗(se). Instead, we factor the returns of execution sequences, the sum of rewards obtained from following state/action sequences, which is optimality preserving. We denote an execution sequence up until time t as θt = [s0,~a0, ..., st−1,~at−1, st] and its return is the sum of its rewards: ∑t−1 x=0R(sθ,x,~aθ,x, sθ,x+1), where sθ,x, ~aθ,x and sθ,x+1 respectively denote the state and joint action at time x, and the resulting state at time x + 1 in this sequence. A seemingly trivial but important observation is that the return of an execution sequence can be written as the sum of local functions:\nZ(θt) = ∑ Re∈R t−1∑ x=0 Re(seθ,x,~a e θ,x, s e θ,x+1), (3)\nwhere seθ,x, ~a e θ,x and s e θ,x+1 denote local states and actions from θt that are relevant for R e. Contrary to the optimal value function, (3) is additive in the reward components and can thus be computed locally.\nNonetheless, the return of (3) does not directly give us the value of an optimal policy. To compute the expected policy value using (3), we sum the expected return of all future execution sequences θh reachable under policy π starting at s0 (denoted θh|π, s0):\nV π(s0) = ∑ θh|π,s0 Pr(θh)Z(θh) = ∑ θh|π,s0 Z(θh) h−1∏ t=0 T (sθ,t, π(sθ,t), sθ,t+1). (4)\nNow, (4) is structured such that it expresses the value in terms of additively factored terms (Z(θh)). However, comparing (2) and (4), we see that the price for this is that we no longer are expressing the optimal value function, but that of a given policy π. In fact, (4) corresponds to an equation for policy evaluation. It is thus not a basis for dynamic programming, but it is usable for policy search. Although policy search methods have their own problems in scaling to large problems, we show that the structure of (4) can be leveraged.\nIn particular, because the return of an execution sequence can be decomposed into additive components (Eq. 3), we decouple and store these returns locally in conditional return graphs (CRGs). The CRG is used in policy search to efficiently compute (4) when, during evaluation, the transition probability Pr(θ) of an execution sequence θ becomes known. Moreover, the returns stored in the CRG can be used to bound the expected value of sequences, allowing branch-and-bound pruning. Finally, when constructing CRGs it is possible to detect the absence future reward interactions between agents and thus optimally decouple the policy search."
    }, {
      "heading" : "4 Conditional Return Graphs",
      "text" : "We now partition the reward function into additive componentsRi and assign them to agents. The local reward for an agent i ∈ N is given by Ri = {Ri} ∪ Rei , whereRei are the interaction rewards assigned\nto i (restricted to Re where i ∈ e). The setsRi are disjoint sub-sets of the reward functionsR such that together they again form the complete set of joint reward functions. Note that such a partitioning can be done in many ways. In our preliminary experiments we observed that for the maintenance planning domain balancing the number of functions per agent works well. Nevertheless, further study is required to establish potentially better or more generic assignment heuristics.\nGiven such a disjoint partition of rewards, a conditional return graph for agent i is a data structure that represents all possible local returns, for all possible local execution histories. Particularly, it is a directed acyclic graph (DAG) with a layer for every stage t = 0, . . . , h−1 of the decision process. Each layer contains nodes corresponding to the reachable local states si ∈ Si of agent i at that stage. As the goal is to include interaction rewards, the CRG includes for every local state si, local action ai, and successor state ŝi a representation of all transitions (se,~a e, ŝe) for which si ∈ se, ai ∈ ~a e, and ŝi ∈ ŝe.\nWhile a direct representation of these transitions captures all the rewards possible, we can achieve a much more compact representation by exploiting sparse interaction rewards, enabling us to group many joint actions ~a e leading to the same rewards. Consider a two-agent example N = {1, 2} with actions A1 = {a1} and A2 = {a2, b2, c2} respectively. Both agents have a local reward function, resp. R1 and R2, and there is one interaction reward R1,2 that is 0 for all transitions but the ones involving joint action {a1, a2}. This could for example be a network cost function of MPP that is non-zero when the maintenance activities corresponding to actions a1 and a2 are performed, e.g. because they take place within close proximity of each other. The interaction reward R1,2 is assigned to agent 1, thus R1i = {Ri, R1,2} and of course R2i = {R2}. A naive representation of all rewards would result in the graph of Figure 1a, illustrating a transition from s10 to s 1 1 where agent 2 may remain in state s 2 4 or transition to state s26. Observe that now there are only three unique rewards (shown in red) whereas there are six possible transitions. Intuitively, all transitions resulting in the same reward should be grouped such that only when the actions/states of other agents influence the reward they should be included in the CRG, resulting in the graph of Figure 1b. To make this explicit, we denote a single transition for agents e ⊆ N by τ e = ({sj}j∈e, {~a j}j∈e, {ŝj}j∈e) = (se,~a e, ŝe). A local transition τ i is said to be contained in τ e, denoted τ i ∈ τ e, if i ∈ e, si ∈ se, ai ∈ ~a e and ŝi ∈ ŝe. Moreover the set set of all available (joint) transitions can be written as\nT e = {(se,~a e, ŝe) | se, ŝe ∈ {sj}j∈e,~a e ∈ {~a j}j∈e, T (se,~a e, se) > 0} (5)\nNow we can formalise the set of actions (state transitions follow briefly afterwards) of other agents\nthat may interact with the rewardsRi, assigned to each of the CRGs, given a current local transition τ i = (si, ai, ŝi). An action aj of agent j 6= i is said to be dependent with respect to local transition τ i if it occurs in one of the available joint transitions that contains τ i, its presence influences the interaction reward and there is at least one other action of agent j that does not cause the same interaction reward. The last condition is included to prevent marking all actions as dependent when actually the interaction reward depends on the state transition of agent j. This leads to the following definition of dependent actions:\nDefinition 2 (Dependent Actions). The set of dependent actions of an agent j ∈ N that may rewardinteract with agent i 6= j when agent i’s local transition is τ i = (si, ai, ŝi) is given by:\nA(τ i, j) = {aj ∈ Aj | ∃Re ∈ Ri, ∃ τ e ∈ T e, ∃ bj 6= aj ∈ Aj : τ i ∈ τ e ∧ aj ∈ ~a e\n∧Re(τ e) 6= Re(τ e \\ {τ j}), s.t. τ j ∈ τ e\n∧Re(τ e) 6= Re(se,~a e \\ {aj} ∪ {bj}, ŝe)}\nActions by other agents that are not dependent with respect to a transition τ i, i.e. the actions Aj \\ A(τ i, j), are (made) anonymous in the CRG for agent i via ‘wildcards’ (e.g. ∗2 of Figure 1b), since they do not influence the reward from the functions in Ri.\nBesides actions, the interaction reward may also be affected by the state transitions of the other agents. This is captured by the transition influence.1 Its definition is rather similar to that of dependent actions. For a state transition sj → ŝj of an agent j 6= i to be considered an influence with respect to local transition τ i, both states must be part of a transition τ j such that both τ i and τ j are contained in a joint transition τ e, such a transition τ j must have an impact on at least one interaction reward and there must exist at least one other transition of agent j that does not have the same interaction reward impact. The latter condition is, as before, to prevent all state transitions being marked an influence whereas the interaction reward depends solely on the action. This is formalised by the following definition:\nDefinition 3. The set of state pairs of an agent j that may lead to a reward interaction when agent i 6= j perform transition τ i = (si, ai, ŝi) and agent j performs action aj ∈ Aj concurrently, is known as the (transition) influence, defined as\nIi(τ i, aj) = {(sj ,ŝj) ∈ Sj × Sj | ∃Re ∈ Ri,∃ τ e ∈ T e, ∃ (sj2, ŝ j 2) 6= (s j , ŝj) ∈ Sj × Sj : τ i ∈ τ e ∧ τ j = (sj , aj , ŝj) ∈ τ e (6) ∧Re(τ e) 6= Re(τ e \\ {τ j}), s.t. τ j ∈ τ e (7) ∧Re(τ e) 6= Re(se \\ {sj} ∪ {sj2},~a e, ŝe \\ {ŝj} ∪ {ŝj2})} (8)\nFinally, for any set of actions Aj of an agent j we define the transition influence of that set with respect to local transition τ i as the union of all influences, or Ii(τ i, Aj) = ⋃ aj∈Aj I\ni(τ i, aj). This last definition is useful to capture the influence of a wildcard set ∗j , which occurs when multiple actions\n1In [Scharpff et al., 2013] only the set of dependent actions was defined. Although the definition in the paper is correct, for some problems it may lead to an unnecessary blow-up of the CRG size. For instance, an interaction reward assigned to agent i may actually depend on the state transition of another agent j, regardless of the action it performs. By the previous definition all actions of agent i would have been marked dependent. Here we strengthen the previous definition by decoupling the influence of actions and state transitions on the interaction reward, such that the CRGs constructed based on these definitions are indeed of minimum size.\nlead to the same state-transition interaction reward. Again, non-influencing state transitions can be grouped. We use the symbol j to denote the set of all non-influencing transitions of agent j given a local transition τ i and action aj (or wildcard ∗j).\nGiven Defs. 2 and 5 above, a conditional return graph (CRG) φi for agent i can be defined as follows. Definition 4 (Conditional Return Graph). Given a disjoint, complete partitioning R = ⋃ i∈N Ri of rewards over agents i ∈ N , the Conditional Return Graph (CRG) φi is a directed acyclic graph with for every stage t of the decision process a node for every reachable local state si ∈ S and for every available local transition τ i = (si, ai, ŝi) a tree compactly representing all joint transitions τ e = (se,~a e, ŝe) of the agents e ⊆ N in the scope ofRi, or e = {i ∈ N | ∃Re ∈ Ri : i ∈ e}.\nThe tree consists of two parts: an action tree that specifies all dependent actions and an influence tree that contains the relevant local state transitions. For every action ai ∈ Ai of agent i, the state si is connected to the root node vai of an action tree by an arc labeled with action ai. For every root node vai , let v = vai be the root of an action tree such that it is defined recursively over the remaining N ′ = e \\ {i} agents as follows:\nA1 If N ′ 6= emptyset take some j ∈ N ′, otherwise stop.\nA2 For every aj ∈ A(τ i, j), create an internal node vaj connected from v by an arc labeled with the action aj .\nA3 Create one internal node v∗j to represent all actions of agent j not in A(τ i, j) (if any), connected by an arc labeled by the ‘other action’ wildcard ∗j from the root node v.\nA4 For each leaf node vaj (or v∗j ) that results from the previous steps, create a sub-tree with N ′ = N ′ \\ {j} and v = vaj as its root using the same procedure.\nWhen all action arcs have been created, each leaf node vax of the action tree is the root node u of an influence tree, where ax is either the last dependent action or wildcard ∗x of the agent x that is visited in the last recursion. Starting again from N ′ = e \\ {i}:\nB1 If N ′ 6= emptyset take some j ∈ N ′, otherwise stop.\nB2 If the path from si to node u contains an arc labelled with action aj ∈ A(τ i, j), create child nodes usj→ŝj to represent all local pairs of state transitions (sj , ŝj) of agent j in the action influence Ii(τ i, aj), connected to node u by arcs labeled sj → ŝj . else\nThe path from si to node u contains the wildcard ∗j . Create child nodes usj→ŝj for all pairs of local state transitions (sj , ŝj) ∈ Ii(τ i, ∗j), i.e. the influence of the set of actions represented by ∗j (all aj /∈ A(τ i, j)), and connect them to u with arcs labelled sj → ŝj .\nB3 If there remains any pair of local states (sj , ŝj) ∈ Sj × Sj with T (sj , aj , ŝj) > 0 that is not in Ii(τ i, aj) or a pair with ∑ aj∈∗j T (s\nj , aj , ŝj) > 0 that is not in Ii(τ i, ∗j), depending on the action of agent j on the path to node u, create another child node u j connected by an arc labeled by the ‘other state pairs’ wildcard j .\nB4 For each leaf node usj→ŝj (or u j ) that results from the previous step, create a sub-tree with N ′ = N ′ \\ {j} and root u = usj→ŝj (resp. u = u j ) repeating the same procedure.\nFinally, each leaf node usx→ŝx (x again being the last agent) of every influence tree is connected to the new local state node ŝi by an arc labeled with the transition reward Ri(τ e) that corresponds to the actions and state pairs on the path from si to ŝi.\nThe labels on the path to a leaf node of an influence tree, via a leaf node of the action tree, sufficiently specify the joint transitions of the agents in scope of the functions Re ∈ Ri, such that we can compute the reward ∑ Re∈Ri R\ne(se,~a e, ŝe). Note that for each Re for which an action is chosen that is not in A(ai, j) (a wildcard ∗j in the action tree), the interaction reward must be 0 by definition (and similarly for state transitions in j).\nIn Figure 1b an example CRG is illustrated. The local state nodes are displayed as circles; the internal nodes as black dots and action tree leaves as black triangles. The action arcs are labelled a1, a2 and ‘wildcard’ ∗2, whereas transition influence arcs are labelled (s24 → s24), (s24 → s26) and j . Note that Definition 4 captures the general case, but often it suffices to consider transitions (si∪F e\\i,~a e, ŝi∪F̂ e\\i), where F e\\i is the set of state features on which the reward functions Ri depend. This is a further abstraction: only feature influence arcs are needed, typically requiring much less arcs (demonstrated later in Figure 2).\nNow we investigate the maximal size of the CRGs to derive a theoretical upper bound. Let |Smax| = maxi∈N |Si| and |Amax| = maxi∈N |Ai| denote respectively the maximal individual state and action space sizes, w = maxRe∈Re |e| − 1 denote the maximal interaction function scope size, α = maxi,j∈N maxai∈Ai |A(ai, j)| the set size of the largest dependent action set, and let finally β = maxi,j∈N maxτ i∈T i maxaj∈Aj |I(τ i, aj)| denote the size of the largest transition influence set. First note that the full joint policy search space is Θ(h|Smax|2n|Amax|n), however we show that the use of CRGs can greatly reduce this:\nTheorem 1. The maximal size of a CRG is\nO( h · |Amax||Smax|2 · (αβ)w ). (9)\nProof. A CRG has as many layers as the planning horizon h. In the worst case, in every stage there are |Smax| local state nodes, each connected to at most |Smax| next-stage local state nodes via multiple arcs. The number of action arcs between two local state nodes si and ŝi is at most |Ai| times the maximal number of dependent actions, αw. Finally, the number of influence arcs is bounded by βw.\nNote that in general all actions and transitions can cause interaction rewards, in which case the size of all n CRGs combined is O(nh|Smax|2+2w|Amax|1+w); typically still much more compact than the full joint policy search space unless w ≈ |N |. For many problems however, the interaction rewards are more sparse and αw |Amax|w. Moreover, (9) gives an upper bound on the CRG size in general, for a specific CRG φi this bound is often expressed more tightly byO(h · |Ai||Si|2 · ∏ j∈N (α iβi)w), where αi and βi denote respectively the maximal dependent action and transition influence set sizes for agent i. Finally, each |Si| can be reduced to |F i| when conditioning on state features is sufficient.\nBounding the optimal value In addition to storing rewards compactly, we use CRGs to bound the optimal policy value. Specifically, the maximal (resp. minimal) return from a joint state st onwards, is an upper (resp. lower) bound on the attainable reward, later to combined with its probability to obtain the expected value. Moreover, the sum of bounds on local returns bounds the global return and thus on the globally optimal joint policy value. We define the bounds recursively:\nU(si) = max (se,~a et ,ŝ e)∈φi(si)\n( Ri(se,~a et , ŝe) + U(ŝi) ) , (10)\nsuch that φi(si) denotes the set of local transitions available from state si ∈ se (ending in ŝi ∈ ŝe) represented in CRG φi. The bound on the optimal value for a joint transition (s,~a, ŝ) of all agents is\nU(s,~at, ŝ) = ∑ i∈N ( Ri(se,~a et , ŝe) + U(ŝi) ) , (11)\nand lower bound L is defined similarly over minimal returns.\nConditional Reward Independence Furthermore, CRGs can exploit independence in local reward functions as a result of past decisions. In many task-modelling MMDPs, e.g. those mentioned in the introduction, actions can be performed a limited amount of times, after which reward interactions involving that action no longer occur. When an agent can no longer perform dependent actions, the expected value of the remaining decisions is found through local optimisation. More generally, when dependencies between groups of agents no longer occur, the policy search space can be decoupled into independent components for which a policy may be found separately while their combination is still globally optimal.\nDefinition 5 (Conditional Reward Independence). Given an execution sequence θt, two agents i, j ∈ N are conditionally reward independent, denoted CRI (i, j, θt), if for all future states st, st+1 ∈ S and every future joint action ~at ∈ A:\n∀Re ∈ R s.t. {i, j} ⊆ e : h−1∑ x=t Re(sx,~ax, sx+1) = 0. (12)\nAlthough reward independence is concluded from joint execution sequence θt, some independence can be detected from the local execution sequence θit, for example when agent i completes its dependent actions. This local conditional reward independence occurs when ∀j ∈ N : CRI (i, j, θit) and is easily detected from the state during CRG generation. For each such state si, we find optimal policy π∗i (s\ni) and add only the optimal transitions dictated by that policy to our CRG, further reducing the CRG size.\nConditional Return Policy Search All the previous combined leads to the Conditional Return Policy Search (CoRe) (Algorithm 1). CoRe performs a branch-and-bound search over the joint policy space, represented as a DAG with nodes st and edges 〈~at, ŝt+1〉, such that finding a joint policy corresponds to selecting a subset of action arcs from the CRGs (corresponding to ~at and ŝt+1). First, however, the CRGs φi are constructed for the local rewards Ri of each agent i ∈ N , assigned heuristically to obtain the CRGs. Preliminary experiments provided evidence that a balanced distribution of rewards over the CRGs leads to the best results in the MPP domain. Further research is required to find effective heuristics for other domains. The generation of the CRGs follows Definition 4 using a recursive procedure, during which we store bounds (Equation 10) and flag local states that are locally conditionally reward independent according to Definition 5. During the subsequent policy search CoRe detects when subsets of agents, N ′ ⊂ N , become conditionally reward independent, and recurses on these subsets separately.\nAfter construction of the CRGs, CoRe performs depth-first policy search (Algorithm 1) over the (disjoint sub-)set of agents N with potential reward interactions (line 2). These subsets are found with a connected component algorithm on a graph with nodesN and an edge (i, j) for every pair of agents i, j ∈ N ′ that are still dependent given the current execution sequence θNt , or ¬CRI (i, j, θNt ). In lines 2 to 10, we thus only consider local state space SN\n′ ⊆ S and joint actions ~a ∈ AN ′. Lines 3 and 4 determine the upper and lower bounds for this subset of agents, retrieved from the CRGs, used to prune in line 5. For the remaining joint actions, CoRe recursively computes the expected value by extending the current execution sequence with the joint action ~at and all possible result states st+1 (line 8), of which the highest is returned (line 10). As an extra, the lower bound is tightened (if possible) after every evaluation (line 9).\nTheorem 2 (CoRe Correctness). Given TI-MMDP M = 〈N,S,A, T,R〉 with (implicit) initial state s0, CoRe always returns the optimal MMDP policy value V ∗(s0) (Eq. 2).\nProof. (Proof Sketch) Conditional reward independence enables optimal decoupling of policy search, the bounds are admissible with respect to the optimal policy value and our pruning does not exclude optimal execution sequences. The full proof can be found in the Appendix.\nAlgorithm 1: CoRe(Φ, θNt , h,N) Input: CRGs Φ, current execution sequence θNt , planning horizon h, agent (sub) set N 1 if t = h then return 0 V ∗ ← 0 2 foreach conditionally independent subset N ′ ⊆ N given θNt do\n// Compute weighted sums of bounds:\n3 ∀~aN ′ t : U(s N′ θ,t ,~a N′ t )← ∑ sN ′\nt+1\nT (sN ′ θ,t ,~a N′ t , s N′ t+1)U(s N′ θ,t ,~a N′ t , s N′ t+1)\n4 Lmax ← maxN ′\n~at ∑ sN ′\nt+1\nT (sN ′ θ,t ,~a N′ t , s N′ t+1)L(s N′ θ,t ,~a N′ t , s N′ t+1)\n// Find joint action maximising expected reward\n5 foreach ~aN ′ t for which U(s N′ θ,t ,~a N′ t ) ≥ Lmax do 6 V ~aN ′ t ← 0 7 foreach sN ′ t+1 reachable from s N′ θ,t and ~a N′ t do 8 V ~aN ′ t + = T (sN ′ θ,t ,~a N′ t , s N′ t+1) ( R(sN ′ θ,t ,~a N′ t , s N′ t+1) + CoRe(Φ, θ N t ′⊕ [~aN ′ t , s N′ t+1], h,N ′) )\n9 Lmax ← max(V~aN′t , Lmax) // update lb 10 V ∗+ = max\n~aN ′ t V ~aN ′ t\n11 return V ∗"
    }, {
      "heading" : "5 CoRe Example",
      "text" : "We present a two-agent example problem in which both agents have actions a, b and c, but every action can be performed only once within a 2 step horizon. Action c2 of agent 2 is (for ease of exposition) the only stochastic action with outcomes c and c′, and corresponding probabilities 0.75 and 0.25. There is only one interaction, between actions a1 and a2, and the reward depends on feature f1 of agent 1 being set from f1? to f1 or ¬f1. Thus we have one interaction reward function with rewards R1,2(f1?, {a1, a2}, f1) andR1,2(f1?, {a1, a2},¬f1), and local rewardsR1 andR2. Without specifying actual rewards, we demonstrate the CRGs and CoRe.\nFigure 2 illustrates the two CRGs. On the left is the CRG φ1 of agent 1 with only its local rewardR1, while the CRG of agent 2 includes both the reward interaction function R1,2 and its local reward R2. Notice that only when sequences start with action a2 additional arcs are included in CRG φ2 to account for reward interactions. The sequence starting with a2 is followed by an after-state node with two arcs: one for agent 1 performing a1 and one for its other actions, ∗1 = {b1, c1}. The interaction reward depends on what feature f1 is (stochastically) set to, thus the influence arcs f1 and ¬f1 are now required. As the interaction reward only occurs when {a1, a2} is executed, the fully-specified after-state node after a2 and ∗1 (the triangle below it) has a no-influence arc 1. All other transitions are reward independent and captured by local transitions (s10, b 1, s1b) and (s 1 0, c\n1, s1c). Locally reward independent states are highlighted green and from each of these states only the optimal action transition is kept in the CRG, e.g. only action arc c1 is included from s1a. This action was determined optimal from the local state by single-agent optimal policy search, and thus no arcs for other actions (here b1) are necessary.\nConsider for example the state s1a of φ 1, in which action a1 has been taken in the first time step. From this state onward, the reward interaction between action a1 and a2 will no longer take place and therefore agent 1 is locally independent. Consequentially, we can remove either the branch for action b1 or c1 based on which action maximises the expected value. In addition, this action will cause both agents to become independent from each other because of the single dependency function between them.\nPolicy Search An example of CoRe policy search is shown in Figure 3, with the policy search space on the left and the CRGs on the right, now annotated with return bounds. Only several of the branches of the full DAG and CRGs are shown to preserve clarity. At t = 0, there are 9 joint actions with 12 result states, while the CRGs need only 3 + 4 states and 3 + 6 transitions to represent all re-\nwards. The execution sequence θh that is evaluated is highlighted in thick red. This sequence starts with non-dependent actions {b1, b2}, resulting in joint state sb,b (ignore the bounds in blue for now). The execution sequence at t = 1 is thus θ1 = [s0, {b1, b2}, sb,b]. In the CRGs the corresponding transitions to states s1b and s 2 b are shown. Now for t = 1 CoRe is evaluating joint action {a1, a2} that is reward-interacting and thus the value of state feature f1 is required to determine the transition in φ2 (here chosen arbitrarily as ¬f1x ). The corresponding execution sequence (of agent 2) is therefore θ22 = [s 2 0, {b1, b2}, s2b ∪ {f1?}, {a1, a2}, s2ba ∪ {¬f1}] If agent 1 had chosen action c1 instead, we would traverse the branches ∗1 and 1 leading to state s2ba without reward interactions. Branch-and-bound is shown (in blue) for state sb,b, with the rewards labelled on transitions and their bounds at the nodes. The bounds for joint actions {a1, a2} and {a1, c2} are [13, 16] and [12.5, 12.5], respectively, found by summing the CRG bounds, hence {a1, c2} can be pruned. Note that we can compute the expected value of {a1, c2} in the CRG, but not that of {a1, a2}. This is because agent 2 knows the transition probability of action c2 but it does not know what value f1 has during CRG generation or with what probability a1 will be performed. Regardless, we can bound the return of action a2 over all possible feature values, stored in φ2, and they can be updated as the probabilities become known during policy search.\nConditional reward independence occurs in the green states of the policy search tree. After joint action {b1, a2}, the agents will no longer interact (a2 is completed and will not be available anymore) and thus the problem is decoupled. From state sb,a CoRe finds optimal policies π∗1(s 1 b) and π ∗ 2(s 2 a) and combines them into the optimal joint policy π∗(sb,a) = 〈π∗1(s1b), π∗2(s2a)〉 for the planning problem remaining from independent state sb,a."
    }, {
      "heading" : "6 Evaluation",
      "text" : "In our experiments we find optimal policies for the maintenance planning problem (MPP, see the introduction) that minimise the (time-dependent) maintenance costs and economic losses due to traffic hindrance. In this problem agents represent contractors that participate in a mechanism and thus it is essential that the planning is done optimally.\nThe problem can be modelled as an MDP, as is explained in full detail in [Scharpff et al., 2013].\nHere we only briefly outline the problem. Agents maintain a state with start and end times of their maintenance tasks. Each of these tasks can be performed only once and agents can perform exactly one task at a time (or do nothing). The individual rewards are given by maintenance costs that are task and time dependent, while interaction rewards model network hindrance due to concurrent maintenance. Maintenance costs are task and time dependent, while interaction rewards model network hindrance due to concurrent maintenance. In this domain we conduct three experiments with CoRe to study 1) the expected value when solving centrally versus decentralised methods, 2) the impact on the number of joint actions evaluated and 3) the scalability in terms of agents.\nFirst, we compare with a decentralised baseline by treating the problem as a (transition and observation independent) Dec-MDP [Becker et al., 2003] in which agents can only observe their local state. Although the (TI-)Dec-MDP model is fundamentally different from the TI-MMDP – in the latter decisions are coordinated on joint (i.e., global) observations – the advances in Dec-MDP solution methods [Dibangoye et al., 2013] may be useful for TI-MMDP problems if they can deliver sufficient quality policies. That is, since they assume less information available, the value of Dec-MDP policies will at best equal that of their MMDP counterparts, but in practice the expected value obtained from following a decentralised policy may be lower. We investigate if this is the case in our first experiment, which compares the expected value of optimal MMDP policies found by CoRe with optimal Dec-MDP policies, as found by the GMAA-ICE* algorithm [Oliehoek et al., 2013a].\nFor this experiment we use two benchmark sets: rand[h], 3 sets of 1000 random two-agent problems with horizons h ∈ [3, 4, 5], and coordint, a set of 1000 coordination-intensive instances. The latter set contains tightly-coupled agents with dependencies constructed in such a way that maintenance delays inevitably lead to hindrance unless agents coordinate their decisions when such a delay becomes known, which is ar the first time step after starting the maintenance task. Figure 4a shows the ratio V π ∗\nDEC /V π∗ MMDP of the expected value of the optimal Dec-MDP policy V π∗ DEC versus that of the optimal\nMMDP policy V π ∗\nMMDP . In the random instances the expected values of both policies equal in approximately half of the instances. For coordination-intensive instances coordint decentralised policies result in worse results – on average the reward loss is about 33%, but it can be 75% – demonstrating that decentralised policies are inadequate for our purposes. As this experiment only served to establish that decentralised methods are indeed not applicable, from now on only centralised methods are considered.\nIn our remaining experiments we used a random test set mpp with 2, 3 and 4-agent problems (400 each) with 3 maintenance tasks, horizons 5 to 10, random delay probabilities and binary reward interactions. We compare CoRe against several other algorithms to investigate the performance of the algorithm. The current state-of-the-art approach to solve MPP, presented in [Scharpff et al., 2013], uses the value iteration algorithm SPUDD [Hoey et al., 1999] and solves an efficient MDP encoding of the problem. SPUDD uses algebraic decision diagrams to compactly represent all rewards and is in this sense somewhat similar to our work, however it does not implicitly partition and decouple rewards. Besides the SPUDD solver we included a dynamic programming algorithm (DP) that uses a depth-first approach to maximise the Bellman equation of Equation 2. In addition to basic value iteration we implemented some domain knowledge in this algorithm to quickly identify and prune sub-optimal and infeasible branches during evaluation. Finally, to analyse the impact that branch-and-bound can have in a task-based planning domain such as MPP we added also a CRG-enabled policy search algorithm (CRG-PS), a variant of our CoRe algorithm that uses CRGs but not branch-and-bound pruning.\nUsing these algorithms, we first study the impact of using CRGs on the number of joint actions that need to be evaluated. SPUDD is not considered in this experiment because because it does not report this information. Figure 4b shows the search space size reduction by CRGs in this domain. Our CRG-enabled algorithm (CRG-PS, blue) approximately decimates the number of evaluated joint actions compared to the DP method (green). Furthermore, when value bounds are used (CoRe, red), this number is reduced even more, although its effect varies per instance.\nHaving observed the policy search space reduction that CoRe can achieved, we are interested in the scalability of the algorithm in terms of number of agents and planning horizon. Figure 4c shows the percentage of problems from the mpp test set that are solved within 30 minutes per method (all twoagent instances were solved and hence omitted). CoRe solves more instances than SPUDD (black) of the 3 agent problems (cross marks), and only CRG-PS and CoRe solve 4-agent instances. This is because CRGs successfully exploit the conditional action independence that decouples the agents for most of the planning decisions. Only when reward interactions may occur actions are coordinated, whereas SPUDD always coordinates every joint decision. Notice also that the use of branch-and-bound enables CoRe to solve more instances, compared to the CRG-enabled policy search.\nNext we investigate the runtime that was required by CoRe versus that by the current best known method based on SPUDD. As CoRe achieves a greater coverage than SPUDD, we compare runtimes only for instances successfully solved by the latter (Figure 4d). We order the instances on their SPUDD runtime (causing the apparent dispersion in CoRe runtimes) and plot runtimes of both. Note that as a result the horizontal axis is not informative, it is the vertical axis plotting the runtime that we are interested in. CoRe solves almost all instances faster than SPUDD, both with 2 and 3 agents, and has a greater solving coverage: CoRe failed to solve 3.4% of the instances solved by SPUDD whereas SPUDD failed 63.9% of the instances that CoRe solved.\nFinally, to study the agent-scalability of CoRe, we generated a test set pyra with a pyramid-like reward interaction structure: every first action of the k-th agent depends on the first action of agent 2k and agent 2k+1. Figure 4e shows the percentage of solved instances from the pyra test for various problem horizons. Whereas previous state-of-the-art solved instances up to only 5 agents, CoRe successfully solved about a quarter of the 10 agent problems (h = 4) and overall solves many of the previously unsolvable instances."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "In this work, we focus on optimally (and centrally) solving fully-observable, stochastic planning problems where agents are dependent only through interaction rewards. We partition individual and interaction rewards per agent in conditional return graphs, a compact and efficient data structure when interactions are sparse and/or non-recurrent. We propose a conditional return policy search algorithm\n(CoRe) that uses reward bounds based on CRGs to reduce the search space size, shown to be by orders of magnitude in the maintenance planning domain. This enables CoRe to overall decrease the runtime required compared to the previously best approach and solve instances previously deemed unsolvable. The reduction in search space follows from three key insights: 1) when interactions are sparse, the number of unique returns per agent is relatively small and can be stored efficiently, 2) we can use CRGs to maintain bounds on the return, and thus the expected value, and use this to guide our search, and 3) in the presence of conditional reward independence, i.e. the absence of further reward interactions, we can decouple agents during policy search.\nOur experiments show that the impact of reduction is by orders of magnitude in the maintenance planning domain. This enables CoRe to solve instances that were previously deemed unsolvable. In addition, to scaling to larger instances, CoRe almost always produces solutions faster than the previously best approach. Moreover, CoRe is able to scale up to 10-agent instances when the reward structure exhibits a high level of conditional reward independence, whereas previous methods did not scale beyond 5 agents. Finally, our experiments also illustrate that using a decentralised MDP approach, a line of research that has seen many scalable approaches in terms of agents and reward structures, leads to suboptimal expected policy values.\nHere only optimal solutions are considered, but CRGs can be combined with approximation in several ways. First, the reward structure of the problem itself may be approximated. For instance, the reward-function approximation of [Koller and Parr, 1999] can be applied to increase reward sparsity, or CRG paths with relatively small reward differences may be grouped, trading off a (bounded) reward loss for compactness. Secondly, the CRG bounds directly lead to a bounded-approximation variant of CoRe, usable in for instance the approximate multi-objective method of [Roijers et al., 2014]. Lastly, the CRG structure can be implemented in any (approximate) TI-MMDP algorithm or, vice versa, any existing approximation scheme for MMDP that preserves TI can be used within CoRe.\nAlthough we focused on transition-independent MMDPs, CRGs may be interesting for general MMDPs when transition dependencies are sparse. This would require including dependent-state transitions in the CRGs similar to reward-interaction paths and is considered to be future work. Another interesting avenue is to exploit conditional reward independence during joint action generation."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This research is supported by the NWO DTC-NCAP (#612.001.109), Next Generation Infrastructures, Almende BV and NWO VENI (#639.021.336) projects."
    }, {
      "heading" : "Appendix: Proof of Theorem 2",
      "text" : "In this appendix, we prove the correctness of the CoRe algorithm (Theorem 2). We define several notational shorthands for convenience. For two (sub)sets of agents A,B ⊆ N , ReAB ⊆ R is the set of all rewards for which A ∩ B ∩ e 6= ∅. ReA 6B ⊆ R is the set of rewards such that A ∩ e 6= ∅ and B ∩ e = ∅. Observe that the individual rewards for all agents a ∈ A are thus contained within ReA 6B (and similarly all rewards Rb are included in Re6AB). Furthermore, two agent sets A and B are conditionally reward independent, denoted CRI (A,B, θt), as result of history θt iff ∀a ∈ A, b ∈ B : CRI (a, b, θt). Finally, τ e = ({sj}j∈e, {~a j}j∈e, {ŝj}j∈e) denotes a transition local to agents e and a global transition, i.e. e = N , is denoted by τ .\nLemma A.1. Given an execution history θt = [s0,~a0, . . . , su, . . . , st] up to time t that can be partitioned into two histories, θu = [s0, . . . , su] and θu′ = [su, . . . , st], and a disjoint partitioning of agents N = N1 ∪ N2 ∪ . . . ∪ Nk such that for every pair Na, Nb ∈ N it holds that CRI (Na, Nb, θu) when a 6= b.\nThe return Z(θt) (as defined in Equation 3) can be decoupled as:\nZ(θu) + k∑ i=1 ZNi(θ Ni u′ ) (A.13)\nwhere θNiu′ is the execution history only containing the states and actions of the agents in the setNi ⊆ N , starting from time u. Proof. Recall from Definition 5 that two agents i, j ∈ N are CRI iff ∀Re ∈ R s.t. {i, j} ⊆ e :∑h x=tR\ne(sx,~ax, sx+1) = 0 for every pair of states st, st+1 and all joint actions ~at, given execution history θt. Moreover, recall that the MMDP rewardsR w.l.o.g. are structured as R(τ) = ∑ Re∈RR\ne(τ e). Let A and B be disjoint subsets of agents such that A ∪ B = N and let the reward functions be accordingly partitioned as disjoint sets: R = ReA 6B∪Re6AB∪ReAB . Now assume that for a given execution history θt = θu∪θu′ we have CRI (A,B, θu). From the state su resulting from the execution history θu, all future rewards can only be local with respect to subsets A and B because every reward ReAB must be zero by definition of CRI. Therefore we can rewrite the (future) global reward R(τ) of every possible transition τ (in every possible θNiu′ ) as:∑\nRe∈Re Re(τ) = ReA 6B(τA) +Re6AB(τB) +ReAB(τAB) (A.14)\n= ∑\nRe∈ReA 6B\nRe(τA) + ∑\nRe∈Re6AB\nRe(τB) (A.15)\nwhere the transition decoupling is possible due to transitional independence. Remember that we can write the returns for an execution history θh as (Eq. 3)\nZ(θt) = t−1∑ x=0 ∑ Re∈R Re(τ eθ,x) (A.16)\nin which τ eθ denotes the transition in the execution history θt local to agents e. Then, for two disjoint agent subsets A ∪B = N that have CRI (A,B, θu) as a result of θu:\nZ(θt) = Z(θu) + Z(θu′) (A.17)\n= Z(θu) + t−1∑ x=u [ ReA 6B(τAθ,x) +Re6AB(τBθ,x) ] (A.18)\n= Z(θu) + t−1∑ x=u ReA 6B(τAθ,x) + t−1∑ x=u Re6AB(τBθ,x) (A.19) = Z(θu) + ZA(θ A u′) + ZB(θ B u′) (A.20)\nAs a consequence, we can decouple the computation of returns for agent sets A and B from time u. For now we have only considered two agent sets A and B, however we can apply this decoupling recursively, in order to obtain an arbitrary disjoint partitioning of agents such thatN1∪N2∪. . .∪Nk = N and ∀Na, Nb ∈ N : CRI (Na, Nb, θu). That is, without loss of generality, we choose A = Na and B = N2∪ . . .∪Nk and decouple the return as Z(θu)+ZA(θAu′)+ZB(θBu′). We now observe that we can rewrite ZB itself as ZN2(θ N2 u′ ) + ZB\\N2(θ B\\N2 u′ ) by following the same steps, because both sets again satisfy conditional reward independence. By continuing this process we obtain Equation A.13.\nAs a result of Lemma A.1, Equation 4 and transitional independence we can now also decouple the policy values of two sets of agents, Na and Na, from time u onwards, when it holds that CRI (Na, Nb, θu).\nCorollary A.1. When at a timestep u, we have observed θu and there is a disjoint partitioning of agents N = N1 ∪ N2 ∪ . . . ∪ Nk such that for every pair Na, Nb ∈ N it holds that CRI (Na, Nb, θu) when a 6= b, the value of a given policy π, V (st) can be decoupled as:\nV π(st) = k∑ i=1 V πNi(s Ni t ) (A.21)\nProof. Starting from Equation 4, we first observe that the return Z(θu′) from timestep u onwards is equal to ∑k i=1 ZNi(θ Ni u′ ) (Lemma A.1) and that, because of transition independence, each set Ni of agents has independent probability distributions over future execution histories θu′ . Thus we have the following equalities:\nV (st) = ∑\nθu′ |π,θt\nPr(θu′)Z(θu′) (A.22)\n= ∑\nθu′ |π,θt\nPr(θu′) k∑ i=1 ZNi(θ Ni u′ ) = ∑ θu′ |π,θt k∑ i=1 Pr(θu′)ZNi(θ Ni u′ ) (A.23)\n= k∑ i=1 ∑ θu′ |π,θt Pr(θu′)ZNi(θ Ni u′ ) = k∑ i=1 ∑ θ Ni u′ |π,θt Pr(θNiu′ )ZNi(θ Ni u′ ) (A.24)\n= k∑ i=1 V πNi(s Ni t ) (A.25)\nLemma A.2. The bounding heuristics L(set ) and U(set ) used to prune during branch-and-bound search are admissible with respect to the expected value V (set ) of state s e t for agents e ⊆ N at time t.\nProof. We proof the admissibility of the bounding heuristics by induction. For sake of brevity, we only show the upper bound proof, but the proof for the lower bound can be written down accordingly. Recall that R = ⋃ i∈N Ri is the disjoint partitioning of reward functions over the CRGs.\nFirst, consider the very last timestep, h− 1, for which there is no future reward, i.e.,\nV (sh−1) = max ~a ∑ sh∈S T (τh−1)R(τh−1) = max ~a ∑ sh∈S T (τh−1) ∑ i∈N Ri(τ eh−1) (A.26)\n≤ max ~a max sh∈S ∑ i∈N Ri(τ eh−1) (A.27)\n≤ ∑ i∈N max τeh−1∈φi(s i h−1) Ri(τ eh−1) (A.28)\n= ∑ i∈N U(sih−1) (A.29)\nThen, we show that if for a next stage t + 1 we have a valid upper bound, the value for a state st is also upper bounded by ∑ i∈N U(s i t). And therefore, that because U(s i h−1) is a valid upper bound on\nV (sh−1), the upper bound is admissible for all stages before h− 1:\nV (st) = max ~a ∑ set+1∈S T (τt) (R(τt) + V (st+1)) (A.30)\n= max ~a e ∑ st+1∈S T (τt)\n( V (st+1) +\n∑ i∈N Ri(τ et )\n) (A.31)\n≤ max ~a e ∑ st+1∈S T (τt) ∑ i∈N ( Ri(τ et ) + U(sit+1) ) (A.32)\n≤ ∑ i∈N max τet ∈φi(sit) ( Ri(τ et ) + U(sit+1) ) (A.33)\n= ∑ i∈N U(sit) (A.34)\nFrom the bounds on the state-values, L(st) = ∑ i∈N L(sit) ≤ V (st) ≤ ∑ i∈N U(sit) = U(st),\nwe can also distill admissible bounds on state-action values, Q(st,~a). Taking the standard MDP definition for Q(st,~a),\nQ(st,~a) = ∑\nŝt+1∈S T (st,~a, ŝt+1) (R(st,~a, ŝt+1) + V (st+1)) ,\nwe replace V (st+1) by the corresponding lower or upper bounds:\nB(st,~a) = ∑\nŝt+1∈S T (st,~a, ŝt+1) (R(st,~a, ŝt+1) +B(st+1)) .\nUsing B(st,~a), CoRe can exclude a joint action ~a after execution history θt from consideration when there is another joint action ~a′ for which U(st,~a) ≤ L(st,~a′), as is standard in branch-and-bound algorithms.\nConcluding the proof of Theorem 2. As a direct consequence of Corollary A.1 agents can be decoupled during policy search without losing optimality. Moreover, Lemma A.2 shows that both the upper and lower bounds are admissible heuristic functions with respect to the expected policy value from a given state. In the main loop, the CoRe algorithm recursively expands and evaluates all possible extensions to the current execution path, except for those starting with actions that lead to a lower upper bound than another action’s lower bound.\nAs the policy search considers all possible execution histories, excluding pruned, non-optimal ones, the search will eventually return the optimal policy value V ∗, and corresponding policy, thus proving Theorem 2.\nMoreover, as there is only a finite number of execution histories, the CoRe algorithm is also guaranteed to terminate in a finite number of recursions."
    } ],
    "references" : [ {
      "title" : "Traffic Light Control by Multiagent Reinforcement Learning Systems, chapter Interactive Collaborative Information Systems, pages 475–510",
      "author" : [ "Bakker et al", "B. 2010] Bakker", "S. Whiteson", "L. Kester", "F. Groen" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Decentralized Markov decision processes with event-driven interactions",
      "author" : [ "Becker et al", "R. 2004] Becker", "S. Zilberstein", "V. Lesser" ],
      "venue" : "In Proceedings of the Int. Conf. on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2004
    }, {
      "title" : "Transition-independent decentralized Markov decision processes",
      "author" : [ "Becker et al", "R. 2003] Becker", "S. Zilberstein", "V. Lesser", "C.V. Goldman" ],
      "venue" : "In Proceedings of the Int. Conf. on Autonomous Agents and Multiagent",
      "citeRegEx" : "al. et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2003
    }, {
      "title" : "Optimal coordinated planning amongst self-interested agents with private state",
      "author" : [ "Cavallo et al", "R. 2006] Cavallo", "D.C. Parkes", "S. Singh" ],
      "venue" : "In Proceedings of Uncertainty in Artificial Intelligence",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "Solving sparse delayed coordination problems in multiagent reinforcement learning",
      "author" : [ "De Hauwere et al", "2012] De Hauwere", "Y.-M", "P. Vrancx", "A. Nowé" ],
      "venue" : "In Adaptive and Learning Agents,",
      "citeRegEx" : "al. et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2012
    }, {
      "title" : "Producing efficient error-bounded solutions for transition independent decentralized MDPs",
      "author" : [ "Dibangoye et al", "J.S. 2013] Dibangoye", "C. Amato", "A. Doniec", "F. Charpillet" ],
      "venue" : "In Proceedings of the Int. Conf. on Autonomous",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Multiagent planning with factored MDPs",
      "author" : [ "Guestrin et al", "C. 2002a] Guestrin", "D. Koller", "R. Parr" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2002
    }, {
      "title" : "Contextspecific multiagent coordination and planning with factored MDPs",
      "author" : [ "Guestrin et al", "C. 2002b] Guestrin", "S. Venkataraman", "D. Koller" ],
      "venue" : "In Proceedings of the Eighteenth National Conference on Artificial",
      "citeRegEx" : "al. et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2002
    }, {
      "title" : "SPUDD: Stochastic planning using decision diagrams",
      "author" : [ "Hoey et al", "J. 1999] Hoey", "R. St-Aubin", "A. Hu", "C. Boutilier" ],
      "venue" : "Proceedings of Uncertainty in Artificial Intelligence",
      "citeRegEx" : "al. et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1999
    }, {
      "title" : "Sparse cooperative Q-learning",
      "author" : [ "Kok", "Vlassis", "J.R. 2004] Kok", "N. Vlassis" ],
      "venue" : "In Proceedings of the Int. Conf. on Machine Learning,",
      "citeRegEx" : "Kok et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Kok et al\\.",
      "year" : 2004
    }, {
      "title" : "Computing factored value functions for policies in structured MDPs",
      "author" : [ "Koller", "Parr", "D. 1999] Koller", "R. Parr" ],
      "venue" : "In Proceedings of the International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Koller et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Koller et al\\.",
      "year" : 1999
    }, {
      "title" : "Learning of coordination: Exploiting sparse interactions in multiagent systems",
      "author" : [ "Melo", "Veloso", "F.S. 2009] Melo", "M. Veloso" ],
      "venue" : "In Proceedings of The 8th Int. Conf. on Autonomous Agents and Multiagent Systems-",
      "citeRegEx" : "Melo et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Melo et al\\.",
      "year" : 2009
    }, {
      "title" : "Decentralized MDPs with sparse interactions",
      "author" : [ "Melo", "Veloso", "F.S. 2011] Melo", "M. Veloso" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Melo et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Melo et al\\.",
      "year" : 2011
    }, {
      "title" : "GSMDPs for multi-robot sequential decision-making",
      "author" : [ "Messias et al", "J.V. 2013] Messias", "M.T.J. Spaan", "P.U. Lima" ],
      "venue" : "In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Solving very large weakly coupled Markov decision processes",
      "author" : [ "Meuleau et al", "N. 1998] Meuleau", "M. Hauskrecht", "Kim", "K.-E", "L. Peshkin", "L.P. Kaelbling", "T.L. Dean", "C. Boutilier" ],
      "venue" : "Proceedings of the Fifteenth",
      "citeRegEx" : "al. et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1998
    }, {
      "title" : "Offline planning for communication by exploiting structured interactions in decentralized MDPs",
      "author" : [ "Mostafa", "Lesser", "H. 2009] Mostafa", "V. Lesser" ],
      "venue" : "In Proceedings of the International Joint Conference on Web",
      "citeRegEx" : "Mostafa et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Mostafa et al\\.",
      "year" : 2009
    }, {
      "title" : "Networked distributed POMDPs: A synthesis of distributed constraint optimization and POMDPs",
      "author" : [ "Nair et al", "R. 2005] Nair", "P. Varakantham", "M. Tambe", "M. Yokoo" ],
      "venue" : "Proceedings of the Twentieth National Conference",
      "citeRegEx" : "al. et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2005
    }, {
      "title" : "Incremental clustering and expansion for faster optimal planning in decentralized POMDPs",
      "author" : [ "Oliehoek et al", "F.A. 2013a] Oliehoek", "M.T.J. Spaan", "C. Amato", "S. Whiteson" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Exploiting locality of interaction in factored DecPOMDPs",
      "author" : [ "Oliehoek et al", "F.A. 2008] Oliehoek", "M.T.J. Spaan", "S. Whiteson", "N. Vlassis" ],
      "venue" : "In Proceedings of the Int. Conf. on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "Factored upper bounds for multiagent planning problems under uncertainty with non-factored value functions",
      "author" : [ "Oliehoek et al", "F.A. 2015] Oliehoek", "M.T.J. Spaan", "S.J. Witwicki" ],
      "venue" : "In Proc. of International Joint Conference",
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximate solutions for factored Dec-POMDPs with many agents",
      "author" : [ "Oliehoek et al", "F.A. 2013b] Oliehoek", "S. Whiteson", "M.T.J. Spaan" ],
      "venue" : "In Proceedings of the Int. Conf. on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Bounded approximations for linear multi-objective planning under uncertainty",
      "author" : [ "Roijers et al", "D.M. 2014] Roijers", "J. Scharpff", "M.T.J. Spaan", "F.A. Oliehoek", "M. De Weerdt", "S. Whiteson" ],
      "venue" : "In Proceedings of the Int. Conf",
      "citeRegEx" : "al. et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2014
    }, {
      "title" : "Planning under uncertainty for coordinating infrastructural maintenance",
      "author" : [ "Scharpff et al", "J. 2013] Scharpff", "M.T.J. Spaan", "M. de Weerdt", "L. Volker" ],
      "venue" : "In Proceedings of the Int. Conf. on Automated Planning and Schedul-",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Decentralized planning under uncertainty for teams of communicating agents",
      "author" : [ "Spaan et al", "M.T.J. 2006] Spaan", "G.J. Gordon", "N. Vlassis" ],
      "venue" : "In Proceedings of the Int. Conf. on Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "Letting loose a SPIDER on a network of POMDPs: Generating quality guaranteed policies",
      "author" : [ "Varakantham et al", "P. 2007] Varakantham", "J. Marecki", "Y. Yabu", "M. Tambe", "M. Yokoo" ],
      "venue" : "Proceedings of the Int",
      "citeRegEx" : "al. et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2007
    }, {
      "title" : "Influence-based policy abstraction for weakly-coupled Dec-POMDPs",
      "author" : [ "Witwicki", "Durfee", "S.J. 2010] Witwicki", "E.H. Durfee" ],
      "venue" : "In Proceedings of the Int. Conf. on Automated Planning and Scheduling,",
      "citeRegEx" : "Witwicki et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Witwicki et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "In cooperative multi-agent sequential decision making under uncertainty, agents must coordinate to find an optimal joint policy that maximises joint value. Typical algorithms exploit additive structure in the value function, but in the fully-observable multi-agent MDP (MMDP) setting such structure is not present. We propose a new optimal solver for transition-independent MMDPs, in which agents can only affect their own state but their reward depends on joint transitions. We represent these dependencies compactly in conditional return graphs (CRGs). Using CRGs the value of a joint policy and the bounds on partially specified joint policies can be efficiently computed. We propose CoRe, a novel branch-and-bound policy search algorithm building on CRGs. CoRe typically requires less runtime than the available alternatives and finds solutions to previously unsolvable problems.",
    "creator" : "LaTeX with hyperref package"
  }
}