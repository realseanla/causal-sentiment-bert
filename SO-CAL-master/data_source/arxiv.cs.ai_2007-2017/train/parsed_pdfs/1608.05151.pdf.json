{
  "name" : "1608.05151.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Effective Multi-step Temporal-Difference Learning for Non-Linear Function Approximation",
    "authors" : [ "Harm van Seijen" ],
    "emails" : [ "harm.vanseijen@maluuba.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Multi-step update targets play an important role in TD learning (Sutton, 1988) and reinforcement learning (Sutton & Barto, 1998; Szepesvári, 2009). The core concept behind TD learning is to bootstrap the value of one state (or state-action pair) from the value of another state (or state-action pair). With one-step update targets the state that is bootstrapped from lies one time step in the future; with multi-step update targets bootstrapping occurs with respect to values of states that lie further in the future. Controlling from which states bootstrapping occurs is important, because it affects the fundamental trade-off between bias and variance of updates. The trade-off that produces the best performance is different from domain to domain, but for most domains the best trade-off lies somewhere in between a one-step update target (high bias, but low variance) and an update with the full return (unbiased, but high variance). This has made TD(λ), where the trade-off between variance and bias of the update target can be controlled by the parameter λ, one of the most popular TD methods in linear function approximation.\nWhile TD(λ) and its control variant Sarsa(λ) are very popular in the case of linear function approximation, when non-linear function approximation is used to represent the value function single-step methods are the norm. A reason could be that in many domains with non-linear function approximation TD(λ) does not perform particularly well. In particular, it is very susceptible to divergence of values. We argue that the underlying reasons for this instability are not unique to non-linear function\nar X\niv :1\n60 8.\n05 15\n1v 1\n[ cs\n.A I]\napproximation; it is a more general phenomenon of traditional TD(λ). However, the issues are more prominent when non-linear function approximation is used for two reasons. First, for table lookup or linear function approximation with binary features, an alternative version of TD(λ) is available (TD(λ) with replacing traces) that is less sensitive to divergence (Singh & Sutton, 1996). Second, value blow-ups occur especially in domains where the same feature is active (i.e., has a value 6= 0) for many subsequent time steps (van Seijen et al., 2015). This is something that occurs often with non-linear function approximation, because features are typically more general in this setting and can be active over a large part of the state-space.\nWe show that the susceptibility of TD(λ) to divergence stems from a deviation of TD(λ) from the general TD update rule based on gradient descent that is formalized by its forward view. Unfortunately, while the forward view is less susceptible to divergence, it is expensive to implement (both the computation time per step and required memory grow over time), making it not a practical alternative to TD(λ). To address this, we present an alternative version of TD(λ), which we call forward TD(λ), that implements the gradient-descent-based update rule exactly and is computationally efficient as well. The price that is payed to achieve this is that updates occur with a delay. However, we show empirically that the advantages of having an exact implementation of the gradient-descent-based update rule substantially outweigh the disadvantages of having a delay in the updates."
    }, {
      "heading" : "2 Related Work",
      "text" : "This work is related to true online temporal-difference learning (van Seijen & Sutton, 2014; van Seijen et al., 2015). The non-linear, online λ-return algorithm presented in Section 4 is a direct extension of the linear, online λ-return algorithm that underlies true online TD(λ). In the linear case, the computationally inefficient forward view equations can be rewritten in computationally efficient backward view equations, yielding the true online TD(λ) algorithm. Unfortunately, this is not possible in the non-linear case, because the derivation of the true online equations makes use of the fact that the gradient with respect to the value function is independent of the weight vector, which does not hold in the case of non-linear function approximation.\nForward TD(λ) is similar to a method introduced by Cichosz (1995). Specifically, Cichosz’s method is based on the same update target as forward TD(λ). Interestingly, Cichosz presents his method in the context of linear function approximation as a computationally efficient alternative to traditional TD(λ). While we focus primarily on sample efficiency in the non-linear setting, like Cichosz’s method, forward TD(λ) also has computational advantages. In fact, forward TD(λ) is more efficient than Cichosz’s method. Forward TD(λ) has the same computation-time complexity as TD(0); by contrast, the computation-time of Cichosz’s method depends on K."
    }, {
      "heading" : "3 Background",
      "text" : "Our problem setting is that of a Markov decision processes (MDP), which can be described as a 5-tuple of the form 〈S,A, p, r, γ〉, consisting of S, the set of all states; A, the set of all actions; p(s′|s, a), the transition probability function, giving for each state s ∈ S and action a ∈ A the probability of a transition to state s′ ∈ S at the next step; r(s, a, s′), the reward function, giving the expected reward for a transition from (s, a) to s′. γ is the discount factor, specifying how future rewards are weighted with respect to the immediate reward. An MDP can contain terminal states, which terminate an episode. Mathematically, a terminal state can be interpreted as a state with a single action that results in a reward of 0 and a transition to itself.\nThe return at time t is defined as the discounted sum of rewards, observed after t:\nGt = Rt+1 + γ Rt+2 + γ 2Rt+3 + ... = ∞∑ i=1 γi−1Rt+i ,\nwhere Rt+1 is the reward received after taking action At in state St.\nActions are taken at discrete time steps t = 0, 1, 2, ... according to a policy π : S × A → [0, 1], which defines for each action the selection probability conditioned on the state. Each policy π has a corresponding state-value function vπ(s), which maps each state s ∈ S to the expected value of the return Gt from that state, when following policy π:\nvπ(s) = E{Gt |St = s, π} .\nThe value of a terminal state is (by definition) 0.\nTemporal-Difference (TD) learning aims to learn the state-value function using a strategy based on stochastic gradient descent (Bertsekas, 1995). Let V̂ (s|θ) be an estimate of vπ(s) given the weight vector θ ∈ Rn. Then, the general form of the TD update is:\nθt+1 = θt + α ( Ut − V̂ (St|θt) ) ∇θV̂ (St|θt) , (1)\nwhere α > 0 is the step-size parameter, Ut is the update target, and∇θV̂ is the gradient of V̂ with respect to the weight vector θ. The update target Ut is some estimate of the value vπ(St). A simple example is the TD(0) update target, which uses the estimate of the next state to bootstrap from:\nUt = Rt+1 + γV̂ (St+1|θt) .\nThe update equations for TD(λ) are:\nδt = Rt+1 + γV̂ (St+1|θt)− V̂ (St|θt) et = γλet−1 +∇θV̂ (St|θt)\nθt+1 = θt + αδt et\nwhere et is called the eligibility-trace vector. While these updates appear to deviate from the gradientdescent-based update rule given in (1), there is a close connection with this update rule. In the next section, we go deeper into the details of this relation."
    }, {
      "heading" : "4 Analysis of TD(λ)",
      "text" : "That TD(λ) is a multi-step method is not immediately obvious, because its update equations are different in form than (1), making it hard to specify what the update target is. That TD(λ) is a multi-step method becomes clear from the fact that the weights computed by TD(λ) are similar to those computed by a different algorithm that does have a well-defined multi-step update target, called the λ-return algorithm. The λ-return algorithm is also referred to as the forward view of TD(λ). While the traditional λ-return algorithm is similar to TD(λ) only at the end of an episode (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996), below we specify a more general version that is similar to TD(λ) at all time steps.\nWe define the λ-return for time step t with horizon h ≥ t+ 1 as follows:\nG λ|h t := (1− λ) h−t−1∑ n=1 λn−1G (n) t + λ h−t−1G (h−t) t (2)\nwhere G(n)t is the n-step return, defined as:\nG (n) t := n∑ k=1 γk−1Rt+k + γ n V̂ (St+n|θt+n−1).\nNote that Gλ|ht uses information only up to the horizon h. We define θt as the result of a sequence of updates of the form (1), based on states S0, . . . , St−1 and update targets G λ|t 0 , . . . , G λ|t t−1, respectively. Formally, we define θt := θtt, with θ t t incrementally defined by: 1\nθtk+1 := θ t k + α ( G λ|t k − V̂ (Sk|θ t k) ) ∇θV̂ (Sk|θtk), for 0 ≤ k < t . (3)\nwith θt0 := θ0 for all t and θ0 being the weight vector at the start of the episode. We call the algorithm that implements these updates the online λ-return algorithm. Furthermore, we define the offline λ-return algorithm as the algorithm that performs (3) only at the end of an episode. That is, θt := θ0 for 0 ≤ t < T , with T the time step of termination, while θT := θTT , with θ T T defined incrementally by (3). 1Note that the sequence of updates is different for each time step, due to the different horizons, requiring the double indices for the weight vectors.\nFigure 1 illustrates the difference between the online and offline λ-return algorithm and TD(λ), by showing the RMS error on a random walk task. The task consists of 10 states laid out in a row plus a terminal state on the left. Each state transitions with 70% probability to its left neighbour and with 30% probability to its right neighbour (or to itself in case of the right-most state). All rewards are 1, and γ = 1. The right-most state is the initial state.\nThe theorem below states that for appropriately small step-sizes TD(λ) behaves like the online λ-return algorithm. We provide the proof for the theorem in Appendix A. The theorem uses the term ∆ti, which we define as:\n∆ti := ( Ḡ λ|t i − V̂ (Si|θ0) ) ∇θV̂ (Si|θ0) ,\nwith Ḡλ|ti the interim λ-return for state Si with horizon t that uses θ0 for all value evaluations. Note that ∆ti is independent of the step-size.\nTheorem 1 Let θ0 be the initial weight vector, θtdt be the weight vector at time t computed by TD(λ), and θλt be the weight vector at time t computed by the online λ-return algorithm. Furthermore, assume that ∇θV̂ is well-defined and continuous everywhere and that ∑t−1 i=0 ∆ t i 6= 0. Then, for all time steps t: ||θtdt − θ λ t ||\n||θtdt − θ0|| → 0 as α→ 0.\nWhile TD(λ) behaves for small step-size like the λ-return algorithm, in practise a small step-size often results in slow learning. Hence, higher step-sizes are desirable. Figure 1 suggests that for higher step-sizes, TD(λ) has a disadvantage with respect to the online λ-return algorithm. We analyze why this is the case, using the one-state example shown in the left of Figure 2.\nThe right of Figure 2 shows the RMS error over the first 10 episodes for different step-sizes and λ = 1. While for small step-sizes, TD(λ) indeed behaves like the λ-return algorithm, for larger step-sizes the difference becomes huge.\nTo understand the reason for the large difference in performance, we derive an analytical expression for the value at the end of an episode. First, we consider the λ-return algorithm. Because there is\nonly one state involved, we indicate the value of this state simply by V̂ . The value at the end of an episode, V̂T , is equal to V̂ TT , resulting from the update sequence:\nV̂ Tk+1 = V̂ T k + α(G λ|T k − V̂ T k ) for 0 ≤ k < T\nBy substitution, we can directly express V̂T in terms of the initial value, V̂0, and the update targets:\nV̂T = (1− α)T V̂0 + α(1− α)T−1Gλ|T0 + α(1− α)T−2G λ|T 1 + · · ·+ αG λ|T T−1\nUsing that Gλ|Tk = 1 for all k, this can be written as a single pseudo-update:\nV̂T = V̂0 + β(1− V̂0) (4)\nwith β = 1 − (1 − α)T . Note that a larger α or T results in a larger β, but its value is bounded. Specifically, 0 ≤ α ≤ 1⇒ 0 ≤ β ≤ 1.\nWe now consider TD(λ). The update at the end of an episode is V̂T = V̂T−1 + αeT−1δT−1 . In our example, δt = 0 for 0 ≤ t < T − 1, while δT−1 = 1 − VT−1. Because δt is 0 for all time steps except the last, VT−1 = V0. Furthermore, ∇θV̂ reduces to 1 in our example, resulting in eT−1 = T . Substituting all this in the above equation also reduces it to pseudo-update (4), but with β = αT . So for TD(λ), β can grow much larger than 1, causing divergence of values, even for α < 1. This is the reason that TD(λ) can be very sensitive to the step-size and it explains why the optimal step-size for TD(λ) is much smaller than the optimal step-size for the λ-return algorithm in Figure 4 (α ≈ 0.15 versus α = 1, respectively). Moreover, because the variance on β is higher for TD(λ) the performance at optimal α of TD(λ) is worse than the performance at optimal α for the λ-return algorithm. In Section 6, we show empirically that the general behaviour of TD(λ) shown in Figure 2 also occurs in more complex domains.\nWhile the online λ-return algorithm has clear advantages over TD(λ), it is not a practical algorithm: the number of updates that need to be performed per time step grows over time, as well as the memory requirements. On the other hand, the offline λ-return algorithm is undesirable, because it performs no updates during an episode and cannot be applied to non-episodic tasks. In the next section, we present forward TD(λ), a computationally efficient algorithm that forms a middle ground between the online and the offline λ-return algorithm."
    }, {
      "heading" : "5 Forward TD(λ)",
      "text" : "The online λ-return algorithm uses update targets that grow with the data horizon. This has the advantage that updates can be performed immediately, but also causes the computation time per time step to grow over time. In this section, we present a computationally efficient method that performs updates using a λ-return with a horizon that lies a fixed number of time steps in the future: Gλ|t+Kt with K ∈ {1, 2, . . . }. We refer to this update target as the K-bounded λ-return.\nA consequence of using update target Gλ|t+Kt with fixed K is that during the first K − 1 time steps no updates occur. In other words, θt := θ0 for 1 ≤ t < K. The weights θK through θT−1 are defined as follows:\nθt+K := θt+K−1 + α ( G λ|t+K t − V̂ (St|θt+K−1) ) ∇θV̂ (St|θt+K−1) , for 0 ≤ t < T −K.\nAt the end of an episode K updates occur. Following the convention of the double indices when multiple updates occur at a single time step, we define θT := θTK , with θ T K defined incrementally by:\nθTk+1 := θ T k + α ( G λ|T T−K+k − V̂ (ST−K+k|θ T k ) ) ∇θV̂ (ST−K+k|θTk ) for 0 ≤ k < K ,\nwith θT0 := θT−1.\nThe question of how to set K involves a trade-off. On the one hand, larger values of K bring the end-of-episode weights closer to those of the λ-return algorithm; on the other hand, smaller values of K result in a shorter delay of updates. In general, K should be set in such a way that G λ|t+K t is an accurate estimate of G λ|T t , while not being unnecessary large. How accurately G λ|t+K t\napproximates Gλ|Tt depends on the value γλ, because the contribution of a reward to the K-bounded λ-return reduces exponentially with γλ (we will show this below). While the immediate reward has a contribution of 1, the contribution of a reward K time steps in the future is only (γλ)K . Hence, a sensible strategy for setting K is to find the smallest value of K that still ensures that the value (γλ)K is smaller than some fraction η. This value can be computed as follows:\nK = ceil ( log(η)/log(γλ) ) , (5)\nwhere ceil(·) rounds up to the nearest integer. Note that for γλ < η , K = 1. The value K = 1 is special becauseGλ|t+1t reduces to the TD(0) update target, independent of the value of λ. Furthermore, there is no delay in updates. Hence, forward TD(λ) behaves exactly like TD(0) in this case. For γλ = 1, no finite value of K can ensure that an accurate estimate of Gλ|Tt is obtained. The only way to resolve this is to postpone all updates to the end of an episode (which can be interpreted as K = ∞). In this case, the performance of forward TD(λ) is equal to that of the offline λ-return algorithm.\nNext, we discuss how forward TD(λ) can be implemented efficiently. Our implementation is based on two ways of computing the K-bounded λ-return. We derive the underlying equations in Appendix B. The first way is based on the equation:\nG λ|h+1 t = G λ|h t + (γλ) h−tδ′h , for h ≥ t+ 1 , (6) with δ′h := Rh+1 + γV̂ (Sh+1|θh)− V̂ (Sh|θh−1) . Note that δ′i differs from δi in the index of the weight vector used for the value of Si. Using (6) incrementally, Gt+Kt can be computed, starting from G λ|t+1 t = Rt+1 + γV̂ (St+1|θt), in K − 1 updates.\nThe second way is based on the equation:\nG λ|h t+1 = (G λ|h t − ρt)/γλ , for h ≥ t+ 2 , (7)\nwith ρt = Rt+1 + γ(1− λ) V̂ (St+1|θt) .\nThis equation can be used to compute Gt+Kt+1 from G t+K t . Performing one more update using (6) results in the K-bounded λ-return for time step t + 1: G t+1+Kt+1 . This way of computing the K-bounded λ-return requires only two updates (for any value of K).\nIn theory, the K-bounded λ-return has to be computed incrementally from scratch (using Equation 6) only for the initial state; for the other states it can be computed efficiently using only 2 updates. Unfortunately, this approach does not work well in practise. The reason is that tiny rounding errors that occur on any computer get blown up by dividing by γλ over and over again. For example, consider γλ = 0.5. Then, rounding errors in the K-bounded λ-return at time t will be blown up by a factor (1/γλ)100 = 2100 at time t+ 100. Fortunately, we can avoid these blow-ups in an elegant way, by recomputing the K-bounded λ-return from scratch every K time steps. This ensures that rounding errors will never grow by a factor larger than (1/γλ)K . Moreover, as we argued in the previous subsection, K is set in such a way that the value γλK is just slightly smaller than the hyper-parameter η. Hence, rounding errors will not grow by a factor larger than approximately 1/η. Because η will typically be set to 0.01 or larger (smaller values of η will result in longer update delays, which is undesirable), no issues with rounding error blow-ups will occur.\nWe now analyze the computational complexity of forward Sarsa(λ). For reference purposes, the pseudocode for implementing forward TD(λ) in provided in Algorithm 1. First, we look at computation time. Between time step K and the end of an episode, exactly one state-value evaluation and one state-value update occur. All other computations have O(1) cost. At the end of the episode an additional K − 1 value updates occur, so there is a spike in computation at the end of an episode, but because during the first K − 1 time steps of an episode no updates occur, on average the algorithm still performs only one value update and one value evaluation per time step. This is the same as for TD(0). Hence, forward TD(λ) is very efficient from a computation time perspective. In terms of memory, forward TD(λ) requires the storage of the K most recent feature vectors. So, if n is the number of features, forward TD(λ) requires additional memory of size O(nK) over TD(0) (note that forward TD(λ) does not require storage of an eligiblity-trace vector). If n is large and memory is scarce, K can be bounded by some value Kmax to deal with this.\nAlgorithm 1 forward TD(λ) INPUT: α, λ, γ,θinit, η,Kmax (optional) θ ← θinit If γλ > 0 then: K = ceil ( log(η)/ log(γλ) ) , else: K = 1\nK = min(Kmax,K) (optional) cfinal ← (γλ)K−1 Loop (over episodes): F ← ∅ // F is a FIFO queue (max length: K) Usync ← 0; i← 0; c← 1; Vcurrent ← 0; ready ← false obtain initial state S // or φ(S) While S is not terminal, do:\nobserve reward R and next state S′ If S′ is terminal: Vnext ← 0 , else: Vnext ← V̂ (S′|θ) ρ← R+ γ(1− λ)Vnext push tuple 〈S, ρ〉 on F // or 〈φ(S), ρ〉 δ′ ← R+ γVnext − Vcurrent Vcurrent ← Vnext If i = K − 1 :\nU ← Usync Usync ← Vcurrent ; i← 0; c← 1; ready ← true\nElse: Usync ← Usync + c · δ′ i← i+ 1; c← γλ · c If ready : U ← U + cfinal · δ′ // Gλ|t+Kt ⇐ G λ|t+K−1 t\npop 〈Sp, ρp〉 from F update θ using Sp and U If K 6= 1 : U ← ( U − ρp ) /(γλ) // Gλ|t+Kt+1 ⇐ G λ|t+K t\nS ← S′ If ready = false: U ← Usync While F not empty:\npop 〈Sp, ρp〉 from F update θ using Sp and U If K 6= 1 : U ← (U − ρp)/γλ"
    }, {
      "heading" : "6 Empirical Comparisons",
      "text" : "In our first experiment, we evaluate the performance of TD(λ), forward TD(λ) and the online/offline λ-return algorithm on the standard mountain car task (Sutton & Barto, 1998). The state-space consists of the position and velocity of the car, scaled to numbers within the range [-1, 1]. The value function is approximated with a neural network that has the two state-variables as input, one output variable representing the state value, and a single hidden layer of 50 nodes in between. The backpropagation algorithm is used for obtaining the derivative of the value function with respect to the weights (in a similar way as done by Tesauro, 1994). The evaluation policy is a near-optimal policy. All rewards are drawn from a normal distribution with mean -1 and standard deviation 2. We fixed λ = 0.9 and set η = 0.01 and show the performance for different step-sizes. Our performance metric is the RMS error (over the state distribution induced by the policy) at the end of an episode, averaged over the first 50 episodes. The left graph of Figure 3 shows the results. The results are averaged over 50 independent runs. TD(λ) shows the same behaviour as in the one-state example (Figure 2). That is, the error quickly diverges. Surprisingly, forward TD(λ) outperforms the online λ-return algorithm. That delaying updates results in better performance in this case is probably related to the reason that the DQN algorithm uses a separate target network that is updated in a delayed way (Mnih et al., 2015). Most likely because it reduces instability.\nFor our second experiment, we compared the performance of forward TD(λ) with η ∈ {0.01, 0.1, 0.3} and no maximum K value, for α = 0.015 and different λ values. In addition, we tested η = 0.01 with Kmax = 50. The experimental settings are the same as in the first experiment, except we average over 200 independent runs instead of 50. The right graph of Figure 3 shows the results. This graph shows that the performance at optimal λ is not really affected by η. Hence, in practise η can just be fixed to some small value.\nFor our third and fourth experiment, we used control tasks. Here the goal is to improve the policy in order to maximize the return. To deal with these tasks, we used one neural network per action to represent the action-value and used -greedy action selection. Effectively, this changes TD(λ) into Sarsa(λ) and forward TD(λ) into forward Sarsa(λ).\nOur first control domain is the mountain car task, but now with deterministic rewards of -1. We compared the average return of Sarsa(λ) and forward Sarsa(λ) over the first 50 episodes for different λ. For each λ and each method we optimized α. We used η = 0.01 and = 0.05. The left graph of Figure 4 shows the results. Results are averaged over 200 independent runs. Forward Sarsa(λ) outperforms Sarsa(λ) for all λ values, except for λ = 1.0. This can be explained by the fact that for λ = 1, all updates are delayed until the end of the episode for forward Sarsa(λ), in contrast to the updates of Sarsa(λ).\nOur second control domain is the cart-pole benchmark task, in which a pole has to be balanced upright on a cart for as long as possible (Barto et al., 1983). The state-space consists of the position and velocity of the cart, as well as the angle and angular velocity of the pole; there are only two actions: move left and move right. An episode ends when the angle of the pole deviates a certain number of degrees from its upright position or when the cart position exceeds certain bounds. We used -greedy exploration with = 0.05, and limited the episode length to 1000 steps. Again, η = 0.01. The networks we used for action-value estimation are the same as in the mountain car experiment (1 hidden layer consisting of 50 nodes), expect that each network now has four input nodes, corresponding with scaled versions of the four state-space parameters. We compared the average return over the first 1000 episodes for different λ with optimized α. The right graph of Figure 4 shows the results, averaged over 200 independent runs. In this domain, higher values of λ actually reduce the performance of Sarsa(λ). By contrast, the optimal performance of forward Sarsa(λ) is obtained around λ = 0.6 and is substantially higher than the performance of Sarsa(0). Overall, these results convincingly show that forward Sarsa(λ) outperforms Sarsa(λ), as predicted by our analysis."
    }, {
      "heading" : "7 Conclusions",
      "text" : "We identified the reason why TD(λ) often performs poorly on domains with non-linear function approximation. Deviations from the general TD update rule make TD(λ) susceptible to divergence of value estimates and causes additional variance that reduces performance. While the λ-return algorithm implements the general update rule exactly, it is not a practical alternative, because its computation-time per step, as well as its memory requirements, are much more expensive. To address this, we presented a new method, called forward TD(λ), that exactly implements the general update rule (like the λ-return algorithm), but is also very efficient (like TD(λ)). Specifically, its computation-time complexity is the same as that of TD(0). While forward TD(λ) performs its updates with a delay, we have shown empirically that the performance increase due to exactly following the general update rule more than makes up for the performance decrease due to the update delays. In fact, one of our experiments suggests that the delay in updates could actually have a positive impact on the performance when non-linear function approximation is used. This surprising result is likely related to the same reason that DQN uses a separate target network that is updated in a delayed way and is an interesting topic for future research."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The author thanks Itamar Arel for discussions leading to the development of forward TD(λ). This work was partly supported by grants from Alberta Innovates – Technology Futures and the National Science and Engineering Research Council of Canada."
    }, {
      "heading" : "A Proof TD(λ) is Similar to the Online λ-Return Algorithm",
      "text" : "Theorem 1 Let θ0 be the initial weight vector, θtdt be the weight vector at time t computed by TD(λ), and θλt be the weight vector at time t computed by the online λ-return algorithm. Furthermore, assume that ∇θV̂ is well-defined and continuous everywhere and that ∑t−1 i=0 ∆ t i 6= 0. Then, for all time steps t: ||θtdt − θ λ t ||\n||θtdt − θ0|| → 0 as α→ 0.\nProof We prove the theorem by showing that ||θtdt − θ λ t ||/||θ td t − θ0|| can be approximated by O(α)/ ( C + O(α) ) as α → 0, with C > 0. For readability, we will not use the ‘td’ and ‘λ’ superscripts; instead, we always use weights with double indices for the online λ-return algorithm and weights with single indices for TD(λ).\nThe update equations for TD(λ) are:\nδt = Rt+1 + γV̂ (St+1|θt)− V̂ (St|θt) et = γλet−1 +∇θV̂ (St|θt)\nθt+1 = θt + αδt et\nBy incremental substitution, we can write θt directly in terms of θ0:\nθt = θ0 + α t−1∑ j=0 δjej\n= θ0 + α t−1∑ j=0 δj j∑ i=0 (γλ)j−i∇θV̂ (Si|θi)\n= θ0 + α t−1∑ j=0 j∑ i=0 (γλ)j−iδj ∇θV̂ (Si|θi)\nUsing the summation rule ∑n j=k ∑j i=k ai,j = ∑n i=k ∑n j=i ai,j we can rewrite this as:\nθt = θ0 + α t−1∑ i=0 t−1∑ j=i (γλ)j−iδj∇θV̂ (Si|θi) (8)\nIn Appendix B, the following relation is proven (see Equation 14):\nG λ|h+1 i = G λ|h i + (γλ) h−iδ′h for h ≥ i+ 1 with δ′h := Rh+1 + γV̂ (Sh+1|θh)− V̂ (Sh|θh−1) . By applying this sequentially for i+ 1 ≤ h < t, we can derive:\nG λ|t i = G λ|i+1 i + t−1∑ j=i+1 (γλ)j−iδ′j (9)\nFurthermore, the following holds:\nG λ|i+1 i = Ri+1 + γV̂ (Si+1|θi)\n= Ri+1 + γV̂ (Si+1|θi)− V̂ (Si|θi−1) + V̂ (Si|θi−1) = δ′i + V̂ (Si|θi−1)\nSubstituting this in (9) yields:\nG λ|t i = V̂ (Si|θi−1) + t−1∑ j=i (γλ)j−iδ′j .\nUsing that δ′j = δj + V̂ (Sj |θj)− V̂ (Sj |θj−1), it follows that\nt−1∑ j=i (γλ)j−iδj = G λ|t i − V̂ (Si|θi−1)− t−1∑ j=i (γλ)j−i ( V̂ (Sj |θj)− V̂ (Sj |θj−1) ) . (10)\nFrom the update equations of TD(λ) it follows that ||θj−θj−1|| → 0 as α→ 0. Furthermore, because ∇θV̂ is well-defined everywhere, V̂ is a continuous function, and therefore if ||θj − θj−1|| → 0 then ||V̂ (Sj |θj−1)− V̂ (Sj |θj)|| → 0. Hence, as α→ 0, we can approximate (10) as:\nt−1∑ j=i (γλ)j−iδj = G λ|t i − V̂ (Si|θi−1) +O(α)\n= Ḡ λ|t i − V̂ (Si|θ0) +O(α)\nwith Ḡλ|ti the interim λ-return that uses θ0 for all value evaluations. Substituting this in (8) yields:\nθt = θ0 + α t−1∑ i=0 ( Ḡ λ|t i − V̂ (Si|θ0) +O(α) ) ∇θV̂ (Si|θi)\nBecause∇θV̂ is a continuous function, if ||θi − θ0|| → 0 then ||∇θV̂ (Si|θi)−∇θV̂ (Si|θ0)|| → 0. Using this, we can approximate the above equation further as:\nθt = θ0 + α t−1∑ i=0 ( Ḡ λ|t i − V̂ (Si|θ0) +O(α) )( ∇θV̂ (Si|θ0) +O(α) · 1 ) = θ0 + α\nt−1∑ i=0 ( Ḡ λ|t i − V̂ (Si|θ0) ) ∇θV̂ (Si|θ0) +O(α2) · 1 , (11)\nwith 1 a vector consisting only of 1’s.\nFor the online λ-return algorithm, we can derive the following by sequential substitution:\nθtt = θ0 + α t−1∑ i=0 ( G λ|t i − V̂ (Si|θ t i) ) ∇θV̂ (Si|θti)\nAs α→ 0, we can approximate this as:\nθtt = θ0 + α t−1∑ i=0 ( Ḡ λ|t i − V̂ (Si|θ0) ) ∇θV̂ (Si|θ0) +O(α2) · 1 . (12)\nCombining (11) and (12), it follows that as α→ 0:\n||θt − θtt|| ||θt − θ0|| = ||(θt − θtt)/α|| ||(θt − θ0)/α|| = O(α) C +O(α) ,\nwith\nC = ∣∣∣∣∣ ∣∣∣∣∣ t−1∑ i=0 ( Ḡ λ|t i − V̂ (Si|θ0) ) ∇θV̂ (Si|θ0) ∣∣∣∣∣ ∣∣∣∣∣ = ∣∣∣∣∣ ∣∣∣∣∣ t−1∑ i=0 ∆ti ∣∣∣∣∣ ∣∣∣∣∣ .\nFrom the condition ∑t−1 i=0 ∆ t i 6= 0 it follows that C > 0.\nB Efficiently Computing the K-bounded λ-Return\nHere, we derive the two update equations that underly forward TD(λ). First, we derive the equation to compute Gλ|h+1t from G λ|h t . We use V̂t as a shorthand for V̂ (St|θt−1). The value G λ|h+1 t can be\nwritten in terms of Gλ|ht as follows:\nG λ|h+1 t := (1− λ) h−t∑ n=1 λn−1G (n) t + λ h−tG (h+1−t) t\n= (1− λ) h−t−1∑ n=1 λn−1G (n) t + λ h−tG (h+1−t) t + (1− λ)λh−t−1G (h−t) t\n= (1− λ) h−t−1∑ n=1 λn−1G (n) t + λ h−t−1G (h−t) t + λ h−t(G(h+1−t)t −G(h−t)t ) = G λ|h t + λ\nh−t(G(h+1−t)t −G(h−t)t ) (13) Furthermore, we can rewrite the difference G(h+1−t)t −G (h−t) t as follows:\nG (h+1−t) t −G (h−t) t = h+1−t∑ k=1 γk−1Rt+k + γ h+1−t V̂h+1 − h−t∑ k=1 γk−1Rt+k − γh−t V̂h\n= γh−t ( Rh+1 + γV̂h+1 − V̂h )\nBy combining this expression with (13), we get:\nG λ|h+1 t = G λ|h t + (γλ) h−tδ′h , (14)\nwith\nδ′h := Rh+1 + γV̂ (Sh+1|θh)− V̂ (Sh|θh−1) .\nNext, the derive the equation to compute Ght+1 from G h t . The first step in the derivation makes use of the fact that the weights of the n-step returns in the K-bounded λ-return always sum to 1. That is, for 0 ≤ λ ≤ 1 and n ∈ N+, the following holds (this can be proven using the geometric series rule):\n(1− λ) n−1∑ i=1 λi−1 + λn−1 = 1 (15)\nIn addition, the derivation makes use of the following relation, for n ≥ 2:\nG (n) t = n∑ i=1 γi−1Rt+i + γ n V̂t+n\n= Rt+1 + n∑ i=2 γi−1Rt+i + γ n V̂t+n\n= Rt+1 + n−1∑ j=1 γjRt+1+j + γ n V̂t+n\n= Rt+1 + γ [ n−1∑ j=1 γj−1Rt+1+j + γ n−1 V̂t+n ] = Rt+1 + γG (n−1) t+1\nThe full derivation is as follows (h ≥ t+ 2):\nG λ|h t := (1− λ) h−t−1∑ i=1 λi−1G (i) t + λ h−t−1G (h−t) t\n= (1− λ) h−t−1∑ i=1 λi−1G (i) t + λ h−t−1G (h−t) t +Rt+1 − [ (1− λ) h−t−1∑ i=1 λi−1 + λh−t−1 ] Rt+1\n= (1− λ) h−t−1∑ i=1 λi−1 [ G (i) t −Rt+1 ] + λh−t−1 [ G (h−t) t −Rt+1 ] +Rt+1 = (1− λ) [ G\n(1) t −Rt+1\n] + (1− λ) h−t−1∑ i=2 λi−1 [ G (i) t −Rt+1 ] +λh−t−1 [ G (h−t) t −Rt+1 ] +Rt+1\n= (1− λ) [ Rt+1 + γV̂t+1 −Rt+1 ] + (1− λ) h−t−1∑ i=2 λi−1γG (i−1) t+1\n+λh−t−1γG (h−t−1) t+1 +Rt+1\n= γ(1− λ)V̂t+1 + (1− λ) h−t−2∑ j=1 λjγG (j) t+1 + λ h−t−1γG (h−t−1) t+1 +Rt+1\n= γ(1− λ)V̂t+1 +Rt+1 + γλ [ (1− λ) h−t−2∑ j=1 λj−1G (j) t+1 + λ h−t−2G (h−t−1) t+1 ] = γ(1− λ)V̂t+1 +Rt+1 + γλGλ|ht+1\nThe above derivation expresses Gλ|ht in terms of G λ|h t+1. G λ|h t+1 expressed in terms of G λ|h t yields:\nG λ|h t+1 = (G λ|h t − ρt)/γλ , for h ≥ t+ 2\nwith ρt = Rt+1 + γ(1− λ) V̂ (St+1|θt) ."
    } ],
    "references" : [ {
      "title" : "Neuronlike adaptive elements that can solve difficult learning control problems",
      "author" : [ "A.G. Barto", "R.S. Sutton", "C.W. Anderson" ],
      "venue" : "IEEE Transactions on Systems, Man and Cybernetics,",
      "citeRegEx" : "Barto et al\\.,? \\Q1983\\E",
      "shortCiteRegEx" : "Barto et al\\.",
      "year" : 1983
    }, {
      "title" : "Truncating temporal differences: On the efficient implementation of TD(λ) for reinforcement learning",
      "author" : [ "P. Cichosz" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Cichosz,? \\Q1995\\E",
      "shortCiteRegEx" : "Cichosz",
      "year" : 1995
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "Kumaran", "H. King D", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature, 518:529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Reinforcement learning with replacing eligibility traces",
      "author" : [ "S.P. Singh", "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Singh and Sutton,? \\Q1996\\E",
      "shortCiteRegEx" : "Singh and Sutton",
      "year" : 1996
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Sutton,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Algorithms for Reinforcement Learning",
      "author" : [ "C. Szepesvári" ],
      "venue" : null,
      "citeRegEx" : "Szepesvári,? \\Q2009\\E",
      "shortCiteRegEx" : "Szepesvári",
      "year" : 2009
    }, {
      "title" : "TD-Gammon, a self-teaching backgammon program, achieves master-level play",
      "author" : [ "G. Tesauro" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Tesauro,? \\Q1994\\E",
      "shortCiteRegEx" : "Tesauro",
      "year" : 1994
    }, {
      "title" : "True online TD(λ)",
      "author" : [ "H. van Seijen", "R.S. Sutton" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Seijen and Sutton,? \\Q2014\\E",
      "shortCiteRegEx" : "Seijen and Sutton",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "1 Introduction Multi-step update targets play an important role in TD learning (Sutton, 1988) and reinforcement learning (Sutton & Barto, 1998; Szepesvári, 2009).",
      "startOffset" : 79,
      "endOffset" : 93
    }, {
      "referenceID" : 6,
      "context" : "1 Introduction Multi-step update targets play an important role in TD learning (Sutton, 1988) and reinforcement learning (Sutton & Barto, 1998; Szepesvári, 2009).",
      "startOffset" : 121,
      "endOffset" : 161
    }, {
      "referenceID" : 1,
      "context" : "Forward TD(λ) is similar to a method introduced by Cichosz (1995). Specifically, Cichosz’s method is based on the same update target as forward TD(λ).",
      "startOffset" : 51,
      "endOffset" : 66
    }, {
      "referenceID" : 2,
      "context" : "That delaying updates results in better performance in this case is probably related to the reason that the DQN algorithm uses a separate target network that is updated in a delayed way (Mnih et al., 2015).",
      "startOffset" : 186,
      "endOffset" : 205
    }, {
      "referenceID" : 0,
      "context" : "Our second control domain is the cart-pole benchmark task, in which a pole has to be balanced upright on a cart for as long as possible (Barto et al., 1983).",
      "startOffset" : 136,
      "endOffset" : 156
    } ],
    "year" : 2016,
    "abstractText" : "Multi-step temporal-difference (TD) learning, where the update targets contain information from multiple time steps ahead, is one of the most popular forms of TD learning for linear function approximation. The reason is that multi-step methods often yield substantially better performance than their single-step counter-parts, due to a lower bias of the update targets. For non-linear function approximation, however, single-step methods appear to be the norm. Part of the reason could be that on many domains the popular multi-step methods TD(λ) and Sarsa(λ) do not perform well when combined with non-linear function approximation. In particular, they are very susceptible to divergence of value estimates. In this paper, we identify the reason behind this. Furthermore, based on our analysis, we propose a new multi-step TD method for non-linear function approximation that addresses this issue. We confirm the effectiveness of our method using two benchmark tasks with neural networks as function approximation.",
    "creator" : "LaTeX with hyperref package"
  }
}