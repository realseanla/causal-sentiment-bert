{
  "name" : "1602.07714.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning functions across many orders of magnitudes",
    "authors" : [ "Hado van Hasselt", "Arthur Guez", "Matteo Hessel", "David Silver" ],
    "emails" : [ "HADO@GOOGLE.COM", "AGUEZ@GOOGLE.COM", "MTTHSS@GOOGLE.COM", "DAVIDSILVER@GOOGLE.COM" ],
    "sections" : [ {
      "heading" : "1. Learning with arbitrary magnitudes",
      "text" : "Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). It is much harder to learn efficiently from a stream of data when we do not know the magnitude of the function we seek to approximate beforehand, or if these magnitudes can change over time.\nConcretely, we are motivated by the work by Mnih et al. (2015), which combines Q-learning (Watkins, 1989) with a deep convolutional neural network (cf. LeCun et al., 2015).\nThe resulting deep Q network (DQN) algorithm was shown to be able to learn to play a varied set of Atari 2600 games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013), by learning action values corresponding to expected sums of future rewards. The ALE was proposed as an evaluation framework to test general learning algorithms on solving many different interesting tasks, and DQN was proposed as a singular solution, using a single set of hyperparameters. However, to achieve this feat the rewards and temporal-difference errors were clipped to [−1, 1] because the magnitudes and frequencies of rewards vary wildly between different games. For instance, in Pong the rewards are sparsely distributed and bounded by −1 and +1 while in Ms. Pac-Man eating a single ghost can yield a reward of up to +1600, but DQN clips the latter to +1 as well. This is not a satisfying solution for two reasons. The first reason is that the clipping introduces a form of domain knowledge. Most games have sparse non-zero rewards outside of [−1, 1]. Clipping the rewards then implies that the algorithm optimizes the frequency of rewards, rather than their sum. This is a good heuristic in many Atari games, but it does not generalize to the full setting of reinforcement learning. More importantly, the clipping changes the problem that the learning agent is solving and in some games the learned behaviour is clearly affected by this.\nIn this paper, we propose a method that adaptively normalizes the targets used in the learning updates. If we know that these targets are guaranteed to fall in some predetermined range, for instance [−1, 1], it is much easier to find suitable hyperparameters. If the magnitudes of incoming targets can vary greatly over time, then a naive adaptive normalization can result in a highly non-stationary and unstable learning problem because it would continually change the outputs of our approximation for all inputs, thereby invalidating earlier learning. To avoid this, the proposed method includes an additional step that ensures that the outputs of the approximation are kept fixed whenever we change the normalization.\nThe proposed technique is not specific to the application to DQN and is applicable more generally in supervised learning and reinforcement learning. There are several reasons\nar X\niv :1\n60 2.\n07 71\n4v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n16\nnormalization of the targets can be desirable. First, it is generally useful when we want a single system to be able to solve multiple different problems with varying natural magnitudes. Second, when learning a multi-variate function we can use the adaptive normalization to disentangle the natural magnitude of each target component from its relative importance in the loss function. This is particularly useful when the targets have different units, such as when we simultaneously predict signals from sensors with different modalities. Finally, adaptive scaling can help in problems that are non-stationary. For instance, this is common in reinforcement learning when the policy of behavior, and therefore the distribution and magnitude of the targets, can change repeatedly during learning."
    }, {
      "heading" : "2. Related work",
      "text" : "Input normalization has long been recognized as important for efficient learning of non-linear approximations such as neural networks (LeCun et al., 1998). This has led to several publications about how to achieve scale-invariance on the inputs (e.g., Ross et al., 2013; Ioffe and Szegedy, 2015; Desjardins et al., 2015). On the other hand, output or target normalization does not seem to have received the same attention. In classification, normalization of the targets is not necessary. In the common supervised regression setting, where a full data set is available before we commence learning, it is straightforward and commonplace to examine the data to determine an appropriate normalization, or to pre-tune appropriate hyper-parameters. However, this assumes the data is available a priori, which is not true in the online (potentially non-stationary) regression settings that we are interested in. In this paper we focus only on target normalization do not investigate the combination and interaction of target and input normalization, except to note that these are compatible and complementary.\nMore generally there has been work on normalizing the whole optimization process, for instance by using natural gradients (Amari, 1998). Natural gradients make the learning invariant to reparameterizations of the function approximation, thereby avoiding many scaling issues. Unfortunately, the pure version is computationally expensive for functions with many parameters, such as deep neural networks. This is why approximations are regularly proposed. Sometimes, these approximations focus on a complementary aspect, such as input normalization in the case of Natural Neural Networks (Desjardins et al., 2015) and Batch Normalization (Ioffe and Szegedy, 2015). In other cases, these algorithms directly approximate the full natural gradient (Martens and Grosse, 2015), but then necessarily have to make trade offs in terms of accuracy to obtain computational gains. In addition, these algorithms generally remain more computationally expensive than vanilla\nstochastic gradients descent and are typically not invariant to rescaling the targets. In contrast, we focus only on target normalization, allowing us to find an effective and computationally efficient algorithm which can potentially be combined with other methods that take care of other aspects of the full learning problem, such as input normalization.\nIn a different strand of research, several algorithms have been proposed to automatically adapt the step size during learning to handle non-stationarity problems (Sutton, 1992; Mahmood et al., 2012; Schaul et al., 2013). These can also be interpreted as attempting to solve multiple problems at once, including scale invariance, speed of learning, non-stationarity, and stability. While potentially powerful, solving these combined issues with a single adaptive algorithm is harder than the problem of target normalization we set out to solve, which is perhaps why there is not a single clear winner in this category. In addition, these algorithms often come with additional assumptions, such as requiring linear function approximation (Sutton, 1992; Mahmood et al., 2012). In addition, the algorithms typically assume stochastic gradient descent updates, or are themselves specific variants thereof, such as RMSprop (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014), which are not themselves target-scale invariant. The target normalization we propose can be combined with any learning algorithm, and therefore is best interpreted as complementary to such methods, rather than a competitor."
    }, {
      "heading" : "3. Preliminaries",
      "text" : "We consider learning from a stream of data {(Xt, Yt)}∞t=1 where the inputs Xt ∈ Rn and targets Yt ∈ Rk are realvalued tensors.1 The objective is to update the parameters θ of a function fθ : Rn → Rk such that the output fθ(Xt) is in some sense close to the corresponding target Yt, for instance as measured by the (expected) squared difference.\nA canonical and popular example of a learning algorithm is stochastic gradient descent (SGD) on a squared loss\nl(fθ) ≡ 1 2 E [ (fθ(Xt)− Yt)>(fθ(Xt)− Yt) ] .\nFor a given tuple (Xt, Yt) the update is then\nθt+1 = θt − α∇>θ lt(fθ) = θt + αJtδt ,\nwhere α ∈ [0, 1] is a step size,2 δt = Yt − fθ(Xt) is the 1Inputs and outputs may have multi-dimensional structure, as matrices or higher-order tensors. Then n and k are the total number of elements. We use upper case letters to denote random inputs and outputs, regardless of their structure.\n2For simplicity the step size is a scalar here. Our discussion generalizes to vector (component-wise) step sizes and (e.g., second-order) methods with matrix step sizes.\nerror, and Jt is the Jacobian defined by\nJt ≡ (∇θfθ,1(Xt), . . . ,∇θfθ,k(Xt))> .\nThe magnitude of this update depends on the magnitudes of both the step size and the error, and it is hard to pick suitable step sizes when nothing is known about the magnitudes of the errors.\nAn important special case is when fθ is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995). These networks are often trained with a form of stochastic gradient descent, with hyperparameters that interact with the scale of the targets. Especially for deep neural networks (LeCun et al., 2015; Schmidhuber, 2015) an update that is too large may harm learning, because these networks are highly non-linear and a large update may ‘bump’ the parameters to regions with high error.3"
    }, {
      "heading" : "4. Adaptive normalization with Pop-Art",
      "text" : "We propose to normalize the targets Yt, where the normalization is learned separately from the approximating function. We consider an affine transformation of the targets\nỸt = Σ −1 t (Yt − µt) ,\nwhere Σt and µt are scale and shift parameters that are learned from the data. The scale matrix Σt can be dense, diagonal, or defined by a scalar σt as Σt = σtI. Similarly, the shift vector µt can contain separate components, or be defined by a scalar µt as µt = µt1. We discuss relative benefits of each approach in Section 4.3.\nWe update the output of a parameterized function f̃θ(Xt) towards the normalized target Ỹt. The unnormalized output for any input x can then be recovered using\nfθ,Σ,µ(x) = Σf̃θ(x) + µ .\nWe call f̃θ the normalized function and fθ,Σ,µ the unnormalized function. The goal is, as before, for the outputs of the unnormalized function for an inputXt to be close to the corresponding unnormalized target Yt.\nAt first glance it may seem we have made little progress. If we learn Σ and µ using the same algorithm as used for the parameters θ of the function f̃θ, then the problem has not become fundamentally different or easier; we would have merely changed the structure of the parameterized function slightly. Conversely, if we consider the scale and shift to be tunable hyperparameters then setting them appropriately is\n3For linear functions, high magnitude updates are not a problem per se, as long as they are proportional to the errors. In other words, SGD on a linear function is invariant to scaling of the output. This does not hold for non-linear functions.\nnot fundamentally easier than tuning other hyperparameters, such as the step size, directly.\nFortunately, there is an alternative. We propose to update Σ and µ according to a separate objective with the aim of making the normalized targets fall approximately into some predetermined range [−1, 1]. Thereby, we decompose the problem of learning an appropriate scale and shift from learning the specific shape of the function.\nUnless care is taken, repeated updates to the scale and shift might make learning harder rather than easier because the normalized targets become non-stationary. More importantly, whenever we adapt the scale and shift based on a certain target, this would simultaneously change the output of the unnormalized function of all inputs. If there is little reason to believe that other unnormalized outputs were incorrect, this is undesirable and may hurt performance in practice, as we will illustrate in Section 5.\nIt may not be immediately obvious that we can prevent changing all outputs when updating the normalization, but we will show how this can be ensured in Section 4.1, by including a step that preserves the outputs of the function whenever we change the scale and shift.\nSummarizing, the two properties that we want to simultaneously achieve are\n(ART) to update scale Σ and shift µ such that Σ−1(Y − µ) is appropriately normalized (e.g., ∈ [−1, 1]), and\n(POP) to preserve the outputs of the unnormalized function whenever we change the scale and shift.\nWe will henceforth refer to algorithms that combine outputpreserving updates and adaptive rescaling, as Pop-Art algorithms, which is an acronym for “Preserving Outputs Precisely, while Adaptively Rescaling Targets”. Next, we first discuss how to achieve the desideratum of output preservation, which has a single elegant solution, and then discuss how to appropriately normalize."
    }, {
      "heading" : "4.1. Preserving outputs precisely",
      "text" : "The only way to avoid changing all outputs of the unnormalized function whenever we update the scale and shift is by changing the normalized function f̃ itself simultaneously. The goal is to exactly preserve the outputs from before the change of normalization, for all inputs. This prevents the normalization from affecting the approximation, which is appropriate because its objective is solely to make learning easier, and to leave solving the approximation itself to the internal optimization algorithm.\nWithout loss of generality we assume the normalized func-\ntion can be written as\nf̃θ,W,b(x) ≡Wg̃θ(x) + b ,\nwhere g̃θ : Rn → Rm is any (non-linear) function, for instance a deep neural network. It is not uncommon for deep neural networks to end in a linear layer, and so g̃θ might be thought of as the output of the last (hidden) layer of nonlinearities. Alternatively, we can always add a square linear layer to any non-linear function g̃θ to ensure this constraint, for instance initialized as W0 = I and b0 = 0.\nThe following fact shows that we can update the parameters W and b to fulfill the second desideratum of preserving outputs precisely for any change in normalization.\nFact 1. Consider a function f : Rn → Rk defined by\nfθ,Σ,µ,W,b(x) ≡ Σ (Wg̃θ(x) + b) + µ ,\nwhere g̃θ : Rn → Rm is any non-linear function of x ∈ Rn, Σ is a k × k matrix, µ and b are k-element vectors, and W is a k × m matrix. Consider any change of the scale and shift parameters from Σ to Σ2 and from µ to µ2, where Σ2 is non-singular. If we then additionally change the parameters W and b to W2 and b2, defined by\nW2 = Σ −1 2 ΣW and b2 = Σ −1 2 (Σb+ µ− µ2) ,\nthen the outputs of the unnormalized function f are preserved precisely in the sense that\nfθ,Σ,µ,W,b(x) = fθ,Σ2,µ2,W2,b2(x) , ∀x .\nThis fact and later highlighted facts are proven in the appendix. For the special case of scalar scale and shift, with Σ ≡ σI and µ ≡ µ1, the updates to W and b become\nW2 = (σ/σ2)W and b2 = (σb+ µ− µ2)/σ2 . (1)\nAfter updating the scale and shift we can update the output of the normalized function f̃θ,W,b(Xt) toward the normalized output Ỹt, using any learning algorithm. Importantly, the normalization can be updated first, thereby avoiding harmful large updates just before they would otherwise occur. This observation will be made more precise in Fact 2 in Section 4.2.\nAlgorithm 1 is an example implementation of SGD with Pop-Art. Notice that W and b are updated twice: first to adapt to the new scale and shift to preserve the outputs of the function, and then by SGD. The order of these updates is important because it allows us to use the new normalization immediately in the subsequent SGD update."
    }, {
      "heading" : "4.2. Adaptively rescaling targets",
      "text" : "A natural choice is to normalize the targets to approximately have zero mean and unit variance. For clarity and\nAlgorithm 1 SGD with Pop-Art For a given differentiable function g̃θ, initialize θ. Initialize W = I , b = 0, Σ = I , and µ = 0. while learning do\nObserve input X and target Y Use Y to compute new scale Σ∗ and new shift µ∗ W← Σ−1∗ ΣW (rescale W to preserve ouputs) b← Σ−1∗ (Σb+ µ− µ∗) (rescale b) g ← g̃θ(X) (store output of g̃θ) J ← (∇g̃θ,1(X), . . . ,∇g̃θ,m(X))> (Jacobian) δ̃ ← Σ−1∗ (Y − µ∗)−Wg − b (normalized error) θ ← θ + αJW>δ̃ (update θ with SGD) W←W + αδ̃g> (update W with SGD) b← b+ αδ̃ (update b with SGD) Σ← Σ∗ (update scale) µ← µ∗ (update shift)\nend while\nconciseness, we consider the single-variate case. If we have data {(Xi, Yi)}ti=1 up to some time t, we then desire\nt∑ i=1 (Yi − µt)/σt = 0 , and 1 t t∑ i=1 (Yi − µt)2/σ2t = 1 .\nSolving for µt and σt gives\nµt = 1\nt t∑ i=1 Yi and σt = 1 t t∑ i=1 Y 2i − µ2t . (2)\nThis can be generalized to incremental updates\nµt = (1− βt)µt−1 + βtYt and (3) σ2t = νt − µ2t where νt = (1− βt)νt−1 + βtY 2t .\nHere νt estimates the second moment of the targets and βt ∈ [0, 1] is a step size. If νt − µ2t is positive initially then it will always remain so, although to avoid issues with numerical precision it can be useful to ensure this explicitly by requiring νt − µ2t ≥ with > 0. For full equivalence to (2), we should use βt = 1/t. If βt = β is constant we instead obtain exponential moving averages, allowing us to place more weight on more recent data points, which is appropriate in non-stationary settings.\nA constant β has the additional benefit of never becoming negligibly small. Consider the first time a target is observed that is much larger than all previously observed targets. If βt is small, our statistics would adapt only slightly, and the resulting update may be large enough to harm the learning. If βt is not too small, the normalization can adapt to the large target before updating, potentially making learning more robust. In particular, the following fact holds.\nFact 2. When using updates (3) to adapt the normalization parameters σ and µ, the normalized targets are bounded\nfor all t by − √ (1− βt)/βt ≤ (Yt − µt)/σt ≤ √ (1− βt)/βt .\nFor instance, if βt = β = 10−4 for all t, then Fact 2 implies that the normalized target is guaranteed to be in (−100, 100). Note that Fact 2 does not rely on any assumptions about the distribution of the targets. This is an important result, because it implies we can bound the potential normalized errors before learning, without any prior knowledge about the actual targets we may observe.\nIn the appendix we discuss others normalization updates, based on percentiles and mini-batches, and derive some interesting correspondences between all of these."
    }, {
      "heading" : "4.3. Multi-variate outputs",
      "text" : "In many applications the targets are vectors or higher-order tensors. Then, there are at least three possibilities for the normalization, depending on how these targets interact. The appropriate choice depends on the domain.\nFirst, we can compute combined statistics to find a global scalar scale and shift. This makes sense when when all targets have the same natural scale and units and we want to preserve relative differences. An example is learning action values in reinforcement learning.\nSecond, we can normalize each element of the target separately. This is useful when the outputs have differing natural scales, for instance when predicting sensory readings of different modalities. Normalizing per component makes it easier to weight each output appropriately in the desired objective, because we can detangle the relative importance of each output component from its magnitude.\nFinally, we can normalize for all elements together, for instance defining as goal that the normalized target vector is distributed according to a standard multi-variate normal distribution. This is a common approach for input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).\nIn the first scenario the scale can be taken to be a scalar such that Σ = σI. In the second scenario the scale is a vector or, equivalently, Σ is diagonal. In the third scenario, Σ can be dense. We can consider combinations of these approaches, for instance normalizing some subsets of outputs together but separately from other subsets.\nIn both the scalar and vector (diagonal) cases, we can treat the input to the normalization as a single stream of data, with a single scalar scale and shift. This stream of data is either combined over several components for the global scalar normalization, or it belongs to a single output in component-wise vector normalization. For clarity and conciseness, in the remainder of this paper we will focus on these cases, and leave the more general setting where Σ\ncan be any non-singular matrix as a natural extension."
    }, {
      "heading" : "4.4. An equivalence for stochastic gradient descent",
      "text" : "We now step back and analyze the effect of the magnitude of the errors on the gradients when using regular SGD. This analysis suggests a different normalization algorithm, which has an interesting correspondence to Pop-Art SGD.\nWe consider SGD updates for an unnormalized multi-layer function of form fθ,W,b(X) = Wg̃θ(X) + b. The update for the weight matrix W is\nWt = Wt−1 + αtδtg̃θt(Xt) > ,\nwhere δt = Yt − fθ,W,b(X) is the unnormalized error. The magnitude of this update depends linearly on the magnitude of the error, which is appropriate when the inputs are normalized, because then the ideal scale of the weights depends linearly on the magnitude of the targets.4\nNow consider the SGD update to the parameters of g̃θ,\nθt = θt−1 + αJtW > t−1δt ,\nwhere Jt = (∇gθ,1(X), . . . ,∇gθ,m(X))> is the Jacobian for g̃θ. The magnitudes of both the weights W and the errors δ depend linearly on the magnitude of the targets. This means that the magnitude of the update for θ depends quadratically on the magnitude of the targets. There is no compelling reason for these updates to depend at all on these magnitudes because the weights in the top layer already ensure appropriate scaling. In other words, for each doubling of the magnitudes of the targets, the updates to the lower layers quadruple for no clear reason.\nThis analysis suggests an algorithmic solution, which seems to be novel in and of itself, in which we track the magnitudes of the targets in a separate parameter σt, and then multiply the updates for all lower layers with a factor σ−2t . A more general version of this for matrix scalings is given in Algorithm 2. We prove an interesting, and perhaps surprising, connection to the Pop-Art algorithm.\nFact 3. Consider two functions defined by\nfθ,Σ,µ,W,b(x) = Σ(Wg̃θ(x) + b) + µ , and\nfθ,W,b(x) = Wg̃θ(x) + b ,\nwhere g̃θ is the same differentiable function in both cases, and the functions are initialized identically, using Σ0 = I and µ = 0, and the same initial θ0, W0 and b0. Consider updating the first function using Algorithm 1 and the second using Algorithm 2. Then, for any sequence of\n4In general care should be taken that the inputs are wellbehaved; this is exactly the point of recent work on input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).\nAlgorithm 2 Normalized SGD Given a function fθ(X) ≡ Wg̃θ(X) + b, where g̃θ is any differentiable function of θ and X . while learning do\nObserve input X and target Y Use Y to compute new scale Σ g ← g̃θ(X) (store output of g̃) δ ← Y −Wg − b (compute unnormalized error) J ← (∇g̃θ,1(X), . . . ,∇g̃θ,m(X))> (compute Jacobian) θ ← θ + αJ(Σ−1W)>Σ−1δ (update θ with scaled SGD) W←W + αδg> (update W with SGD) b← b+ αδ (update b with SGD)\nend while\nnon-singular scales {Σt}∞t=1 and shifts {µt}∞t=1, the algorithms are equivalent in the sense that 1) the sequences {θt}∞t=0 are identical, 2) the outputs of the functions are identical, for any input.\nThis fact shows that Algorithms 1 and 2 are fully equivalent, allowing us to make other interesting comparisons. Consider RMSprop (Tieleman and Hinton, 2012), AdaGrag (Duchi et al., 2011), and Adam (Kingma and Ba, 2014), all of which divide the updates by the square root of a recency-weighted empirical second moment such that\nθt ≈ θt − α∇>θt lt(θt)/ √ E[(∇>θt lt(θt))2] ,\nwhere the division is per component. This is somewhat similar to the normalization discussed above, which also uses a trace of the recent empirical second moment. However, then Pop-Art can be interpreted as not applying the scaling to the updates at the top layer of the network but only to the gradient passing through the top layer into the rest of the network, as can be seen by its equivalence to Algorithm 2. This means the updates in the top layer are allowed to be big if the targets are big, whereas the aforementioned algorithms all scale down these updates. The effect is that RMSprop, AdaGrad, and Adam will slow down the updates to weights in the top layer when the magnitudes of the errors are bigger, which means these algorithms are not scale-invariant, while Pop-Art is not similarly affected. This argument is even more important when the non-linearities of the last hidden layer are bounded, for instance when using the tanh function, because then the top layer is the only layer that can scale the output of the network up to the appropriate range.\nThe above discussion compares RMSprop, AdaGrad, and Adam to the combination of SGD with Pop-Art. However, Pop-Art is agnostic to the choice of optimization algorithm and it is possible, and straightforward, to combine it with these other methods."
    }, {
      "heading" : "5. Binary regression experiments",
      "text" : "In our first experiment, we analyze the effect of rare events in online learning, when infrequently a target is observed with a much higher magnitude. Such events are expected to occur, for instance, when we try to learn from a sensor that is mostly white noise but sometimes captures an actual signal. Additionally, such settings occur in reinforcement learning with sparse rewards.\nWe empirically compare three variants of SGD: without normalization, with normalization but without preserving outputs precisely (i.e., with ‘Art’, but without ‘Pop’), and with Pop-Art. The setup is as follows. The inputs are binary representations of integers drawn uniformly randomly between 0 and n = 210−1. The desired outputs are the corresponding integer values. Every 1000 samples, we present one much larger target by giving as input the binary representation of 216 − 1 (i.e., all 16 inputs are 1) and as target 216 − 1 = 65, 535. The approximating function approximation is a fully connected neural network with 16 inputs, 3 hidden layers with 10 nodes per layer, and tanh internal activation functions. This simple setup allows for extensive sweeps over hyperparameters, to avoid inadvertent bias towards any of the algorithms by the way we tune these. The step sizes α for SGD and β for the normalization are both tuned by a grid search over {10−5, 10−4.5, . . . , 10−1, 10−0.5, 1}. Figure 1 shows results for online learning on 5000 samples. The x-axis shows the number of samples observed so far, and the y-axis shows the root mean squared error (RMSE) on a log scale for the current sample, before up-\ndating the function (so this is a test error, not a train error). The solid line is the median of 50 repetitions of the experiment. The shaded region covers the 10th to 90th percentiles over all repetitions. The plotted results correspond to the best hyper-parameters according to the overall RMSE (i.e., area under the curve). The lines are slightly smoothed by averaging over each 10 consecutive samples.\nSGD favors a relatively small step size (α = 10−3.5) to avoid harmful large updates from the larger targets, but this slows learning on the other samples; the error curve is almost flat in between spikes. SGD with adaptive normalization (labeled ‘Art’) can use a higher step size and learns faster, but has high error after the spikes because the changing scale also changes the outputs of the smaller inputs, increasing the errors on these. In comparison, Pop-Art performs much better. Both Art and Pop-Art used a step size of α = 10−2.5, but Pop-Art could exploit a much faster rate for the statistics (β = 10−0.5 for Pop-Art, 10−4 for Art). The faster tracking of statistics protects Pop-Art from the much higher spikes, while the output preservation avoids invalidating the outputs at smaller magnitudes. We also ran experiments with RMSprop but chose to leave these out of the figure because the results were very similar to SGD."
    }, {
      "heading" : "6. Atari 2600 experiments",
      "text" : "An important motivation for this work is reinforcement learning with non-linear function approximators such as neural networks (sometimes called deep reinforcement learning). The goal is to predict and optimize action values defined as the expected sum of future rewards. These rewards can differ arbitrarily from one domain to the next,\nand non-zero rewards can be sparse. As a result, the action values can span a varied and wide range.\nAs a successful example, Mnih et al. (2015) used a form of Q-learning (Watkins, 1989) together with a deep neural network (LeCun et al., 2015) in an algorithm called DQN. Impressively, DQN learned to play many games using a single set of hyper-parameters. However, to handle the different reward structures with a single system, DQN clips all rewards to the interval [−1, 1]. This is harmless in some games, such as Pong where no reward is ever higher than 1 or lower than −1, but it is not satisfactory as it introduces a form of domain knowledge that optimizing the number of rewards is a good proxy for optimizing the sum of rewards. In addition, it makes the DQN algorithm blind to differences between certain actions, such as the difference in reward between eating a ghost (reward>= 100) and eating a pellet (reward = 25) in Ms. Pac-Man. The hypotheses are that 1) overall performance decreases when we turn off clipping, because it is not possible to tune a step size that works on many games, 2) that we can regain the lost performance by then applying Pop-Art, thereby opening up the real problem as defined by the real rewards and reducing the dependence on a domain-specific heuristic without suffering a large performance loss.\nWe ran the Double DQN algorithm (van Hasselt et al., 2016) in three versions: without changes, with unclipped rewards and temporal difference errors, and unclipped but additionally adding Pop-Art. The targets are the cumulation of a reward and the discounted value at the next state:\nYt = Rt+1 + γQ(St, argmax a\nQ(St, a;θ);θ −) , (4)\nwhereQ(s, a;θ) is the estimated action value of action a in state s according to current parameters θ, and where θ− is a more stable periodic copy of these parameters (cf. Mnih et al., 2015; van Hasselt et al., 2016, for more details). This is a form of Double Q-learning (van Hasselt, 2011). We roughly tuned the main step size and the step size for the normalization to 10−4. It is not straightforward to tune the unclipped version, for reasons that will become clear soon.\nFigure 2 shows `2 norm of the gradient of Double DQN during learning as a function of number of training steps. The left plot corresponds to no reward clipping, middle to clipping (as per original DQN and Double DQN), and right to using Pop-Art instead of clipping. Each faint dashed lines corresponds to the median norms (where the median is taken over time) on one game. The shaded areas correspond to 50%, 90%, and 95% of games.\nWithout clipping the rewards, Pop-Art produces a much narrower band within which the gradients fall. Across games, 95% of median norms range over less than two orders of magnitude (roughly between 1 and 20), compared to\nalmost four orders of magnitude for clipped Double DQN, and more than six orders of magnitude for unclipped Double DQN without Pop-Art. The wide range for the latter shows why it is impossible to find a suitable step size with neither clipping nor Pop-Art: the updates are either far too small on some games or far too large on others.\nAfter 200M frames, we evaluated the actual scores of the best performing agent in each game on 100 episodes of up to 30 minutes of play, and then normalized by human and random scores as described by Mnih et al. (2015). Figure 3 shows the differences in normalized scores between (clipped) Double DQN and Double DQN with Pop-Art. There is a slight gain for using Pop-Art: on 32 out of 57 games Double DQN with Pop-Art is at least as good as Double DQN and the median (+0.4%) and mean (+34%) differences are both positive. This means we have suc-\ncessfully removed the domain-dependent arbitrary clipping without suffering an overall loss in performance.\nHowever, the main eye-catching result is that the distribution in performance drastically changed. On some games (e.g., Gopher, Centipede) we observe dramatic improvements, while on other games (e.g., Video Pinball, Star Gunner) we see a substantial decrease.\nPerformance on some games improves greatly because that uncovering the true problem immediately results in better behavior. For instance, in Ms. Pac-Man the original DQN and Double DQN agents do not care more about ghosts than pellets, but Double DQN with Pop-Art learned to actively hunt ghosts. This results in higher scores. Especially remarkable is the improved performance on games like Centipede and Gopher, but also notable are games like Frostbite which went from below 50% to a near-human performance level. Raw scores can be found in the appendix.\nOn the other end of the spectrum, we see that some games suffer from the unclipping of the rewards. The reason is similar to the reason performance increases on others. Consider Breakout as an example. In this game the aim is to hit blocks, where higher-placed blocks are worth more than lower-placed blocks. The original DQN was blind to this fact, and would happily and diligently work at removing many blocks regardless of position. The unclipped agent is greedier, in a sense, and tries to get to the higher-valued blocks quickly. This strategy is riskier, however, and therefore the unclipped agent loses more lives in the process and ultimately performs worse. One potential reason the agent adopts this riskier strategy in pursuit of higher rewards is that the learning process is fairly myopic: the discount used in target (4) was γ = 0.99, as per DQN and Double DQN, which along with the knowledge that the agent takes 15 actions per second translates into a horizon of perhaps a dozen seconds.\nIn other words, we see that in several games the heuristic of optimizing the number of rewards, rather than their sum, actually makes the learning problem easier. However, in order to make real progress, for instance when we consider how to efficiently explore to get to rare high rewards, ultimately we are going to have to consider the real rewards of the domains. These results show that Pop-Art can be a useful tool for this."
    }, {
      "heading" : "7. Discussion",
      "text" : "We have demonstrated that Pop-Art methods can be used to adapt to different and non-stationary target magnitudes. As discussed above, this seems especially useful for the combination with deep reinforcement learning, although PopArt is not specific to this setting.\nWe saw that Pop-Art can successfully replace the clipping of rewards as done by DQN to handle the various magnitudes of the targets used in the Q-learning update. Now that the true problem is exposed to the learning algorithm we can hope to make further progress, for instance by combining Pop-Art with other recent advances, such as dueling architectures (Wang et al., 2015) and prioritized replay (Schaul et al., 2015). In addition, it is important to investigate methods to escape from the sub-optimal policies these algorithms currently seems to get stuck in, for which we need better exploration. Fortunately, such exploration can now be informed by the true problem we aim to solve."
    }, {
      "heading" : "Appendix",
      "text" : "In this appendix, we introduce and analyze several extensions and variations, including normalizing based on percentiles or minibatches. Additionally, we prove all facts in the main text and the appendix."
    }, {
      "heading" : "Experiment setup",
      "text" : "For the experiments described in Section 6, we closely followed the setup described in Mnih et al. (2015) and van Hasselt et al. (2016). In particular, the Double DQN algorithm is identical to that described by van Hasselt et al. The shown results were obtained by running the trained agent for 30 minutes of simulated play (or 108,000 frames). This was repeated 100 times, where diversity over different runs was ensured by a small probability of exploration on each step ( -greedy exploration with = 0.01), as well as by\nperforming up to 30 ‘no-op’ actions, as also used and described by Mnih et al. In summary, the evaluation setup was the same as used by Mnih et al., except that we allowed more evaluation time per game (30 minutes instead of 5 minutes), as also used by Wang et al. (2015).\nThe results in Figure 3 were obtained by normalizing the raw scores by first subtracting the score by a random agent, and then dividing by the absolute difference between human and random agents, such that\nscorenormalized ≡ score agent − scorerandom\n|scorehuman − scorerandom| .\nThe raw scores are given below, in Table 1."
    }, {
      "heading" : "Generalizing normalization by variance",
      "text" : "We can change the variance of the normalized targets to influence the magnitudes of the updates. For a desired standard deviation of s > 0, we can use\nσt = √ νt − µ2t s ,\nwith the updates for νt and µt as normal. It is straightforward to show that then a generalization of Fact 2 holds with a bound of\n−s √\n1− βt βt ≤ Yt − µt σt\n≤ s √\n1− βt βt .\nThis additional parameter is for instance useful when we desire fast tracking in non-stationary problems. We then want a large step size α, but without risking overly large updates.\nThe new parameter s may seem superfluous because increasing the normalization step size β also reduces the hard bounds on the normalized targets. However, β additionally influences the distribution of the normalized targets. The histograms in the left-most plot in Figure 4 show what happens when we try to limit the magnitudes using only β. The red histogram shows normalized targets where the unnormalized targets come from a normal distribution, shown in blue. The normalized targets are contained in [−1, 1], but the distribution is very non-normal even though the actual targets are normal. Conversely, the red histogram in the middle plot shows that the distribution remains approximately normal if we instead use s to reduce the magnitudes. The right plot shows the effect on the variance of normalized targets for either approach. When we change β while keeping s = 1 fixed, the variance of the normalized targets can drop far below the desired variance of one (magenta curve). When we use change s while keeping β = 0.01 fixed, the variance remains predictably at approximately s\n(black line). The difference in behavior of the resulting normalization demonstrates that s gives us a potentially useful additional degree of freedom.\nSometimes, we can simply roll the additional scaling s into the step size, such that without loss of generality we can use s = 1 and decrease the step size to avoid overly large updates. However, sometimes it is easier to separate the magnitude of the targets, as influenced by s, from the magnitude of the updates, for instance when using an adaptive step-size algorithm. In addition, the introduction of an explicit scaling s allows us to make some interesting connections to normalization by percentiles, in the next section."
    }, {
      "heading" : "Adaptive normalization by percentiles",
      "text" : "Instead of normalizing by mean and variance, we can normalize such that a given ratio p of normalized targets is inside the predetermined interval. The per-output objective is then\nP ( Y − µ σ ∈ [−1, 1] ) = p .\nFor normally distributed targets, there is a direct correspondence to normalizing by means and variance.\nFact 4. If scalar targets {Yt}∞t=1 are distributed according to a normal distribution with arbitrary finite mean and variance, then the objective P((Y − µ)/σ ∈ [−1, 1]) = p is equivalent to the joint objective E [Y − µ] = 0 and E [ σ−2(Y − µ)2 ] = s2 with\np = erf (\n1√ 2s\n) .\nFor example, percentiles of p = 0.99 and p = 0.95 correspond to s ≈ 0.4 and s ≈ 0.5, respecticely. Conversely,\ns = 1 corresponds to p ≈ 0.68. The fact only applies when the targets are normal. For other distributions the two forms of normalization differ even in terms of their objectives.\nWe now discuss a concrete algorithm to obtain normalization by percentiles. Let Y (n)t denote order statistics of the targets up to time t,5 such that Y (1)t = mini{Yi}ti=1, Y\n(t) t = maxi{Yi}ti=1, and Y ((t+1)/2) t = mediani{Yi}ti=1. For notational simplicity, define n+ ≡ t+12 + p t−12 and n− ≡ t+12 − p t−12 . Then, for data up to time t, the goal is\nY (n+) t − µt\nσt = −1 , and Y\n(n−) t − µt\nσt = 1 .\nSolving for σt and µt gives\nµt = 1\n2\n( Y (n+) t + Y (n−) t ) , and\nσt = 1\n2\n( Y (n+) t − Y (n −) t ) .\nIn the special case where p = 1 we get µt = 12 (maxiYi + miniYi) and σt = 12 (maxiYi −miniYi). We are then guaranteed that all normalized targets fall in [−1, 1], but this could result in an overly conservative normalization that is sensitive to outliers and may reduce the overall magnitude of the updates too far. In other words, learning will then be safe in the sense that no updates will be too big, but it may be slow because many updates may be very small. In general it is probably typically better to use a ratio p < 1.\nExact order statistics are hard to compute online, because we would need to store all previous targets. To obtain\n5For non-integer x we can define Y (x) by either rounding x to an integer or, perhaps more appropriately, by linear interpolation between the values for the nearest integers.\nmore memory-efficient online updates for percentiles we can store two values ymint and y max t , which should eventually have the property that a proportion of (1− p)/2 values is larger than ymaxt and a proportion of (1 − p)/2 values is smaller than ymint , such that\nP (Y > ymaxt ) = P ( Y < ymint ) = (1− p)/2 . (5)\nThis can be achieved asymptotically by updating ymint and ymaxt according to\nymaxt = y max t−1 + βt ( I(Yt > ymaxt−1)−\n1− p 2\n) and (6)\nymint = y min t−1 − βt ( I(Yt < ymint−1)−\n1− p 2\n) , (7)\nwhere the indicator function I(·) is equal to one when its argument is true and equal to zero otherwise.\nFact 5. If ∑∞ t=1 βt and ∑∞ t=1 β 2 t , and the distribution of targets is stationary, then the updates in (6) converge to values such that (5) holds.\nIf the step size βt is too small it will take long for the updates to converge to appropriate values. In practice, it might be better to let the magnitude of the steps depend on the actual errors, such that the update takes the form of an asymmetrical least-squares update (Newey and Powell, 1987; Efron, 1991).\nOnline learning with minibatches\nOnline normalization by mean and variance with minibatches {Yt,1, . . . , Yt,B} of size B can be achieved by using the updates\nµt = (1− βt)µt−1 + βt 1\nB B∑ b=1 Yt,b , and\nσt = √ νt − µ2t s , where\nνt = (1− βt)νt−1 + βt 1\nB B∑ b=1 Y 2t,b .\nAnother interesting possibility is to update ymint and y max t towards the extremes of the minibatch such that\nymint = (1− βt)ymint−1 + βtminbYt,b , and (8) ymaxt = (1− βt)ymaxt−1 + βtmaxbYt,b ,\nand then use\nµt = 1\n2 (ymaxt + y min t ) , and σt =\n1 2 (ymaxt − ymint ) .\nThe statistics of this normalization depend on the size of the minibatches, and there is an interesting correspondence to normalization by percentiles.\nFact 6. Consider minibatches {{Yt,1, . . . , Yt,B}}∞t=1 of size B ≥ 2 whose elements are drawn i.i.d. from a uniform distribution with support on [a, b]. If ∑ t βt =∞ and∑\nt β 2 t < ∞, then in the limit the updates (8) converge to\nvalues such that (5) holds, with p = (B − 1)/(B + 1).\nThis fact connects the online minibatch updates (8) to normalization by percentiles. For instance, a minibatch size of B = 20 would correspond roughly to online percentile updates with p = 19/21 ≈ 0.9 and, by Fact 4, to a normalization by mean and variance with a s ≈ 0.6. These different normalizations are not strictly equivalent, but may behave similarly in practice.\nFact 6 quantifies an interesting correspondence between minibatch updates and normalizing by percentiles. Although the fact as stated holds only for uniform targets, the proportion of normalized targets in the interval [−1, 1] more generally becomes larger when we increase the minibatch size, just as when we increase p or decrease s, potentially resulting in better robustness to outliers at the possible expense of slower learning."
    }, {
      "heading" : "A note on initialization",
      "text" : "When using constant step sizes it is useful to be aware of the start of learning, to trust the data rather than arbitrary initial values. This can be done by using a step size as defined in the following fact.\nFact 7. Consider a recency-weighted running average z̄t updated from a stream of data {Zt}∞t=1 using z̄t = (1 − βt)z̄t−1 + βtZt, with βt defined by\nβt = β(1− (1− β)t)−1 . (9)\nThen 1) the relative weights of the data in Zt are the same as when using a constant step size β, and 2) the estimate z̄t does not depend on the initial value z̄0.\nA similar result was derived to remove the effect of the initialization of certain parameters by Kingma and Ba (2014) for a stochastic optimization algorithm called Adam. In that work, the initial values are assumed to be zero and a standard exponentially weighted average is explicitly computed and stored, and then divided by a term analogous to 1−(1−β)t. The step size (9) corrects for any initialization in place, without storing auxiliary variables, but for the rest the method and its motivation are very similar.\nAlternatively, it is possible to initialize the normalization safely, by choosing a scale that is relatively high initially. This can be beneficial when at first the targets are relatively small and noisy. If we would then use the step size in (9), the updates would treat these initial observations as important, and would try to fit our approximating function to the noise. A high initialization (e.g., ν0 = 104 or ν0 = 106)\nwould instead reduce the effect of the first targets on the learning updates, and would instead use these only to find an appropriate normalization. Only after finding this normalization the actual learning would then commence."
    }, {
      "heading" : "Deep Pop-Art",
      "text" : "Sometimes it makes sense to apply the normalization not to the output of the network, but at a lower level. For instance, the ith output of a neural network with a soft-max on top can be written\nf̃θ,i(X) = e[Wg̃θ(X)+b]i∑ j e [Wg̃θ(X)+b]j ,\nwhere W is the weight matrix of the last linear layer before the soft-max. The actual outputs are already normalized by using the soft-max, but the outputs Wg̃θ(X) + b of the layer below the soft-max may still benefit from normalization. To determine the targets to be normalized, we can either back-propagate the gradient of our loss through the soft-max or invert the function.\nMore generally, we can consider applying normalization at any level of a hierarchical non-linear function. This seems a promising way to counteract undesirable characteristics of back-propagating gradients, such as vanishing or exploding gradients (Hochreiter, 1998).\nIn addition, normalizing gradients further down in a network can provide a straightforward way to combine gradients from different sources in more complex network graphs than a standard feedforward multi-layer network. First, the normalization allows us to normalize the gradient from each source separately before merging gradients, thereby avoiding one source to fully drown out any others and allowing us to weight the gradients by actual relative importance, rather than implicitly relying on the current magnitude of each as a proxy for this. Second, the normalization can prevent undesirably large gradients when many gradients come together at one point of the graph, by normalizing again after merging gradients."
    }, {
      "heading" : "Proofs",
      "text" : "Fact 1. Consider a function f : Rn → Rk defined by\nfθ,Σ,µ,W,b(x) ≡ Σ (Wg̃θ(x) + b) + µ ,\nwhere g̃θ : Rn → Rm is any non-linear function of x ∈ Rn, Σ is a k × k matrix, µ and b are k-element vectors, and W is a k × m matrix. Consider any change of the scale and shift parameters from Σ to Σ2 and from µ to µ2, where Σ2 is non-singular. If we then additionally change the parameters W and b to W2 and b2, defined by\nW2 = Σ −1 2 ΣW and b2 = Σ −1 2 (Σb+ µ− µ2) ,\nthen the outputs of the unnormalized function f are preserved precisely in the sense that\nfθ,Σ,µ,W,b(x) = fθ,Σ2,µ2,W2,b2(x) , ∀x .\nProof. The stated result follows from\nfθ,Σ2,µ2,W2,b2(x)\n= Σ2f̃θ,W2,b2(x) + µ2\n= Σ2 (W2g̃θ(x) + b2) + µ2 = Σ2 ( Σ−12 ΣWg̃θ(x) + Σ −1 2 (Σb+ µ− µ2) ) + µ2 = (ΣWg̃θ(x) + Σb+ µ− µ2) + µ2 = ΣWg̃θ(x) + Σb+ µ\n= Σf̃θ,W,b(x) + µ\n= fθ,Σ,µ,W,b(x) .\nFact 2. When using updates (3) to adapt the normalization parameters σ and µ, the normalized target σ−1t (Yt−µt) is bounded for all t by\n− √\n1− βt βt ≤ Yt − µt σt\n≤ √\n1− βt βt .\nProof.( Yt − µt σt )2 = ( Yt − (1− βt)µt−1 − βtYt\nσt )2 =\n(1− βt)2(Yt − µt−1)2 νt − µ2t\n= (1− βt)2(Yt − µt−1)2\n(1− βt)νt−1 + βtY 2t − ((1− βt)µt−1 + βtYt)2\n= (1− βt)2(Yt − µt−1)2 (1− βt) ( νt−1 + βtY 2t − (1− βt)µ2t−1 − 2βtµt−1Yt ) =\n(1− βt)(Yt − µt−1)2 νt−1 + βtY 2t − (1− βt)µ2t−1 − 2βtµt−1Yt\n≤ (1− βt)(Yt − µt−1) 2\nµ2t−1 + βtY 2 t − (1− βt)µ2t−1 − 2βtµt−1Yt\n= (1− βt)(Yt − µt−1)2\nβtY 2t + βtµ 2 t−1 − 2βtµt−1Yt\n= (1− βt)(Yt − µt−1)2 βt(Yt − µt−1)2 = (1− βt) βt ,\nThe inequality follows from the fact that νt−1 ≥ µ2t−1.\nFact 3. Consider two functions defined by\nfθ,Σ,µ,W,b(x) = Σ(Wg̃θ(x) + b) + µ , and\nfθ,W,b(x) = Wg̃θ(x) + b ,\nwhere g̃θ is the same differentiable function in both cases, and the functions are initialized identically, using Σ0 = I and µ = 0, and the same initial θ0, W0 and b0. Consider updating the first function using Algorithm 1 and the second using Algorithm 2. Then, for any sequence of non-singular scales {Σt}∞t=1 and shifts {µt}∞t=1, the algorithms are equivalent in the sense that 1) the sequences {θt}∞t=0 are identical, 2) the outputs of the functions are identical, for any input.\nProof. Let θ1t and θ 2 t denote the parameters of g̃θ for Algorithms 1 and 2, respectively. Similarly, let W1 and b1 be parameters of the first function, while W2 and b2 are parameters of the second function. It is enough to show that single updates of both Algorithms 1 and 2 from the same starting points have equivalent results. That is, if\nθ2t−1 = θ 1 t−1 , and\nfθ2t−1,W2t−1,b2t−1(x) = fθ1t−1,Σt−1,µt−1,W1t−1,b1t−1(x) ,\nthen it must follow that\nθ2t = θ 1 t , and\nfθ2t ,W2t ,b2t (x) = fθ1t ,Σt,µt,W1t ,b1t (x) ,\nwhere the quantities θ2, W2, and b2 are updated with Algorithm 2 and quantities θ1, W1, and b1 are updated with Algorithm 1. We do not require W2t = W 1 t or b 2 t = b 1 t , and indeed these quantities will generally differ.\nWe use the shorthands f1t and f 2 t for the first and second function, respectively. First, we show that W1t = Σ −1 t W 2 t , for all t. For t = 0, this holds trivially because W10 = W20 = W0, and Σ0 = I . Now assume that W 1 t−1 = Σ−1t−1W 2 t−1. Let δt = Yt − f1t (Xt) be the unnormalized error at time t. Then, Algorithm 1 results in\nW1t = Σ −1 t Σt−1W 1 t−1 + αΣ −1 t δtgθt−1(Xt) > = Σ−1t ( Σt−1W 1 t−1 + αδtgθt−1(Xt) >) = Σ−1t ( W2t−1 + αδtgθt−1(Xt)\n>) = Σ−1t W 2 t .\nSimilarly, b10 = Σ −1 0 (b 2 0−µ0) and if b1t−1 = Σ−1t−1(b2t−1− µt−1) then\nb1t = Σ −1 t (Σt−1b 1 t−1 + µt−1 − µt) + αΣ−1t δt\n= Σ−1t (b 2 t−1 − µt) + αΣ−1t δt = Σ−1t (b 2 t−1 − µt + αδt) = Σ−1t (b 2 t − µt) .\nNow, assume that θ1t−1 = θ 2 t−1. Then,\nθ1t = θ 1 t−1 + αJt(W 1 t−1) >Σ−1t δ\n= θ2t−1 + αJt(Σ −1 t−1W 2 t−1) >Σ−1t δ = θ2t .\nAs θ10 = θ 2 0 by assumption, θ 1 t = θ 2 t for all t.\nFinally, we put everything together and note that f10 = f 2 0 and that\nf1t (x) = Σt(W 1 t g̃θ1t (x) + b 1 t ) + µt\n= Σt(Σ −1 t W 2 t g̃θ2t (x) + Σ −1 t (b 2 t − µt)) + µt = W2t g̃θ2t (x) + b 2 t\n= f2t (x) ∀x, t . Fact 4. If the targets {Yt}∞t=1 are distributed according to a normal distribution with arbitrary finite mean and variance, then the objective P(σ−1(Y − µ) ∈ [−1, 1]) = p is equivalent to the joint objective E [Y − µ] = 0 and E [ σ−2(Y − µ)2 ] = s2 for\np = erf (\n1√ 2s ) Proof. For any µ and σ, the normalized targets are distributed according to a normal distribution because the targets themselves are normally distributed and the normalization is an affine transformation. For a normal distribution with mean zero and variance v, the values 1 and −1 are both exactly 1/√v standard deviations from the mean, implying that the ratio of data between these points is Φ(1/ √ v)− Φ(−1/√v), where\nΦ(x) = 1\n2\n( 1 + erf ( x√ 2 )) is the standard normal cumulative distribution. The normalization by mean and variance is then equivalent to a normalization by percentiles with a ratio p defined by\np = Φ(1/ √ v)− Φ(−1/√n)\n= 1\n2\n( 1 + erf ( 1√ 2v )) − 1 2 ( 1 + erf ( − 1√ 2v )) = erf ( 1√ 2v ) ,\nwhere we used the fact that erf is odd, such that erf(x) = − erf(−x). Fact 5. If ∑∞ t=1 βt and ∑∞ t=1 β 2 t , and the distribution of targets is stationary, then the updates\nymaxt = y max t−1 + βt ( I(Yt > ymaxt−1)−\n1− p 2\n) and\nymint = y min t−1 − βt ( I(Yt < ymint−1)−\n1− p 2\n) ,\nconverge to values such that\nP (Y > ymaxt ) = P ( Y < ymint ) =\n1− p 2 .\nProof. Note that E [ ymaxt = y max t−1 ] ⇐⇒ E [I(Y > ymaxt )] = (1− p)/2 ⇐⇒ P (Y > ymaxt ) = (1− p)/2 ,\nso this is a fixed point of the update. Note further that the variance of the stochastic update is finite, and that the expected direction of the updates is towards the fixed point, so that this fixed point is an attractor. The conditions on the step sizes ensure that the fixed point is reachable ( ∑∞ t=1 βt = ∞) and that we converge upon it in the limit\n( ∑∞ t=1 β 2 t < ∞). For more detail and weaker conditions, we refer to reader to the extensive literature on stochastic approximation (Robbins and Monro, 1951; Kushner and Yin, 2003). The proof for the update for ymint is exactly analogous.\nFact 6. Consider minibatches {{Yt,1, . . . , Yt,B}}∞t=1 of size B ≥ 2 whose elements are drawn i.i.d. from a uniform distribution with support on [a, b]. If ∑ t βt =∞ and∑\nt β 2 t <∞, then in the limit the updates\nymint = (1− βt)ymint−1 + βtminbYt,b , and ymaxt = (1− βt)ymaxt−1 + βtmaxbYt,b\nconverge to values such that P (Y > ymaxt ) = P ( Y < ymint ) = (1− p)/2 ,\nwith p = (B − 1)/(B + 1).\nProof. Because of the conditions on the step size, the quantities ymint and y max t will converge to the expected value for the minimum and maximum of a set of B i.i.d. random variables. The cumulative distribution function (CDF) for the maximum of B i.i.d. random variables with CDF F (x) is F (x)B , since\nP (x < max1≤b≤BYb) = B∏ b=1 P (x < Yb) = F (x) B\nThe CDF for a uniform random variables with support on [a, b] is\nF (x) =  0 if x < a, x−a b−a if a ≤ x ≤ b, 1 if x > b.\nTherefore,\nP (x ≤ max1≤b≤BYb) =  0 if x < a,( x−a b−a )B if a ≤ x ≤ b,\n1 if x > b.\nThe associated expected value can then be calculated to be\nE [max1≤b≤BYb] = a+ B\nB + 1 (b− a) ,\nso that a fraction of 1B+1 of samples will be larger than this value. Through a similar reasoning, an additional fraction of 1B+1 will be smaller than the minimum, and a ratio of p = B−1B+1 will on average fall between these values.\nFact 7. Consider a weighted running average xt updated from a stream of data {Zt}∞t=1 using\n(1− βt)xt−1 + βtZt ,\nwith\nβt ≡ β\n1− (1− β)t ,\nwhere β is a constant. Then 1) the relative weights of the data in xt are the same as when only the constant step size β is used, and 2) the average does not depend on the initial value x0.\nProof. The point of the fact is to show that\nµt = µβt − (1− β)tµ0\n1− (1− β)t , ∀t , (10)\nwhere\nµβt = (1− β)µβt−1 + βZt , ∀t ,\nand where µ0 = µ β 0 . Note that µt as defined by (10) exactly removes the contribution of the initial value µ0, which at time t have weight (1 − β)t in the exponential moving average µβt , and then renormalizes the remaining value by dividing by 1 − (1 − β)t, such that the relative weights of the observed samples {Zt}∞t=1 is conserved.\nIf (10) holds for µt−1, then\nµt =\n( 1− β 1− (1− β)t ) µt−1 +\nβ\n1− (1− β)tYt\n= ( 1− β 1− (1− β)t ) µβt−1 − (1− β)t−1µ0 1− (1− β)t−1\n+ β\n1− (1− β)tYt\n= 1− (1− β)t − β 1− (1− β)t µβt−1 − (1− β)t−1µ0 1− (1− β)t−1\n+ β\n1− (1− β)tYt\n= (1− β)(1− (1− β)t−1) 1− (1− β)t µβt−1 − (1− β)t−1µ0 1− (1− β)t−1\n+ β\n1− (1− β)tYt\n= (1− β)(µβt−1 − (1− β)t−1µ0)\n1− (1− β)t + β 1− (1− β)tYt\n= (1− β)µβt−1 + βYt − (1− β)tµ0\n1− (1− β)t\n= µβt − (1− β)tµ0\n1− (1− β)t ,\nso that then (10) holds for µt. Finally, verify that µ1 = Y1. Therefore, (10) holds for all t by induction."
    } ],
    "references" : [ {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "S.I. Amari" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Amari.,? \\Q1998\\E",
      "shortCiteRegEx" : "Amari.",
      "year" : 1998
    }, {
      "title" : "The arcade learning environment: An evaluation platform for general agents",
      "author" : [ "M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling" ],
      "venue" : "J. Artif. Intell. Res. (JAIR),",
      "citeRegEx" : "Bellemare et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2013
    }, {
      "title" : "Random search for hyperparameter optimization",
      "author" : [ "J. Bergstra", "Y. Bengio" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Bergstra and Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bergstra and Bengio.",
      "year" : 2012
    }, {
      "title" : "Algorithms for hyper-parameter optimization",
      "author" : [ "J.S. Bergstra", "R. Bardenet", "Y. Bengio", "B. Kégl" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2011
    }, {
      "title" : "Neural networks for pattern recognition",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 1995
    }, {
      "title" : "Natural neural networks",
      "author" : [ "G. Desjardins", "K. Simonyan", "R. Pascanu", "K. Kavukcuoglu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Desjardins et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Desjardins et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Regression percentiles using asymmetric squared error loss",
      "author" : [ "B. Efron" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Efron.,? \\Q1991\\E",
      "shortCiteRegEx" : "Efron.",
      "year" : 1991
    }, {
      "title" : "The vanishing gradient problem during learning recurrent neural nets and problem solutions",
      "author" : [ "S. Hochreiter" ],
      "venue" : "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,",
      "citeRegEx" : "Hochreiter.,? \\Q1998\\E",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1998
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv preprint arXiv:1502.03167,",
      "citeRegEx" : "Ioffe and Szegedy.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe and Szegedy.",
      "year" : 2015
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "D.P. Kingma", "J. Ba" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Stochastic approximation and recursive algorithms and applications, volume 35",
      "author" : [ "H.J. Kushner", "G. Yin" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Kushner and Yin.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kushner and Yin.",
      "year" : 2003
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1998
    }, {
      "title" : "Tuning-free step-size adaptation",
      "author" : [ "A.R. Mahmood", "R.S. Sutton", "T. Degris", "P.M. Pilarski" ],
      "venue" : "In Proceedings of the IEEE International Conference on Acoustics,",
      "citeRegEx" : "Mahmood et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimizing neural networks with kronecker-factored approximate curvature",
      "author" : [ "J. Martens", "R.B. Grosse" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Martens and Grosse.,? \\Q2015\\E",
      "shortCiteRegEx" : "Martens and Grosse.",
      "year" : 2015
    }, {
      "title" : "A logical calculus of the ideas immanent in nervous activity",
      "author" : [ "W.S. McCulloch", "W. Pitts" ],
      "venue" : "The bulletin of mathematical biophysics,",
      "citeRegEx" : "McCulloch and Pitts.,? \\Q1943\\E",
      "shortCiteRegEx" : "McCulloch and Pitts.",
      "year" : 1943
    }, {
      "title" : "Asymmetric least squares estimation and testing",
      "author" : [ "W.K. Newey", "J.L. Powell" ],
      "venue" : "Econometrica: Journal of the Econometric Society,",
      "citeRegEx" : "Newey and Powell.,? \\Q1987\\E",
      "shortCiteRegEx" : "Newey and Powell.",
      "year" : 1987
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Robbins and Monro.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins and Monro.",
      "year" : 1951
    }, {
      "title" : "Principles of Neurodynamics",
      "author" : [ "F. Rosenblatt" ],
      "venue" : "Spartan, New York,",
      "citeRegEx" : "Rosenblatt.,? \\Q1962\\E",
      "shortCiteRegEx" : "Rosenblatt.",
      "year" : 1962
    }, {
      "title" : "Normalized online learning",
      "author" : [ "S. Ross", "P. Mineiro", "J. Langford" ],
      "venue" : "In Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Ross et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning internal representations by error propagation",
      "author" : [ "D.E. Rumelhart", "G.E. Hinton", "R.J. Williams" ],
      "venue" : "In Parallel Distributed Processing,",
      "citeRegEx" : "Rumelhart et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Rumelhart et al\\.",
      "year" : 1986
    }, {
      "title" : "No More Pesky Learning Rates",
      "author" : [ "T. Schaul", "S. Zhang", "Y. LeCun" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Schaul et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Schaul et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep learning in neural networks: An overview",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Schmidhuber.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 2015
    }, {
      "title" : "Practical bayesian optimization of machine learning algorithms",
      "author" : [ "J. Snoek", "H. Larochelle", "R.P. Adams" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "Adapting bias by gradient descent: An incremental version of delta-bar-delta",
      "author" : [ "R.S. Sutton" ],
      "venue" : "In Proceedings of the Tenth National Conference on Artificial Intelligence,",
      "citeRegEx" : "Sutton.,? \\Q1992\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1992
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "Tieleman and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton.",
      "year" : 2012
    }, {
      "title" : "Deep reinforcement learning with Double Q-learning",
      "author" : [ "H. van Hasselt", "A. Guez", "D. Silver" ],
      "venue" : null,
      "citeRegEx" : "Hasselt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2016
    }, {
      "title" : "Insights in Reinforcement Learning",
      "author" : [ "H.P. van Hasselt" ],
      "venue" : "PhD thesis, Utrecht University,",
      "citeRegEx" : "Hasselt.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hasselt.",
      "year" : 2011
    }, {
      "title" : "Dueling network architectures for deep reinforcement learning",
      "author" : [ "Z. Wang", "N. de Freitas", "M. Lanctot" ],
      "venue" : "CoRR, abs/1511.06581,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning from delayed rewards",
      "author" : [ "C.J.C.H. Watkins" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Watkins.,? \\Q1989\\E",
      "shortCiteRegEx" : "Watkins.",
      "year" : 1989
    }, {
      "title" : "Adaptive switching circuits",
      "author" : [ "B. Widrow", "M.E. Hoff" ],
      "venue" : "IRE WESCON Convention Record,",
      "citeRegEx" : "Widrow and Hoff.,? \\Q1960\\E",
      "shortCiteRegEx" : "Widrow and Hoff.",
      "year" : 1960
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012).",
      "startOffset" : 139,
      "endOffset" : 209
    }, {
      "referenceID" : 2,
      "context" : "Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012).",
      "startOffset" : 139,
      "endOffset" : 209
    }, {
      "referenceID" : 23,
      "context" : "Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012).",
      "startOffset" : 139,
      "endOffset" : 209
    }, {
      "referenceID" : 29,
      "context" : "(2015), which combines Q-learning (Watkins, 1989) with a deep convolutional neural network (cf.",
      "startOffset" : 34,
      "endOffset" : 49
    }, {
      "referenceID" : 1,
      "context" : "The resulting deep Q network (DQN) algorithm was shown to be able to learn to play a varied set of Atari 2600 games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013), by learning action values corresponding to expected sums of future rewards.",
      "startOffset" : 159,
      "endOffset" : 183
    }, {
      "referenceID" : 12,
      "context" : "Input normalization has long been recognized as important for efficient learning of non-linear approximations such as neural networks (LeCun et al., 1998).",
      "startOffset" : 134,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "This has led to several publications about how to achieve scale-invariance on the inputs (e.g., Ross et al., 2013; Ioffe and Szegedy, 2015; Desjardins et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "This has led to several publications about how to achieve scale-invariance on the inputs (e.g., Ross et al., 2013; Ioffe and Szegedy, 2015; Desjardins et al., 2015).",
      "startOffset" : 89,
      "endOffset" : 164
    }, {
      "referenceID" : 0,
      "context" : "More generally there has been work on normalizing the whole optimization process, for instance by using natural gradients (Amari, 1998).",
      "startOffset" : 122,
      "endOffset" : 135
    }, {
      "referenceID" : 5,
      "context" : "Sometimes, these approximations focus on a complementary aspect, such as input normalization in the case of Natural Neural Networks (Desjardins et al., 2015) and Batch Normalization (Ioffe and Szegedy, 2015).",
      "startOffset" : 132,
      "endOffset" : 157
    }, {
      "referenceID" : 9,
      "context" : ", 2015) and Batch Normalization (Ioffe and Szegedy, 2015).",
      "startOffset" : 32,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "In other cases, these algorithms directly approximate the full natural gradient (Martens and Grosse, 2015), but then necessarily have to make trade offs in terms of accuracy to obtain computational gains.",
      "startOffset" : 80,
      "endOffset" : 106
    }, {
      "referenceID" : 24,
      "context" : "In a different strand of research, several algorithms have been proposed to automatically adapt the step size during learning to handle non-stationarity problems (Sutton, 1992; Mahmood et al., 2012; Schaul et al., 2013).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 13,
      "context" : "In a different strand of research, several algorithms have been proposed to automatically adapt the step size during learning to handle non-stationarity problems (Sutton, 1992; Mahmood et al., 2012; Schaul et al., 2013).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 21,
      "context" : "In a different strand of research, several algorithms have been proposed to automatically adapt the step size during learning to handle non-stationarity problems (Sutton, 1992; Mahmood et al., 2012; Schaul et al., 2013).",
      "startOffset" : 162,
      "endOffset" : 219
    }, {
      "referenceID" : 24,
      "context" : "In addition, these algorithms often come with additional assumptions, such as requiring linear function approximation (Sutton, 1992; Mahmood et al., 2012).",
      "startOffset" : 118,
      "endOffset" : 154
    }, {
      "referenceID" : 13,
      "context" : "In addition, these algorithms often come with additional assumptions, such as requiring linear function approximation (Sutton, 1992; Mahmood et al., 2012).",
      "startOffset" : 118,
      "endOffset" : 154
    }, {
      "referenceID" : 25,
      "context" : "In addition, the algorithms typically assume stochastic gradient descent updates, or are themselves specific variants thereof, such as RMSprop (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014), which are not themselves target-scale invariant.",
      "startOffset" : 143,
      "endOffset" : 170
    }, {
      "referenceID" : 10,
      "context" : "In addition, the algorithms typically assume stochastic gradient descent updates, or are themselves specific variants thereof, such as RMSprop (Tieleman and Hinton, 2012) and Adam (Kingma and Ba, 2014), which are not themselves target-scale invariant.",
      "startOffset" : 180,
      "endOffset" : 201
    }, {
      "referenceID" : 15,
      "context" : "An important special case is when fθ is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).",
      "startOffset" : 57,
      "endOffset" : 163
    }, {
      "referenceID" : 30,
      "context" : "An important special case is when fθ is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).",
      "startOffset" : 57,
      "endOffset" : 163
    }, {
      "referenceID" : 18,
      "context" : "An important special case is when fθ is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).",
      "startOffset" : 57,
      "endOffset" : 163
    }, {
      "referenceID" : 20,
      "context" : "An important special case is when fθ is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).",
      "startOffset" : 57,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "An important special case is when fθ is a neural network (McCulloch and Pitts, 1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986; Bishop, 1995).",
      "startOffset" : 57,
      "endOffset" : 163
    }, {
      "referenceID" : 22,
      "context" : "Especially for deep neural networks (LeCun et al., 2015; Schmidhuber, 2015) an update that is too large may harm learning, because these networks are highly non-linear and a large update may ‘bump’ the parameters to regions with high error.",
      "startOffset" : 36,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "This is a common approach for input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).",
      "startOffset" : 50,
      "endOffset" : 100
    }, {
      "referenceID" : 5,
      "context" : "This is a common approach for input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).",
      "startOffset" : 50,
      "endOffset" : 100
    }, {
      "referenceID" : 9,
      "context" : "In general care should be taken that the inputs are wellbehaved; this is exactly the point of recent work on input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "In general care should be taken that the inputs are wellbehaved; this is exactly the point of recent work on input normalization (Ioffe and Szegedy, 2015; Desjardins et al., 2015).",
      "startOffset" : 129,
      "endOffset" : 179
    }, {
      "referenceID" : 25,
      "context" : "Consider RMSprop (Tieleman and Hinton, 2012), AdaGrag (Duchi et al.",
      "startOffset" : 17,
      "endOffset" : 44
    }, {
      "referenceID" : 6,
      "context" : "Consider RMSprop (Tieleman and Hinton, 2012), AdaGrag (Duchi et al., 2011), and Adam (Kingma and Ba, 2014), all of which divide the updates by the square root of a recency-weighted empirical second moment such that",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 10,
      "context" : ", 2011), and Adam (Kingma and Ba, 2014), all of which divide the updates by the square root of a recency-weighted empirical second moment such that",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 29,
      "context" : "(2015) used a form of Q-learning (Watkins, 1989) together with a deep neural network (LeCun et al.",
      "startOffset" : 33,
      "endOffset" : 48
    }, {
      "referenceID" : 28,
      "context" : "Now that the true problem is exposed to the learning algorithm we can hope to make further progress, for instance by combining Pop-Art with other recent advances, such as dueling architectures (Wang et al., 2015) and prioritized replay (Schaul et al.",
      "startOffset" : 193,
      "endOffset" : 212
    } ],
    "year" : 2016,
    "abstractText" : "Learning non-linear functions can be hard when the magnitude of the target function is unknown beforehand, as most learning algorithms are not scale invariant. We propose an algorithm to adaptively normalize these targets. This is complementary to recent advances in input normalization. Importantly, the proposed method preserves the unnormalized outputs whenever the normalization is updated to avoid instability caused by non-stationarity. It can be combined with any learning algorithm and any non-linear function approximation, including the important special case of deep learning. We empirically validate the method in supervised learning and reinforcement learning and apply it to learning how to play Atari 2600 games. Previous work on applying deep learning to this domain relied on clipping the rewards to make learning in different games more homogeneous, but this uses the domainspecific knowledge that in these games counting rewards is often almost as informative as summing these. Using our adaptive normalization we can remove this heuristic without diminishing overall performance, and even improve performance on some games, such as Ms. Pac-Man and Centipede, on which previous methods did not perform well. 1. Learning with arbitrary magnitudes Many current learning algorithms rely on a priori access to actual or sufficiently similar data to properly tune relevant hyper-parameters (Bergstra et al., 2011; Bergstra and Bengio, 2012; Snoek et al., 2012). It is much harder to learn efficiently from a stream of data when we do not know the magnitude of the function we seek to approximate beforehand, or if these magnitudes can change over time. Concretely, we are motivated by the work by Mnih et al. (2015), which combines Q-learning (Watkins, 1989) with a deep convolutional neural network (cf. LeCun et al., 2015). The resulting deep Q network (DQN) algorithm was shown to be able to learn to play a varied set of Atari 2600 games from the Arcade Learning Environment (ALE) (Bellemare et al., 2013), by learning action values corresponding to expected sums of future rewards. The ALE was proposed as an evaluation framework to test general learning algorithms on solving many different interesting tasks, and DQN was proposed as a singular solution, using a single set of hyperparameters. However, to achieve this feat the rewards and temporal-difference errors were clipped to [−1, 1] because the magnitudes and frequencies of rewards vary wildly between different games. For instance, in Pong the rewards are sparsely distributed and bounded by −1 and +1 while in Ms. Pac-Man eating a single ghost can yield a reward of up to +1600, but DQN clips the latter to +1 as well. This is not a satisfying solution for two reasons. The first reason is that the clipping introduces a form of domain knowledge. Most games have sparse non-zero rewards outside of [−1, 1]. Clipping the rewards then implies that the algorithm optimizes the frequency of rewards, rather than their sum. This is a good heuristic in many Atari games, but it does not generalize to the full setting of reinforcement learning. More importantly, the clipping changes the problem that the learning agent is solving and in some games the learned behaviour is clearly affected by this. In this paper, we propose a method that adaptively normalizes the targets used in the learning updates. If we know that these targets are guaranteed to fall in some predetermined range, for instance [−1, 1], it is much easier to find suitable hyperparameters. If the magnitudes of incoming targets can vary greatly over time, then a naive adaptive normalization can result in a highly non-stationary and unstable learning problem because it would continually change the outputs of our approximation for all inputs, thereby invalidating earlier learning. To avoid this, the proposed method includes an additional step that ensures that the outputs of the approximation are kept fixed whenever we change the normalization. The proposed technique is not specific to the application to DQN and is applicable more generally in supervised learning and reinforcement learning. There are several reasons ar X iv :1 60 2. 07 71 4v 1 [ cs .L G ] 2 4 Fe b 20 16 Learning functions across many orders of magnitudes normalization of the targets can be desirable. First, it is generally useful when we want a single system to be able to solve multiple different problems with varying natural magnitudes. Second, when learning a multi-variate function we can use the adaptive normalization to disentangle the natural magnitude of each target component from its relative importance in the loss function. This is particularly useful when the targets have different units, such as when we simultaneously predict signals from sensors with different modalities. Finally, adaptive scaling can help in problems that are non-stationary. For instance, this is common in reinforcement learning when the policy of behavior, and therefore the distribution and magnitude of the targets, can change repeatedly during learning.",
    "creator" : "TeX"
  }
}