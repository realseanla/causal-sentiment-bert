{
  "name" : "1602.05897.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity",
    "authors" : [ "Amit Daniely", "Roy Frostig", "Yoram Singer" ],
    "emails" : [ "amitdaniely@google.com", "rf@cs.stanford.edu.", "singer@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗Email: amitdaniely@google.com †Email: rf@cs.stanford.edu. Work performed at Google. ‡Email: singer@google.com\nar X\niv :1\n60 2.\n05 89\n7v 1\n[ cs\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : ""
    }, {
      "heading" : "2 Related work 2",
      "text" : ""
    }, {
      "heading" : "3 Setting 3",
      "text" : ""
    }, {
      "heading" : "4 Computation skeletons 4",
      "text" : "4.1 From computation skeletons to neural networks . . . . . . . . 6 4.2 From computation skeletons to reproducing kernels . . . . . . 7"
    }, {
      "heading" : "5 Main results 9",
      "text" : ""
    }, {
      "heading" : "6 Mathematical background 11",
      "text" : ""
    }, {
      "heading" : "7 Compositional kernel spaces 12",
      "text" : ""
    }, {
      "heading" : "8 The dual activation function 14",
      "text" : ""
    }, {
      "heading" : "9 Proofs 19",
      "text" : "9.1 Well-behaved activations . . . . . . . . . . . . . . . . . . . . . 19 9.2 Proofs of Thms. 2 and 3 . . . . . . . . . . . . . . . . . . . . . 22 9.3 Proofs of Thms. 4 and 5 . . . . . . . . . . . . . . . . . . . . . 25\n10 Discussion 28"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]). Nonetheless, neural network learning remains rather poorly understood in several regards. Notably, it remains unclear why training algorithms find good weights, how learning is impacted by the network architecture and activations, what is the role of random weight initialization, and how to choose a concrete optimization procedure for a given architecture.\nWe start by analyzing the expressive power of NNs subsequent to the random weight initialization. The motivation is the empirical success of training algorithms despite inherent computational intractability, and the fact that they optimize highly non-convex objectives with potentially many local minima. Our key result shows that random initialization already positions learning algorithms at a good starting point. We define an object termed a computation skeleton that describes a distilled structure of feed-forward networks. A skeleton induces a family of network architectures along with a hypothesis class H of functions obtained by certain non-linear compositions according to the skeleton’s structure. We show that the representation generated by random initialization is sufficiently rich to approximately express the functions in H. Concretely, all functions in H can be approximated by tuning the weights of the last layer, which is a convex optimization task.\nIn addition to explaining in part the success in finding good weights, our study provides an appealing perspective on neural network learning. We establish a tight connection between network architectures and their dual kernel spaces. This connection generalizes several previous constructions (see Sec 2). As we demonstrate, our dual view gives rise to design principles for NNs, supporting current practice and suggesting new ideas. We outline below a few points.\n• Duals of convolutional networks appear a more suitable fit for vision and acoustic tasks than those of fully connected networks.\n• Our framework surfaces a principled initialization scheme. It is very similar to common practice, but incorporates a small correction.\n• By modifying the activation functions, two consecutive fully connected layers can be replaced with one while preserving the network’s dual kernel.\n• The ReLU activation, i.e. x 7→ max(x, 0), possesses favorable properties. Its dual kernel is expressive, and it can be well approximated by random initialization, even when the initialization’s scale is moderately changed.\n• As the number of layers in a fully connected network becomes very large, its dual kernel converges to a degenerate form for any non-linear activation.\n• Our result suggests that optimizing the weights of the last layer can serve as a convex proxy for choosing among different architectures prior to training. This idea was advocated and tested empirically in [49]."
    }, {
      "heading" : "2 Related work",
      "text" : "Current theoretical understanding of NN learning. Understanding neural network learning, particularly its recent successes, commonly decomposes into the following research questions.\n(i) What functions can be efficiently expressed by neural networks?\n(ii) When does a low empirical loss result in a low population loss?\n(iii) Why and when do efficient algorithms, such as stochastic gradient, find good weights?\nThough still far from being complete, previous work provides some understanding of questions (i) and (ii). Standard results from complexity theory [28] imply that essentially all functions of interest (that is, any efficiently computable function) can be expressed by a network of moderate size. Biological phenomena show that many relevant functions can be expressed by even simpler networks, similar to convolutional neural networks [32] that are dominant in ML tasks today. Barron’s theorem [7] states that even two-layer networks can express a very rich set of functions. As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss. In contrast to the first two, question (iii) is rather poorly understood. While learning algorithms succeed in practice, theoretical analysis is overly pessimistic. Direct interpretation of theoretical results suggests that when going slightly deeper beyond single layer networks, e.g. to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16]. Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].\nCompositional kernels and connections to networks. The idea of composing kernels has repeatedly appeared throughout the machine learning literature, for instance in early work by Schölkopf et al. [51], Grauman and Darrell [21]. Inspired by deep networks’ success, researchers considered deep composition of kernels [36, 13, 11]. For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56]. Notably, Rahimi and Recht [46] proved a formal connection (similar to ours) for the RBF kernel. Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5]. Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36]. Our work sets a common foundation for and expands on these ideas. We extend the analysis from fully-connected and convolutional networks to a rather broad family of architectures. In addition, we prove approximation guarantees between a network and its corresponding kernel in our more general setting. We thus extend previous analyses that only applies to fully connected two-layer networks. Finally, we use the connection as an analytical tool to reason about architectural design choices."
    }, {
      "heading" : "3 Setting",
      "text" : "Notation. We denote vectors by bold-face letters (e.g. x), and matrices by upper case Greek letters (e.g. Σ). The 2-norm of x ∈ Rd is denoted by ‖x‖. For functions σ : R → R we let\n‖σ‖ := √ EX∼N (0,1) σ2(X) = √ 1√ 2π ∫∞ −∞ σ 2(x)e− x2 2 dx .\nLet G = (V,E) be a directed acyclic graph. The set of neighbors incoming to a vertex v is denoted in(v) := {u ∈ V | uv ∈ E}. The d − 1 dimensional sphere is denoted Sd−1 = {x ∈ Rd | ‖x‖ = 1}. We provide a brief overview of reproducing kernel Hilbert spaces in the sequel and merely introduce notation here. In a Hilbert space H, we use a slightly non-standard notation HB for the ball of radius B, {x ∈ H | ‖x‖H ≤ B}. We use [x]+ to denote max(x, 0) and 1[b] to denote the indicator function of a binary variable b.\nInput space. Throughout the paper we assume that each example is a sequence of n elements, each of which is represented as a unit vector. Namely, we fix n and take the input space to be X = Xn,d = ( Sd−1 )n . Each input example is denoted,\nx = (x1, . . . ,xn), where xi ∈ Sd−1 . (1)\nWe refer to each vector xi as the input’s ith coordinate, and use xij to denote it jth scalar entry. Though this notation is slightly non-standard, it unifies input types seen in various domains. For example, binary features can be encoded by taking d = 1, in which case X = {±1}n. Meanwhile, images and audio signals are often represented as bounded and continuous numerical values—we can assume in full generality that these values lie in [−1, 1]. To match the setup above, we embed [−1, 1] into the circle S1, e.g. via the map x 7→( sin ( πx 2 ) , cos ( πx 2 )) . When each coordinate is categorical—taking one of d values—we can represent category j ∈ [d] by the unit vector ej ∈ Sd−1. When d may be very large or the basic units exhibits some structure, such as when the input is a sequence of words, a more concise encoding may be useful, e.g. as unit vectors in a low dimension space Sd′ where d′ d (see for instance Mikolov et al. [37], Levy and Goldberg [34]).\nSupervised learning. The goal in supervised learning is to devise a mapping from the input space X to an output space Y based on a sample S = {(x1, y1), . . . , (xm, ym)}, where (xi, yi) ∈ X × Y , drawn i.i.d. from a distribution D over X × Y . A supervised learning problem is further specified by an output length k and a loss function ` : Rk × Y → [0,∞), and the goal is to find a predictor h : X → Rk whose loss, LD(h) := E(x,y)∼D `(h(x), y), is small. The empirical loss LS(h) := 1m ∑m i=1 `(h(xi), yi) is commonly used as a proxy for the loss LD. Regression problems correspond to Y = R and, for instance, the squared loss `(ŷ, y) = (ŷ − y)2. Binary classification is captured by Y = {±1} and, say, the zero-one loss `(ŷ, y) = 1[ŷy ≤ 0] or the hinge loss `(ŷ, y) = [1 − ŷy]+, with standard extensions to the multiclass case. A loss ` is L-Lipschitz if |`(y1, y)− `(y2, y)| ≤ L|y1 − y2| for all y1, y2 ∈ Rk, y ∈ Y , and it is convex if `(·, y) is convex for every y ∈ Y .\nNeural network learning. We define a neural network N to be directed acyclic graph (DAG) whose nodes are denoted V (N ) and edges E(N ). Each of its internal units, i.e. nodes with both incoming and outgoing edges, is associated with an activation function σv : R→ R. In this paper’s context, an activation can be any function that is square integrable with respect to the Gaussian measure on R. We say that σ is normalized if ‖σ‖ = 1. The set of nodes having only incoming edges are called the output nodes. To match the setup of a supervised learning problem, a network N has nd input nodes and k output nodes, denoted o1, . . . , ok. A network N together with a weight vector w = {wuv | uv ∈ E} defines a predictor hN ,w : X → Rk whose prediction is given by “propagating” x forward through the network. Formally, we define hv,w(·) to be the output of the subgraph of the node v as follows: for an input node v, hv,w is the identity function, and for all other nodes, we define hv,w recursively as\nhv,w(x) = σv (∑ u∈in(v) wuv hu,w(x) ) .\nFinally, we let hN ,w(x) = (ho1,w(x), . . . , hok,w(x)). We also refer to internal nodes as hidden units. The output layer of N is the sub-network consisting of all output neurons of N along with their incoming edges. The representation induced by a network N is the network rep(N ) obtained from N by removing the output layer. The representation function induced by the weights w is RN ,w := hrep(N ),w. Given a sample S, a learning algorithm searches for weights w having small empirical loss LS(w) = 1m ∑m i=1 `(hN ,w(xi), yi). A popular approach is to randomly initialize the weights and then use a variant of the stochastic gradient method to improve these weights in the direction of lower empirical loss.\nKernel learning. A function κ : X × X → R is a reproducing kernel, or simply a kernel, if for every x1, . . . ,xr ∈ X , the r × r matrix Γi,j = {κ(xi,xj)} is positive semi-definite. Each kernel induces a Hilbert space Hκ of functions from X to R with a corresponding norm ‖ · ‖Hκ . A kernel and its corresponding space are normalized if ∀x ∈ X , κ(x,x) = 1. Given a convex loss function `, a sample S, and a kernel κ, a kernel learning algorithm finds a function f = (f1, . . . , fk) ∈ Hkκ whose empirical loss, LS(f) = 1m ∑ i `(f(xi), yi), is minimal\namong all functions with ∑\ni ‖fi‖2κ ≤ R2 for some R > 0. Alternatively, kernel algorithms minimize the regularized loss,\nLRS (f) = 1\nm m∑ i=1 `(f(xi), yi) + 1 R2 k∑ i=1 ‖fi‖2κ ,\na convex objective that often can be efficiently minimized."
    }, {
      "heading" : "4 Computation skeletons",
      "text" : "In this section we define a simple structure which we term a computation skeleton. The purpose of a computational skeleton is to compactly describe a feed-forward computation from an input to an output. A single skeleton encompasses a family of neural networks that share the same skeletal structure. Likewise, it defines a corresponding kernel space.\nDefinition. A computation skeleton S is a DAG whose non-input nodes are labeled by activations.\nThough the formal definition of neural networks and skeletons appear identical, we make a conceptual distinction between them as their role in our analysis is rather different. Accompanied by a set of weights, a neural network describes a concrete function, whereas the skeleton stands for a topology common to several networks as well as for a kernel. To further underscore the differences we note that skeletons are naturally more compact than networks. In particular, all examples of skeletons in this paper are irreducible, meaning that for each two nodes v, u ∈ V (S), in(v) 6= in(u). We further restrict our attention to skeletons with a single output node, showing later that single-output skeletons can capture supervised problems with outputs in Rk. We denote by |S| the number of non-input nodes of S.\nFigure 1 shows four example skeletons, omitting the designation of the activation functions. The skeleton S1 is rather basic as it aggregates all the inputs in a single step. Such topology can be useful in the absence of any prior knowledge of how the output label may be computed from an input example, and it is commonly used in natural language processing where the input is represented as a bag-of-words [23]. The only structure in S1 is a single fully connected layer:\nTerminology (Fully connected layer of a skeleton). An induced subgraph of a skeleton with r + 1 nodes, u1, . . . , ur, v, is called a fully connected layer if its edges are u1v, . . . , urv.\nThe skeleton S2 is slightly more involved: it first processes consecutive (overlapping) parts of the input, and the next layer aggregates the partial results. Altogether, it corresponds to networks with a single one-dimensional convolutional layer, followed by a fully connected layer. The two-dimensional (and deeper) counterparts of such skeletons correspond to networks that are common in visual object recognition.\nTerminology (Convolution layer of a skeleton). Let s, w, q be positive integers and denote n = s(q − 1) + w. A subgraph of a skeleton is a one dimensional convolution layer of width w and stride s if it has n + q nodes, u1, . . . , un, v1, . . . , vq, and qw edges, us(i−1)+j vi, for 1 ≤ i ≤ q, 1 ≤ j ≤ w.\nThe skeleton S3 is a somewhat more sophisticated version of S2: the local computations are first aggregated, then reconsidered with the aggregate, and finally aggregated again. The last skeleton, S4, corresponds to the networks that arise in learning sequenceto-sequence mappings as used in translation, speech recognition, and OCR tasks (see for example Sutskever et al. [55])."
    }, {
      "heading" : "4.1 From computation skeletons to neural networks",
      "text" : "The following definition shows how a skeleton, accompanied with a replication parameter r ≥ 1 and a number of output nodes k, induces a neural network architecture. Recall that inputs are ordered sets of vectors in Sd−1.\nDefinition (Realization of a skeleton). Let S be a computation skeleton and consider input coordinates in Sd−1 as in (1). For r, k ≥ 1 we define the following neural network N = N (S, r, k). For each input node in S, N has d corresponding input neurons. For each internal node v ∈ S labeled by an activation σ, N has r neurons v1, . . . , vr, each with an activation σ. In addition, N has k output neurons o1, . . . , ok with the identity activation σ(x) = x. There is an edge viuj ∈ E(N ) whenever uv ∈ E(S). For every output node v in S, each neuron vj is connected to all output neurons o1, . . . , ok. We term N the (r, k)-fold realization of S. We also define the r-fold realization of S as1 N (S, r) = rep (N (S, r, 1)).\nNote that the notion of the replication parameter r corresponds, in the terminology of convolutional networks, to the number of channels taken in a convolutional layer and to the number of hidden units taken in a fully-connected layer.\nFigure 2 illustrates a (5, 4)- and 5-realizations of a skeleton with coordinate dimension d = 2. The (5, 4)-realization is a network with a single (one dimensional) convolutional layer having 5 channels, stride of 2, and width of 4, followed by three fully-connected layers. The global replication parameter r in a realization is used for brevity; it is straightforward to extend results when the different nodes in S are each replicated to a different extent.\nWe next define a scheme for random initialization of the weights of a neural network, that is similar to what is often done in practice. We employ the definition throughout the paper whenever we refer to random weights.\n1Note that for every k, rep (N (S, r, 1)) = rep (N (S, r, k)).\nDefinition (Random weights). A random initialization of a neural network N is a multivariate Gaussian w = (wuv)uv∈E(N ) such that each weight wuv is sampled independently from a normal distribution with mean 0 and variance 1/(‖σu‖2 |in(v)|).\nArchitectures such as convolutional nets have weights that are shared across different edges. Again, it is straightforward to extend our results to these cases and for simplicity we assume no explicit weight sharing."
    }, {
      "heading" : "4.2 From computation skeletons to reproducing kernels",
      "text" : "In addition to networks’ architectures, a computation skeleton S also defines a normalized kernel κS : X ×X → [−1, 1] and a corresponding norm ‖ · ‖S on functions f : X → R. This norm has the property that ‖f‖S is small if and only if f can be obtained by certain simple compositions of functions according to the structure of S. To define the kernel, we introduce a dual activation and dual kernel. For ρ ∈ [−1, 1], we denote by Nρ the multivariate Gaussian distribution on R2 with mean 0 and covariance matrix ( 1 ρ ρ 1 ) .\nDefinition (Dual activation and kernel). The dual activation of an activation σ is the function σ̂ : [−1, 1]→ R defined as\nσ̂(ρ) = E (X,Y )∼Nρ σ(X)σ(Y ) .\nThe dual kernel w.r.t. to a Hilbert space H is the kernel κσ : H1 ×H1 → R defined as\nκσ(x,y) = σ̂(〈x,y〉H) .\nSection 7 shows that κσ is indeed a kernel for every activation σ that adheres with the square-integrability requirement. In fact, any continuous µ : [−1, 1]→ R, such that (x,y) 7→ µ(〈x,y〉H) is a kernel for all H, is the dual of some activation. Note that κσ is normalized iff σ is normalized. We show in Section 8 that dual activations are closely related to Hermite polynomial expansions, and that these can be used to calculate the duals of activation\nfunctions analytically. Table 1 lists a few examples of normalized activations and their corresponding dual (corresponding derivations are in Section 8). The following definition gives the kernel corresponding to a skeleton having normalized activations.2\nDefinition (Compositional kernels). Let S be a computation skeleton with normalized activations and (single) output node o. For every node v, inductively define a kernel κv : X×X → R as follows. For an input node v corresponding to the ith coordinate, define κv(x,y) = 〈xi,yi〉. For a non-input node v, define\nκv(x,y) = σ̂v\n(∑ u∈in(v) κu(x,y)\n|in(v)|\n) .\nThe final kernel κS is κo, the kernel associated with the output node o. The resulting Hilbert space and norm are denoted HS and ‖ · ‖S respectively, and Hv and ‖ · ‖v denote the space and norm when formed at node v.\nAs we show later, κS is indeed a (normalized) kernel for every skeleton S. To understand the kernel in the context of learning, we need to examine which functions can be expressed as moderate norm functions in HS . As we show in section 7, these are the functions obtained by certain simple compositions according to the feed-forward structure of S. For intuition, the following example contrasts two commonly used skeletons.\nExample 1 (Convolutional vs. fully connected skeletons). Consider a network whose activations are all ReLU, σ(z) = [z]+, and an input space Xn,1 = {±1}n. Say that S1 is a skeleton comprising a single fully connected layer, and that S2 is one comprising a convolutional layer of stride 1 and width q = log0.999(n), followed by a single fully-connected layer. (The skeleton S2 from Figure 1 is a concrete example of the convolutional skeleton with q = 2 and n = 4.) The kernel κS1 takes the form κS1(x,y) = σ̂ (〈x,y〉/n). It is a symmetric kernel and therefore functions with small norm in HS1 are essentially low-degree polynomials. For instance, fix a\n2For a skeleton with unnormalized activations, the corresponding kernel is the kernel of the skeleton S ′ obtained by normalizing the activations of S.\nbound R = n1.001 on the norm of the functions. In this case, the space HRS1 contains multiplication of one or two input coordinates. However, multiplication of 3 or more coordinates are no-longer in HRS1 . Moreover, this property holds true regardless of the choice of activation function. On the other hand, HRS2 contains functions whose dependence on adjacent input coordinates is far more complex. It includes, for instance, any function f : X → {±1} that is symmetric (i.e. f(x) = f(−x)) and that depends on q adjacent coordinates xi, . . . ,xi+q. Furthermore, any sum of n such functions is also in HRS2 ."
    }, {
      "heading" : "5 Main results",
      "text" : "We review our main results. Let us fix a compositional kernel S. There are a few upshots to underscore upfront. First, our analysis implies that a representation generated by a random initialization of N = N (S, r, k) approximates the kernel κS . The sense in which the result holds is twofold. First, with the proper rescaling we show that 〈RN ,w(x),RN ,w(x′)〉 ≈ κS(x,x\n′). Then, we also show that the functions obtained by composing bounded linear functions with RN ,w are approximately the bounded-norm functions in HS . In other words, the functions expressed by N under varying the weights of the last layer are approximately bounded-norm functions in HS . For simplicity, we restrict the analysis to the case k = 1. We also confine the analysis to either bounded activations, with bounded first and second derivatives, or the ReLU activation. Extending the results to a broader family of activations is left for future work. Through this and remaining sections we use & to hide universal constants.\nDefinition. An activation σ : R→ R is C-bounded if it is twice continuously differentiable and ‖σ‖∞, ‖σ′‖∞, ‖σ′′‖∞ ≤ ‖σ‖C.\nNote that many activations are C-bounded for some constant C > 0. In particular, most of the popular sigmoid-like functions such as 1/(1 + e−x), erf(x), x/ √ 1 + x2, tanh(x), and tan−1(x) satisfy the boundedness requirements. We next introduce terminology that parallels the representation layer of N with a kernel space. Concretely, let N be a network whose representation part has q output neurons. Given weights w, the normalized representation Ψw is obtained from the representation RN ,w by dividing each output neuron v by ‖σv‖ √ q. The empirical kernel corresponding to w is defined as κw(x,x ′) = 〈Ψw(x),Ψw(x′)〉. We also define the empirical kernel space corresponding to w as Hw = Hκw . Concretely,\nHw = {hv(x) = 〈v,Ψw(x)〉 | v ∈ Rq} ,\nand the norm of Hw is defined as ‖h‖w = inf{‖v‖ | h = hv}. Our first result shows that the empirical kernel approximates the kernel kS .\nTheorem 2. Let S be a skeleton with C-bounded activations. Let w be a random initialization of N = N (S, r) with\nr ≥ (4C 4)depth(S)+1 log (8|S|/δ)\n2 .\nThen, for all x,x′, with probability of at least 1− δ,\n|kw(x,x′)− kS(x,x′)| ≤ .\nWe note that if we fix the activation and assume that the depth of S is logarithmic, then the required bound on r is polynomial. For the ReLU activation we get a stronger bound with only quadratic dependence on the depth. However, it requires that ≤ 1/depth(S).\nTheorem 3. Let S be a skeleton with ReLU activations. Let w be a random initialization of N (S, r) with\nr & depth2(S) log (|S|/δ)\n2 .\nThen, for all x,x′ and . 1/depth(S), with probability of at least 1− δ,\n|κw(x,x′)− κS(x,x′)| ≤ .\nFor the remaining theorems, we fix a L-Lipschitz loss ` : R×Y → [0,∞). For a distribution D on X × Y we denote by ‖D‖0 the cardinality of the support of the distribution. We note that log (‖D‖0) is bounded by, for instance, the number of bits used to represent an element in X × Y . We use the following notion of approximation.\nDefinition. Let D be a distribution on X ×Y. A space H1 ⊂ RX -approximates the space H2 ⊂ RX w.r.t. D if for every h2 ∈ H2 there is h1 ∈ H1 such that LD(h1) ≤ LD(h2) + .\nTheorem 4. Let S be a skeleton with C-bounded activations. Let w be a random initialization of N (S, r) with\nr & L4R4 (4C4)depth(S)+1 log\n( LRC|S| δ ) 4 .\nThen, with probability of at least 1−δ over the choices of w we have that H √\n2R w -approximates\nHRS and H √ 2R S -approximates HRw.\nTheorem 5. Let S be a skeleton with ReLU activations and . 1/depth(C). Let w be a random initialization of N (S, r) with\nr & L4R4 depth2(S) log\n( ‖D‖0|S|\nδ ) 4 .\nThen, with probability of at least 1−δ over the choices of w we have that H √\n2R w -approximates\nHRS and H √ 2R S -approximates HRw.\nAs in Theorems 2 and 3, for a fixed C-bounded activation and logarithmically deep S, the required bounds on r are polynomial. Analogously, for the ReLU activation the bound is polynomial even without restricting the depth. However, the polynomial growth in Theorems 4 and 5 is rather large. Improving the bounds, or proving their optimality, is left to future work."
    }, {
      "heading" : "6 Mathematical background",
      "text" : "Reproducing kernel Hilbert spaces (RKHS). The proofs of all the theorems we quote here are well-known and can be found in Chapter 2 of [48] and similar textbooks. Let H be a Hilbert space of functions from X to R. We say that H is a reproducing kernel Hilbert space, abbreviated RKHS or kernel space, if for every x ∈ X the linear functional f 7→ f(x) is bounded. The following theorem provides a one-to-one correspondence between kernels and kernel spaces.\nTheorem 6. (i) For every kernel κ there exists a unique kernel space Hκ such that for every x ∈ X , κ(·,x) ∈ Hκ and for all f ∈ Hκ, f(x) = 〈f(·), κ(·,x)〉Hκ. (ii) A Hilbert space H ⊆ RX is a kernel space if and only if there exists a kernel κ : X × X → R such that H = Hκ.\nThe following theorem describes a tight connection between embeddings of X into a Hilbert space and kernel spaces.\nTheorem 7. A function κ : X × X → R is a kernel if and only if there exists a mapping Φ : X → H to some Hilbert space for which κ(x,x′) = 〈Φ(x),Φ(x′)〉H. In addition, the following two properties hold,\n• Hκ = {fv : v ∈ H}, where fv(x) = 〈v,Φ(x)〉H.\n• For every f ∈ Hκ, ‖f‖Hκ = inf{‖v‖H | f = fv}.\nPositive definite functions. A function µ : [−1, 1]→ R is positive definite (PSD) if there are non-negative numbers b0, b1, . . . such that\n∞∑ i=0 bi <∞ and ∀x ∈ [−1, 1], µ(x) = ∞∑ i=0 bix i .\nThe norm of µ is defined as ‖µ‖ := √∑ i bi = √ µ(1). We say that µ is normalized if ‖µ‖ = 1\nTheorem 8 (Schoenberg, [50]). A continuous function µ : [−1, 1] → R is PSD if and only if for all d = 1, 2, . . . ,∞, the function κ : Sd−1 × Sd−1 → R defined by κ(x,x′) = µ(〈x,x′〉) is a kernel.\nThe restriction to the unit sphere of many of the kernels used in machine learning applications corresponds to positive definite functions. An example is the Gaussian kernel,\nκ(x,x′) = exp ( −‖x− x ′‖2\n2σ2\n) .\nIndeed, note that for unit vectors x,x′ we have\nκ(x,x′) = exp ( −‖x‖\n2 + ‖x′‖2 − 2〈x,x′〉 2σ2\n) = exp ( −1− 〈x,x\n′〉 σ2\n) .\nAnother example is the Polynomial kernel κ(x,x′) = 〈x,x′〉d.\nHermite polynomials. The normalized Hermite polynomials is the sequence h0, h1, . . . of orthonormal polynomials obtained by applying the Gram-Schmidt process to the sequence 1, x, x2, . . . w.r.t. the inner-product 〈f, g〉 = 1√ 2π ∫∞ −∞ f(x)g(x)e −x 2\n2 dx. Recall that we define activations as square integrable functions w.r.t. the Gaussian measure. Thus, Hermite polynomials form an orthonormal basis to the space of activations. In particular, each activation σ can be uniquely described in the basis of Hermite polynomials,\nσ(x) = a0h0(x) + a1h1(x) + a2h2(x) + . . . , (2)\nwhere the convergence holds in `2 w.r.t. the Gaussian measure. This decomposition is called the Hermite expansion. Finally, we use the following facts (see Chapter 11 in [41] and the relevant entry in Wikipedia):\n∀n ≥ 1, hn+1(x) = x√ n+ 1\nhn(x)− √ n\nn+ 1 hn−1(x) , (3)\n∀n ≥ 1, h′n(x) = √ nhn−1(x) (4)\nE (X,Y )∼Nρ hm(X)hn(Y ) =\n{ ρn n = m\n0 n 6= m where n,m ≥ 0, ρ ∈ [−1, 1] , (5)\nhn(0) =\n{ 0, if n is odd\n1√ n!\n(−1) n 2 (n− 1)!! if n is even\n, (6)\nwhere\nn!! =  1 n ≤ 0 n · (n− 2) · · · 5 · 3 · 1 n > 0 odd n · (n− 2) · · · 6 · 4 · 2 n > 0 even ."
    }, {
      "heading" : "7 Compositional kernel spaces",
      "text" : "We now describe the details of compositional kernel spaces. Let S be a skeleton with normalized activations and n input nodes associated with the input’s coordinates. Throughout the rest of the section we study the functions in HS and their norm. In particular, we show that κS is indeed a normalized kernel. Recall that κS is defined inductively by the equation,\nκv(x,x ′) = σ̂v\n(∑ u∈in(v) κu(x,x ′)\n|in(v)|\n) . (7)\nThe recursion (7) describes a means for generating a kernel form another kernel. Since kernels correspond to kernel spaces, it also prescribes an operator that produces a kernel space from other kernel spaces. If Hv is the space corresponding to v, we denote this operator by\nHv = σ̂v ( ⊕u∈in(v)Hu |in(v)| ) . (8)\nThe reason for using the above notation becomes clear in the sequel. The space HS is obtained by starting with the spacesHv corresponding to the input nodes and propagating them according to the structure of S, where at each node v the operation (8) is applied. Hence, to understand HS we need to understand this operation as well as the spaces corresponding to input nodes. The latter spaces are rather simple: for an input node v corresponding to the variable xi, we have that Hv = {fw | ∀x, fw(x) = 〈w,xi〉} and ‖fw‖Hv = ‖w‖. To understand (8), it is convenient to decompose it into two operations. The first operation, termed the direct average, is defined through the equation κ̃v(x,x ′) = ∑ u∈in(v) κu(x,x ′)\n|in(v)| , and the\nresulting kernel space is denoted Hṽ = ⊕u∈in(v)Hu |in(v)| . The second operation, called the extension according to σ̂v, is defined through κv(x,x ′) = σ̂v (κ̃v(x,x\n′)). The resulting kernel space is denoted Hv = σ̂v (Hṽ). We next analyze these two operations.\nThe direct average of kernel spaces. Let H1, . . . ,Hn be kernel spaces with kernels κ1, . . . , κn : X × X → R. Their direct average, denoted H = H1⊕···⊕Hnn , is the kernel space corresponding to the kernel κ(x,x′) = 1\nn ∑n i=1 κi(x,x ′).\nLemma 9. The function κ is indeed a kernel. Furthermore, the following properties hold.\n1. If H1, . . . ,Hn are normalized then so is H. 2. H = { f1+...+fn\nn | fi ∈ Hi } 3. ‖f‖2H = inf {‖f1‖2H1+...+‖fn‖2Hn n s.t. f = f1+...+fn n , fi ∈ Hi }\nProof. (outline) The fact that κ is a kernel follows directly from the definition of a kernel and the fact that an average of PSD matrices is PSD. Also, it is straight forward to verify item 1. We now proceed to items 2 and 3. By Theorem 7 there are Hilbert spaces G1, . . . ,Gn and mappings Φi : X → Gi such that κi(x,x′) = 〈Φi(x),Φi(x′)〉Gi . Consider now the mapping\nΨ(x) = ( Φ1(x)√\nn , . . . , Φn(x)√ n\n) .\nIt holds that κ(x,x′) = 〈Ψ(x),Ψ(x′)〉. Properties 2 and 3 now follow directly form Thm. 7 applied to Ψ.\nThe extension of a kernel space. Let H be a normalized kernel space with a kernel κ. Let µ(x) = ∑ i bix\ni be a PSD function. As we will see shortly, a function is PSD if and only if it is a dual of an activation function. The extension of H w.r.t. µ, denoted µ (H), is the kernel space corresponding to the kernel κ′(x,x′) = µ(κ(x,x′)).\nLemma 10. The function κ′ is indeed a kernel. Furthermore, the following properties hold.\n1. µ(H) is normalized if and only if µ is.\n2. µ(H) = span {∏ g∈A g | A ⊂ H, b|A| > 0 } where span(A) is the closure of the span of A.\n3. ‖f‖µ(H) ≤ inf {∑ A ∏ g∈A ‖g‖H√ b|A| s.t. f = ∑ A ∏ g∈A g, A ⊂ H }\nProof. (outline) Let Φ : X → G be a mapping from X to the unit ball of a Hilbert space G such that κ(x,x′) = 〈Φ(x),Φ(x′)〉. Define\nΨ(x) = (√ b0, √ b1Φ(x), √ b2Φ(x)⊗ Φ(x), √ b3Φ(x)⊗ Φ(x)⊗ Φ(x), . . . ) It is not difficult to verify that 〈Ψ(x),Ψ(x′)〉 = µ(κ(x,x′)). Hence, by Thm. 7, κ′ is indeed a kernel. Verifying property 1 is a straightforward task. Properties 2 and 3 follow by applying Thm. 7 on the mapping Ψ."
    }, {
      "heading" : "8 The dual activation function",
      "text" : "The following lemma describes a few basic properties of the dual activation. These properties follow easily from the definition of the dual activation and equations (2), (4), and (5).\nLemma 11. The following properties of the mapping σ 7→ σ̂ hold: (a) If σ = ∑ i aihi is the Hermite expansion of σ, then σ̂(ρ) = ∑ i a 2 i ρ i.\n(b) For every σ, σ̂ is positive definite.\n(c) Every positive definite function is a dual of some activation.\n(d) The mapping σ 7→ σ̂ preserves norms.\n(e) The mapping σ 7→ σ̂ commutes with differentiation.\n(f) For a ∈ R, âσ = a2σ̂.\n(g) For every σ, σ̂ is continuous in [−1, 1] and smooth in (−1, 1).\n(h) For every σ, σ̂ is non-decreasing and convex in [0, 1].\n(i) For every σ, the range of σ̂ is [−‖σ‖2, ‖σ‖2].\n(j) For every σ, σ̂(0) = ( EX∼N(0,1) σ(X) )2 and σ̂(1) = ‖σ‖2.\nWe next discuss a few examples for activations and calculate their dual activation and kernel. Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13]. Here, our derivations are different and may prove useful for future calculations of duals for other activations.\nThe exponential activation. Consider the activation function σ(x) = Ceax where C > 0 is a normalization constant such that ‖σ‖ = 1. The actual value of C is e−2a2 but it will not be needed for the derivation below. From properties (e) and (f) of Lemma 11 we have that,\n(σ̂)′ = σ̂′ = âσ = a2σ̂ .\nThe the solution of ordinary differential equation (σ̂)′ = a2σ̂ is of the form σ̂(ρ) = b exp (a2ρ). Since σ̂(1) = 1 we have b = e−a 2 . We therefore obtain that the dual activation function is\nσ̂(ρ) = ea 2ρ−a2 = ea 2(ρ−1) .\nNote that the kernel induced by σ is the RBF kernel, restricted to the d-dimensional sphere,\nκσ(x,x ′) = ea 2(〈x,x′〉−1) = e− a2‖x−x′‖2 2 .\nThe Sine activation and the Sinh kernel. Consider the activation σ(x) = sin(ax). We can write sin(ax) = e iax−e−iax\n2i . We have\nσ̂(ρ) = E (X,Y )∼Nρ\n( eiaX − e−iaX\n2i\n)( eiaY − e−iaY\n2i ) = −1\n4 E (X,Y )∼Nρ\n( eiaX − e−iaX ) ( eiaY − e−iaY ) = −1\n4 E (X,Y )∼Nρ\n[ eia(X+Y ) − eia(X−Y ) − eia(−X+Y ) + eia(−X−Y ) ] .\nRecall that the characteristic function, E[eitX ], when X is distributed N(0, 1) is e− 12 t2 . Since X+Y and −X−Y are normal variables with expectation 0 and variance of 2+2ρ, it follows that,\nE (X,Y )∼Nρ eia(X+Y ) = E (X,Y )∼Nρ\ne−ia(X+Y ) = e− a2(2+2ρ) 2 .\nSimilarly, since the variance of X − Y and Y −X is 2− 2ρ, we get\nE (X,Y )∼Nρ eia(X−Y ) = E (X,Y )∼Nρ\neia(−X+Y ) = e− a2(2−2ρ) 2 .\nWe therefore obtain that\nσ̂(ρ) = e−a 2(1−ρ) − e−a2(1+ρ)\n2 = e−a 2 sinh(a2ρ) .\nHermite activations and polynomial kernels. From Lemma 11 it follows that the dual activation of the Hermite polynomial hn is ĥn(ρ) = ρ\nn. Hence, the corresponding kernel is the polynomial kernel.\nThe normalized step activation. Consider the activation\nσ(x) =\n{√ 2 x > 0\n0 x ≤ 0 .\nTo calculate σ̂ we compute the Hermite expansion of σ. For n ≥ 0 we let\nan = 1√ 2π ∫ ∞ −∞ σ(x)hn(x)e −x 2 2 dx = 1√ π ∫ ∞ 0 hn(x)e −x 2 2 dx .\nSince h0(x) = 1, h1(x) = x, and h2(x) = x2−1√\n2 , we get the corresponding coefficients,\na0 = E X∼N(0,1) [σ(X)] = 1√ 2\na1 = E X∼N(0,1) [σ(X)X] = 1√ 2 E X∼N(0,1) [|X|] = 1√ π a2 = 1√ 2 E X∼N(0,1) [σ(X)(X2 − 1)] = 1 2 E X∼N(0,1) [X2 − 1] = 0 .\nFor n ≥ 3 we write gn(x) = hn(x)e− x2 2 and note that\ng′n(x) = [h ′ n(x)− xhn(x)] e−\nx2\n2 = [√ nhn−1(x)− xhn(x) ] e− x2 2 = − √ n+ 1hn+1(x)e −x 2 2 = − √ n+ 1 gn+1(x) .\nHere, the second equality follows from (4) and the third form (3). We therefore get\nan = 1√ π ∫ ∞ 0 gn(x)dx\n= − 1√ nπ ∫ ∞ 0 g′n−1(x)dx\n= 1√ nπ\ngn−1(0)− =0︷ ︸︸ ︷gn−1(∞) \n= 1√ nπ hn−1(0)\n=  (−1) n−1 2 (n−2)!! √ nπ √ (n−1)! = (−1) n−1 2 (n−2)!!√ πn! if n is odd\n0 if n is even .\nThe second equality follows from (3) and the last equality follows from (6). Finally, from Lemma 11 we have that σ̂(ρ) = ∑∞ n=0 bnρ n where\nbn =  ((n−2)!!)2 πn! if n is odd 1 2 if n = 0\n0 if n is even ≥ 2 .\nIn particular, (b0, b1, b2, b3, b4, b5, b6) = ( 1 2 , 1 π , 0, 1 6π , 0, 3 40π , 0 ) . Note that from the Taylor expansion of cos−1 it follows that σ̂(ρ) = 1− cos −1(ρ) π .\nThe normalized ReLU activation. Consider the activation σ(x) = √\n2 max(0, x). We now write σ̂(ρ) = ∑ i biρ i. The first coefficient is\nb0 =\n( E\nX∼N(0,1) σ(X)\n)2 = 1\n2\n( E X∼N(0,1) |X| )2 = 1 π .\nTo calculate the remaining coefficients we simply note that the derivative of the ReLU activation is the step activation and the mapping σ 7→ σ̂ commutes with differentiation. Hence, from the calculation of the step activation we get,\nbn =  ((n−3)!!)2 πn! if n is even 1 2 if n = 1\n0 if n is odd ≥ 3 .\nIn particular, (b0, b1, b2, b3, b4, b5, b6) = ( 1 π , 1 2 , 1 2π , 0, 1 24π , 0, 1 80π ) . We see that the coefficients corresponding to the degrees 0, 1, and 2 sum to 0.9774. The sums up to degrees 4 or 6 are 0.9907 and 0.9947 respectively. That is, we get an excellent approximation of less than 1% error with a dual activation of degree 4.\nThe collapsing tower of fully connected layers. To conclude this section we discuss the case of very deep networks. The setting is taken for illustrative purposes but it might surface when building networks with numerous fully connected layers. Indeed, most deep architectures that we are aware of do not employ more than five consecutive fully connected layers.\nConsider a skeleton Sm consisting of m fully connected layers, each layer associated with the same (normalized) activation σ. We would like to examine the form of the compositional kernel as the number of layers becomes very large. Due to the repeated structure and activation we have\nκSm(x,y) = αm ( 〈x,y〉 n ) where αm = σ̂ m = m times︷ ︸︸ ︷ σ̂ ◦ . . . ◦ σ̂ .\nHence, the limiting properties of κSm can be understood from the limit of αm. In the case that σ(x) = x or σ(x) = −x, σ̂ is the identity function. Therefore αm(ρ) = σ̂(ρ) = ρ for all m and κSm is simply the linear kernel. Assume now that σ is neither the identity nor its negation. The following claim shows that αm has a point-wise limit corresponding to a degenerate kernel.\nClaim 1. There exists a constant 0 ≤ ασ ≤ 1 such that for all −1 < ρ < 1,\nlim m→∞ αm(ρ) = ασ\nBefore proving the claim, we note that for ρ = 1, αm(1) = 1 for all m, and therefore limm→∞ αm(1) = 1. For ρ = −1, if σ is anti-symmetric then αm(−1) = −1 for all m, and in particular limm→∞ αm(−1) = −1. In any other case, our argument can show that limm→∞ αm(−1) = ασ.\nProof. Recall that σ̂(ρ) = ∑∞\ni=0 biρ i where the bi’s are non-negative numbers that sum to 1.\nBy the assumption that σ is not the identity or its negation, b1 < 1. We first claim that there is a unique ασ ∈ [0, 1] such that\n∀x ∈ (−1, ασ) , σ̂(ρ) > ρ and ∀x ∈ (ασ, 1) , ασ < σ̂(ρ) < ρ (9)\nTo prove (9) it suffices to prove the following properties.\n(a) σ̂(ρ) > ρ for ρ ∈ (−1, 0)\n(b) σ̂ is non-decreasing and convex in [0, 1]\n(c) σ̂(1) = 1\n(d) the graph of σ̂ has at most a single intersection in [0, 1) with the graph of f(ρ) = ρ\nIf the above properties hold we can take ασ to be the intersection point or 1 if such a point does not exist. We first show (a). For ρ ∈ (−1, 0) we have that\nσ̂(ρ) = b0 + ∞∑ i=1 biρ i ≥ b0 − ∞∑ i=1 bi|ρ|i > − ∞∑ i=1 bi|ρ| ≥ −|ρ| = ρ .\nHere, the third inequality follows form the fact that b0 ≥ 0 and for all i, −bi|ρ|i ≥ −bi|ρ|. Moreover since b1 < 1, one of these inequalities must be strict. Properties (b) and (c) follows from Lemma 11. Finally, to show (d), we note that the second derivative of σ̂(ρ) − ρ is∑\ni≥2 i(i− 1)biρi−2 which is non-negative in [0, 1). Hence, σ̂(ρ)− ρ is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1]. As we assume that σ̂ is not the identity, we can rule out the option of infinitely many intersections. Also, since σ̂(1) = 1, we know that there is at least one intersection in [0, 1]. Hence, there are 1 or 2 intersections in [0, 1] and because one of them is in ρ = 1, we conclude that there is at most one intersection in [0, 1).\nLastly, we derive the conclusion of the claim from equation (9). Fix ρ ∈ (−1, 1). Assume first that ρ ≥ ασ. By (9), αm(ρ) is a monotonically non-increasing sequence that is lower bounded by ασ. Hence, it has a limit ασ ≤ τ ≤ ρ < 1. Now, by the continuity of σ̂ we have\nσ̂(τ) = σ̂ (\nlim m→∞\nαm(ρ) )\n= lim m→∞ σ̂(αm(ρ)) = lim m→∞ αm+1(ρ) = τ .\nSince the only solution to σ̂(ρ) = ρ in (−1, 1) is ασ, we must have τ = ασ. We next deal with the case that −1 < ρ < ασ. If for some m, αm(ρ) ∈ [ασ, 1), the argument for ασ ≤ ρ shows that ασ = limm→∞ αm(ρ). If this is not the case, we have that for all m, αm(ρ) ≤ αm+1(ρ) ≤ ασ. As in the case of ρ ≥ ασ, this can be used to show that αm(ρ) converges to ασ."
    }, {
      "heading" : "9 Proofs",
      "text" : ""
    }, {
      "heading" : "9.1 Well-behaved activations",
      "text" : "The proof of our main results applies to activations that are decent, i.e. well-behaved, in a sense defined in the sequel. We then show that C-bounded activations as well as the ReLU activation are decent. We first need to extend the definition of the dual activation and kernel to apply to vectors in Rd, rather than just Sd. We denote by M+ the collection of 2 × 2 positive semi-define matrices and by M++ the collection of positive definite matrices.\nDefinition. Let σ be an activation. Define the following,\nσ̄ :M2+ → R , σ̄(Σ) = E (X,Y )∼N(0,Σ) σ(X)σ(Y ) , kσ(x,y) = σ̄ ( ‖x‖2 〈x,y〉 〈x,y〉 ‖y‖2 ) .\nWe underscore the following properties of the extension of a dual activation.\n(a) The following equality holds,\nσ̂(ρ) = σ̄ ( 1 ρ ρ 1 ) (b) The restriction of the extended kσ to the sphere agrees with the restricted definition.\n(c) The extended dual activation and kernel are defined for every activation σ such that for all a ≥ 0, x 7→ σ(ax) is square integrable with respect to the Gaussian measure.\n(d) For x,y ∈ Rd, if w ∈ Rd is a multivariate normal distribution with zero mean vector and identity covariance matrix, then\nkσ(x,y) = E w σ(〈w,x〉)σ(〈w,y〉) .\nDenote\nMγ+ := {(\nΣ11 Σ12 Σ12 Σ22\n) ∈M+ | 1− γ ≤ Σ11,Σ22 ≤ 1 + γ } .\nDefinition. A normalized activation σ is (α, β, γ)-decent for α, β, γ ≥ 0 if the following conditions hold.\n(i) The dual activation σ̄ is β-Lipschitz in Mγ+ with respect to the ∞-norm.\n(ii) If (X1, Y1), . . . , (Xr, Yr) are independent samples from N (0,Σ) for Σ ∈Mγ+ then\nPr (∣∣∣∣∑ri=1 σ(Xi)σ(Yi)r − σ̄(Σ) ∣∣∣∣ ≥ ) ≤ 2 exp(− r 22α2 ) .\nLemma 12 (Bounded activations are decent). Let σ : R → R be a C-bounded normalized activation. Then, σ is (C2, 2C2, γ)-decent for all γ ≥ 0.\nProof. It is enough to show that the following properties hold.\n1. The (extended) dual activation σ̄ is 2C2-Lipschitz in M++ w.r.t. the ∞-norm.\n2. If (X1, Y1), . . . , (Xr, Yr) are independent samples from N (0,Σ) then\nPr (∣∣∣∣∑ri=1 σ(Xi)σ(Yi)r − σ̄(Σ) ∣∣∣∣ ≥ ) ≤ 2 exp(− r 22C4 ) From the boundedness of σ it holds that |σ(X)σ(Y )| ≤ C2. Hence, the second property follows directly from Hoeffding’s bound. We next prove the first part. Let z = (x, y) and φ(z) = σ(x)σ(y). Note that for Σ ∈M++ we have\nσ̄(Σ) = 1 2π √ det(Σ) ∫ R2 φ(z)e− z>Σ−1z 2 dz .\nThus we get that,\n∂σ̄ ∂Σ = 1 2π ∫ R2 φ(z) [ 1 2 √ det(Σ)Σ−1 − 1 2 √ det(Σ)(Σ−1zz>Σ−1) det(Σ) ] e− z>Σ−1z 2 dz\n= 1 2π √ det(Σ) ∫ R2 φ(z) 1 2 [ Σ−1 − Σ−1zz>Σ−1 ] e− z>Σ−1z 2 dz\nLet g(z) = e− z>Σ−1z 2 . Then, the first and second order partial derivatives of g are\n∂g ∂z = −Σ−1ze− z>Σ−1z 2\n∂2g ∂2z =\n[ −Σ−1 + Σ−1zz>Σ−1 ] e− z>Σ−1z 2 .\nWe therefore obtain that,\n∂σ̄ ∂Σ = − 1 4π √ det(Σ) ∫ R2 φ ∂2g ∂2z dz .\nBy the product rule we have\n∂σ̄ ∂Σ = − 1 2π √ det(Σ) 1 2 ∫ R2 ∂2φ ∂2z gdz = −1 2 E (X,Y )∼N(0,Σ) [ ∂2φ ∂2z (X, Y ) ] We conclude that σ̄ is differentiable in M++ with partial derivatives that are point-wise bounded by C 2\n2 . Thus, σ̄ is 2C2-Lipschitz in M+ w.r.t. the ∞-norm.\nWe next show that the ReLU activation is decent.\nLemma 13 (ReLU is decent). There exists a constant αReLU ≥ 1 such that for 0 ≤ γ ≤ 1, the normalized ReLU activation σ(x) = √ 2 max(0, x) is (αReLU, 1 + o(γ), γ)-decent.\nProof. The measure concentration property follows from standard concentration bounds for sub-exponential random variables (e.g. [53]). It remains to show that σ̄ is (1+o(γ))-Lipschitz in Mγ+. We first calculate an exact expression for σ̄. The expression was already calculated in [13], yet we give here a derivation for completeness.\nClaim 2. The following equality holds for all Σ ∈M2+,\nσ̄(Σ) = √\nΣ11Σ22 σ̂\n( Σ12√\nΣ11Σ22\n) .\nProof. Let us denote\nΣ̃ =\n( 1 Σ12√\nΣ11Σ12 Σ12√\nΣ11Σ12 1\n) .\nBy the positive homogeneity of the ReLU activation we have\nσ̄ (Σ) = E (X,Y )∼N(0,Σ) σ(X)σ(Y )\n= √\nΣ11Σ22 E (X,Y )∼N(0,Σ) σ ( X√ Σ11 ) σ ( Y√ Σ22 ) = √ Σ11Σ22 E (X̃,Ỹ )∼N(0,Σ̃) σ ( X̃ ) σ ( Ỹ )\n= √\nΣ11Σ22 σ̂\n( Σ12√\nΣ11Σ22\n) .\nwhich concludes the proof.\nFor brevity, we henceforth drop the argument from σ̄(Σ) and use the abbreviation σ̄. In order to show that σ̄ is (1 + o(γ))-Lipschitz w.r.t. the ∞-norm it is enough to show that for every Σ ∈Mγ+ we have,\n‖∇σ̄‖1 = ∣∣∣∣ ∂σ̄∂Σ12 ∣∣∣∣+ ∣∣∣∣ ∂σ̄∂Σ11 ∣∣∣∣+ ∣∣∣∣ ∂σ̄∂Σ22 ∣∣∣∣ ≤ 1 + o(γ) . (10)\nFirst, Note that ∂σ̄/∂Σ11 and ∂σ̄/∂Σ22 have the same sign, hence, ‖∇σ̄‖1 = ∣∣∣∣ ∂σ̄∂Σ12 ∣∣∣∣+ ∣∣∣∣ ∂σ̄∂Σ11 + ∂σ̄∂Σ22 ∣∣∣∣ .\nNext we get that,\n∂σ̄\n∂Σ11 =\n1\n2 √ Σ22 Σ11 σ̂ ( Σ12√ Σ11Σ22 ) − 1 2 √ Σ22 Σ11 Σ12√ Σ11Σ22 σ̂′ ( Σ12√ Σ11Σ22 ) ∂σ̄\n∂Σ22 =\n1\n2 √ Σ11 Σ22 σ̂ ( Σ12√ Σ11Σ22 ) − 1 2 √ Σ11 Σ22 Σ12√ Σ11Σ22 σ̂′ ( Σ12√ Σ11Σ22 ) ∂σ̄\n∂Σ12 = σ̂′\n( Σ12√\nΣ11Σ22\n) .\nWe therefore get that the 1-norm of ∇σ̄ is,\n‖∇σ̄‖1 = 1\n2 Σ11 + Σ22√ Σ11Σ22 ∣∣∣∣σ̂( Σ12√Σ11Σ22 ) − Σ12√ Σ11Σ22 σ̂′ ( Σ12√ Σ11Σ22 )∣∣∣∣+ σ̂′( Σ12√Σ11Σ22 ) .\nThe gradient of 1 2 Σ11+Σ22√ Σ11Σ22 at (Σ11,Σ22) = (1, 1) is (0, 0). Therefore, from the mean value theorem we get, 1 2 Σ11+Σ22√ Σ11Σ22 = 1 + o(γ). Furthermore, σ̂, σ̂′ and Σ12√ Σ11Σ22\nare bounded by 1 in absolute value. Hence, we can write,\n‖∇σ̄‖1 = ∣∣∣∣σ̂( Σ12√Σ11Σ22 ) − Σ12√ Σ11Σ22 σ̂′ ( Σ12√ Σ11Σ22 )∣∣∣∣+ σ̂′( Σ12√Σ11Σ22 ) + o(γ) .\nFinally, if we let t = Σ12√ Σ11Σ22 , we can further simply the expression for ∇σ̄,\n‖∇σ̄(Σ)‖1 = |σ̂(t)− tσ̂′(t)|+ |σ̂′(t)|+ o(γ)\n= √ 1− t2 π + 1− cos −1(t) π + o(γ) .\nFinally, the proof is obtained from the fact that the function f(t) = √\n1−t2 π + 1 − cos −1(t) π\nsatisfies 0 ≤ f(t) ≤ 1 for every t ∈ [−1, 1]. Indeed, it is simple to verify that f(−1) = 0 and f(1) = 1. Hence, it suffices to show that f ′ is non-negative in [−1, 1] which is indeed the case since,\nf ′(t) = 1\nπ 1− t√ 1− t2 = 1 π √ 1− t 1 + t ≥ 0 ."
    }, {
      "heading" : "9.2 Proofs of Thms. 2 and 3",
      "text" : "We start by an additional theorem which serves as a simple stepping stone for proving the aforementioned main theorems.\nTheorem 14. Let S be a skeleton with (α, β, γ)-decent activations, 0 < ≤ γ, and Bd =∑d−1 i=0 β i. Let w be a random initialization of the network N = N (S, r) with\nr ≥ 2α2B2depth(S) log\n( 8|S| δ ) 2 .\nThen, for every x,y with probability of at least 1− δ, it holds that\n|κw(x,y)− κS(x,y)| ≤ .\nBefore proving the theorem we show that together with Lemmas 12 and 13, Theorems 2 and 3 follow from Theorem 14. We restate them as corollaries, prove them, and then proceed to the proof of Theorem 14.\nCorollary 15. Let S be a skeleton with C-bounded activations. Let w be a random initialization of N = N (S, r) with\nr ≥ (4C4)depth(S)+1 log\n( 8|S| δ ) 2 .\nThen, for every x,y, w.p. ≥ 1− δ,\n|κw(x,y)− κS(x,y)| ≤ .\nProof. From Lemma 12, for all γ > 0, each activation is (C2, 2C2, γ)-decent. By Theorem 14, it suffices to show that\n2 ( C2 )2depth(S)−1∑\ni=0\n(2C2)i 2 ≤ (4C4)depth(S)+1 . The sum of can be bounded above by,\ndepth(S)−1∑ i=0 (2C2)i = (2C2)depth(S) − 1 2C2 − 1 ≤ (2C 2)depth(S) C2 .\nTherefore, we get that,\n2 ( C2 )2depth(S)−1∑\ni=0\n(2C2)i 2 ≤ 2C4(4C4)depth(S) C4 ≤ (4C4)depth(S)+1 ,\nwhich concludes the proof.\nCorollary 16. Let S be a skeleton with ReLU activations, and w a random initialization of N (S, r) with r ≥ c1 depth2(S) log( 8|S|δ ) 2 . For all x,y and ≤ min(c2, 1depth(S)), w.p. ≥ 1− δ,\n|κw(x,y)− κS(x,y)| ≤\nHere, c1, c2 > 0 are universal constants.\nProof. From Lemma 13, each activation is (αReLU, 1 + o( ), )-decent. By Theorem 14, it is enough to show that\ndepth(S)−1∑ i=0 (1 + o( ))i = O(depth(S)) .\nThis claim follows from the fact that (1 + o( ))i ≤ eo( )depth(S) as long as i ≤ depth(S). Since we assume that ≤ 1/depth(S), the expression is bounded by e for sufficiently small . We next prove Theorem 14.\nProof. (Theorem 14) For a node u ∈ S we denote by Ψu,w : X → Rr the normalized representation of S’s sub-skeleton rooted at u. Analogously, κu,w denotes the empirical kernel of that network. When u is the output node of S we still use Ψw and κw for Ψu,w and κu,w. Given two fixed x,y ∈ X and a node u ∈ S, we denote\nKuw = ( κu,w(x,x) κu,w(x,y) κu,w(x,y) κu,w(y,y) ) , Ku = ( κu(x,x) κu(x,y) κu(x,y) κu(y,y) )\nK←uw = ∑ v∈in(u)Kvw |in(u)| , K←u = ∑ v∈in(u)Kv |in(u)| .\nFor a matrix K ∈M+ and a function f :M+ → R, we denote\nfp(K) = f ( K11 K11 K11 K11 ) f(K)\nf(K) f ( K22 K22 K22 K22\n) \nNote that Ku = σ̄pu(K←u) and Kuw = σ̄pu(K←uw ). We say that a node u ∈ S, is well-initialized if\n‖Kuw −Ku‖∞ ≤ Bdepth(u) Bdepth(S) . (11)\nHere, we use the convention that B0 = 0. It is enough to show that with probability of at least ≥ 1− δ all nodes are well-initialized. We first note that input nodes are well-initialized by construction since Kuw = Ku. Next, we show that given that all incoming nodes for a certain node are well-initialized, then w.h.p. the node is well-initialized as well.\nClaim 3. Assume that all the nodes in in(u) are well-initialized. Then, the node u is wellinitialized with probability of at least 1− δ|S| .\nProof. It is easy to verify that Kuw is the empirical covariance matrix of r independent variables distributed according to (σ(X), σ(Y )) where (X, Y ) ∼ N (0,K←uw ). Given the assumption that all nodes incoming to u are well-initialized, we have,\n‖K←uw −K←u‖∞ = ∥∥∥∥∥ ∑ v∈in(v)Kvw |in(v)| − ∑ v∈in(v)Kv |in(v)| ∥∥∥∥∥ ∞\n≤ 1 |in(v)| ∑ v∈in(v) ‖Kvw −Kv‖∞ (12)\n≤ Bdepth(u)−1 Bdepth(S) .\nFurther, since ≤ γ then K←uw ∈ M γ +. Using the fact that σu is (α, β, γ)-decent and that r ≥ 2α2B2 depth(S) log( 8|S| δ )\n2 , we get that w.p. of at least 1− δ|S| ,\n‖Kuw − σ̄pu (K←uw )‖∞ ≤\nBdepth(S) . (13)\nFinally, using (12) and (13) along with the fact that σ̄ is β-Lipschitz, we have\n‖Kuw −Ku‖∞ = ‖Kuw − σ̄pu (K←u)‖∞ ≤ ‖Kuw − σ̄pu (K←uw )‖∞ + ‖σ̄ p u (K←uw )− σ̄pu (K←u)‖∞\n≤ Bdepth(S) + β ‖K←uw −K←u‖∞\n≤ Bdepth(S) + β Bdepth(u)−1 Bdepth(S) = Bdepth(u) Bdepth(S) .\nWe are now ready to conclude the proof. Let u1, . . . , u|S| be an ordered list of the nodes in S in accordance to their depth, starting with the shallowest nodes, and ending with the output node. Denote by Aq the event that u1, . . . , uq are well-initialized. We need to show that Pr(A|S|) ≥ 1− δ. We do so using an induction on q for the inequality Pr(Aq) ≥ 1− qδ|S| . Indeed, for q = 1, . . . , n, uq is an input node and Pr(Aq) = 1. Thus, the base of the induction hypothesis holds. Assume that q > n. By Claim (3) we have that Pr(Aq|Aq−1) ≥ 1 − δ|S| . Finally, from the induction hypothesis we have,\nPr(Aq) ≥ Pr(Aq|Aq−1) Pr(Aq−1) ≥ (\n1− δ |S|\n)( 1− (q − 1)δ\n|S|\n) ≥ 1− qδ\n|S| ."
    }, {
      "heading" : "9.3 Proofs of Thms. 4 and 5",
      "text" : "Theorems 4 and 5 follow from using the following lemma combined with Theorems 5 and 3. When we apply the lemma, we always focus on the special case where one of the kernels is constant w.p. 1.\nLemma 17. Let D be a distribution on X × Y, ` : R × Y → R be an L-Lipschitz loss, δ > 0, and κ1, κ2 : X × X → R be two independent random kernels sample from arbitrary distributions. Assume that the following properties hold.\n• For some C > 0, ∀x ∈ X , κ1(x,x), κ2(x,x) ≤ C.\n• ∀x,y ∈ X , Prκ1,κ2 (|κ1(x,y)− κ2(x,y)| ≥ ) ≤ δ̃ for δ̃ < c2 2δ\nC2 log2( 1δ ) where c2 > 0 is a\nuniversal constant.\nThen, w.p. ≥ 1 − δ over the choices of κ1, κ2, for every f1 ∈ HMκ1 there is f2 ∈ H √ 2M κ2\nsuch that LD(f2) ≤ LD(f1) + √ 4LM .\nTo prove the above lemma, we state another lemma below followed by a basic measure concentration result.\nLemma 18. Let x1, . . . ,xm ∈ Rd, w∗ ∈ Rd and > 0. There are weights α1, . . . , αm such that for w := ∑m i=1 αixi we have,\n• L(w) := 1 m ∑m i=1 |〈w,xi〉 − 〈w∗,xi〉| ≤\n• ∑\ni |αi| ≤ ‖w∗‖2\n• ‖w‖ ≤ ‖w∗‖\nProof. Denote M = ‖w∗‖, C = maxi ‖xi‖, and yi = 〈w∗,xi〉. Suppose that we run stochastic gradient decent on the sample {(x1, y1), . . . , (xm, ym)} w.r.t. the loss L(w), with learning rate η =\nC2 , and with projections onto the ball of radius M . Namely, we start with w0 = 0 and\nat each iteration t ≥ 1, we choose at random it ∈ [m] and perform the update,\nw̃t = { wt−1 − ηxit 〈wt−1,xit〉 ≥ yit wt−1 + ηxit 〈wt−1,xit〉 < yit\nwt = { w̃t ‖w̃t‖ ≤M Mw̃t ‖w̃t‖ ‖w̃t‖ > M\nAfter T = M 2C2\n2 iterations the loss in expectation would be at most (see for instance\nChapter 14 in [53]). In particular, there exists a sequence of at most M 2C2\n2 gradient steps\nthat attains a solution w with L(w) ≤ . Each update adds or subtracts C2 xi from the current solution. Hence w can be written as a weighted sum of xi’s where the sum of each coefficient is at most T\nC2 = M\n2\n.\nTheorem 19 (Bartlett and Mendelson [8]). Let D be a distribution over X×Y, ` : R×Y → R a 1-Lipschitz loss, κ : X × X → R a kernel, and , δ > 0. Let S = {(x1, y1), . . . , (xm, ym)} be i.i.d. samples from D such that m ≥ cM 2 maxx∈X κ(x,x)+log( 1δ ) 2\nwhere c is a constant. Then, with probability of at least 1− δ we have,\n∀f ∈ HMκ , |LD(f)− LS(f)| ≤ .\nProof. (of Lemma 17) By rescaling `, we can assume w.l.o.g that L = 1. Let 1 = √ M and S = {(x1, y1), . . . , (xm, ym)} ∼ D be i.i.d. samples which are independent of the choice of κ1, κ2. By Theorem 19, for a large enough constant c, if m = c\nCM2 log( 1δ ) 21 = c C log( 1δ ) , then\nw.p. ≥ 1− δ 2 over the choice of the samples we have,\n∀f ∈ HMκ1 ∪H √ 2M κ2 , |LD(f)− LS(f)| ≤ 1 (14)\nNow, if we choose c2 = 1 2c2 then w.p. ≥ 1 −m2δ̃ ≥ 1 − δ 2 (over the choice of the examples and the kernel), we have that\n∀i, j ∈ [m], |κ1(xi,xj)− κ2(xi,xj)| < . (15) In particular, w.p. ≥ 1−δ (14) and (15) hold and therefore it suffices to prove the conclusion of the theorem under these conditions. Indeed, let Ψ1,Ψ2 : X → H be two mapping from X to a Hilbert space H so that κi(x,y) = 〈Ψi(x),Ψi(y)〉. Let f1 ∈ HMκ1 . By lemma 18 there are α1, . . . , αm so that for the vector w = ∑m i=1 α1Ψ1(xi) we have\n1\nm m∑ i=1 |〈w,Ψ1(xi)〉 − f1(xi)| ≤ 1, ‖w‖ ≤M , (16)\nand m∑ i=1 |αi| ≤ M2 1 . (17)\nConsider the function f2 ∈ H2 defined by f2(x) = ∑m i=1 α1〈Ψ2(xi),Ψ2(x)〉. We note that\n‖f2‖2Hk2 ≤ ∥∥∥∥∥ m∑ i=1 αiΨ2(xi) ∥∥∥∥∥ 2\n= m∑\ni,j=1\nαiαjκ2(xi,xj)\n≤ m∑\ni,j=1\nαiαjκ1(xi,xj) + m∑\ni,j=1\n|αiαj|\n= ‖w‖2 + ( m∑ i=1 |αi| )2 ≤ M2 + M 4\n21 = 2M2 .\nDenote by f̃1(x) = 〈w,Ψ1(x)〉 and note that for every i ∈ [m] we have,\n|f̃1(xi)− f2(xi)| = ∣∣∣∣∣ m∑ j=1 αj (κ1(xi,xj)− κ2(xi,xj)) ∣∣∣∣∣ ≤\nm∑ i=1 |αi| ≤ M2 1 = 1 .\nFinally, we get that,\nLD(f2) ≤ LS(f2) + 1\n= 1\nm m∑ i=1 ` (f2(xi), yi) + 1\n≤ 1 m m∑ i=1 ` ( f̃1(xi), yi ) + 1 + 1\n≤ 1 m m∑ i=1 ` (f1(xi), yi) + |f̃1(xi)− f1(xi)|+ 2 1\n≤ 1 m m∑ i=1 ` (f1(xi), yi) + 3 1 ≤ LS(f1) + 3 1 ≤ LD(f1) + 4 1 ,\nwhich concludes the proof."
    }, {
      "heading" : "10 Discussion",
      "text" : "Role of initialization and training. Our results surface the question of the extent to which random initialization accounts for the success of neural networks. While we mostly leave this question for future research, we would like to point to empirical evidence supporting the important role of initialization. First, numerous researchers and practitioners demonstrated that random initialization, similar to the scheme we analyze, is crucial to the success of neural network learning (see for instance [20]). This suggests that starting from arbitrary weights is unlikely to lead to a good solution. Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15]. For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49]. Furthermore, Saxe et al. [49] show that the performance of training the last layer is quite correlated with training the entire network. The effectiveness of optimizing solely the last layer is also manifested by the popularity of the random features paradigm [46]. Finally, other studies show that the metrics induced by the initial and fully trained representations are not substantially different. Indeed, Giryes et al. [19] demonstrated that for the MNIST and CIFAR-10 datasets the distances’ histogram of different examples barely changes when moving from the initial to the trained representation. For the ImageNet dataset the difference is more pronounced yet still moderate.\nThe role of architecture. By using skeletons and compositional kernel spaces, we can reason about functions that the network can actually learn rather than merely express. This may explain in retrospect past architectural choices and potentially guide future choices. Let us consider for example the task of object recognition. It appears intuitive, and is supported by visual processing mechanisms in mammals, that in order to perform object recognition,\nthe first processing stages are confined to local receptive fields. Then, the result of the local computations are applied to detect more complex shapes which are further combined towards a prediction. This processing scheme is naturally expressed by convolutional skeletons. A two dimensional version of Example 1 demonstrates the usefulness of convolutional networks for vision and speech applications.\nThe rationale we described above was pioneered by LeCun and colleagues [32]. Alas, the mere fact that a network can express desired functions does not guarantee that it can actually learn them. Using for example Barron’s theorem [7], one may claim that visionrelated functions are expressed by fully connected two layer networks, but such networks are inferior to convolutional networks in machine vision applications. Our result mitigates this gap. First, it enables use of the original intuition behind convolutional networks in order to design function spaces that are provably learnable. Second, as detailed in Example 1, it also explains why convolutional networks perform better than fully connected networks.\nThe role of other architectural choices. In addition to the general topology of the network, our theory can be useful for understanding and guiding other architectural choices. We give two examples. First, suppose that a skeleton S has a fully connected layer with the dual activation σ̂1, followed by an additional fully connected layer with dual activation σ̂2. It is straightforward to verify that if these two layers are replaced by a single layer with dual activation σ̂2 ◦ σ̂1, the corresponding compositional kernel space remains the same. This simple observation can be useful in potentially saving a whole layer in the corresponding networks.\nThe second example is concerned with the ReLU activation, which is one of the most common activations used in practice. Our theory suggests a somewhat surprising explanation for its usefulness. First, the dual kernel of the ReLU activation enables expression of non-linear functions. However, this property holds true for many activations. Second, Theorem 3 shows that even for quite deep networks with ReLU activations, random initialization approximates the corresponding kernel. While we lack a proof at the time of writing, we conjecture that this property holds true for many other activations. What is then so special about the ReLU? Well, an additional property of the ReLU is being positive homogeneous, i.e. satisfying σ(ax) = aσ(x) for all a ≥ 0. This fact makes the ReLU activation robust to small perturbations in the distribution used for initialization. Concretely, if we multiply the variance of the random weights by a constant, the distribution of the generated representation and the space Hw remain the same up to a scaling. Note moreover that training algorithms are sensitive to the initialization. Our initialization is very similar to approaches used in practice, but encompasses a small “correction”, in the form of a multiplication by a small constant which depends on the activation. For most activations, ignoring this correction, especially in deep networks, results in a large change in the generated representation. The ReLU activation is more robust to such changes. We note that similar reasoning applies to the max-pooling operation.\nFuture work. Though our formalism is fairly general, we mostly analyzed fully connected and convolutional layers. Intriguing questions remain, such as the analysis of max-pooling and recursive neural network components from the dual perspective. On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25]. Beside studying existing elements of neural network learning, it would be interesting to devise new architectural components inspired by duality. More concrete questions are concerned with quantitative improvements of the main results. In particular, it remains open whether the dependence on 2O(depth(S)) can be made polynomial and the quartic dependence on 1/ , R, and L can be improved. In addition to being interesting in their own right, improving the bounds may further underscore the effectiveness of random initialization as a way of generating low dimensional embeddings of compositional kernel spaces. Randomly generating such embeddings can be also considered on its own, and we are currently working on design and analysis of random features a la Rahimi and Recht [45]."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Yossi Arjevani, Elad Eban, Moritz Hardt, Elad Hazan, Percy Liang, Nati Linial, Ben Recht, and Shai Shalev-Shwartz for fruitful discussions, comments, and suggestions."
    } ],
    "references" : [ {
      "title" : "Learning polynomials with neural networks",
      "author" : [ "A. Andoni", "R. Panigrahy", "G. Valiant", "L. Zhang" ],
      "venue" : "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1908–1916,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep convolutional networks are hierarchical kernel machines",
      "author" : [ "F. Anselmi", "L. Rosasco", "C. Tan", "T. Poggio" ],
      "venue" : "arXiv:1508.01084,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural Network Learning: Theoretical Foundations",
      "author" : [ "M. Anthony", "P. Bartlet" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "S. Arora", "A. Bhaskara", "R. Ge", "T. Ma" ],
      "venue" : "Proceedings of The 31st International Conference on Machine Learning, pages 584–592,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Breaking the curse of dimensionality with convex neural networks",
      "author" : [ "F. Bach" ],
      "venue" : "arXiv:1412.8690,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the equivalence between kernel quadrature rules and random feature expansions",
      "author" : [ "F. Bach" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Universal approximation bounds for superposition of a sigmoidal function",
      "author" : [ "A.R. Barron" ],
      "venue" : "IEEE Transactions on Information Theory, 39(3):930–945,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Rademacher and Gaussian complexities: Risk bounds and structural results",
      "author" : [ "P.L. Bartlett", "S. Mendelson" ],
      "venue" : "Journal of Machine Learning Research, 3:463–482,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network",
      "author" : [ "P.L. Bartlett" ],
      "venue" : "IEEE Transactions on Information Theory, 44(2):525–536, March",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "What size net gives valid generalization",
      "author" : [ "E.B. Baum", "D. Haussler" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1989
    }, {
      "title" : "Object recognition with hierarchical kernel descriptors",
      "author" : [ "L. Bo", "K. Lai", "X. Ren", "D. Fox" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1729–1736. IEEE,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1872–1886,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Y. Cho", "L.K. Saul" ],
      "venue" : "Advances in neural information processing systems, pages 342–350,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "A. Choromanska", "M. Henaff", "M. Mathieu", "G. Ben Arous", "Y. LeCun" ],
      "venue" : "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, pages 192–204,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Beyond simple features: A large-scale feature search approach to unconstrained face recognition",
      "author" : [ "D. Cox", "N. Pinto" ],
      "venue" : "Automatic Face & Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on, pages 8–15. IEEE,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Complexity theoretic limitations on learning halfspaces",
      "author" : [ "A. Daniely" ],
      "venue" : "STOC,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Complexity theoretic limitations on learning DNFs",
      "author" : [ "A. Daniely", "S. Shalev-Shwartz" ],
      "venue" : "arXiv:1404.3378 v1,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "From average case complexity to improper learning complexity",
      "author" : [ "A. Daniely", "N. Linial", "S. Shalev-Shwartz" ],
      "venue" : "STOC,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Deep neural networks with random gaussian weights: A universal classification strategy",
      "author" : [ "R. Giryes", "G. Sapiro", "A.M. Bronstein" ],
      "venue" : "arXiv preprint arXiv:1504.08291,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "X. Glorot", "Y. Bengio" ],
      "venue" : "International conference on artificial intelligence and statistics, pages 249–256,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "The pyramid match kernel: Discriminative classification with sets of image features",
      "author" : [ "K. Grauman", "T. Darrell" ],
      "venue" : "Computer Vision, 2005. ICCV 2005. Tenth IEEE International Conference on, volume 2, pages 1458–1465. IEEE,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "M. Hardt", "B. Recht", "Y. Singer" ],
      "venue" : "arXiv:1509.01240,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distributional structure",
      "author" : [ "Z.S. Harris" ],
      "venue" : "Word,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1954
    }, {
      "title" : "Steps toward deep kernel methods from infinite neural networks",
      "author" : [ "T. Hazan", "T. Jaakkola" ],
      "venue" : "arXiv:1508.05133,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "S. Ioffe", "C. Szegedy" ],
      "venue" : "arXiv:1502.03167,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "What is the best multi-stage architecture for object recognition? In Computer Vision, 2009 IEEE 12th International Conference on, pages 2146–2153",
      "author" : [ "K. Jarrett", "K. Kavukcuoglu", "M. Ranzato", "Y. LeCun" ],
      "venue" : "IEEE,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Random feature maps for dot product kernels",
      "author" : [ "P. Kar", "H. Karnick" ],
      "venue" : "arXiv:1201.6530,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Some connections between nonuniform and uniform complexity classes",
      "author" : [ "R.M. Karp", "R.J. Lipton" ],
      "venue" : "Proceedings of the twelfth annual ACM symposium on Theory of computing, pages 302–309. ACM,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1980
    }, {
      "title" : "Cryptographic limitations on learning Boolean formulae and finite automata",
      "author" : [ "M. Kearns", "L.G. Valiant" ],
      "venue" : "STOC, pages 433–444, May",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Cryptographic hardness for learning intersections of halfspaces",
      "author" : [ "A.R. Klivans", "A.A. Sherstov" ],
      "venue" : "FOCS,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "Advances in neural information processing systems, pages 1097–1105,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, 86(11):2278–2324,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Deep learning",
      "author" : [ "Y. LeCun", "Y. Bengio", "G. Hinton" ],
      "venue" : "Nature, 521(7553):436–444,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "O. Levy", "Y. Goldberg" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2177–2185,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "R. Livni", "S. Shalev-Shwartz", "O. Shamir" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 855–863,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convolutional kernel networks",
      "author" : [ "J. Mairal", "P. Koniusz", "Z. Harchaoui", "Cordelia Schmid" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "Advances in neural information processing systems, pages 3111–3119,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Bayesian learning for neural networks, volume 118",
      "author" : [ "R.M. Neal" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Path-SGD: Path-normalized optimization in deep neural networks",
      "author" : [ "B. Neyshabur", "R. R Salakhutdinov", "N. Srebro" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2413–2421,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Norm-based capacity control in neural networks",
      "author" : [ "B. Neyshabur", "N. Srebro", "R. Tomioka" ],
      "venue" : "COLT,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Analysis of boolean functions",
      "author" : [ "R. O’Donnell" ],
      "venue" : null,
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2014
    }, {
      "title" : "Spherical random features for polynomial kernels",
      "author" : [ "J. Pennington", "F. Yu", "S. Kumar" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1837–1845,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An evaluation of the invariance properties of a biologicallyinspired system for unconstrained face recognition",
      "author" : [ "N. Pinto", "D. Cox" ],
      "venue" : "Bio-Inspired Models of Network, Information, and Computing Systems, pages 505–518. Springer,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A high-throughput screening approach to discovering good forms of biologically inspired visual representation",
      "author" : [ "N. Pinto", "D. Doukhan", "J.J. DiCarlo", "D.D. Cox" ],
      "venue" : "PLoS Computational Biology, 5(11):e1000579,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS, pages 1177–1184,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "Advances in neural information processing systems, pages 1313–1320,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "On the quality of the initial basin in overspecified neural networks",
      "author" : [ "I. Safran", "O. Shamir" ],
      "venue" : "arxiv:1511.04210,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Theory of reproducing kernels and its applications",
      "author" : [ "S. Saitoh" ],
      "venue" : "Longman Scientific & Technical England,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "On random weights and unsupervised feature learning",
      "author" : [ "A. Saxe", "P.W. Koh", "Z. Chen", "M. Bhand", "B. Suresh", "A.Y. Ng" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1089–1096,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Positive definite functions on spheres",
      "author" : [ "I.J. Schoenberg" ],
      "venue" : "Duke Mathematical Journal,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1942
    }, {
      "title" : "Prior knowledge in support vector kernels",
      "author" : [ "B. Schölkopf", "P. Simard", "A. Smola", "Vladimir Vapnik" ],
      "venue" : "In Proceedings of the 1997 conference on Advances in neural information processing systems",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 1998
    }, {
      "title" : "Provable methods for training neural networks with sparse connectivity",
      "author" : [ "H. Sedghi", "A. Anandkumar" ],
      "venue" : "arXiv:1412.2693,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "S. Shalev-Shwartz", "S. Ben-David" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Dropout: A simple way to prevent neural networks from overfitting",
      "author" : [ "N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov" ],
      "venue" : "The Journal of Machine Learning Research, 15(1):1929–1958,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in neural information processing systems, pages 3104–3112,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Computation with infinite neural networks",
      "author" : [ "C.K.I. Williams" ],
      "venue" : "pages 295–301,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]).",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 32,
      "context" : "Neural network (NN) learning has underpinned state of the art empirical results in numerous applied machine learning tasks (see for instance [31, 33]).",
      "startOffset" : 141,
      "endOffset" : 149
    }, {
      "referenceID" : 48,
      "context" : "This idea was advocated and tested empirically in [49].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : "Standard results from complexity theory [28] imply that essentially all functions of interest (that is, any efficiently computable function) can be expressed by a network of moderate size.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 31,
      "context" : "Biological phenomena show that many relevant functions can be expressed by even simpler networks, similar to convolutional neural networks [32] that are dominant in ML tasks today.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 6,
      "context" : "Barron’s theorem [7] states that even two-layer networks can express a very rich set of functions.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.",
      "startOffset" : 37,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.",
      "startOffset" : 37,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.",
      "startOffset" : 37,
      "endOffset" : 47
    }, {
      "referenceID" : 39,
      "context" : "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 21,
      "context" : "As for question (ii), both classical [10, 9, 3] and more recent [40, 22] results from statistical learning theory show that as the number of examples grows in comparison to the size of the network the empirical loss must be close to the population loss.",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 28,
      "context" : "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 29,
      "context" : "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 16,
      "context" : "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 15,
      "context" : "to depth two networks with very few hidden units, it is hard to predict even marginally better than random [29, 30, 17, 18, 16].",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 46,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 0,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 11,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 38,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 34,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 18,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 51,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 13,
      "context" : "Finally, we note that the recent empirical successes of NNs have prompted a surge of theoretical work around NN learning [47, 1, 4, 12, 39, 35, 19, 52, 14].",
      "startOffset" : 121,
      "endOffset" : 155
    }, {
      "referenceID" : 50,
      "context" : "[51], Grauman and Darrell [21].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[51], Grauman and Darrell [21].",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 35,
      "context" : "Inspired by deep networks’ success, researchers considered deep composition of kernels [36, 13, 11].",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 12,
      "context" : "Inspired by deep networks’ success, researchers considered deep composition of kernels [36, 13, 11].",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "Inspired by deep networks’ success, researchers considered deep composition of kernels [36, 13, 11].",
      "startOffset" : 87,
      "endOffset" : 99
    }, {
      "referenceID" : 45,
      "context" : "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].",
      "startOffset" : 136,
      "endOffset" : 152
    }, {
      "referenceID" : 44,
      "context" : "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].",
      "startOffset" : 136,
      "endOffset" : 152
    }, {
      "referenceID" : 37,
      "context" : "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].",
      "startOffset" : 136,
      "endOffset" : 152
    }, {
      "referenceID" : 55,
      "context" : "For fully connected two-layer networks, the correspondence between kernels and neural networks with random weights has been examined in [46, 45, 38, 56].",
      "startOffset" : 136,
      "endOffset" : 152
    }, {
      "referenceID" : 45,
      "context" : "Notably, Rahimi and Recht [46] proved a formal connection (similar to ours) for the RBF kernel.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 26,
      "context" : "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 41,
      "context" : "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].",
      "startOffset" : 88,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "Their work was extended to include polynomial kernels [27, 42] as well as other kernels [6, 5].",
      "startOffset" : 88,
      "endOffset" : 94
    }, {
      "referenceID" : 12,
      "context" : "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 23,
      "context" : "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].",
      "startOffset" : 149,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].",
      "startOffset" : 149,
      "endOffset" : 160
    }, {
      "referenceID" : 35,
      "context" : "Several authors have further explored ways to extend this line of research to deeper, either fully-connected networks [13] or convolutional networks [24, 2, 36].",
      "startOffset" : 149,
      "endOffset" : 160
    }, {
      "referenceID" : 36,
      "context" : "[37], Levy and Goldberg [34]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[37], Levy and Goldberg [34]).",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : "Such topology can be useful in the absence of any prior knowledge of how the output label may be computed from an input example, and it is commonly used in natural language processing where the input is represented as a bag-of-words [23].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 54,
      "context" : "[55]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "= √ 1−ρ2+(π−cos−1(ρ))ρ π arccos1 [13] Step √ 2 · 1[x ≥ 0] 1 2 + ρ π + ρ 3 6π + 3ρ 5 40π + .",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "= π−cos −1(ρ) π arccos0 [13] Exponential ex−2 1 e + ρ e + ρ 2 2e + ρ 3 6e + .",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 35,
      "context" : "= eρ−1 RBF [36]",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 47,
      "context" : "The proofs of all the theorems we quote here are well-known and can be found in Chapter 2 of [48] and similar textbooks.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 49,
      "context" : "Theorem 8 (Schoenberg, [50]).",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 40,
      "context" : "Finally, we use the following facts (see Chapter 11 in [41] and the relevant entry in Wikipedia):",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "(h) For every σ, σ̂ is non-decreasing and convex in [0, 1].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 35,
      "context" : "Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "Note that the dual of the exponential activation was calculated in [36] and the duals of the step and the ReLU activations were calculated in [13].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 0,
      "context" : "We first claim that there is a unique ασ ∈ [0, 1] such that ∀x ∈ (−1, ασ) , σ̂(ρ) > ρ and ∀x ∈ (ασ, 1) , ασ < σ̂(ρ) < ρ (9)",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "(b) σ̂ is non-decreasing and convex in [0, 1]",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Hence, σ̂(ρ)− ρ is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1].",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Hence, σ̂(ρ)− ρ is convex in [0, 1] and in particular intersects with the x-axis at either 0, 1, 2 or infinitely many times in [0, 1].",
      "startOffset" : 127,
      "endOffset" : 133
    }, {
      "referenceID" : 0,
      "context" : "Also, since σ̂(1) = 1, we know that there is at least one intersection in [0, 1].",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Hence, there are 1 or 2 intersections in [0, 1] and because one of them is in ρ = 1, we conclude that there is at most one intersection in [0, 1).",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 52,
      "context" : "[53]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "The expression was already calculated in [13], yet we give here a derivation for completeness.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 52,
      "context" : "wt = { w̃t ‖w̃t‖ ≤M Mw̃t ‖w̃t‖ ‖w̃t‖ > M After T = M 2C2 2 iterations the loss in expectation would be at most (see for instance Chapter 14 in [53]).",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 7,
      "context" : "Theorem 19 (Bartlett and Mendelson [8]).",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "First, numerous researchers and practitioners demonstrated that random initialization, similar to the scheme we analyze, is crucial to the success of neural network learning (see for instance [20]).",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 48,
      "context" : "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 25,
      "context" : "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 43,
      "context" : "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 14,
      "context" : "Second, several studies show that the contribution of optimizing the representation layers is relatively small [49, 26, 44, 43, 15].",
      "startOffset" : 111,
      "endOffset" : 131
    }, {
      "referenceID" : 35,
      "context" : "For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 48,
      "context" : "For example, competitive accuracy on CIFAR-10, STL-10, MNIST and MONO datasets can be achieved by optimizing merely the last layer [36, 49].",
      "startOffset" : 131,
      "endOffset" : 139
    }, {
      "referenceID" : 48,
      "context" : "[49] show that the performance of training the last layer is quite correlated with training the entire network.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 45,
      "context" : "The effectiveness of optimizing solely the last layer is also manifested by the popularity of the random features paradigm [46].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 18,
      "context" : "[19] demonstrated that for the MNIST and CIFAR-10 datasets the distances’ histogram of different examples barely changes when moving from the initial to the trained representation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "The rationale we described above was pioneered by LeCun and colleagues [32].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "Using for example Barron’s theorem [7], one may claim that visionrelated functions are expressed by fully connected two layer networks, but such networks are inferior to convolutional networks in machine vision applications.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 53,
      "context" : "On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "On the algorithmic side, it is yet to be seen whether our framework can help in understanding procedures such as dropout [54] and batch-normalization [25].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 44,
      "context" : "Randomly generating such embeddings can be also considered on its own, and we are currently working on design and analysis of random features a la Rahimi and Recht [45].",
      "startOffset" : 164,
      "endOffset" : 168
    } ],
    "year" : 2016,
    "abstractText" : "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power. ∗Email: amitdaniely@google.com †Email: rf@cs.stanford.edu. Work performed at Google. ‡Email: singer@google.com ar X iv :1 60 2. 05 89 7v 1 [ cs .L G ] 1 8 Fe b 20 16",
    "creator" : "LaTeX with hyperref package"
  }
}