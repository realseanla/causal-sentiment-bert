{
  "name" : "1612.00094.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimizing Quantiles in Preference-based Markov Decision Processes",
    "authors" : [ "Hugo Gilbert", "Paul Weng", "Yan Xu" ],
    "emails" : [ "hugo.gilbert@lip6.fr", "paweng@cmu.edu", "xuyan@cmu.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Sequential decision-making in uncertain environments is an important task in artificial intelligence. Such problems can be modeled as Markov Decision Processes (MDPs). In an MDP, an agent chooses at every time step actions to perform according to the current state of the world in order to optimize a criterion in the long run. In standard MDPs, uncertainty is described by probabilities over the possible action outcomes, preferences are represented by numeric rewards and the expectation of future cumulated rewards is used as the decision criterion. And yet, for numerous applications, the expectation of cumulated rewards may not be the most appropriate criterion. For instance, in one-shot decision-making problems an alternative and well motivated objective for the agent is to insure a certain level of satisfaction with high probability.\nIn this paper we focus on the decision criterion that consists in maximizing a quantile. Intuitively, the τ th quantile of a population is the value x such that 100 · τ percent of the population is equal or lower than x and 100 · (1 − τ) percent of the population is equal or greater than x. Optimizing a quantile criterion offers nice properties: i) no assumption is made about the commensurability between preferences and uncertainty, ii) preferences over actions or trajectories can be expressed on a purely ordinal scale, iii) preferences induced over policies are more robust than with the standard criterion of maximizing the expectation of cumulated rewards. ∗hugo.gilbert@lip6.fr †paweng@cmu.edu ‡xuyan@cmu.edu\nar X\niv :1\n61 2.\n00 09\n4v 1\n[ cs\n.A I]\n1 D\nAs a result, maximizing a quantile is used in many applications. For instance, the Value-at-Risk criterion [Jorion, 2006] widely used in finance is in fact a quantile. Moreover, in the Web industry [Wolski and Brevik, 2014; DeCandia et al., 2007], decisions about performance or Quality-Of-Service are often made based on quantiles. For instance, Amazon reports [DeCandia et al., 2007] that they optimize the 99.9% quantile for their cloud services. More generally, in the service industry, because of skewed distributions [Benoit and Van den Poel, 2009], one generally does not want that customers are satisfied on average, but rather that most customers (e.g., 99% of them) to be as satisfied as possible.\nOur contribution: We show that optimizing the quantile criterion amounts to solving a sequence of MDP problems using an Expected Utility criterion with a target utility function. We provide a binary search algorithm using functional backward induction [Liu and Koenig, 2006] as a subroutine for computing an optimal policy. Moreover, we investigate some properties of the optimal policies in the finite and infinite cases. Finally, we provide the results of experiments testing our algorithm in a variety of settings.\nThe paper is organized as follows. Section 2 introduces the necessary background to present our approach and state formally our problem. Section 3 presents the details of our solving algorithm for the finite horizon case. Section 4 provides some theoretical results in the infinite horizon case. In Section 5, we experimentally evaluate our proposition. Section 6 discusses the related work and Section 7 concludes."
    }, {
      "heading" : "2 Background",
      "text" : "In this section, we provide the background information necessary for the sequel."
    }, {
      "heading" : "2.1 Markov Decision Process",
      "text" : "Markov Decision Processes (MDPs) offer a general and powerful formalism to model and solve sequential decision-making problems [Puterman, 1994]. An MDP is formally defined as a tupleMT = (S,A,P, r, s0) where T is a time horizon, S is a finite set of states, A is a finite set of actions, P : S × A × S → R is a transition function with P(s, a, s′) being the probability of reaching state s′ when action a is performed in state s, r : S × A → R is a bounded reward function and s0 ∈ S is a particular state called initial state.\nIn a nutshell, at each time step t, the agent knows her current state st. According to this state, she decides to perform an action at. This action results in a new state st+1 ∈ S according to probability distributionP(st, at, .), and a reward signal r(st, at) which penalizes or reinforces the choice of this action. At time step t = 0, the agent is in the initial state s0. We will call t-history ht a succession of t state-action pairs starting from state s0 (e.g., ht = (s0, a0, s1, . . . , st−1, at−1, st)). We call episode a T -history and denote E the set of episodes.\nThe goal of the agent is to determine a policy, i.e., a procedure to select an action in a state, that is optimal for a given criterion. More formally, a policy π at an horizon T is a sequence of T decision rules (δ1, . . . , δT ). Decision rules are functions which\nprescribe the actions that the agent should perform. They are Markovian if they only depend on the current state. Moreover, a decision rule is either deterministic if it always selects the same action in a given state or randomized if it can prescribe a probability distribution over possible actions. A policy can be Markovian, deterministic or randomized according to the type of its decision rules. Lastly, a policy is stationary if it uses the same decision rule at every time step, i.e., π = (δ, δ, . . .).\nDifferent criteria can be defined in order to compare policies. One standard criterion is expected cumulated reward, for which it is known that an optimal deterministic Markovian policy exists at any horizon T . This criterion is defined as follows. First, the value of a history ht = (s0, a0, s1, . . . , st−1, at−1, st) is described as the sum of rewards obtained along it, i.e., r(ht) = ∑t−1 i=0 r(si, ai). Then, the value of a policy π = (δ1, . . . , δT ) in a state s is set to be the expected value of the histories that can be generated by π from s. This value, given by the value function vπ1 : S → R can be computed iteratively as follows:\nvπT+1(s) = 0 vπt (s) = r(s, δt(s)) + ∑ s′∈S P(s, δt(s), s′)vπt+1(s′) (1)\nThe value vπt (s) is the expectation of cumulated rewards obtained by the agent if she performs action δt(s) in state s at time step t and continues to follow policy π thereafter. The higher the values of vπt (s) are, the better. Therefore, value functions induce a preference relation %π over policies in the following way:\nπ %π π ′ ⇔ ∀s ∈ S,∀t = 1, . . . , T, vπt (s) ≥ vπ\n′\nt (s)\nA solution to an MDP is a policy, called optimal policy, that ranks the highest with respect to %π . Such a policy can be found by solving the Bellman equations.\nv∗T+1(s) = 0\nv∗t (s) = max a∈A r(s, a) + ∑ s′∈S P(s, a, s′)v∗t+1(s′)\nAs can be seen, the preference relation %π over policies is directly induced by the reward function r.\nThe decision criterion, based on the expectation of cumulated rewards, may not always be suitable. Firstly, unfortunately, in many cases, the reward function r is not known. One can therefore try to uncover the reward function by interacting with an expert of the domain considered [Regan and Boutilier, 2009; Weng and Zanuttini, 2013]. However, even for an expert user, the elicitation of the reward function can be burdensome. Indeed, this process can be cognitively very complex as it requires to balance several criteria in a complex manner and as it can imply a large number of parameters. In this paper, we address this problem by only assuming that we have a strict weak ordering on episodes.\nSecondly, for numerous applications, the expectation of cumulated reward, as used in Equation 1, may not be the most appropriate criterion (even when a numeric reward\nfunction is defined). For instance, in the Web industry, most decisions about performance are based on the minimal quality of 99% of the possible outcomes. Therefore, in this article we aim at using a quantile (defined in Section 2.3) as a decision criterion to solve an MDP."
    }, {
      "heading" : "2.2 Preferences over Histories",
      "text" : "For generality’s sake, contrary to standard MDPs, we define in this work the reward function to take values in a set R. Moreover, we assume that the values of histories take values in a set W , called the wealth level space, and that the value of a history ht = (s0, a0, s1, . . . , st) is defined by:\nw(h0) = w0 w(ht) = w(ht−1) ◦ r(st−1, at−1)\nwhere ht−1 = (s0, a0, s1, . . . , st−1), ◦ is a binary operation from W × R to W and w0 ∈ W is the left identity element of ◦. LetWT ⊂ W be the set of wealth levels of T -histories. We make three assumptions aboutWT :\n• It is ordered by a total order W , which defines how T -histories are compared,\n• It admits a lowest element, denoted wmin and a greatest element, denoted wmax for order W .\n• A distance consistent with W is defined over WT . It is denoted d(w,w′) for any pair (w,w′) ∈ WT ×WT .\nNote that when a distance is defined, for any pair (w,w′), its set of mid-elements is also defined mid(w,w′) = arg inf{max(d(w,w′′), d(w′, w′′)) |w′′ ∈ WT }.\nIn a numerical context, the possible wealth levels of a state are the possible sums (resp. γ-discounted sums) of rewards that can be obtained during an episode. We have wmax = RmaxT (resp. wmax = Rmax (1−γ)T 1−γ ) withRmax being the highest reward and mid(w,w′) = {(w + w′)/2}. In the most general case, the possible wealth levels of a state are the possible histories (or more precisely their equivalent classes) that can be obtained during an episode. Here, if the equivalence classes are known and denoted by w1 ≺W w2 ≺W . . . ≺W wm and if d(wi, wj) = |j−i|, then wmin = w1, wmax = wm and mid(wi, wj) = {wb(i+j)/2c, wd(i+j)/2e} (where bxcis the greatest integer smaller than x and dxe is the smallest integer greater than x).\nThe goal of the agent is then to make sure that most of the time, it will generate episodes that have the highest possible wealth levels. This can be implemented by optimizing a quantile criterion as explained in the next subsection."
    }, {
      "heading" : "2.3 Quantile Criterion",
      "text" : "Intuitively, the τ -quantile of a population of ordered elements, for τ ∈ [0, 1], is the value q such that 100 · τ% of the population is equal or lower than q and 100 · (1− τ)% of the population is equal or greater than q. The 0.5-quantile, also known as the median, can be seen as the ordinal counterpart of the mean. More generally, quantiles define\ndecision criteria that have the nice property of not requiring numeric valuations, but only an order. They have been axiomatically studied as decision criteria by Rostek [2010].\nWe now give a formal definition of quantiles. For this purpose we define the probability distribution pπ over wealth levels induced by a policy π, i.e., pπ(w) is the probability of getting a wealth level w ∈ WT when applying policy π from the initial state. The cumulative distribution induced by pπ is then defined as Fπ where Fπ(w) = ∑ w′ Ww p\nπ(w′) is the probability of getting a wealth level not preferred to w when applying policy π. Similarly, the decumulative distribution induced by pπ is defined as Gπ(w) = ∑ w Ww′ p\nπ(w′) is the probability of getting a wealth level “not lower” than w.\nThese two notions of cumulative and decumulative enable us to define two kinds of criteria. First, given a policy π, we define the lower τ -quantile for τ ∈ (0, 1] as:\nqπ τ = min{w ∈ WT |Fπ(w) ≥ τ} (2)\nwhere the min operator is with respect to ≺W . Then, given a policy π, we define the upper τ -quantile for τ ∈ [0, 1) as:\nqπτ = max{w ∈ WT |Gπ(w) ≥ 1− τ} (3)\nwhere the max operator is with respect to ≺W . If τ = 0 or τ = 1 only one of qπ\nτ or qπτ is defined and we define the τ -quantile q π τ\nas that value. When both are defined, by construction, we have qπ τ W qπτ . If those two values are equal, qπτ is defined as equal to them. For instance, this is always the case in continuous settings for continuous distributions. However, in our discrete setting, it could happen that those values differ, as shown by Example 1.\nExample 1. Consider an MDP whereWT = {w1 ≺W w2 ≺W w3}. Now assume a policy π attains each wealth level with probabilities 0.5, 0.2 and 0.3 respectively. Then it is easy to see that qπ\n0.5 = w1 whereas qπ0.5 = w2.\nWhen the lower and upper quantiles differ, one may define the quantile as a function of the lower and upper quantiles [Weng, 2012]. For simplicity, we show in this paper how to optimize (approximately) the lower and the upper quantiles.\nDefinition 1. A policy π∗ is optimal for the lower (resp. upper) τ -quantile criterion if:\nqπ ∗\nτ = max π qπ τ\n(resp. qπ ∗\nτ = max π\nqπτ ) (4)\nwhere the max operator is with respect to ≺W and taken over all policies π at horizon T .\nEven in a numerical context where a numerical reward function is given and the quality of an episode is defined as the cumulative of rewards received along the episode, this criterion is difficult to optimize, notably due to the two following related points:\n• It is non-linear meaning for instance that the τ -quantile qπ̃τ of the mixed policy π̃ that generates an episode using policy π with probability p and π′ with probability 1− p is not given by pqπτ + (1− p)qπ ′ τ .\n• It is non-dynamically consistent meaning that at time step t, an optimal policy computed in s0 with horizon T might not prescribe in state st to follow a policy optimal in st for horizon T − t.\nThree solutions are then possible [McClennen, 1990]: 1) adopting a consequentialist approach, i.e., at each time step t we follow an optimal policy for the problem with horizon T − t and initial state st even if the resulting policy is not optimal at horizon T ; 2) adopting a resolute choice approach, i.e., at time step t = 0 we apply an optimal policy for the problem with horizon T and initial state s0 and do not deviate from it; 3) adopting a sophisticated resolute choice approach [Jaffray, 1998; Fargier et al., 2011], i.e., we apply a policy π (chosen at the beginning) that trades off between how much π is optimal for all horizons T, T − 1, . . . , 1.\nWith non-dynamically consistent preferences, it is debatable to adopt a consequentialist approach, as the sequence of decisions may lead to dominated results. In this paper, we adopt a resolute choice point of view. We leave the third approach for future work.\nAs optimizing exactly a (lower or upper) quantile is hard, we aim at finding an approximate solution. Let q∗\nτ and q∗τ be equal to the optimal lower and upper quantile\nrespectively.\nDefinition 2. Let ε > 0. A policy π∗ε is said to be ε-optimal for the lower (resp. upper) τ -quantile criterion if d(qπ\n∗ ε τ , q∗ τ ) ≤ ε (resp. d(qπ ∗ ε τ , q ∗ τ ) ≤ ε)."
    }, {
      "heading" : "3 Solving Algorithm",
      "text" : "In this section, we present a technique for computing an ε-optimal policy for the quantile criterion. Our approach amounts to solving a sequence of MDPs optimizing EU with target utility functions (see Section 3.2)."
    }, {
      "heading" : "3.1 Binary Search",
      "text" : "In order to justify our algorithm, we introduce two lemmas that characterize the optimal lower and upper quantiles1:\nLemma 1. The optimal lower τ -quantile q∗ satisfies:\nq∗ = min{w : F ∗(w) ≥ τ} (5) F ∗(w) = min\nπ Fπ(w) ∀w ∈ W (6)\nNote the last two equations can be equivalently rewritten:\nq∗ = min{w : G∗≺(w) ≤ 1− τ} (7) G∗≺(w) = max\nπ Gπ≺(w) ∀w ∈ W (8)\nwhere Gπ≺(w) = 1− Fπ(w) = ∑ w≺Ww′ p π(w′).\n1For lack of space, all proofs are in the supplementary material.\nAlgorithm 1: Binary Search for the Lower Quantile (resp. Upper Quantile) Data: MDPM, τ , ε Result: an ε-optimal policy π 1 w ← wmax; w ← wmin; w ← mid(w,w) 2 while d(w,w) > ε do 3 (π, p) = solve(M, w); 4 if p > 1− τ (resp. p ≥ 1− τ ) then 5 w ← w; w ← max(mid(w,w)); π∗ ← π; 6 else 7 w ← w; w ← min(mid(w,w));\n8 return π∗\nLemma 2. The optimal upper τ -quantile q∗ satisfies:\nq∗ = max{w : G∗(w) ≥ 1− τ} (9) G∗(w) = max\nπ Gπ(w) ∀w ∈ W (10)\nGiven Lemmas 1 and 2 the problem now reduces to finding the right value of w ∈ W that solves the problems defined by Equation 7 or 9. Our solving method is based on binary search (see Algorithm 1) and on the function solve(M, w) that returns a pair (π, p), the solution of the problems defined by Equation 8 or 10 for a fixedw, i.e., the max is equal to p and attained at π. Note that while for the upper quantile criterion, solve(M, qπ ∗ τ ) returns an optimal policy, for the lower quantile, solve(M, qπ ∗ τ ) may not if qπ ∗\nτ W min(WT ). However, solve(M, prec(qπ\n∗\nτ )) returns an optimal policy\nwhere prec(w) is the most preferred element such that prec(w) ≺W w (see supplementary material).\nIn the next subsection, we show how function solve can be computed for the lower and upper quantile.\nNote that whenWT is defined on the real line, Algorithm 1 needs only\ndlog2 d(wmax, wmin)/εe\niterations to terminate by using [wmin, wmax] as WT . In the case where WT is finite, binary search can of course determine the optimal policy with ε = 1 and needs dlog2(|WT |)e iterations.\nThe next proposition asserts that Algorithm 1 is correct:\nProposition 1. Algorithm 1 returns an ε-optimal policy for the lower (or upper) quantile criterion."
    }, {
      "heading" : "3.2 Dynamic Programming",
      "text" : "For / ∈ {≺W , W}, we denote by U/w : W → R the function, called target utility function, defined as follows:\nU/w(x) = 1 if w / x and 0 else. (11)\nAlgorithm 2: FunctionalBackwardInduction Data: MDPM, wealth w Result: an optimal policy π 1 for all s ∈ S do 2 VT+1(s, .)← U/w(.) 3 for t = T to 1 do 4 for all s ∈ S do 5 Vt(s, ·)← max\na ∑ s′∈S P(s, a, s′)Vt+1(s′, · ◦ r(s, a))\n6 return (πV1 , V1(s0, w0)) \\\\ πV1= policy corresponding to V1\nWhen optimizing the lower (resp. upper) quantile, function solve(M, w) can be computed by solving MDPM using EU as a decision criterion with U≺Ww (resp. U Ww ) as a utility function. Indeed, we have:\nEπ[U/w ( w(HT ) ) ] = P[w / w(HT ) |π]\nwhere HT is a random variable representing a T -history and P[w /w(HT ) |π] denotes the probability that π generates a history whose wealth is strictly better (resp. at least better) than w when / =≺W (resp. / = W ).\nFollowing [Liu and Koenig, 2006], this problem can be solved with a functional backward induction (Algorithm 2). For each state s, it maintains a function Vt(s, .) which associates to each possible wealth level w the expected utility obtained by applying an optimal policy in state s for the remaining T − t time steps with w as initial wealth level. At each time step (t = T, . . . , 1) this function is updated similarly as in backward induction except that operations are not applied to scalars but to functions. The max and × operations are extended over functions as pointwise operations. As utility functions defined by Equation 11 are piecewise-linear, Vt(s, .) is also piecewiselinear because all the operations in Line 5 of Algorithm 2 preserve this property.\nPolicies returned by Algorithm 2 have a special structure. They are deterministic and wealth-Markovian:\nDefinition 3. A policy is said to be wealth-Markovian if its decision rules are functions of both the current state and the current wealth level.\nBesides, this is also the case for policies optimal with respect to the quantile criterion.\nProposition 2. Optimal policies for the lower or upper quantile at horizon T can be found as deterministic wealth-Markovian policies."
    }, {
      "heading" : "4 Infinite Horizon",
      "text" : "We present in this section some results regarding the infinite horizon case. Similarly to the finite horizon setting, the situation for the quantile criterion is not as simple as for\nthe standard case. Indeed, in the infinite horizon case, it may happen that there is no stationary deterministic Markovian policy that is optimal (w.r.t. the quantile criterion) among all policies, contrary to standard MDPs.\nExample 2. Consider an MDP with two states s1 and s2 and two actions a1 and a2. In s1, the transition probabilities are P(s1, a1, s1) = 0.1, P(s1, a1, s2) = 0.9 and P(s1, a2, s2) = 1. To make this example shorter, we assume that rewards depend on next states. The rewards are r(s1, a1, s1) = 1, r(s1, a1, s2) = −1 and r(s1, a2, s2) = 1. In s2, the transition probabilities are P(s2, a1, s2) = P(s2, a2, s2) = 1. Rewards are null for both actions in s2. Among all decision rules, there are only two distinct rules: δ1(s1) = a1 and δ2(s1) = a2. To ensure that the values of histories are welldefined, we assume that they are defined as discounted sum of rewards with a discount factor γ = 0.9. One can then check that the 0.95-quantile of the stationary policy using δ1 is 0.1, that of the stationary policy using δ2 is 1. Finally, the 0.95-quantile of the policy applying first δ1 and then δ2 is 1.9. Therefore, no stationary deterministic Markovian policy is optimal for the quantile criterion.\nHowever, considering wealth-Markovian policies, some results can be given when rewards are numeric and wealth levels are undiscounted:\nProposition 3. Optimal policies for the lower or upper quantile can be found as stationary deterministic wealth-Markovian policies in the two following cases:\n(i) ∀(s, a) ∈ S ×A, r(s, a) ≤ 0.\n(ii) ∀(s, a) ∈ S × A, r(s, a) ≥ 0. Furthermore, we require the existence of a finite upper bound on the optimal lower and upper quantiles.\nThen, a solving algorithm can be obtained from Algorithm 1 by replacing functional backward induction (Alg. 2) by functional value iteration [Liu and Koenig, 2006] in the binary search. This amounts to do the for loop over t (line 4) until convergence\nof Vt, i.e., ‖Vt − Vt−1‖∞ ≤ ′. Binary search will then return an ( + ′)-optimal for the τ -quantile. However, note that in the first (resp. second) case, a lower (resp. upper) bound on the optimal lower or upper quantile is required to do the binary search."
    }, {
      "heading" : "5 Experimental Results",
      "text" : "We experimentally evaluated our approach on a server equipped with four Intel(R) Xeon(R) CPU E5-2640 v3 @ 2.60GHz and 64Gb of RAM. The algorithms were implemented in Matlab and ran only on one core. We expect the running times to be improved with a more efficient programming language and by exploiting a multicore architecture.\nWe designed three sets of experiments. Although our approach could be used in a preference-based setting, we performed the experiments with numerical rewards for simplicity. The first shows the running time of functional backward induction for different varying state sizes on random MDPs. The second set of experiments shows the running time of functional backward induction for different horizons on a data center control problem with various number of servers. Finally, the third compares the cumulative distributions of a policy optimal for the quantile criterion and a policy optimal for the standard criterion on a fixed MDP.\nThe first set of experiments was conducted on Garnets [McKinnon and Thomas, 1995], which designate random MDPs with a constrained branching factor. A Garnet G(nS , nA, b) is characterized by nS a number of states, nA a number of actions and b the number of successor states for every state and action. For our experiments, nS ∈ {250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250} and we set nA = 5 and b = dlog2 nSe. Rewards are randomly chosen in [0, 1] and the values of histories are simply cumulated rewards. The horizon of the problem was set to 5. The results are presented in Figure 1 where the x-axis represents the state size and the y-axis the com-\nputation time. Each point is the average over 10 runs. Naturally, computation times increases with state sizes. In this setting, binary search would call functional backward induction dlog2(1/ε)e = 10 times if ε = 10−3.\nThe second set of experiments was performed on a more realistic domain, which is a data center control problem inspired by the model proposed by Yin and Sinopoli [2014]. In this problem, one needs to decide every time step how many servers to switch on or off, while maximizing Quality-of-Service and minimizing power consumption. In the model proposed by Yin and Sinopoli, the two objectives are simply combined into one cost, which defines our reward signal. The state is defined as the number of servers that are currently on and the number of jobs that needs to be processed during a time step. The action represents the number of servers that will be on at the next time step. We assume for simplicity that the maximum number of jobs that can arrive at one timestep is three times the total number of servers. For instance, in a problem with n = 30 servers, the total number of states is 30×3×30 = 2700. Besides, the distribution of the next number of jobs is modeled as a Poisson distribution whose parameter can be dn/2e, d3n/2e or d5n/2e (to model different regimes) depending on the current number of jobs. Figure 2 shows the computation times of functional backward induction for n ∈ {20, 30, 40} and different horizons. We can see that for more structured problems, the computation time is much more reasonable than on random MDPs.\nIn the last set of experiments, to give an intuition of the kind of policy obtained when optimizing a quantile, we compare the cumulative distribution of a policy optimal for the quantile criterion and that of a policy optimal for the standard criterion. This experiment is performed on an instance of GarnetG(100, 5, dlog2 100e) whose rewards are slightly modified to make the distribution of the optimal policy skewed, as it is often the case in some real applications [Benoit and Van den Poel, 2009]. The horizon is set to 5 and we optimize the 0.1-quantile with ε = 0.001 in binary search. The two cumulative distributions are plotted in Figure 3. We can observe that although the optimal policy for the standard criterion maximizes the expectation, it may be a risky policy to apply as the probability of obtaining a high reward is low. On the contrary, the optimal policy for the τ -quantile criterion will guarantee a reward as high as possible with probability at least 1− τ ."
    }, {
      "heading" : "6 Related Work",
      "text" : "Much work in the MDP literature [Boussard et al., 2010] considered decision criteria different to the standard ones (i.e., expected discounted sum of rewards, expected total rewards or expected average rewards). For instance, in the operations research community, White [1987] considered different cases where preferences over policies only depend on sums of rewards: Expected Utility (EU), probabilistic constraints and mean-variance formulations. In this context, he showed the sufficiency of working in a state space augmented with the sum of rewards obtained so far. Recently, [Prashanth and Ghavamzadeh, 2013] and [Mannor and Tsitsiklis, 2011] provided algorithms for this mean-variance formulation. Filar et al. [1989] investigated decision criteria that are variance-penalized versions of the standard ones. They formulated the obtained optimization problem as a non-linear program. Several researchers [White, 1993; Bouakiz\nand Kebir, 1995; Yu et al., 1998; Wu and Lin, 1999; Ohtsubo and Toyonaga, 2002; Hou et al., 2014; Fan et al., 2005] worked on the problem of optimizing the probability that the total (discounted) reward exceeds a given threshold.\nAdditionally, in the artificial intelligence community, [Liu and Koenig, 2005; Liu and Koenig, 2006; Ermon et al., 2012] also investigated the use of EU as a decision criterion in MDPs. In the continuation of this work, Gilbert et al. [2015] investigated the use of Skew-Symmetric Bilinear (SSB) utility [Fishburn, 1981] functions — a generalization of EU with stronger descriptive abilities — as decision criteria in finite-horizon MDPs. Interestingly, SSB also encompasses probabilistic dominance, a decision criterion that can be employed in preference-based sequential decision-making [BusaFekete et al., 2014].\nRecent work in MDP and reinforcement learning considered conditional Valueat-risk (CVaR), a criterion related to quantile, as a risk measure. Bäuerle and Ott [2011] proved the existence of deterministic wealth-Markovian policies optimal with respect to CVaR. Chow and Ghavamzadeh [2014] proposed gradient-based algorithms for CVaR optimization. In contrast, Borkar and Jain [2014] used CVaR in inequality constraints instead of as objective function.\nCloser to our work, several quantile-based decision models have been investigated in different contexts. In uncertain MDPs where the parameters of the transition and reward functions are imprecisely known, Delage and Mannor [2007] presented and investigated a quantile-like criterion to capture the trade-off between optimistic and pessimistic viewpoints on an uncertain MDP. The quantile criterion they use is different to ours as it takes into account the uncertainty present in the parameters of the MDP. Filar et al. [1995] proposed an algorithm for optimizing the quantile criterion when histories are valued by average rewards. In that setting, they showed that an optimal stationary deterministic Markovian policy exists. In MDPs with ordinal rewards [Weng, 2011; Weng, 2012; Filar, 1983], quantile-based decision models were proposed to compute\npolicies that maximize a quantile using linear programming. While quantiles in those works are defined on distributions over ordinal rewards, we defined them as distributions over histories.\nMore recently, in the machine learning community, quantile-based criteria have been proposed in the multi-armed bandit (MAB) setting, a special case of reinforcement learning. Yu and Nikolova [2013] proposed an algorithm in the pure exploration setting for different risk measures, including Value-at-Risk. Carpentier and Valko [2014] studied the problem of identifying arms with extreme payoffs, a particular case of quantiles. Finally, Szörenyi et al. [2015] investigated MAB problems where a quantile is optimized instead of the mean."
    }, {
      "heading" : "7 Conclusion",
      "text" : "In this paper we have developed a framework to solve sequential decision problems in a very general setting according to a quantile criterion. Modeling those problems as MDPs we developed an offline algorithm in order to compute an -optimal policy and investigated the properties of the optimal policies in the finite and infinite horizon cases. Lastly, we provided experimental results, testing those two algorithms in a variety of settings.\nAs future work, we plan to investigate how this work can be extended to the case of reinforcement learning, a framework more involved than the one of MDPs where the dynamics of the problems are unknown and must be learned."
    }, {
      "heading" : "8 Supplementary material of “Optimizing Quantiles in Preference-based Markov Decision Processes”",
      "text" : "We provide in this section the proofs of our lemmas and propositions.\nLemma 1. The optimal lower τ -quantile q∗ satisfies:\nq∗ = min{w : F ∗(w) ≥ τ} F ∗(w) = min\nπ Fπ(w) ∀w ∈ W\nProof. We recall that for any policy π, Fπ is nondecreasing and that consequently F ∗ is also nondecreasing. Let w1 = maxπminw{w ∈ WT |Fπ(w) ≥ τ} and let w2 = min{w : F ∗(w) ≥ τ}. By contradiction, assume w1 > w2. Then there exists π such that Fπ(w1) ≥ τ and Fπ(w) < τ , ∀w < w1. Thus Fπ(w2) < τ which contradicts the definition of w2. Now, assume w2 > w1. Then F ∗(w1) < τ . Thus, there exists π such that Fπ(w1) < τ and qπτ > w1 which contradicts the definition of w1.\nLemma 2. The optimal upper τ -quantile q∗ satisfies:\nq∗ = max{w : G∗(w) ≥ 1− τ} G∗(w) = max\nπ Gπ(w) ∀w ∈ W\nProof. We recall that for any policy π, Gπ is nonincreasing and that consequently G∗ is also nonincreasing. Let w1 = maxπmaxw{w ∈ WT |Gπ(w) ≥ 1 − τ} and let w2 = max{w : G∗(w) ≥ 1− τ}. By definition of w1, there exists a policy π such that Gπ(w1) ≥ 1− τ , thus G∗(w1) ≥ 1− τ and w2 ≥ w1. By definition of w2, there exists a policy π such that Gπ(w2) ≥ 1 − τ , thus maxw{w ∈ WT |Gπ(w) ≥ 1 − τ} ≥ w2 and w1 ≥ w2.\nThe following example shows that F ∗(q∗) (see Equation 6) may not be attained by an optimal policy (for the lower quantile):\nExample 3. Let F1 and F2 be two cumulatives defined over three elements w1 ≺W w2 ≺W w3 with the following probabilities: F1 = (0.5, 0.5, 1) and F2 = (0, 0.6, 1). The lower 0.5-quantile of F1 is w1 and that of F2 is w2. Therefore the optimal lower quantile is q∗ = w2. We have F ∗ = (0, 0.5, 1) and F ∗(q∗) = 0.5, which is attained by F1.\nThis implies that solve(M, q∗) may return a non-optimal policy when q∗ W min(WT ). For w ∈ WT , we define prec(w) as the most preferred element of WT such that prec(w) ≺ w. If there are no element w′ ∈ WT such that w′ ≺ w, prec(w) is defined as w. The optimal policy can be found using the following property:\nLemma 3. Any policy π∗ such that Fπ ∗ (prec(q∗)) = F ∗(prec(q∗)) is an optimal policy with regard to the lower quantile criterion.\nProof. Assume that q∗ W min(WT ). Otherwise the lemma is clearly true. Assume by contradiction that there is a non-optimal policy π such that Fπ(prec(q∗)) = F ∗(prec(q∗)). Let q be the lower τ - quantile of policy π, q∗ be the optimal lower quantile and π∗ be an optimal policy. By assumption, we have q W prec(q∗) ≺W q∗ and Fπ ∗ (prec(q∗)) ≥ Fπ(prec(q∗)). As a cumulative is non-decreasing, we have Fπ(prec(q∗)) ≥ Fπ(q) ≥ τ , which contradicts the fact that the lower quantile of π∗ is q∗.\nBefore proving that Algorithm 1 is correct, we introduce a lemma that gives sufficient conditions for a policy to be approximately optimal.\nLemma 4. Let π be a policy for which there exists w such that d(w, q∗ τ ) ≤ ε (resp. d(w, q∗τ ) ≤ ε) and:\nFπ(w) < τ (resp. Gπ(w) ≥ 1− τ).\nThen π is ε-optimal for the lower (resp. upper) τ -quantile criterion.\nProof. Indeed, for such a policy, as Fπ(w) is nondecreasing (resp. Gπ(w) is nonincreasing), we have that qπ\nτ ∈ [w, q∗ τ ] (resp. qπτ ∈ [w, q∗τ ]) and thus d(qπτ , q ∗ τ ) ≤ ε (resp.\nd(qπτ , q ∗ τ ) ≤ ε) .\nProposition 1. Algorithm 1 returns an ε-optimal policy for the lower (or upper) quantile criterion.\nProof. If WT ⊂ R we have seen that the algorithm terminates in ⌈ log2 d(wmax,0) ε ⌉ iterations. In the most general setting, the algorithm terminates, because in the worst case we will check all the m possible final wealth values. Let π be the policy returned by the algorithm. For the lower (resp. upper) quantile, when the algorithm terminates, d(q∗\nτ , w) (resp. d(q∗τ , w)) ≤ d(w,w) ≤ ε and Fπ ∗ (w) < τ (resp. Gπ(w) ≥ 1 − τ ).\nThus, we can apply Lemma 4 which concludes the proof.\nProposition 2. Optimal policies for the lower or upper quantile at horizon T can be found as deterministic wealth-Markovian policies.\nProof. We recall that for the lower (resp. upper) quantile criterion, procedure solve(M, w) returns the policy which minimizes Fπ(w) (resp. maximizes Gπ(w)). Thus, for any policy π, by definition of quantiles, solve(M, prec(qπ\nτ )) (resp. solve(M, qπτ )) returns\na deterministic wealth-Markovian policy, which is at least as good as π regarding the lower (resp. upper) quantile criterion. As the set of deterministic wealth-Markovian policies is finite in the finite horizon case, taking the one with highest lower (resp. upper) quantile concludes the proof.\nProposition 3. Optimal policies for the lower or upper quantile in the infinite horizon setting can be found as stationary deterministic wealth-Markovian policies in the two following cases:\n(i) ∀(s, a) ∈ S ×A, r(s, a) ≤ 0.\n(ii) ∀(s, a) ∈ S × A, r(s, a) ≥ 0. Furthermore, we require the existence of a finite upper bound on the optimal lower and upper quantiles.\nProof. We prove for the upper quantile and case (i), the other cases are similar. If all policies have −∞ as quantile, they are all optimal. Now, if one policy has a finite lower quantile q ∈ R−, the optimal quantile must be greater than or equal to q. From the original MDP, consider the state-augmented MDP whose state space is defined by S = {(s, w)|s, w ∈ S ×W}. In S, regroup all states having a wealth level strictly less than q in a single absorbing state. Indeed, as the optimal upper quantile is greater than q and r(s, a) ≤ 0, ∀s, a, the choices of the policies in those states are irrelevant to find an optimal policy w.r.t the upper quantile criterion. Note that the resulting augmented state space S<q is finite. In this MDP, we use reward functions parametrized by a value x ∈ W defined as follows :\nrx((s, w), a) = { −1 if w ≥ x and w + r(s, a) < x 0 else.\nA policy solving this MDP w.r.t the expectation of total reward criterion maximizes the probability of getting an episode with a wealth level greater than or equal to x. According to Puterman (1994, Theorem 7.1.9), such a policy can be found as a stationary deterministic Markovian policy in the augmented MDP. Stated differently, there exists a stationary deterministic wealth-Markovian optimal policy in the original MDP. Then, for any policy π, the stationary deterministic wealth-Markovian policy which is optimal when using reward function rqπτ (and expectation of total reward) is at least as good as π regarding the upper quantile criterion. By partitioning those policies by regrouping the ones that agree on S<q we reduce the set of stationary deterministic wealth-Markovian policies to a finite set. By taking the “best one” in this set, we obtain a stationary deterministic wealth-Markovian optimal policy."
    } ],
    "references" : [ {
      "title" : "Mathematical Methods of Operations Research",
      "author" : [ "Nicole Bäuerle", "Jonathan Ott. Markov decision processes with average value-at-risk criteria" ],
      "venue" : "74(3):361–379,",
      "citeRegEx" : "Bäuerle and Ott. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Benefits of quantile regression for the analysis of customer lifetime value in a contractual setting: An application in financial services",
      "author" : [ "D.F. Benoit", "D. Van den Poel" ],
      "venue" : "Expert Systems with Applications, 36:10475–10484,",
      "citeRegEx" : "Benoit and Van den Poel. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "on Automatic Control",
      "author" : [ "V. Borkar", "Rahul Jain. Risk-constrained Markov decision processes. IEEE Trans" ],
      "venue" : "59(9):2574–2579,",
      "citeRegEx" : "Borkar and Jain. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Target-level criterion in Markov decision processes",
      "author" : [ "M. Bouakiz", "Y. Kebir" ],
      "venue" : "Journal of Optimization Theory and Applications, 86(1):1–15",
      "citeRegEx" : "Bouakiz and Kebir. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "chapter Non-Standard Criteria",
      "author" : [ "Matthieu Boussard", "Maroua Bouzid", "Abdel-Illah Mouaddib", "Régis Sabbadin", "Paul Weng. Markov Decision Processes in Artificial Intelligence" ],
      "venue" : "pages 319–359. Wiley,",
      "citeRegEx" : "Boussard et al.. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Preference-based Reinforcement Learning: Evolutionary Direct Policy Search using a Preference-based Racing Algorithm",
      "author" : [ "Róbert Busa-Fekete", "Balázs Szörenyi", "Paul Weng", "Weiwei Cheng", "Eyke Hüllermeier" ],
      "venue" : "Machine Learning, 97(3):327–351,",
      "citeRegEx" : "Busa.Fekete et al.. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Extreme bandits",
      "author" : [ "Alexandra Carpentier", "Michal Valko" ],
      "venue" : "NIPS,",
      "citeRegEx" : "Carpentier and Valko. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Algorithms for CVaR optimization in MDPs",
      "author" : [ "Yinlam Chow", "Mohammad Ghavamzadeh" ],
      "venue" : "NIPS,",
      "citeRegEx" : "Chow and Ghavamzadeh. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Dynamo: Amazon’s highly available key-value store",
      "author" : [ "G. DeCandia", "D. Hastorun", "M. Jampani", "G. Kakulapati", "A. Lakshman", "A. Pilchin", "S. Sivasubramanian", "P. Vosshall", "W. Vogels" ],
      "venue" : "ACM SIGOPS Operating Systems Review, 41(6):205– 220",
      "citeRegEx" : "DeCandia et al.. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Percentile optimization in uncertain Markov decision processes with application to efficient exploration",
      "author" : [ "E. Delage", "S. Mannor" ],
      "venue" : "ICML, pages 225–232",
      "citeRegEx" : "Delage and Mannor. 2007",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "In AAMAS",
      "author" : [ "Stefano Ermon", "Carla Gomes", "Bart Selman", "Alexander Vladimirsky. Probabilistic planning with non-linear utility functions", "worst-case guarantees" ],
      "venue" : "pages 965–972,",
      "citeRegEx" : "Ermon et al.. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Arriving on time",
      "author" : [ "YY Fan", "RE Kalaba", "JE Moore II" ],
      "venue" : "Journal of Optimization Theory and Applications, 127(3):497–513",
      "citeRegEx" : "Fan et al.. 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Resolute choice in sequential decision problems with multiple priors",
      "author" : [ "Hélène Fargier", "Gildas Jeantet", "Olivier Spanjaard" ],
      "venue" : "IJCAI,",
      "citeRegEx" : "Fargier et al.. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Mathematics of Operations Research",
      "author" : [ "Jerzy A. Filar", "L.C.M. Kallenberg", "Huey-Miin Lee. Variancepenalized Markov decision processes" ],
      "venue" : "14:147– 161,",
      "citeRegEx" : "Filar et al.. 1989",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Percentile performance criteria for limiting average Markov decision processes",
      "author" : [ "J.A. Filar", "D. Krass", "K.W. Ross" ],
      "venue" : "IEEE Trans. on Automatic Control, 40(1):2–10",
      "citeRegEx" : "Filar et al.. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Operations Research Letters",
      "author" : [ "Jerzy A. Filar. Percentiles", "Markovian decision processes" ],
      "venue" : "2(1):13 – 15,",
      "citeRegEx" : "Filar. 1983",
      "shortCiteRegEx" : null,
      "year" : 1983
    }, {
      "title" : "An axiomatic characterization of skew-symmetric bilinear functionals",
      "author" : [ "P.C. Fishburn" ],
      "venue" : "with applications to utility theory. Economics Letters, 8(4):311–313",
      "citeRegEx" : "Fishburn. 1981",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "In IJCAI",
      "author" : [ "Hugo Gilbert", "Olivier Spanjaard", "Paolo Viappiani", "Paul Weng. Solving MDPs with skew symmetric bilinear utility functions" ],
      "venue" : "pages 1989– 1995,",
      "citeRegEx" : "Gilbert et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Revisiting risk-sensitive MDPs: New algorithms and results",
      "author" : [ "Ping Hou", "William Yeoh", "Pradeep Reddy Varakantham" ],
      "venue" : "ICAPS,",
      "citeRegEx" : "Hou et al.. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Implementing resolute choice under uncertainty",
      "author" : [ "Jean-Yves Jaffray" ],
      "venue" : "UAI,",
      "citeRegEx" : "Jaffray. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Value-at-Risk: The New Benchmark for Managing Financial Risk",
      "author" : [ "Philippe Jorion" ],
      "venue" : "McGraw-Hill,",
      "citeRegEx" : "Jorion. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Risk-sensitive planning with one-switch utility functions: Value iteration",
      "author" : [ "Y. Liu", "S. Koenig" ],
      "venue" : "AAAI, pages 993–999",
      "citeRegEx" : "Liu and Koenig. 2005",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Functional value iteration for decisiontheoretic planning with general utility functions",
      "author" : [ "Y. Liu", "S. Koenig" ],
      "venue" : "AAAI, pages 1186–1193",
      "citeRegEx" : "Liu and Koenig. 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Mean-variance optimization in Markov decision processes",
      "author" : [ "Shie Mannor", "John Tsitsiklis" ],
      "venue" : "ICML,",
      "citeRegEx" : "Mannor and Tsitsiklis. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Rationality and dynamic choice: Foundational explorations",
      "author" : [ "E. McClennen" ],
      "venue" : "Cambridge university press",
      "citeRegEx" : "McClennen. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "In Journal of the Operational Research Society",
      "author" : [ "T.W. Archibald K. McKinnon", "L.C. Thomas. On the generation of Markov decision processes" ],
      "venue" : "pages 354–361,",
      "citeRegEx" : "McKinnon and Thomas. 1995",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Optimal policy for minimizing risk models in Markov decision processes",
      "author" : [ "Y. Ohtsubo", "K. Toyonaga" ],
      "venue" : "Journal of mathematical analysis and applications, 271:66–81",
      "citeRegEx" : "Ohtsubo and Toyonaga. 2002",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "In NIPS",
      "author" : [ "LA Prashanth", "Mohammad Ghavamzadeh. Actorcritic algorithms for risk-sensitive MDPs" ],
      "venue" : "pages 252–260,",
      "citeRegEx" : "Prashanth and Ghavamzadeh. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Markov decision processes: discrete stochastic dynamic programming",
      "author" : [ "M.L. Puterman" ],
      "venue" : "Wiley",
      "citeRegEx" : "Puterman. 1994",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Regret based reward elicitation for Markov decision processes",
      "author" : [ "K. Regan", "C. Boutilier" ],
      "venue" : "UAI, pages 444–451. Morgan Kaufmann",
      "citeRegEx" : "Regan and Boutilier. 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Quantile maximization in decision theory",
      "author" : [ "M.J. Rostek" ],
      "venue" : "Review of Economic Studies, 77(1):339–371",
      "citeRegEx" : "Rostek. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Qualitative multi-armed bandits: A quantile-based approach",
      "author" : [ "Balázs Szörenyi", "Róbert Busa-Fekete", "Paul Weng", "Eyke Hüllermeier" ],
      "venue" : "ICML, pages 1660–1668,",
      "citeRegEx" : "Szörenyi et al.. 2015",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "In IJCAI",
      "author" : [ "Paul Weng", "Bruno Zanuttini. Interactive value iteration for Markov decision processes with unknown rewards" ],
      "venue" : "pages 2415–2421,",
      "citeRegEx" : "Weng and Zanuttini. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Markov decision processes with ordinal rewards: Reference point-based preferences",
      "author" : [ "Paul Weng" ],
      "venue" : "ICAPS, volume 21, pages 282–289,",
      "citeRegEx" : "Weng. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "volume 20",
      "author" : [ "Paul Weng. Ordinal decision models for Markov decision processes. In ECAI" ],
      "venue" : "pages 828–833,",
      "citeRegEx" : "Weng. 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Utility",
      "author" : [ "D.J. White" ],
      "venue" : "probabilistic constraints, mean and variance of discounted rewards in Markov decision processes. OR Spektrum, 9:13–22",
      "citeRegEx" : "White. 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "Minimising a threshold probability in discounted Markov decision processes",
      "author" : [ "D.J. White" ],
      "venue" : "Journal of mathematical analysis and applications, 173(634–646)",
      "citeRegEx" : "White. 1993",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "QPRED: Using quantile predictions to improve power usage for private clouds",
      "author" : [ "R. Wolski", "J. Brevik" ],
      "venue" : "Technical report, UCSB",
      "citeRegEx" : "Wolski and Brevik. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Journal of mathematical analysis and applications",
      "author" : [ "Congbin Wu", "Yuanlie Lin. Minimizing risk models in Markov decision processes with policies depending on target values" ],
      "venue" : "231:41–67,",
      "citeRegEx" : "Wu and Lin. 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Adaptive robust optimization for coordinated capacity and load control in data centers",
      "author" : [ "Xiaoqi Yin", "Bruno Sinopoli" ],
      "venue" : "International Conference on Decision and Control,",
      "citeRegEx" : "Yin and Sinopoli. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sample complexity of riskaverse bandit-arm selection",
      "author" : [ "Jia Yuan Yu", "Evdokia Nikolova" ],
      "venue" : "IJCAI,",
      "citeRegEx" : "Yu and Nikolova. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Journal of mathematical analysis and applications",
      "author" : [ "Stella X. Yu", "Yuanlie Lin", "Pingfan Yan. Optimization models for the first arrival target distribution function in discrete time" ],
      "venue" : "225:193–223,",
      "citeRegEx" : "Yu et al.. 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "For instance, the Value-at-Risk criterion [Jorion, 2006] widely used in finance is in fact a quantile.",
      "startOffset" : 42,
      "endOffset" : 56
    }, {
      "referenceID" : 37,
      "context" : "Moreover, in the Web industry [Wolski and Brevik, 2014; DeCandia et al., 2007], decisions about performance or Quality-Of-Service are often made based on quantiles.",
      "startOffset" : 30,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "Moreover, in the Web industry [Wolski and Brevik, 2014; DeCandia et al., 2007], decisions about performance or Quality-Of-Service are often made based on quantiles.",
      "startOffset" : 30,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "For instance, Amazon reports [DeCandia et al., 2007] that they optimize the 99.",
      "startOffset" : 29,
      "endOffset" : 52
    }, {
      "referenceID" : 1,
      "context" : "More generally, in the service industry, because of skewed distributions [Benoit and Van den Poel, 2009], one generally does not want that customers are satisfied on average, but rather that most customers (e.",
      "startOffset" : 73,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : "We provide a binary search algorithm using functional backward induction [Liu and Koenig, 2006] as a subroutine for computing an optimal policy.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 28,
      "context" : "Markov Decision Processes (MDPs) offer a general and powerful formalism to model and solve sequential decision-making problems [Puterman, 1994].",
      "startOffset" : 127,
      "endOffset" : 143
    }, {
      "referenceID" : 29,
      "context" : "One can therefore try to uncover the reward function by interacting with an expert of the domain considered [Regan and Boutilier, 2009; Weng and Zanuttini, 2013].",
      "startOffset" : 108,
      "endOffset" : 161
    }, {
      "referenceID" : 32,
      "context" : "One can therefore try to uncover the reward function by interacting with an expert of the domain considered [Regan and Boutilier, 2009; Weng and Zanuttini, 2013].",
      "startOffset" : 108,
      "endOffset" : 161
    }, {
      "referenceID" : 34,
      "context" : "When the lower and upper quantiles differ, one may define the quantile as a function of the lower and upper quantiles [Weng, 2012].",
      "startOffset" : 118,
      "endOffset" : 130
    }, {
      "referenceID" : 24,
      "context" : "Three solutions are then possible [McClennen, 1990]: 1) adopting a consequentialist approach, i.",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 19,
      "context" : ", at time step t = 0 we apply an optimal policy for the problem with horizon T and initial state s0 and do not deviate from it; 3) adopting a sophisticated resolute choice approach [Jaffray, 1998; Fargier et al., 2011], i.",
      "startOffset" : 181,
      "endOffset" : 218
    }, {
      "referenceID" : 12,
      "context" : ", at time step t = 0 we apply an optimal policy for the problem with horizon T and initial state s0 and do not deviate from it; 3) adopting a sophisticated resolute choice approach [Jaffray, 1998; Fargier et al., 2011], i.",
      "startOffset" : 181,
      "endOffset" : 218
    }, {
      "referenceID" : 22,
      "context" : "Following [Liu and Koenig, 2006], this problem can be solved with a functional backward induction (Algorithm 2).",
      "startOffset" : 10,
      "endOffset" : 32
    }, {
      "referenceID" : 22,
      "context" : "2) by functional value iteration [Liu and Koenig, 2006] in the binary search.",
      "startOffset" : 33,
      "endOffset" : 55
    }, {
      "referenceID" : 25,
      "context" : "The first set of experiments was conducted on Garnets [McKinnon and Thomas, 1995], which designate random MDPs with a constrained branching factor.",
      "startOffset" : 54,
      "endOffset" : 81
    }, {
      "referenceID" : 1,
      "context" : "This experiment is performed on an instance of GarnetG(100, 5, dlog2 100e) whose rewards are slightly modified to make the distribution of the optimal policy skewed, as it is often the case in some real applications [Benoit and Van den Poel, 2009].",
      "startOffset" : 216,
      "endOffset" : 247
    }, {
      "referenceID" : 4,
      "context" : "Much work in the MDP literature [Boussard et al., 2010] considered decision criteria different to the standard ones (i.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "Recently, [Prashanth and Ghavamzadeh, 2013] and [Mannor and Tsitsiklis, 2011] provided algorithms for this mean-variance formulation.",
      "startOffset" : 10,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "Recently, [Prashanth and Ghavamzadeh, 2013] and [Mannor and Tsitsiklis, 2011] provided algorithms for this mean-variance formulation.",
      "startOffset" : 48,
      "endOffset" : 77
    }, {
      "referenceID" : 21,
      "context" : "Additionally, in the artificial intelligence community, [Liu and Koenig, 2005; Liu and Koenig, 2006; Ermon et al., 2012] also investigated the use of EU as a decision criterion in MDPs.",
      "startOffset" : 56,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "Additionally, in the artificial intelligence community, [Liu and Koenig, 2005; Liu and Koenig, 2006; Ermon et al., 2012] also investigated the use of EU as a decision criterion in MDPs.",
      "startOffset" : 56,
      "endOffset" : 120
    }, {
      "referenceID" : 10,
      "context" : "Additionally, in the artificial intelligence community, [Liu and Koenig, 2005; Liu and Koenig, 2006; Ermon et al., 2012] also investigated the use of EU as a decision criterion in MDPs.",
      "startOffset" : 56,
      "endOffset" : 120
    }, {
      "referenceID" : 16,
      "context" : "[2015] investigated the use of Skew-Symmetric Bilinear (SSB) utility [Fishburn, 1981] functions — a generalization of EU with stronger descriptive abilities — as decision criteria in finite-horizon MDPs.",
      "startOffset" : 69,
      "endOffset" : 85
    }, {
      "referenceID" : 33,
      "context" : "In MDPs with ordinal rewards [Weng, 2011; Weng, 2012; Filar, 1983], quantile-based decision models were proposed to compute",
      "startOffset" : 29,
      "endOffset" : 66
    }, {
      "referenceID" : 34,
      "context" : "In MDPs with ordinal rewards [Weng, 2011; Weng, 2012; Filar, 1983], quantile-based decision models were proposed to compute",
      "startOffset" : 29,
      "endOffset" : 66
    }, {
      "referenceID" : 15,
      "context" : "In MDPs with ordinal rewards [Weng, 2011; Weng, 2012; Filar, 1983], quantile-based decision models were proposed to compute",
      "startOffset" : 29,
      "endOffset" : 66
    } ],
    "year" : 2016,
    "abstractText" : "In the Markov decision process model, policies are usually evaluated by expected cumulative rewards. As this decision criterion is not always suitable, we propose in this paper an algorithm for computing a policy optimal for the quantile criterion. Both finite and infinite horizons are considered. Finally we experimentally evaluate our approach on random MDPs and on a data center control problem.",
    "creator" : "LaTeX with hyperref package"
  }
}