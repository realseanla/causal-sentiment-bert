{
  "name" : "1611.03451.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Importance Sampling with Unequal Support",
    "authors" : [ "Philip S. Thomas", "Emma Brunskill" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "A key challenge in artificial intelligence is to estimate the expectation of a random variable. Instances of this problem arise in areas ranging from planning and decision making (e.g., estimating the expected sum of rewards produced by a policy for decision making under uncertainty) to probabilistic inference. Although the estimation of an expected value is straightforward if we can generate many independent and identically distributed (i.i.d.) samples from the relevant probability distribution (which we refer to as the target distribution), we may not have generative access to the target distribution. Instead, we might only have data from a different distribution that we call the sampling distribution.\nFor example, in off-policy evaluation for reinforcement learning, the goal is to estimate the expected sum of rewards that a decision policy will produce, given only data gathered using some other policy. Similarly, in supervised learning, we may wish to predict the performance of a regressor or classifier if it were to be applied to data that comes from a distribution that differs from the distribution of the available data (e.g., we might predict the accuracy of a classifier for hand-written letters given that observed letter frequencies come from English, using a corpus of labeled letters collected from German documents).\nMore precisely, we consider the problem of estimating θ := E[h(X)], where h is a real-valued function and the\nexpectation is over the random variable X , which is a sample from the target distribution. As input we assume access to n i.i.d. samples from a sampling distribution that is different from the target distribution. A classical approach to this problem is to use importance sampling (IS), which reweighs the observed samples to account for the difference between the target and sampling distributions (Kahn, 1955). Importance sampling produces an unbiased but often highvariance estimate of θ.\nWe introduce importance sampling with unequal support (US)—a simple new importance sampling estimator that can drastically reduce the variance of importance sampling when the supports of the sampling and target distributions differ. This setting with unequal support can occur, for example, in our earlier example where German documents might include symbols like ß, that the classifier will not encounter. US essentially performs importance sampling only on the data that falls within the support of the target distribution, and then scales this estimate by a constant that reflects the relative support of the target and sampling distributions.\nUS typically has lower variance than ordinary importance sampling (sometimes by orders of magnitude), and is unbiased in the important setting where at least one sample falls within the support of the target distribution. If no samples do, then none of the available data could have been generated by the target distribution, and so it is unclear what would make for a reasonable estimate. Furthermore, the conditionally unbiased nature of US is sufficient to allow for its use with concentration inequalities like Hoeffding’s inequality to construct confidence bounds on θ. By contrast, weighted importance sampling (Rubinstein, 1981) is another variant of importance sampling that can reduce variance, but which introduces bias that makes it incompatible with Hoeffding’s inequality."
    }, {
      "heading" : "Problem Setting and Importance Sampling",
      "text" : "Let f and g be probability density functions (PDFs) for two distributions that we call the target distribution and sampling distribution, respectively. Let h : R → R be called the evaluation function. Let θ := Ef [h(X)], where Ef denotes the expected value given that f is the PDF of the random variable(s) in the expectation (in this case, just X). Let F := {x ∈ R : f(x) 6= 0}, G := {x ∈ R : g(x) 6= 0}, and H := {x ∈ R : h(x) 6= 0} be the supports of the target and\nar X\niv :1\n61 1.\n03 45\n1v 1\n[ cs\n.L G\n] 1\n0 N\nov 2\n01 6\nsampling distributions, and the evaluation function, respectively. In this paper we will discuss techniques for estimating θ given n ∈ N>0 i.i.d. samples, Xn := {X1, . . . , Xn}, from the sampling distribution, and we focus on the setting where F ∩H ⊂ G—where the joint support of F and H is a strict subset of the support of G.\nThe importance sampling estimator,\nIS(Xn) := t+ 1\nn\nn∑\ni=1\nf(Xi) g(Xi) (h(Xi)− t), (1)\nis a widely used estimator of θ, where t = 0 (we consider non-zero values of t later). If F ∩ H ⊆ G, then IS(Xn) is a consistent and unbiased estimator of θ. That is, IS(Xn)\na.s.−→ θ and Eg[IS(Xn)] = θ (we review this latter result in Property 1 in the supplemental document).\nA control variate is a constant, t ∈ R, that is subtracted from each h(Xi) and then added back to the final estimate, as in (1) (Hammersley, 1960; Hammersley and Handscomb, 1964). Although control variates, t(Xi), that depend on the sample, Xi, can be beneficial, for our later purposes we only consider constant control variates. Intuitively, including a constant control variate equates to estimating θ′ := Ef [h′(X)] using importance sampling without a control variate, where h′(x) = h(x) − t, and then adding t to the resulting estimate to get an estimate of θ.\nLater we show that the variance of importance sampling increases with θ2, and so applying importance sampling to h results in higher variance than applying importance sampling to h′ with t ≈ θ, since then θ′ ≈ 0. That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias—a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dudı́k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016). Although later we discuss control variates more, for simplicity our derivations focus on importance sampling estimators without control variates. There are also other extensions of the importance sampling estimator that can reduce variance—notably the weighted importance sampling estimator, which we compare to later, and which can provide large reductions of variance and mean squared error, but which introduces bias.\nAn Illustrative Example In this section we present an example that highlights the peculiar behavior of the IS estimator when F ∩ H 6= G. Let g(x) = 0.5 if x ∈ [0, 2] and g(x) = 0 otherwise, and let f(x) = 1 if x ∈ [0, 1] and f(x) = 0 otherwise. So, F = [0, 1] and G = [0, 2]. Let h(x) = 1 if x ∈ [0, 1] and h(x) = 0 otherwise, so that H = [0, 1]. Notice that θ = 1.\nSince the sampling and target distributions are both uniform, an obvious estimator of θ (if f and g are known but h is not) would be the average of the points that fall within F . Let (#Xi ∈ F ) denote the number of samples in Xn that are in F . Formally, the obvious estimator is\nθ̂ := 1 (#Xi ∈ F ) n∑\ni=1\n1F (Xi)h(Xi),\nwhere 1A(x) = 1 if x ∈ A and 1A(x) = 0 otherwise. Given our knowledge of h, it is straightforward to show that this estimator is equal to 1 if (#Xi ∈ F ) > 0 and is undefined otherwise—it is exactly correct (has zero bias and variance) as long as at least one sample falls within F . If no samples fall within F , then we have only observed data that will never occur under the target distribution, and so we have no useful information about θ. In this case, we might define our obvious estimator to return an arbitrary value, e.g., zero.\nPerhaps surprisingly, the importance sampling estimator does not degenerate to this obvious estimator:\nIS(Xn) = 1\nn\nn∑\ni=1\n1F (Xi)2h(Xi) = 2(#Xi ∈ F )\nn .\nSince Eg[(#Xi ∈ F )/n] = 1/2, this estimate is correct in expectation, but does not have zero variance given that at least one sample falls within F . If more than 1/2 of the samples fall within F , this estimate will be an over-estimate of θ, and if fewer than 1/2 of the samples fall within F , this estimate will be an under-estimate. Although correct on average, the importance sampling estimator has unnecessary additional variance relative to the obvious estimator."
    }, {
      "heading" : "Importance Sampling with Unequal Support",
      "text" : "We propose a new importance sampling estimator, importance sampling with unequal support (ISUS, or US for brevity), that does degenerate to the obvious estimator for our illustrative example. Intuitively, US prunes from Xn the samples that are outside F (or more generally, outside some set C, that we define later) to construct a new data set, X′n, that has fewer samples. This new data set can be viewed as (#Xi ∈ F ) i.i.d. samples from a different sampling distribution—a distribution with PDF g′, which is simply g, but truncated to only have support on F and re-normalized to integrate to one. US then applies ordinary importance sampling to this new data set.\nFor generality, we allow US to prune from Xn all of the points that are not in a set, C, which can be defined many different ways, including C := F (as in our previous example). Our only requirement is that F ∩H ⊆ C ⊆ G. In order to compute US, we must compute a value,\nc :=\n∫\nC\ng(x) dx,\nwhich is the probability that a sample from the sampling distribution will be in C. In general, C should be chosen to be as small as possible while still ensuring that both 1) F ∩H ⊆ C ⊆ G (so that informative samples are not discarded) and 2) c can be computed. Ideally, we would select C = F ∩H , however in some cases c cannot be computed for this value of C. For example, in our later experiments we consider a problem where h and H are not known, but F is, and so we can compute c using C = F , but not C = F ∩H .\nLet k(Xn) := ∑n i=1 1C(Xi) be the number of Xi that\nare in C. The US estimator is then defined as:\nUS(Xn) := c\nk(Xn)\nn∑\ni=1\nf(Xi) g(Xi) h(Xi), (2)\nif k(Xn) > 0, and US(Xn) := 0 if k(Xn) = 0. This is equivalent to applying importance sampling to the pruned data set, X′n, since then g\n′(x) = g(x)/c for x ∈ C. Also, in (2) we sum over all n samples rather than just the k(Xn) samples in C because f(Xi)h(Xi) = 0 for all Xi not in C."
    }, {
      "heading" : "Theoretical Analysis of US",
      "text" : "We begin with two simple theorems that elucidate the relationship between IS and US. The proofs of both theorems are straightforward, but deferred to the supplemental document. First, Theorem 1 shows that, when C = G, US degenerates to IS. One case where C = G is when the support of the target distribution and evaluation function are both equal to the support of the sampling distribution, i.e., when F = H = G, and so C = G necessarily. Theorem 1. If C = G, then US(Xn) = IS(Xn).\nTheorem 2 shows that, if we replace c in the definition of US with an empirical estimate, ĉ(Xn) := k(Xn)/n, then US and IS are equivalent. This provides some intuition for why US tends to outperform IS when C ⊂ G—IS is US, but using an empirical estimate of c (the probability that a sample falls within C), in place of its known value. Theorem 2. If we replace c with an empirical estimate, ĉ(Xn) := k(Xn)/n, then US(Xn) = IS(Xn).\nIn Table 1 we summarize more theoretical results that clarify the differences between IS and US in several settings. The first setting (denoted by a † in Table 1) is the standard setting where we consider the ordinary expected value and variance of the two estimators. The second setting (denoted by a ‡ in Table 1) conditions on the event that at least one sample falls withinC, that is, the event that k(Xn) > 0. This is a reasonable setting to consider if one takes the view that no estimate should be returned if all of the samples are outside C. That is, if the pruned data set, X′n, is empty, then no estimate should be produced or considered (just as IS does not produce an estimate when n = 0—when there are no samples at all). Finally, the third setting (denoted by a ? in Table 1) conditions on the event that k(Xn) = κ—that a specific constant number of the n samples are in C.\nTable 1 and the theorems that it references use additional symbols that we review here. Let ρ := Pr(k(Xn) > 0) = 1− (1− c)n be the probability that at least one of n samples is in C. Let Varg(·) denote the variance given that the random variables within the parenthesis are sampled from the distribution with PDF g. Let\nv := Varg\n( f(X)\ng(X) h(X)\n∣∣∣∣X ∈ C )\nbe the conditional variance of the importance sampling estimate when using a single sample and given that the sample is inC. LetB(n, c) denote the binomial distribution with parameters n and c and let EB(n,c) denote the expected value given that κ ∼ B(n, c).\nAlthough the proofs of the claims in Table 1 are some of the primary contributions of this work, we defer them to the supplemental document because they are straightforward (though lengthy) and do not provide further insights\ninto the results. The primary result of Table 1 is that US is unbiased and often has lower variance in the key setting of interest: when at least one sample is in the support of the target distribution—when k(Xn) > 0. We find this setting compelling because, when no samples are in F , little can be inferred about Ef [h(X)].\nIn this setting (denoted by ‡ in Table 1) US is an unbiased estimator, while IS is not (although the bias of IS does go to zero as n → ∞).1 To understand the source of this bias, consider the bias of IS given that k(Xn) = κ—the ? setting in Table 1. In this case, Eg[IS(Xn)] = κcnθ. Recall that IS uses an empirical estimate of c, i.e., ĉ ≈ κn (as discussed in Theorem 2). When this estimate is correct, terms in κcnθ cancel, making IS unbiased. Thus, the bias of IS when conditioning on the event that k(Xn) > 0 stems from IS’s use of an estimate of c.\nNext we discuss the variance of the two estimators given that at least one sample falls within C, i.e., in the ‡ setting. First consider how the variances of IS and US change as c → 0—that is, as the differences between the supports of the sampling and target distributions increases. Specifically, let ci := 1i for i ∈ N>0. We then have that: Var(IS(Xn)|k(Xn) > 0, ci) ≥ civnρ = vnρi ≥ vni , since ρ ∈ (0, 1], and Var(US(Xn)|k(Xn) > 0, ci) = (v/i2)EB(n,c)[1/κ|κ > 0] ≤ v/i2, since EB(n,c)[κ−1|κ > 0] ≤ 1. Thus, as i → ∞ (as c → 0 logarithmically), and given some fixed n and v, the variance of US goes to zero much faster than the variance of IS. The variance of US (as a function of i) converges to zero linearly (or faster) with a rate of at most 1 while the variance of IS converges to zero sublinearly (at best, logarithmically).\nNext note that the variance of US in this setting is independent of θ2, but the variance of IS increases with θ2 (see Property 3 in the supplemental document, applied to Theorem 9). To ameliorate this issue, a control variate, t, can be used to center the data so that θ ≈ 0. However, since θ is not known a priori, selecting t = θ is not practical. The term that scales with θ2 in the variance of IS given that k(Xn) > 0 therefore means that the variance of IS depends on the quality of the control variate—poor control variates can cause IS to have high variance. By contrast, the variance of US in this setting does not have a term that scales with θ2, and so the quality of the control variate is less important.2\nThere is a rare case when IS can have a lower variance than US. First, we assume that the control variate is perfect so that θ = 0 (which, as discussed before, is impractical) and consider the term that scales with v. From this term, it is clear that US will have lower variance that IS if:\nc2EB(n,c)[κ −1|κ > 0] ≤ c\nnρ . (3)\n1If we do not condition on the event that k(Xn) > 0, then US is a biased estimator of θ. This is because it is unclear how to define US(Xn) when k(Xn) = 0, and we chose (arbitrarily) to define it to be 0. However, the bias of IS(Xn) in this setting converges quickly to zero, since ρ (the probability that no samples fall within C) converges quickly to one as n→∞.\n2The quality of the control variate can still impact the variance of estimates though, since it can change v.\nNotice that this inequality depends only on n and c, which must both be known in order to implement US, and so we can test a priori whether US will have lower variance than IS. That is, if (3) holds, then US will have lower variance than IS, given that k(Xn) > 0. However, if (3) does not hold, it does not mean that IS will have lower variance than US unless the perfect (typically unknown) control variate is used so that θ = 0."
    }, {
      "heading" : "Application to Illustrative Example",
      "text" : "Because neither method is always superior, here we consider the application of IS and US to the illustrative example to see when each method works best, and by how much. We consider the setting where C = F , but modify the example slightly. First, although the target distribution is always uniform, we allow for its support to be scaled. Specifically, we define the support of f to be [0, Fmax], where Fmax ∈ (0, 2]. When Fmax is small, it corresponds to significant differences in support, while large Fmax correspond to small differences (when Fmax = 2, C = F = G and so the two estimators are equivalent). We also modify h to allow for various values of θ. Specifically, we define h(x) = −1 + θ if x < Fmax/2 and h(x) = 1 + θ if x ≥ Fmax/2. Notice that, although we defined h in terms of θ, θ remains Ef [h(X)], and also that using this definition of h and θ = 0 is an instance that is particularly favorable to IS.\nFor this example, it is straightforward to verify that v = 4/F 2max for any definition of θ, and c = Fmax/2. Given these two values (and θ), we can compute the bias and variance of each estimator. The biases and variances of the two estimators for various settings are depicted in Figure 1. Notice that US is always competitive with IS, although the reverse is not true. Particularly, when Fmax is small (so that c is small), or when θ is large, US can have orders of magnitude lower variance than IS. Also, as n increases, the two estimators become increasingly similar, since the empirical estimate of c used by IS becomes increasingly accurate, although US is still vastly superior to IS even when n is large if c is correspondingly small. This matches our theoretical analysis from the previous section: we expect US to perform better when c is small (by our convergence rate analysis) or when θ2 is large (due to US’s lesser dependence on the quality of the control variate), and we expect the two estimators to becomes increasingly similar as n → ∞ (because ĉ becomes increasingly similar to c).\nNotice also that gains are not only obtained when c is\nso small relative to n that no samples are expected to fall within C (a relatively uninteresting setting). For example, the right-most plot in Figure 1 shows that with Fmax = 0.5, where Pr(k(Xn) > 0) = ρ = 1 − 1250 ≈ 1, the MSE of US is approximately 0.086, while the MSE of IS is approximately 6.08—US is has roughly 1/70 the MSE of IS (1/8 the RMSE).\nPerhaps surprisingly, there are cases where IS has lower variance than US (even when both are unbiased, since θ = 0). For example, consider the plot with θ = 0 and n = 10, and the position on the horizontal axis that corresponds to Fmax = 1.0. This is one case where IS is marginally better than US (it has lower variance in both settings, and neither estimator is biased). Intuitively, the IS estimator includes the points outside the support of F , although they have associated values, h(Xi) = 0, which pulls the importance sampling estimate towards zero. In this case, when θ = 0, this extra pull towards zero happens to be beneficial. However, to remain unbiased given the pull towards zero, IS also increases the magnitudes of the weights associated with points in F , which incurs additional variance. When Fmax is small enough, this additional variance outweighs the variance reduction that results from the extra pull towards zero, and so US is again superior. This intuition is supported by the fact that in Figure 1 IS does not outperform US for small Fmax or θ ≥ 1, since then a pull towards zero is detrimental.\nFinally, we consider the use of IS and US to create highconfidence upper and lower bounds on θ using a concentration inequality (Massart, 2007) like Hoeffding’s inequality (Hoeffding, 1963). If b denotes the range of the function f(x)h(x)/g(x), for x ∈ G, then using Hoeffding’s inequality, we have that IS(Xn)− b √ ln(1/δ)/(2n) is a 1− δ confidence lower bound on θ. Similarly, we can use US with Hoeffding’s inequality to create a 1 − δ confidence lower bound: US(Xn) − cb √ ln(1/δ)/(2k(Xn)), since the range of the k(Xn) i.i.d. random variables averaged by US(Xn) is cb. Notice that, if k(Xn) = 0, then this second estimator is undefined (one might define the lower bound to be a known lower bound on θ in this setting). Although we expect that k(Xn) ≈ cn, the resulting c in the denominator of the US-based bound is within the square root, while the c in the numerator is not, and so the bound constructed using US should tend to be tighter when c is small."
    }, {
      "heading" : "Application to Diabetes Treatment",
      "text" : "We applied US and IS to the problem of predicting the effectiveness of altering the treatment policy for diabetes 1 for a particular individual. That is, we would like to use prior data from when the individual was treated with one treatment policy to estimate how well a related policy would work. The treatment policy is parameterized by two numbers, CR and CF, and dictates how much insulin a person should inject prior to eating a meal in order to keep his or her blood glucose close to optimum levels. CR and CF are typically specified by a diabetologist and tweaked during follow-up visits every 3–6 months. If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014).\nHere we focus on a sub-problem of improving CR and CF—using data collected from an initial range of admissible values of CR and CF to predict how well a new range of values for CR and CF would perform. When collecting data, CR and CF are drawn uniformly from an initial admissible range, and then used for one day (which we view as one episode of a Markov decision process). The performance during each day is measured using an objective function similar to the reward function proposed by Bastani (2014), which measures the deviation of blood glucose from optimum levels, with larger penalties for low blood glucose levels. We refer to the measure of how good the outcome was from one day as the return associated with that day, with larger values being better. Using approximately 30 days of data, our goal is to estimate the expected return if a different distribution of CR and CF were to be used.\nWe consider a specific in silico person—a person sim-\nulated using a metabolic simulator. We used the subject “Adult#003” in the Type 1 Diabetes Metabolic Simulator (T1DMS) (Dalla Man et al., 2014)—a simulator that has been approved by the US Food and Drug Administration as a substitute for animal trials in pre-clinical testing of treatment policies for type 1 diabetes. During each day, the subject is given three or four meals of randomized sizes at randomized times, similar to the experimental setup proposed by Bastani (2014). As a result of this randomness, and the stochastic nature of the T1DMS model, applying the same values of CR and CF can produce different returns if used for multiple days. After analyzing the performance of many CR and CF pairs, we selected an initial range that results in good performance: CR ∈ [8.5, 11] and CF ∈ [10, 15]. Using a large number of samples, we computed an estimate of the expected return if different CR and CF values are used for a single day—this estimate is depicted in Figure 2.\nAs described by Bastani (2014), when the value of CR is set appropriately, performance is robust to changes in CF. We therefore focus on possible changes to CR. Specifically, we consider new treatment policies where CF remains sampled from the uniform distribution over [10, 15], but where CR is sampled from the truncated normal distribution over [CRmin, 11], with mean 11 and standard deviation 11 − CRmin. This distribution places the largest probability densities at the upper end of the range of CR, which favors better policies. As CRmin increases towards 11, the support of the sampling distribution and target distribution become increasingly different (c = (11 − CRmin)/2.5) and the expected return increases.\nFor each value of CRmin (each of which corresponds to\na value of c), we performed 2,433 trials, each of which involved sampling 30 days of data from the sampling distribution and then using IS, US, and weighted importance sampling (WIS) to estimate the expected return if CR and CF were sampled from the target distribution. Figure 2 displays the bias, variance and mean squared error (MSE) of these 2,433 estimates, using an estimate of ground truth computed using Monte Carlo sampling. Figure 2 also shows the impact of providing a constant control variate to all the estimators: the chosen control variate was the expected return under the sampling distribution.\nNotice that we see the same trend as in the illustrative example—for small c (the best treatment policies, which have small ranges of CR), US significantly outperforms IS. Furthermore, when a decent control variate is not used, the benefits of US are increased, even when controlling for the resulting bias by measuring the mean squared error. We also computed the biases and variances given that k(Xn) > 0, and observed similar results (not shown), which favored US slightly more. Notice that WIS and US perform very similarly. Indeed, if the sampling and target distributions are both uniform, it is straightforward to verify that WIS and US are equivalent. In other experiments (not shown) we found that WIS yields lower variance than US when the target distribution is modified to be even less like the uniform distribution.\nHowever, it is often important to be able to produce confidence intervals around estimates (especially when data is limited), and since WIS is biased, it cannot be used with standard concentration inequalities. We used Hoeffding’s inequality to compute a 90% confidence interval around the estimates produced by IS and US (without control variates and with CRmin = 10.375, so that c = 1/4) using various numbers of samples (days of data). The mean confidence intervals are depicted in Figure 3, which also shows a Monte Carlo estimate of θ, as well as deterministic domain-specific upper and lower bounds on h(X) (denoted by “h range” in the legend). If k(Xn) = 0, then US is not defined, and so the confidence intervals shown for US are averaged only over the instances where k(Xn) > 0. To show how often US returns a solution, Figure 3 also shows ρ—the probability that US will produce a confidence bound—using the right verti-\ncal axis for scale. US produces a much tighter confidence interval than IS in all cases. Furthermore, the setting where US often does not return a bound corresponds to the setting where IS produces a confidence interval that is outside the deterministic bound on h(X)—a trivial confidence interval. In additional experiments (not shown) we defined the bounds to be truncated to always be within the deterministic bounds on h(X) and define the bound produced using US to be conservative (equal to the deterministic bounds) when k(Xn) = 0. In this experiment we saw similar results—the confidence intervals produced using US were much tighter than those using IS."
    }, {
      "heading" : "Conclusion and Future Work",
      "text" : "We have presented a simple new variant of importance sampling, US. Our analytical and empirical results suggest that US can significantly outperform ordinary importance sampling, and we provide an a priori calculation to check for the rare cases where it can perform slightly worse. Unlike some other IS estimators that have been developed to reduce variance (like WIS), US is unbiased given mild conditions that still permit the easy computation of confidence intervals."
    }, {
      "heading" : "Supplemental Document",
      "text" : "In this supplemental document we prove the various properties and theorems referenced earlier (particularly those in Table 1).\nProperty 1. If F ∩H ⊆ G then Eg[IS(Xn)] = θ."
    }, {
      "heading" : "Proof.",
      "text" : "Eg[IS(Xn)] (a) =Eg\n[ f(X)\ng(X) h(X)\n] = ∫\nG\ng(x) f(x)\ng(x) h(x) dx\n(b) =\n∫\nF∩H f(x)h(x) dx = Ef [h(X)] = θ,\nwhere (a) holds because IS(Xn) is the mean of n independent and identically distributed random variables, and (b) holds because ∀x ∈ G \\ (F ∩H), f(x) = 0.\nWe now provide a proof of Theorem 1, which states that if C = G, then US(Xn) = IS(Xn).\nProof. In this setting, c = ∫ G g(x) dx = 1 and since every Xi must be within C, k(Xn) = n. So,\nUS(Xn) = c\nk(Xn)\nn∑\ni=1\nf(Xi) g(Xi) h(Xi)\n= 1\nn\nn∑\ni=1\nf(Xi) g(Xi) h(Xi).\nWe now provide a proof of Theorem 2, which states that if we replace c with an empirical estimate, ĉ(Xn) := n−1k(Xn), then US(Xn) = IS(Xn).\nProof. Using the empirical estimate, ĉ(Xn), in place of c within US we have:\nUS(Xn) = ĉ(Xn)\nk(Xn)\nn∑\ni=1\nf(Xi) g(Xi) h(Xi)\n= k(Xn)\nnk(Xn)\nn∑\ni=1\nf(Xi) g(Xi) h(Xi)\n= 1\nn\nn∑\ni=1\nf(Xi) g(Xi) h(Xi)\n= IS(Xn).\nTheorem 3. If F ∩H ⊆ G and κ ∈ N>0, then\nEg[US(Xn)|k(Xn) = κ] = θ.\nProof. Let Prg(X ∈ C) denote the probability that a sample, X , from the sampling distribution is in C.\nEg[ US(Xn)|k(Xn) = κ]\n=Eg\n[ c\nκ\nn∑\ni=1\nf(Xi) g(Xi) h(Xi) ∣∣∣∣∣k(Xn) = κ ]\n(a) =Eg\n[ c\nκ\nκ∑\ni=1\nf(Xi) g(Xi) h(Xi) ∣∣∣∣∣∀i ∈ {1, . . . , κ}, Xi ∈ C ]\n(b) =Eg [ c f(X)\ng(X) h(X)\n∣∣∣∣X ∈ C ]\n(c) =\n∫\nC\ng(x) Prg(X ∈ C) c f(x) g(x) h(x) dx\n(d) =\n∫\nC\ng(x) c c f(x) g(x) h(x) dx\n=\n∫\nC\nf(x)h(x) dx\n(e) =Ef [h(X)],\nwhere (a) holds because f(Xi) = 0 for all but κ of the terms in the summation, and so (by re-ordering the Xi so that these κ terms have indices 1, . . . , κ) we need only sum to κ rather than n, (b) holds because the summation is over κ independent and identically distributed random variables, (c) holds by the definition of conditional expectations, (d) holds because Prg(X ∈ C) = c, and (e) holds because F ∩H ⊆ C.\nTheorem 4. If F ∩H ⊆ G then\nEg[US(Xn)|k(Xn) > 0] = θ."
    }, {
      "heading" : "Proof.",
      "text" : "Eg[ US(Xn)|k(Xn) > 0]\n= n∑\nκ=1\nPr(k(Xn) = κ|k(Xn) > 0) Pr(k(Xn) > 0) Eg[US(Xn)|k(Xn) = κ]\n(a) =\nn∑\nκ=1\nPr(k(Xn) = κ|k(Xn) > 0) Pr(k(Xn) > 0) θ\n=θ n∑\nκ=1\nPr(k(Xn) = κ|k(Xn) > 0) Pr(k(Xn) > 0)\n=θ,\nwhere (a) holds because, by Theorem 3, E[US(Xn)|k(Xn) = κ] = θ.\nTheorem 5. If F ∩H ⊆ G and κ ∈ N>0, then\nEg[IS(Xn)|k(Xn) = κ]− θ = ( κ cn − 1 ) θ. (4)\nProof. Following roughly the same steps as used to prove\nTheorem 3 we have that:\nEg[ IS(Xn)|k(Xn) = κ]\n=Eg\n[ 1\nn\nn∑\ni=1\nf(Xi) g(Xi) h(Xi) ∣∣∣∣∣k(Xn) = κ ]\n=Eg\n[ 1\nn\nκ∑\ni=1\nf(Xi) g(Xi) h(Xi) ∣∣∣∣∣∀i ∈ {1, . . . , κ}, Xi ∈ C ]\n=Eg\n[ κ\nn\nf(X1) g(X1) h(X1)\n∣∣∣∣X1 ∈ C ]\n=\n∫\nC\ng(x)\nc\nκ\nn\nf(x) g(x) h(x) dx\n= κ\ncn Ef [h(X)]\n= κ\ncn θ,\nand so (4) follows.\nTheorem 6. If F ∩H ⊆ G then\nEg[IS(Xn)|k(Xn) > 0] = 1\n1− (1− c)n θ.\nProof. Recall from Property 1 that Eg[IS(Xn)] = θ. By marginalizing over whether or not k(Xn) > 0, we also have that:\nEg[IS(Xn)] =Pr(k(Xn) > 0)Eg[IS(Xn)|k(Xn) > 0] + Pr(k(Xn) = 0)Eg[IS(Xn)|k(Xn) = 0].\nSo,\nEg[ IS(Xn)|k(Xn) > 0]\n= θ − Pr(k(Xn) = 0)Eg[IS(Xn)|k(Xn) = 0]\nPr(k(Xn) > 0)\n(a) =\nθ\n1− (1− c)n ,\nwhere (a) holds because Eg[IS(Xn)|k(Xn) = 0] = 0 and Pr(k(Xn) > 0) = 1−Pr(k(Xn) = 0) = 1− (1− c)n.\nTheorem 7. If F ∩H ⊆ G, then Eg[US(Xn)] = (1− (1− c)n)θ."
    }, {
      "heading" : "Proof.",
      "text" : "Eg[ US(Xn)]\n=Pr(k(Xn) > 0)︸ ︷︷ ︸ =1−(1−c)n Eg[US(Xn)|k(Xn) > 0]︸ ︷︷ ︸ =θ, by Theorem 4\n+ Pr(k(Xn) = 0)Eg[US(Xn)|k(Xn) = 0]︸ ︷︷ ︸ =0\n=(1− (1− c)n)θ. Before continuing, recall the following property (which\nwe prove for completeness):\nProperty 2. Let X1, . . . , Xn be n independent and identically distributed random variables, each with finite mean and variance. Then,\nE\n  ( 1\nn\nn∑\ni=1\nXi\n)2  = 1\nn Var (X1) +E [X1]\n2 .\nProof. Recall that\nVar\n( 1\nn\nn∑\ni=1\nXi ) = E [( 1\nn\nn∑\ni=1\nXi )2] −E [ 1\nn\nn∑\ni=1\nXi ]2 .\nSo, by rearranging terms:\nE\n[( 1\nn\nn∑\ni=1\nXi )2] = 1\nn2 Var\n( n∑\ni=1\nXi ) + 1\nn2 E\n[ n∑\ni=1\nXi ]2 .\nSince the Xi are independent and identically distributed, we therefore have that:\nE\n  ( 1\nn\nn∑\ni=1\nXi\n)2  = 1\nn2 nVar (X1) +\n1\nn2 n2E [X1]\n2\n= 1\nn Var (X1) +E [X1]\n2 .\nTheorem 8. If F ∩H ⊆ G then\nVarg(US(Xn)|k(Xn > 0)) =c2vEB(n,c) [ 1\nκ\n∣∣∣∣κ > 0 ] ."
    }, {
      "heading" : "Proof.",
      "text" : "Varg(US(Xn)|k(Xn) > 0) =Eg[US(Xn) 2|k(Xn) > 0]−Eg[US(Xn)|k(Xn) > 0]2\n=Eg[US(Xn) 2|k(Xn) > 0]− θ2\n=\n( n∑\nκ=1\nPr(k(Xn) = κ) Pr(k(Xn) > 0) Eg[US(Xn)\n2|k(Xn) = κ] ) − θ2.\n(5)\nWe will write y to denote a vector in Rn, the elements of which are y1, . . . , yn ∈ R. We also write yi:j to denote the ith through jth entries of y, i.e., yi:j := [yi, yi+1, . . . , yj−1, yj ]. Let Gnκ = {y ∈ Gn : k(y) = κ} be the set of all possible tuples of n samples where exactly κ are in C. We also overload the definition of g by defining g(y) := ∏n i=1 g(yi). Using this notation, we have that (where . . . are used to denote that a long line is split across multiple lines via scalar multiplication):\nEg[ US(Xn) 2|k(Xn) = κ]\n=\n∫\nGnκ\ng(y)\nPr(k(Xn) = κ) US(y)2 dy\n(a) =\n( n κ )\nPr(k(Xn) = κ)\n∫\nCκ\n∫\n(G\\C)n−κ g(y)US(y)2 dy1:κ dyκ+1:n\n(b) =\n( n κ )\nPr(k(Xn) = κ)\n∫\nCκ\n∫\n(G\\C)n−κ g(y1:κ)g(yκ+1:n) . . .\nUS(y1:κ) 2 dy1:κ dyκ+1:n\n=\n( n κ ) ( n κ ) cκ(1− c)n−κ ∫ Cκ g(y1:κ)US(y1:κ) 2 dy1:κ . . . ∫\n(G\\C)n−κ g(yκ+1:n) dyκ+1:n ︸ ︷︷ ︸ =(1−c)n−κ\n=\n( n κ ) (1− c)n−k(\nn κ\n) cκ(1− c)n−κ\n∫\nCκ g(y1:κ)\n( c\nκ\nκ∑\ni=1\nf(yi) g(yi) h(yi)\n)2 dy1:κ\n= c2\ncκ\n∫\nCκ g(y1:κ)\n( 1\nκ\nκ∑\ni=1\nf(yi) g(yi) h(yi)\n)2 dy1:κ\n(c) =c2\n∫\nCκ\ng(y1:κ)\nPr(k(Xκ) = κ)\n( 1\nκ\nκ∑\ni=1\nf(yi) g(yi) h(yi)\n)2 dy1:κ\n=c2Eg\n[( 1\nκ\nκ∑\ni=1\nf(Xi) g(Xi) h(Xi) )2∣∣∣∣∣Xκ ∈ C κ ]\n(d) =c2\n( 1\nκ v +E\n[ f(X)\ng(X) h(X)\n∣∣∣∣X ∼ g,X ∈ C ]2)\n=c2 ( 1\nκ v +\n(∫\nC\ng(x)\nc\nf(x) g(x) h(x) dx\n)2) = c2\nκ v + θ2, (6)\nwhere (a) comes from 1) the fact that there are ( n κ ) ways of ordering n elements such that κ are in C and n − κ are in G \\ C, and 2) the fact that US does not depend on the order of its inputs, (b) comes from 1) the property that US(y) does not change if additional samples are appended to y that are not in C and 2) the fact that g(y) can be decomposed into g(y1:κ)g(yκ+1:n) since it represents the joint probability density function for n independent and identically distributed random variables, (c) comes from the fact that Pr(k(Xκ) = κ) = cκ, and (d) comes from Property 2.\nCombining (5) with (6) we have that\nVarg(US(Xn)|k(Xn) > 0)\n=\n( n∑\nκ=1\nPr(k(Xn) = κ)\nPr(k(Xn) > 0)\n( c2\nκ v + θ2\n)) − θ2\n=c2v\n( n∑\nκ=1\nPr(k(Xn) = κ)\nPr(k(Xn) > 0)\n1\nκ\n)\n+ θ2\n( n∑\nκ=1\nPr(k(Xn) = κ)\nPr(k(Xn) > 0)\n)\n︸ ︷︷ ︸ =1\n−θ2\n=c2v n∑\nκ=1\nPr(k(Xn) = κ)\nPr(k(Xn) > 0)\n1\nκ\n=c2vEB(n,c)\n[ 1\nκ\n∣∣∣∣κ > 0 ] .\nTheorem 9. If F ∩H ⊆ G then\nVarg(IS(Xn)|k(Xn > 0)) = v c nρ + θ2 cρ(n− 1) + ρ− cn cnρ2 .\nProof. At a high level, this proof is similar to the proof of Theorem 8, but uses the property that IS(Xn) = k(Xn) cn US(Xn).\nVarg(IS(Xn)|k(Xn) > 0) =Eg[IS(Xn) 2|k(Xn) > 0]−Eg[IS(Xn)|k(Xn) > 0]2\n(a) =Eg[IS(Xn) 2|k(Xn) > 0]− (\nθ 1− (1− c)n )2\n=\n( n∑\nκ=1\nPr(k(Xn) = κ) Pr(k(Xn) > 0) Eg[IS(Xn)\n2|k(Xn) = κ] )\n− (\nθ 1− (1− c)n )2 , (7)\nwhere (a) comes from Theorem 6. Also,\nEg[ IS(Xn) 2|k(Xn) = κ]\n(a) =Eg\n[( k(Xn)\ncn US(Xn) )2∣∣∣∣∣k(Xn) = κ ]\n= κ2\nc2n2 Eg[US(Xn)\n2|k(Xn) = κ] (b)= κ 2\nc2n2\n( c2\nκ v + θ2\n) ,(8)\nwhere (a) holds because IS(Xn) = k(Xn)cn US(Xn) and (b) follows from (6). Using the shorthand, ρ := Pr(k(Xn) > 0) = 1 − (1 − c)n and by combining (7) with (8) we have\nthat:\nVarg(IS(Xn)|k(Xn) > 0)\n=\n( n∑\nκ=1\nPr(k(Xn) = κ)\nPr(k(Xn) > 0)\nκ2\nc2n2\n( c2\nκ v + θ2\n))\n− (\nθ 1− (1− c)n )2\n= v\nn2ρ\n( n∑\nκ=1\nPr(k(Xn) = κ)κ\n)\n︸ ︷︷ ︸ =EB(n,c)[κ]=nc\n+ θ2\nc2n2ρ\n( n∑\nκ=1\nPr(k(Xn) = κ)κ 2\n)\n︸ ︷︷ ︸ =EB(n,c)[κ2]=nc((n−1)c+1)\n− ( θ\nρ\n)2\n=v c nρ + θ2((n− 1)c+ 1) cnρ − θ 2 ρ2\n=v c\nnρ + θ2 cρ(n− 1) + ρ− cn cnρ2 .\nTheorem 10. If F ∩H ⊆ G then\nVarg(US(Xn)) = ρc 2vEB(n,c)\n[ 1\nκ\n∣∣∣∣κ > 0 ] +θ2ρ(1−ρ)."
    }, {
      "heading" : "Proof.",
      "text" : "Varg(US(Xn)) = Eg[US(Xn) 2]−Eg[US(Xn)]2\n(a) =Eg[US(Xn) 2]− ρ2θ2\n=\n( n∑\nκ=0\nPr(k(Xn) = κ)Eg[US(Xn) 2|k(Xn) = κ]\n)\n− ρ2θ2\n=Pr(k(Xn) = 0)Eg[US(Xn) 2|k(Xn) = 0]︸ ︷︷ ︸ =0\n+\n( n∑\nκ=1\nPr(k(Xn) = κ)Eg[US(Xn) 2|k(Xn) = κ]\n)\n− ρ2θ2\n(b) =ρ\n( n∑\nκ=1\nPr(k(Xn) = κ)\nρ\n( c2\nκ v + θ2\n)) − ρ2θ2\n=ρc2v\n( n∑\nκ=1\nPr(k(Xn) = κ)\nρ\n1\nκ\n)\n+ ρθ2\n( n∑\nκ=1\nPr(k(Xn) = κ)\nρ\n)\n︸ ︷︷ ︸ =1\n−ρ2θ2\n=ρc2vEB(n,c)\n[ 1\nκ\n∣∣∣∣κ > 0 ] + θ2ρ(1− ρ),\nwhere (a) comes from Theorem 7, (b) comes from (6) and from multiplying one term by ρ/ρ = 1.\nTheorem 11. If F ∩H ⊆ G then\nVarg(IS(Xn)) = 1\nn\n( cv + θ2 ( 1 c − 1 )) ."
    }, {
      "heading" : "Proof.",
      "text" : "Varg(IS(Xn)) (a) =\n1 n Varg(IS(X))\n= 1\nn\n( Eg[IS(X) 2]−Eg[IS(X)]2 )\n(b) = 1\nn\n( Eg[IS(X) 2]− θ2 )\n= 1\nn\n( Pr(X ∈ C|X ∼ g)Eg[IS(X)2|X ∈ C]\n+ Pr(X 6∈ C|X ∼ g)Eg[IS(X)2|X 6∈ C]︸ ︷︷ ︸ =0\n−θ2 )\n= 1\nn\n( cEg[IS(X) 2|X ∈ C]− θ2 )\n(c) = 1\nn\n( c ( v + θ2\nc2\n) − θ2\n)\n= 1\nn\n( cv + θ2 ( 1 c − 1 )) ,\nwhere (a) holds because IS(Xn) is the sum of n independent and identically distributed random variables, (b) comes from Property 1, and (c) comes from applying (8) with n = 1 and κ = 1. Property 3. cρ(n− 1) + ρ− cn ≥ 0, Proof. Recall that ρ := 1− (1− c)n, so we have that: cρ(n− 1) + ρ− cn = c(1− (1− c)n)(n− 1) + 1− (1− c)n − cn\n=(cn− c)(1− (1− c)n) + 1− (1− c)n − cn =cn− cn(1− c)n − c+ c(1− c)n + 1− (1− c)n − cn =(1− c)n(−cn+ c− 1)− c+ 1. (9) We will show by induction that (9) is non-negative for all n ≥ 1. First, notice that for the base case where n = 1, (9) is equal to zero. For the inductive step we will show that (9) is non-negative for n+1 given that it is non-negative for n.\n(1− c)n+1(−c(n+ 1) + c− 1)− c+ 1 =(1− c)(1− c)n(−cn+ c− 1)− (1− c)n+1c\n+ (−c+ 1)(1− c+ c) =(1− c) ( (1− c)n(−cn+ c− 1)− c+ 1 )\n︸ ︷︷ ︸ (a)\n− (1− c)n+1c+ c(1− c), where (a) is positive by the inductive hypothesis, and so we need only show that −(1− c)n+1c+ c(1− c) ≥ 0. Since −(1− c)n+1c+ c(1− c) =c ( (1− c)− (1− c)n+1 ) ,\nand 1−c ≥ (1−c)n+1 because c ∈ (0, 1], we conclude."
    } ],
    "references" : [ {
      "title" : "Model-free intelligent diabetes management using machine learning",
      "author" : [ "M. Bastani" ],
      "venue" : "Master’s thesis, Department of Computing Science, University of Alberta,",
      "citeRegEx" : "Bastani.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bastani.",
      "year" : 2014
    }, {
      "title" : "The uva/padova type 1 diabetes simulator new features",
      "author" : [ "C. Dalla Man", "F. Micheletto", "D. Lv", "M. Breton", "B. Kovatchev", "C. Cobelli" ],
      "venue" : "Journal of diabetes science and technology,",
      "citeRegEx" : "Man et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Man et al\\.",
      "year" : 2014
    }, {
      "title" : "Doubly robust policy evaluation and learning",
      "author" : [ "M. Dudı́k", "J. Langford", "L. Li" ],
      "venue" : "In Proceedings of the TwentyEighth International Conference on Machine Learning,",
      "citeRegEx" : "Dudı́k et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudı́k et al\\.",
      "year" : 2011
    }, {
      "title" : "Monte carlo methods for solving multivariable problems",
      "author" : [ "J.M. Hammersley" ],
      "venue" : "Annals of the New York Academy of Sciences,",
      "citeRegEx" : "Hammersley.,? \\Q1960\\E",
      "shortCiteRegEx" : "Hammersley.",
      "year" : 1960
    }, {
      "title" : "Monte carlo methods, methuen & co",
      "author" : [ "J.M. Hammersley", "D.C. Handscomb" ],
      "venue" : null,
      "citeRegEx" : "Hammersley and Handscomb.,? \\Q1964\\E",
      "shortCiteRegEx" : "Hammersley and Handscomb.",
      "year" : 1964
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Hoeffding.,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding.",
      "year" : 1963
    }, {
      "title" : "Doubly robust off-policy value evaluation for reinforcement learning",
      "author" : [ "N. Jiang", "L. Li" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Jiang and Li.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jiang and Li.",
      "year" : 2016
    }, {
      "title" : "Use of different Monte Carlo sampling techniques",
      "author" : [ "H. Kahn" ],
      "venue" : "Technical Report P-766,",
      "citeRegEx" : "Kahn.,? \\Q1955\\E",
      "shortCiteRegEx" : "Kahn.",
      "year" : 1955
    }, {
      "title" : "Concentration Inequalities and Model Selection",
      "author" : [ "P. Massart" ],
      "venue" : null,
      "citeRegEx" : "Massart.,? \\Q2007\\E",
      "shortCiteRegEx" : "Massart.",
      "year" : 2007
    }, {
      "title" : "Simulation and the Monte Carlo method",
      "author" : [ "R. Rubinstein" ],
      "venue" : null,
      "citeRegEx" : "Rubinstein.,? \\Q1981\\E",
      "shortCiteRegEx" : "Rubinstein.",
      "year" : 1981
    }, {
      "title" : "Data-efficient off-policy policy evaluation for reinforcement learning",
      "author" : [ "P.S. Thomas", "E. Brunskill" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Thomas and Brunskill.,? \\Q2016\\E",
      "shortCiteRegEx" : "Thomas and Brunskill.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "A classical approach to this problem is to use importance sampling (IS), which reweighs the observed samples to account for the difference between the target and sampling distributions (Kahn, 1955).",
      "startOffset" : 185,
      "endOffset" : 197
    }, {
      "referenceID" : 9,
      "context" : "By contrast, weighted importance sampling (Rubinstein, 1981) is another variant of importance sampling that can reduce variance, but which introduces bias that makes it incompatible with Hoeffding’s inequality.",
      "startOffset" : 42,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "A control variate is a constant, t ∈ R, that is subtracted from each h(Xi) and then added back to the final estimate, as in (1) (Hammersley, 1960; Hammersley and Handscomb, 1964).",
      "startOffset" : 128,
      "endOffset" : 178
    }, {
      "referenceID" : 4,
      "context" : "A control variate is a constant, t ∈ R, that is subtracted from each h(Xi) and then added back to the final estimate, as in (1) (Hammersley, 1960; Hammersley and Handscomb, 1964).",
      "startOffset" : 128,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias—a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dudı́k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016).",
      "startOffset" : 248,
      "endOffset" : 317
    }, {
      "referenceID" : 6,
      "context" : "That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias—a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dudı́k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016).",
      "startOffset" : 248,
      "endOffset" : 317
    }, {
      "referenceID" : 10,
      "context" : "That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias—a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dudı́k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016).",
      "startOffset" : 248,
      "endOffset" : 317
    }, {
      "referenceID" : 8,
      "context" : "Finally, we consider the use of IS and US to create highconfidence upper and lower bounds on θ using a concentration inequality (Massart, 2007) like Hoeffding’s inequality (Hoeffding, 1963).",
      "startOffset" : 128,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "Finally, we consider the use of IS and US to create highconfidence upper and lower bounds on θ using a concentration inequality (Massart, 2007) like Hoeffding’s inequality (Hoeffding, 1963).",
      "startOffset" : 172,
      "endOffset" : 189
    }, {
      "referenceID" : 0,
      "context" : "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014).",
      "startOffset" : 127,
      "endOffset" : 142
    }, {
      "referenceID" : 0,
      "context" : "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014). Here we focus on a sub-problem of improving CR and CF—using data collected from an initial range of admissible values of CR and CF to predict how well a new range of values for CR and CF would perform. When collecting data, CR and CF are drawn uniformly from an initial admissible range, and then used for one day (which we view as one episode of a Markov decision process). The performance during each day is measured using an objective function similar to the reward function proposed by Bastani (2014), which measures the deviation of blood glucose from optimum levels, with larger penalties for low blood glucose levels.",
      "startOffset" : 128,
      "endOffset" : 649
    }, {
      "referenceID" : 0,
      "context" : "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014). Here we focus on a sub-problem of improving CR and CF—using data collected from an initial range of admissible values of CR and CF to predict how well a new range of values for CR and CF would perform. When collecting data, CR and CF are drawn uniformly from an initial admissible range, and then used for one day (which we view as one episode of a Markov decision process). The performance during each day is measured using an objective function similar to the reward function proposed by Bastani (2014), which measures the deviation of blood glucose from optimum levels, with larger penalties for low blood glucose levels. We refer to the measure of how good the outcome was from one day as the return associated with that day, with larger values being better. Using approximately 30 days of data, our goal is to estimate the expected return if a different distribution of CR and CF were to be used. We consider a specific in silico person—a person simulated using a metabolic simulator. We used the subject “Adult#003” in the Type 1 Diabetes Metabolic Simulator (T1DMS) (Dalla Man et al., 2014)—a simulator that has been approved by the US Food and Drug Administration as a substitute for animal trials in pre-clinical testing of treatment policies for type 1 diabetes. During each day, the subject is given three or four meals of randomized sizes at randomized times, similar to the experimental setup proposed by Bastani (2014). As a result of this randomness, and the stochastic nature of the T1DMS model, applying the same values of CR and CF can produce different returns if used for multiple days.",
      "startOffset" : 128,
      "endOffset" : 1577
    }, {
      "referenceID" : 0,
      "context" : "If follow-up visits are not an option, recent research has suggested using reinforcement learning algorithms to tune CR and CF (Bastani, 2014). Here we focus on a sub-problem of improving CR and CF—using data collected from an initial range of admissible values of CR and CF to predict how well a new range of values for CR and CF would perform. When collecting data, CR and CF are drawn uniformly from an initial admissible range, and then used for one day (which we view as one episode of a Markov decision process). The performance during each day is measured using an objective function similar to the reward function proposed by Bastani (2014), which measures the deviation of blood glucose from optimum levels, with larger penalties for low blood glucose levels. We refer to the measure of how good the outcome was from one day as the return associated with that day, with larger values being better. Using approximately 30 days of data, our goal is to estimate the expected return if a different distribution of CR and CF were to be used. We consider a specific in silico person—a person simulated using a metabolic simulator. We used the subject “Adult#003” in the Type 1 Diabetes Metabolic Simulator (T1DMS) (Dalla Man et al., 2014)—a simulator that has been approved by the US Food and Drug Administration as a substitute for animal trials in pre-clinical testing of treatment policies for type 1 diabetes. During each day, the subject is given three or four meals of randomized sizes at randomized times, similar to the experimental setup proposed by Bastani (2014). As a result of this randomness, and the stochastic nature of the T1DMS model, applying the same values of CR and CF can produce different returns if used for multiple days. After analyzing the performance of many CR and CF pairs, we selected an initial range that results in good performance: CR ∈ [8.5, 11] and CF ∈ [10, 15]. Using a large number of samples, we computed an estimate of the expected return if different CR and CF values are used for a single day—this estimate is depicted in Figure 2. As described by Bastani (2014), when the value of CR is set appropriately, performance is robust to changes in CF.",
      "startOffset" : 128,
      "endOffset" : 2111
    } ],
    "year" : 2016,
    "abstractText" : "Importance sampling is often used in machine learning when training and testing data come from different distributions. In this paper we propose a new variant of importance sampling that can reduce the variance of importance samplingbased estimates by orders of magnitude when the supports of the training and testing distributions differ. After motivating and presenting our new importance sampling estimator, we provide a detailed theoretical analysis that characterizes both its bias and variance relative to the ordinary importance sampling estimator (in various settings, which include cases where ordinary importance sampling is biased, while our new estimator is not, and vice versa). We conclude with an example of how our new importance sampling estimator can be used to improve estimates of how well a new treatment policy for diabetes will work for an individual, using only data from when the individual used a previous treatment policy. Introduction A key challenge in artificial intelligence is to estimate the expectation of a random variable. Instances of this problem arise in areas ranging from planning and decision making (e.g., estimating the expected sum of rewards produced by a policy for decision making under uncertainty) to probabilistic inference. Although the estimation of an expected value is straightforward if we can generate many independent and identically distributed (i.i.d.) samples from the relevant probability distribution (which we refer to as the target distribution), we may not have generative access to the target distribution. Instead, we might only have data from a different distribution that we call the sampling distribution. For example, in off-policy evaluation for reinforcement learning, the goal is to estimate the expected sum of rewards that a decision policy will produce, given only data gathered using some other policy. Similarly, in supervised learning, we may wish to predict the performance of a regressor or classifier if it were to be applied to data that comes from a distribution that differs from the distribution of the available data (e.g., we might predict the accuracy of a classifier for hand-written letters given that observed letter frequencies come from English, using a corpus of labeled letters collected from German documents). More precisely, we consider the problem of estimating θ := E[h(X)], where h is a real-valued function and the expectation is over the random variable X , which is a sample from the target distribution. As input we assume access to n i.i.d. samples from a sampling distribution that is different from the target distribution. A classical approach to this problem is to use importance sampling (IS), which reweighs the observed samples to account for the difference between the target and sampling distributions (Kahn, 1955). Importance sampling produces an unbiased but often highvariance estimate of θ. We introduce importance sampling with unequal support (US)—a simple new importance sampling estimator that can drastically reduce the variance of importance sampling when the supports of the sampling and target distributions differ. This setting with unequal support can occur, for example, in our earlier example where German documents might include symbols like ß, that the classifier will not encounter. US essentially performs importance sampling only on the data that falls within the support of the target distribution, and then scales this estimate by a constant that reflects the relative support of the target and sampling distributions. US typically has lower variance than ordinary importance sampling (sometimes by orders of magnitude), and is unbiased in the important setting where at least one sample falls within the support of the target distribution. If no samples do, then none of the available data could have been generated by the target distribution, and so it is unclear what would make for a reasonable estimate. Furthermore, the conditionally unbiased nature of US is sufficient to allow for its use with concentration inequalities like Hoeffding’s inequality to construct confidence bounds on θ. By contrast, weighted importance sampling (Rubinstein, 1981) is another variant of importance sampling that can reduce variance, but which introduces bias that makes it incompatible with Hoeffding’s inequality. Problem Setting and Importance Sampling Let f and g be probability density functions (PDFs) for two distributions that we call the target distribution and sampling distribution, respectively. Let h : R → R be called the evaluation function. Let θ := Ef [h(X)], where Ef denotes the expected value given that f is the PDF of the random variable(s) in the expectation (in this case, just X). Let F := {x ∈ R : f(x) 6= 0}, G := {x ∈ R : g(x) 6= 0}, and H := {x ∈ R : h(x) 6= 0} be the supports of the target and ar X iv :1 61 1. 03 45 1v 1 [ cs .L G ] 1 0 N ov 2 01 6 sampling distributions, and the evaluation function, respectively. In this paper we will discuss techniques for estimating θ given n ∈ N>0 i.i.d. samples, Xn := {X1, . . . , Xn}, from the sampling distribution, and we focus on the setting where F ∩H ⊂ G—where the joint support of F and H is a strict subset of the support of G. The importance sampling estimator, IS(Xn) := t+ 1 n n ∑ i=1 f(Xi) g(Xi) (h(Xi)− t), (1) is a widely used estimator of θ, where t = 0 (we consider non-zero values of t later). If F ∩ H ⊆ G, then IS(Xn) is a consistent and unbiased estimator of θ. That is, IS(Xn) a.s. −→ θ and Eg[IS(Xn)] = θ (we review this latter result in Property 1 in the supplemental document). A control variate is a constant, t ∈ R, that is subtracted from each h(Xi) and then added back to the final estimate, as in (1) (Hammersley, 1960; Hammersley and Handscomb, 1964). Although control variates, t(Xi), that depend on the sample, Xi, can be beneficial, for our later purposes we only consider constant control variates. Intuitively, including a constant control variate equates to estimating θ′ := Ef [h′(X)] using importance sampling without a control variate, where h′(x) = h(x) − t, and then adding t to the resulting estimate to get an estimate of θ. Later we show that the variance of importance sampling increases with θ, and so applying importance sampling to h results in higher variance than applying importance sampling to h′ with t ≈ θ, since then θ′ ≈ 0. That is, by inducing a kind of normalization, a control variate can reduce the variance of estimates without introducing bias—a property that has made the inclusion of control variates a popular topic in some recent works using importance sampling (Dudı́k et al., 2011; Jiang and Li, 2016; Thomas and Brunskill, 2016). Although later we discuss control variates more, for simplicity our derivations focus on importance sampling estimators without control variates. There are also other extensions of the importance sampling estimator that can reduce variance—notably the weighted importance sampling estimator, which we compare to later, and which can provide large reductions of variance and mean squared error, but which introduces bias. An Illustrative Example In this section we present an example that highlights the peculiar behavior of the IS estimator when F ∩ H 6= G. Let g(x) = 0.5 if x ∈ [0, 2] and g(x) = 0 otherwise, and let f(x) = 1 if x ∈ [0, 1] and f(x) = 0 otherwise. So, F = [0, 1] and G = [0, 2]. Let h(x) = 1 if x ∈ [0, 1] and h(x) = 0 otherwise, so that H = [0, 1]. Notice that θ = 1. Since the sampling and target distributions are both uniform, an obvious estimator of θ (if f and g are known but h is not) would be the average of the points that fall within F . Let (#Xi ∈ F ) denote the number of samples in Xn that are in F . Formally, the obvious estimator is θ̂ := 1 (#Xi ∈ F ) n ∑ i=1 1F (Xi)h(Xi), where 1A(x) = 1 if x ∈ A and 1A(x) = 0 otherwise. Given our knowledge of h, it is straightforward to show that this estimator is equal to 1 if (#Xi ∈ F ) > 0 and is undefined otherwise—it is exactly correct (has zero bias and variance) as long as at least one sample falls within F . If no samples fall within F , then we have only observed data that will never occur under the target distribution, and so we have no useful information about θ. In this case, we might define our obvious estimator to return an arbitrary value, e.g., zero. Perhaps surprisingly, the importance sampling estimator does not degenerate to this obvious estimator:",
    "creator" : "LaTeX with hyperref package"
  }
}