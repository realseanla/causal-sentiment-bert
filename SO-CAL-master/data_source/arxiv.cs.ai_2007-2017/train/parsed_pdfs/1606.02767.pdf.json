{
  "name" : "1606.02767.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Theoretical Robopsychology: Samu Has Learned Turing Machines",
    "authors" : [ "Norbert Bátfai" ],
    "emails" : [ "batfai.norbert@inf.unideb.hu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Samu is a disembodied developmental robotic experiment to develop a family chatterbot agent who will be able to talk in a natural language like humans do [Bát15a]. At this moment it is only an utopian idea of the project Samu. The practical purpose of Samu projects is to develop computational mental organs that can support software agents to acquire higher-order knowledge from their input [BB16]. The activities have been conducted during the development of such mental organs may be considered as first efforts to create on demand the Asimovian profession called robopsychology1 [Bát16a].\nThe roots of this paper lie in the two new software experiments Samu Turing [Bát16c] and Samu C. Turing [Bát16b]. These are very simplified versions of the former habituation-sensitization [CS14] based (like for example SamuBrain [Bát16d] or SamuKnows [Bát16e]) learning projects of Samu. Their common feature is that they use the same COP-based Q-learning engine that the chatbot Samu does. To be more precise the mental organs use the same code\n1https://en.wikipedia.org/wiki/Robopsychology\nar X\niv :1\n60 6.\n02 76\n7v 2\n[ cs\n.A I]\n2 3\nJu n\n(to see this compare https://github.com/nbatfai/SamuLife/blob/master/ SamuQl.h with https://github.com/nbatfai/nahshon/blob/master/ql.hpp) as the chatbot does. The term “COP-based” (Consciousness Oriented Programming [Bát11]) means that the engine predicts its future input. The engine itself is based on the Q-learning that receives positive reinforcement if the chatbot (or a mental organ) can successfully forecast the next input of Q-learning in the actual step. In this case the previous output (the previous prediction) is the same as the actual input, for precise details see [Bát15a] and [BB16]). In the two new experiments in question the transition rules of Turing machines (TMs) have been learned as it is illustrated in Fig 1. It should be noticed that neither these experiments nor this paper focus on the habituation-based learning because the learning agent knows the model (TM) that generates the reality. Our motivation\nto write this paper stems from the last paragraph of the work of Neumann on the general theory of automata [vN51] where Neumann had suggested that there is a complexity level above which the machines can reproduce themselves and even more complicated ones. Neumann investigated the self-reproducing automata [vN51] roughly a decade after Alan Turing had published his work on universal simulation theorem [Tur36]. The Turing machine is a precise form of the informal notion of the algorithm to describe algorithms. If this description algorithm has been applied to describe itself then this brings us to Turing’s notion of the universal machine. In an intuitive sense we can say that Neumann replaced Turing’s notion of simulation with the notion of reproduction. In this work we would like to replace the reproduction with the learning. To be more precise we investigate algorithms to write algorithms. For simplicity of our discussion the scope of this paper is constrained to Turing machines. It should be noticed that we could have used other universal computing models such as the Cellular Automata. For example, the first mental organs had learned the Conway’s Game of Life [BB16] (or see the YouTube video at https://youtu.be/_W0Ep2HpJSQ). But in spite of this, we chose Turing machines because they are closer to the programmers’ intuition.\nThe structure of this paper is as follows: the next section introduces the basic notations. Then, in Sect. 3 we present the results of two Samu-based developmental robotic software experiments to learn how Turing machines operate. Here we investigate some specific TMs. It should be noticed that some of them, such as the machines of Schult and Uhing or the Marxen and Buntrock’s BB5 champion machine are famous in the field of the Radó Tibor’s Busy Beaver problem [LV08]. It is worth noting that despite that this problem is a very interesting theoretical computer science problem we do not address it in this paper. We introduce of the learning problem and give the basic notions of this subject. Finally we present a new complexity measure called self-reproduction complexity and we show in Subsect. 3.2.3 that it is reasonable to use machine\nlearning algorithm for learning Turing machines. The paper is closed by a short conclusion in which some possible directions for further work are pointed out."
    }, {
      "heading" : "2 Notations and Technical Background",
      "text" : "Throughout both this article and our software experiments we use the definition of the Turing machine (TM) that was introduced in [Bát09] and also used in [Bát15b] where the Turing machine was defined by a quadruple T = (Q, 0, {0, 1}, f) where f : Q× {0, 1} → Q× {0, 1} × {←, ↑,→} is a partial transition function and 0 ∈ Q ⊂ N is the starting state. As usual a configuration determines the actual state of the head, the position of the head and the contents of the tape. With the notation of [Bát09] a configuration can be written in the form wbefore[q > wafter, where wbefore, wafter ∈ {0, 1}∗ and q ∈ Q.\nIn some proofs for simplicity’s sake we use multitape Turing machines or the blank symbol on the tape (that is the tape alphabet is extended by the symbol ). In addition, without limiting the generality, we may assume that halting Turing machines (with a given input) do not contain unused transition rules. The notation T (x) < ∞ denotes that the machine T with the input x halts.\nDefinition 2.0.1 (configN). The word bN . . . b1[q > a0a1 . . . aN over the alphabet {0, 1, [, >} ∪Q where ai, bj ∈ {0, 1} is referred to as a configN configuration if there is a configuration wbefore[q > wafter such that wbefore[q > wafter = w,beforebN . . . b1[q > a0a1 . . . aNw , after.\nRemark 2.0.1 (config∞). In some cases, see for example Remark 3.2.1, we extend the definition of the configuration as follows ∞wbefore[q > wafter\n∞. In this sense a usual configuration corresponds to a config∞ configuration where w,before = w , after = λ the empty word.\nWe may note that the release of the project Samu C. Turing used in Fig. 2 uses config4 configurations."
    }, {
      "heading" : "3 Learning by Listening and Doing",
      "text" : "In the aforementioned projects Samu Turing and Samu C. Turing we programmed the Samu agent to work in a similar way as, for example, Professor James Harland did in his work [Har16] where he observed and studied the configurations of Marxen and Buntrock’s Busy Beaver champion machines [MB90]. In our experiments the agent Samu observes (listening) the consecutive subconfigurations of a given investigated Turing machine and try to predict (doing) the next rule of the machine that will be applied. From this viewpoint this whole learning process can be seen as a way of learning by listening and doing where the listening part is the sensation of the agent and doing is the prediction of the agent. But the question may naturally be raised why should we use agent technology and machine learning algorithms to learn Turing machines? Our explicit answer is based on the following intuitive results and it will be found in Sect. 3.2.3.\nSome running and learning times"
    }, {
      "heading" : "3.1 Some Intuitive Results",
      "text" : "Fig. 2 summarizes and compares some running results produced by the project Samu C. Turing. The numbers of two kinds of running times (usual time complexity and “learning complexity”, see the caption of the figure for details) are not directly comparable because they use different scales to compute the y-axis values. One of the two curves is computed by the number of steps of a Turing machine and the other by the number of sensory-action pairs of the reinforcement learning agent Samu C. Turing. The exact values can be found in Table 1. One of the notions of cognitive complexity defined in Subsect. 3.2.3 will be based on this intuitive “learning complexity”. In Fig. 2, it seems that the growth rate of the learning time is related to the running time. It is worth to compare this with Fig. 6 where the growth rate of an another (the “self-reproducing”) complexity has already been separated from the running time."
    }, {
      "heading" : "3.2 The Basic Notions of the Subject",
      "text" : "From the observations of the two experiments above, we can build the abstract model of learning that is referred to as the learning problem. The learning\nproblem of learning TMs is divided into two parts. The first is a simulation of the TM to be learned. The second is the actual learning problem itself. Fig. 3 shows the schematic of the learning problem where the UTM R takes the description of the machine T and an x input of T . Then R has collected the configurations of T whilst it is simulating T with x. After the simulation S takes the collected configurations and it must try to figure out what TM was actually simulating."
    }, {
      "heading" : "3.2.1 The Running Problem",
      "text" : "It is obvious that the running problem trivially contains the halting problem. Therefore we may notice that similar undecidable statements can be made for this case as well but in this paper we only focus on halting machines.\nLemma 3.2.1. Apart from the trivial case of the empty tapes, the transition rule between two consecutive configurations ci and ci+1 is uniquely determined by the configurations ci and ci+1.\nProof. Suppose that there are two transition rules (q, r) → (q1, w1, d1) and (q, r) → (q2, w2, d2) where q, q1, q2 ∈ Q, r, w1, w2 ∈ {0, 1, }, d1, d2 ∈ {←, ↑,→} and then we show that q1 = q2, w1 = w2 and d1 = d2.\nLet ci = Ll[q > rR where l ∈ {0, 1, }, L,R ∈ {0, 1, }∞, Then the following cases are possible\nci+1 = Ll[q1 > w1R, (d1 =↑)  = Ll[q2 > w2R, (d2 =↑) ⇔ (q1 = q2, w1 = w2) = L[q2 > lw2R, (←) ⇔ (q1 = q2, Ll = L, w1R = lw2R that is, iff l, w1, w2 = and R,L = ∞) = Llw2[q2 > R, (→) ⇔ (q1 = q2, Ll = Llw2, w1R = R that is, iff\nl, w1, w2 = and R,L = ∞)\nci+1 = L[q1 > lw1R, (←)  = L[q2 > lw2R, (←) ⇔ (q1 = q2, w1 = w2) = Llw2[q2 > R, (→)⇔ (q1 = q2, L = Llw2, lw1R = R that is, iff\nl, w1, w2 = and R,L = ∞)\nci+1 = Llw1[q1 > R, (→)\n{ = Llw2[q2 > R, (→) ⇔ (q1 = q2,\nw1 = w2)\nRemark 3.2.1. It is noted that we may give an even more simpler lemma and proof using the usual ∗ and {0, 1, }∗ instead of ∞ and {0, 1, }∞. We use the latter because they are closer to the programmers’ intuition.\nTheorem 3.2.2 (Universal Learning). There exist an universal running machine R and a learning machine S such that, for all halting Turing machines T , it holds that S(R(T, x)) = T .\nProof. The proof is divided into two parts: in the first one, we modify the usual proof of Turing’s universal simulation theorem (see for example the textbook [ISR00]) to produce the sequence of configurations of T by the universal machine R. In the other part we focus the learning of S by using the previous lemma.\nWe provide only an outline of the first part. We use a multitape TM for the implementation of R. Fig. 4 shows the preparation of the tapes before starting the simulation of T . The tapes are shown in Fig. 5 after the simulation of the i-th step of T .\nThen the theorem follows from Lemma 3.2.1."
    }, {
      "heading" : "3.2.2 The Learning Problem",
      "text" : "The previous theorem shows that there is no problem with learning if we use config∞ (or the usual) configurations. But otherwise, as shown in the following two simple examples of config2 configurations (Example 3.2.1 and 3.2.2) the applied transition rule between two consecutive configN configurations may be\nnot uniquely determined by the configN configurations. If we use configN configurations instead of the usual or config∞ configurations then the Lemma 3.2.1 does not hold. In the next subsection a notion of complexity will be exactly based on this property.\nExample 3.2.1. Let ci = ∞11111[q > 11111 ∞ be a config∞ configuration and c,i be a corresponded config2 configuration. Then the rules (q, 1)→ (q, 1,←), (q, 1)→ (q, 1,→), and (q, 1)→ (q, 1, ↑) yield the same c,i+1 = 11[q > 11 config2 configuration.\nExample 3.2.2. Let ci = ∞0101[q > 1101 ∞ be a config∞ configuration and c,i be a corresponded config2 configuration. Then the rules (q, 1)→ (q, 0,←) and (q, 1)→ (q, 0,→) yield the same c,i+1 = 10[q > 10 config2 configuration."
    }, {
      "heading" : "3.2.3 Cognitive Complexities",
      "text" : "As has already been mentioned in Sect. 3.1 we intuitively use the running time of the learning machines as a complexity measure that may be formulated as follows cc(T, x) = min{tS (〈ci〉xT ) |T (x) <∞, S(R(T, x)) = T} but it does not seem very helpful because it is probably correlated with the usual time complexity of T as it is suggested by Fig. 2. The next type of complexity tells what is the first finite N for which Lemma 3.2.1 holds with using the configurations configN . To be more precise, it is defined as\ncc∗(T, x) = min{N |T (x) <∞, S(R(T, x)) = T and for configN the lemma 3.2.1 holds}\nthat has shown different behavior than the previous one as it can be seen in Fig. 6 The growth rate of the investigated cc∗ values not related to the number of ones rather than to the running time (see “14”, “21” and “1471”).\nThe results shown in Fig. 6 also suggest that it is hopeless to handle the learning problem with the universal learning machine S of Lemma 3.2.1. This justifies the using of agent technology (an agent observes the operation of the investigated TMs) and machine learning algorithms (such as Q-learning) to learn Turing machines instead of searching for suitable configNs for any universal learning machine S."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this paper, we started with two developmental robotic software experiments Samu Turing [Bát16c] and Samu C. Turing [Bát16b] to learn how Turing ma-\nSome self−reproduction complexities\nchines operate. This subject of the experiments itself enabled us to investigate the theoretical properties of learning. First, we have eliminated from our software experiments the developmental robotic processes (for example the habituation-sensitization parts) and then we introduced the problem of learning and some complexity measures based on it. For some cases of given TMs we also determine these complexities. The cc∗ of machines of greater sophistication cannot easily be computed by the universal learning machine S of Theorem 3.2.2. This justifies the usage of agent technology and machine learning for learning Turing machines. We have provided only an outline of the proof of Theorem 3.2.2. To complete it may be a further theoretical computer science work. Further work of a practical robopsychological nature is also needed. For example, we are going to investigate using Samu’s neural architecture [Bát15a], Samu mental organs (like MPUs) [BB16] and deep learning to learn how TMs operate.\nTo return to Neumann’s train of thought mentioned in the introduction it seems to be interesting to study when the learning algorithm has been applied to write itself. Let’s start from a machine T that halts with x. It follows from Theorem 3.2.2 that R(T, x) = 〈ci〉xT and S(R(T, x)) = T . But then we can also learn this learning of T , that is R(S, 〈ci〉xT ) = 〈ci〉 〈ci〉xT S and S(R(S, 〈ci〉 x T )) = S. And then we can learn again the learning of learning of T , that is, to be more\nprecise R(S, 〈ci〉 〈ci〉xT S ) = 〈ci〉\n〈ci〉 〈ci〉 x T\nS\nS and so on. If we introduce the notation\nyj = 〈ci〉 . . . 〈ci〉\n〈ci〉 x T S\nS\nthen we can easily write that cc(S, yj) < cc(S, yj+1) because tS(y j) < tS(y j+1) but the similar relation between cc∗(S, yj) and cc∗(S, yj+1) is an open question at this moment.\nIt is clear, of course, that further work of a theoretical robopsychological nature is required as well. For example, we are going to find possible relations among the time, space, Kolmogorov and cognitive complexities. We believe that this is a necessary step towards achieving the situation that has been defined as “Programs hacking programs” by Neo in the movie “The Matrix Reloaded”. In the framework of Turing machines and Busy Beaver problem this quotation has a special meaning namely that can we program a computer program not only to discover a BB machine but to build it from scratch?"
    }, {
      "heading" : "5 Acknowledgment",
      "text" : "The author would like to thank his students in “High Level Programming Languages” course in the spring semester of 2015/2016 at the University of Debrecen for testing the Samu projects. He would also like to thank the members of some AI-specific communities on Facebook, Google+ and Linkedin and especially his group called DevRob2Psy at https://www.facebook.com/groups/ devrob2psy/ for their interest."
    } ],
    "references" : [ {
      "title" : "CoRR",
      "author" : [ "Norbert Bátfai. On the Running Time of the Shortest Programs" ],
      "venue" : "abs/0908.1159,",
      "citeRegEx" : "Bát09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "CoRR",
      "author" : [ "Norbert Bátfai. Conscious Machines", "Consciousness Oriented Programming" ],
      "venue" : "abs/1108.2865, 2011. http://arxiv.org/abs/",
      "citeRegEx" : "Bát11",
      "shortCiteRegEx" : null,
      "year" : 1108
    }, {
      "title" : "CoRR",
      "author" : [ "Norbert Bátfai. A disembodied developmental robotic agent called Samu Bátfai" ],
      "venue" : "abs/1511.02889, 2015. http://arxiv.org/abs/",
      "citeRegEx" : "Bát15a",
      "shortCiteRegEx" : null,
      "year" : 1511
    }, {
      "title" : "Are there intelligent Turing machines? CoRR",
      "author" : [ "Norbert Bátfai" ],
      "venue" : "abs/1503.03787,",
      "citeRegEx" : "Bát15b",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "GitHub Project",
      "author" : [ "Norbert Bátfai. How to Become a Robopsychologist" ],
      "venue" : "https://github.com/nbatfai/Robopsychology/files/169195/ robopsychology.pdf,",
      "citeRegEx" : "Bát16a",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "https://github",
      "author" : [ "Norbert Bátfai. Samu C. Turing. GitHub Project" ],
      "venue" : "com/nbatfai/SamuCTuring, (visited: 2016-06-04),",
      "citeRegEx" : "Bát16b",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "https://github",
      "author" : [ "Norbert Bátfai. Samu Turing. GitHub Project" ],
      "venue" : "com/nbatfai/SamuTuring, (visited: 2016-06-04),",
      "citeRegEx" : "Bát16c",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "GitHub Project",
      "author" : [ "Norbert Bátfai. SamuBrain" ],
      "venue" : "https://github.com/ nbatfai/SamuBrain, (visited: 2016-06-04),",
      "citeRegEx" : "Bát16d",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "GitHub Project",
      "author" : [ "Norbert Bátfai. SamuKnows" ],
      "venue" : "https://github.com/ nbatfai/SamuKnows, (visited: 2016-06-04),",
      "citeRegEx" : "Bát16e",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Robopsychology Manifesto: Samu in His Prenatal Development",
      "author" : [ "Norbert Bátfai", "Renátó Besenczi" ],
      "venue" : "submitted manuscript,",
      "citeRegEx" : "BB16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Developmental Robotics: From Babies to Robots",
      "author" : [ "Angelo Cangelosi", "Matthew Schlesinger" ],
      "venue" : "The MIT Press,",
      "citeRegEx" : "CS14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Busy Beaver Machines and the Observant Otter Heuristic (or How to Tame Dreadful Dragons)",
      "author" : [ "James Harland" ],
      "venue" : "CoRR, abs/1602.03228,",
      "citeRegEx" : "Har16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Algoritmusok",
      "author" : [ "Gábor Ivanyos", "Réka Szabó", "Lajos Rónyai" ],
      "venue" : "Typotex,",
      "citeRegEx" : "ISR00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Incorporated",
      "author" : [ "Ming Li", "Paul M.B. Vitányi. An Introduction to Kolmogorov Complexity", "Its Applications. Springer Publishing Company" ],
      "venue" : "3 edition,",
      "citeRegEx" : "LV08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Attacking the busy beaver 5",
      "author" : [ "Heiner Marxen", "Jürgen Buntrock" ],
      "venue" : "Bull EATCS, 40:247–251,",
      "citeRegEx" : "MB90",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "On computable numbers with an application to the Entscheidungsproblem",
      "author" : [ "Alan Turing" ],
      "venue" : "Proceeding of the London Mathematical Society,",
      "citeRegEx" : "Tur36",
      "shortCiteRegEx" : null,
      "year" : 1936
    }, {
      "title" : "editor",
      "author" : [ "John von Neumann. The general", "logical theory of automata. In L.A. Jeffress" ],
      "venue" : "Cerebral Mechanisms in Behaviour – The Hixon Symposium, pages 1–31. Wiley,",
      "citeRegEx" : "vN51",
      "shortCiteRegEx" : null,
      "year" : 1951
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Samu is a disembodied developmental robotic experiment to develop a family chatterbot agent who will be able to talk in a natural language like humans do [Bát15a].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 9,
      "context" : "The practical purpose of Samu projects is to develop computational mental organs that can support software agents to acquire higher-order knowledge from their input [BB16].",
      "startOffset" : 165,
      "endOffset" : 171
    }, {
      "referenceID" : 4,
      "context" : "The activities have been conducted during the development of such mental organs may be considered as first efforts to create on demand the Asimovian profession called robopsychology [Bát16a].",
      "startOffset" : 182,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "The roots of this paper lie in the two new software experiments Samu Turing [Bát16c] and Samu C.",
      "startOffset" : 76,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "Turing [Bát16b].",
      "startOffset" : 7,
      "endOffset" : 15
    }, {
      "referenceID" : 10,
      "context" : "These are very simplified versions of the former habituation-sensitization [CS14] based (like for example SamuBrain [Bát16d] or SamuKnows [Bát16e]) learning projects of Samu.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "These are very simplified versions of the former habituation-sensitization [CS14] based (like for example SamuBrain [Bát16d] or SamuKnows [Bát16e]) learning projects of Samu.",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 8,
      "context" : "These are very simplified versions of the former habituation-sensitization [CS14] based (like for example SamuBrain [Bát16d] or SamuKnows [Bát16e]) learning projects of Samu.",
      "startOffset" : 138,
      "endOffset" : 146
    }, {
      "referenceID" : 1,
      "context" : "The term “COP-based” (Consciousness Oriented Programming [Bát11]) means that the engine predicts its future input.",
      "startOffset" : 57,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "In this case the previous output (the previous prediction) is the same as the actual input, for precise details see [Bát15a] and [BB16]).",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "In this case the previous output (the previous prediction) is the same as the actual input, for precise details see [Bát15a] and [BB16]).",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 16,
      "context" : "to write this paper stems from the last paragraph of the work of Neumann on the general theory of automata [vN51] where Neumann had suggested that there is a complexity level above which the machines can reproduce themselves and even more complicated ones.",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "Neumann investigated the self-reproducing automata [vN51] roughly a decade after Alan Turing had published his work on universal simulation theorem [Tur36].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 15,
      "context" : "Neumann investigated the self-reproducing automata [vN51] roughly a decade after Alan Turing had published his work on universal simulation theorem [Tur36].",
      "startOffset" : 148,
      "endOffset" : 155
    }, {
      "referenceID" : 9,
      "context" : "For example, the first mental organs had learned the Conway’s Game of Life [BB16] (or see the YouTube video at https://youtu.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "It should be noticed that some of them, such as the machines of Schult and Uhing or the Marxen and Buntrock’s BB5 champion machine are famous in the field of the Radó Tibor’s Busy Beaver problem [LV08].",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 0,
      "context" : "Throughout both this article and our software experiments we use the definition of the Turing machine (TM) that was introduced in [Bát09] and also used in [Bát15b] where the Turing machine was defined by a quadruple T = (Q, 0, {0, 1}, f) where f : Q× {0, 1} → Q× {0, 1} × {←, ↑,→} is a partial transition function and 0 ∈ Q ⊂ N is the starting state.",
      "startOffset" : 130,
      "endOffset" : 137
    }, {
      "referenceID" : 3,
      "context" : "Throughout both this article and our software experiments we use the definition of the Turing machine (TM) that was introduced in [Bát09] and also used in [Bát15b] where the Turing machine was defined by a quadruple T = (Q, 0, {0, 1}, f) where f : Q× {0, 1} → Q× {0, 1} × {←, ↑,→} is a partial transition function and 0 ∈ Q ⊂ N is the starting state.",
      "startOffset" : 155,
      "endOffset" : 163
    }, {
      "referenceID" : 0,
      "context" : "With the notation of [Bát09] a configuration can be written in the form wbefore[q > wafter, where wbefore, wafter ∈ {0, 1}∗ and q ∈ Q.",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "Turing we programmed the Samu agent to work in a similar way as, for example, Professor James Harland did in his work [Har16] where he observed and studied the configurations of Marxen and Buntrock’s Busy Beaver champion machines [MB90].",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 14,
      "context" : "Turing we programmed the Samu agent to work in a similar way as, for example, Professor James Harland did in his work [Har16] where he observed and studied the configurations of Marxen and Buntrock’s Busy Beaver champion machines [MB90].",
      "startOffset" : 230,
      "endOffset" : 236
    }, {
      "referenceID" : 12,
      "context" : "The proof is divided into two parts: in the first one, we modify the usual proof of Turing’s universal simulation theorem (see for example the textbook [ISR00]) to produce the sequence of configurations of T by the universal machine R.",
      "startOffset" : 152,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "In this paper, we started with two developmental robotic software experiments Samu Turing [Bát16c] and Samu C.",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 5,
      "context" : "Turing [Bát16b] to learn how Turing ma-",
      "startOffset" : 7,
      "endOffset" : 15
    }, {
      "referenceID" : 2,
      "context" : "For example, we are going to investigate using Samu’s neural architecture [Bát15a], Samu mental organs (like MPUs) [BB16] and deep learning to learn how TMs operate.",
      "startOffset" : 74,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "For example, we are going to investigate using Samu’s neural architecture [Bát15a], Samu mental organs (like MPUs) [BB16] and deep learning to learn how TMs operate.",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "The combine columns show the given TM in the form of rule-index notation [Bát15b].",
      "startOffset" : 73,
      "endOffset" : 81
    } ],
    "year" : 2016,
    "abstractText" : "From the point of view of a programmer, the robopsychology is a synonym for the activity is done by developers to implement their machine learning applications. This robopsychological approach raises some fundamental theoretical questions of machine learning. Our discussion of these questions is constrained to Turing machines. Alan Turing had given an algorithm (aka the Turing Machine) to describe algorithms. If it has been applied to describe itself then this brings us to Turing’s notion of the universal machine. In the present paper, we investigate algorithms to write algorithms. From a pedagogy point of view, this way of writing programs can be considered as a combination of learning by listening and learning by doing due to it is based on applying agent technology and machine learning. As the main result we introduce the problem of learning and then we show that it cannot easily be handled in reality therefore it is reasonable to use machine learning algorithm for learning Turing machines.",
    "creator" : "LaTeX with hyperref package"
  }
}