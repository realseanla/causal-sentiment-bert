{
  "name" : "1703.08262.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Supervisor Synthesis of POMDP based on Automata Learning",
    "authors" : [ "Xiaobin Zhang", "Bo Wu", "Hai Lin" ],
    "emails" : [ "(xzhang11@nd.edu;", "bwu3@nd.edu;", "hlin1@nd.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—partially observable Markov decision process, supervisory control, formal methods, automata learning.\nI. INTRODUCTION\nIN real world applications, autonomous systems alwayscontain uncertainties. The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4], [5], and human-robot collaboration [6]–[8]. Different system models have been considered to capture uncertainties, and partially observable Markov decision process (POMDP) has emerged as one of the most general and thus popular models. POMDP models system software and hardware statuses with discrete states. Between different states, probabilistic transitions are triggered by different system actions to describe uncertainties from the system actuation behavior. Compared to Markov decision process (MDP), POMDP considers partial observability on its states that can model sensing noises and observation errors, which makes MDP a special case of POMDP. This property is very useful in modeling autonomous systems with hidden states, such as advanced driver assistant system (ADAS) [5] and human-robot collaboration [7], [9] where human intention can not be directly observed. Together\nThe authors are with Department of Electrical Engineering, University of Notre Dame, Notre Dame, IN 46556, USA. (xzhang11@nd.edu; bwu3@nd.edu; hlin1@nd.edu).\nThe financial supports from NSF-EECS-1253488 and NSF-CNS-1446288 for this work are greatly acknowledged.\nwith probabilistic transitions between states and nondeterminism in action selections, POMDP can capture uncertainties from various sources, such as sensing, actuation, and the environment. Meanwhile, a reward function can be defined that assigns real value to each state transition to represent additional information in POMDP.\nIn this paper, we study formal design methods for POMDPs with control tasks given by Probabilistic Computation Tree Logic (PCTL). While most of the ω−regular properties are undecidable for POMDPs [35], we consider PCTL specifications with finite horizons which can bound the searching space with finite memory in POMDP model checking following the philosophy of Bounded Model Checking [36]. Meanwhile, a lot of robotics applications require task completion with finite time, such as motion planning [37], which also makes PCTL specification with finite horizons suitable to describe our control tasks. With a finite planning horizon, the model checking problem of POMDP is decidable, but a historydependent controller instead of a memoryless one is necessary. To regulate POMDP to satisfy a finite horizon PCTL, we propose a novel supervisor control framework with a special type of deterministic finite automaton (DFA), za-DFA, as the supervisor to achieve history-dependent planning. After defining the probability space of POMDP, PCTL satisfaction over POMDP is established on the product model between za-DFA and POMDP. To check the satisfaction relation efficiently, we show the connection between the model checking and the optimal policy computation, then modify a state-ofart POMDP solving algorithm, Partially Observable MonteCarlo Planning (POMCP) [38], to reduce the computational complexity for POMDP model checking. After that, an L∗ learning based supervisor synthesis algorithm is proposed to synthesize a za-DFA to satisfy the given specification. To guarantee the soundness and completeness of the supervisor synthesis, we design novel algorithms to answer membership queries and conjectures from L∗ learning. The returned zaDFA will also be permissive by enabling more than one action for POMDP to select given a history."
    }, {
      "heading" : "A. Related Work",
      "text" : "Traditional planning and control problems in POMDP target to find a policy that maximizes the expectation of accumulated rewards. Since states are not directly observable, the available information for a control policy is an observation-action sequence up to current time instance, and such a sequence is called history. History can be represented in a compact form called belief state, which is a probability distribution\nar X\niv :1\n70 3.\n08 26\n2v 1\n[ cs\n.S Y\n] 2\n4 M\nar 2\n01 7\n2 over the state space of POMDP. Since belief state is sufficient statistics for history [10], POMDP can be viewed as MDP with a continuous state space formed by belief states. This inspires solving POMDP planning by finding the optimal control policy over the continuous belief state space. Exact planning of POMDP [11] can be intractable with the size of state space and planning horizon exploding quickly. Therefore, approximation methods are proposed to approximate the value function or limit the policy search space to alleviate the computational complexity. As one of the most popular approaches, pointbased value iteration (PBVI) optimizes the value function only over a selected finite set of belief states and provides the optimization result with a bounded error [12]–[15].\nCompared to the point-based approach that solves POMDP on the continuous state space of belief states, the controllerbased approach [16] finds the optimal policy represented by a finite state controller (FSC) with finite memory. An FSC can be defined as a directed graph G = 〈N , E〉 with each node n ∈ N being labeled by an action a and each edge e ∈ E by an observation z in POMDP. Each node has one outward edge per observation, and a policy can be executed by taking action associated with the node at current time instance and updating the current node by following the edge labeled by the observation made [17]. This representation is equivalent to Moore machine [18] from automata theory [16]. There are two types of approaches to find an FSC: policy iteration and gradient search. The policy iteration tends to find the optimal controller, but the size of the controller can grow exponentially fast and turns intractable. The gradient search usually leads to a suboptimal solution that often traps in local optimum [19], [20]. To combine the advantages from gradient ascent and policy iteration, bounded policy iteration (BPI) is proposed in [17] to limit the size of the controller and provide evidence to help escape local optimum. Besides direct graph as the controller form for FSC, DFA and Mealy machine have also been considered in [21] and [16], respectively.\nCurrently, most existing results on the control problem of POMDP focus on the reward-based planning. However, for some safety-critical applications like autonomous driving, a guaranteed system performance is crucial. This motivates us to consider formal methods. In robotics, formal methods are used to generate controllers that can guarantee the system performance to satisfy high-level mission requirements [22]– [25]. For complicated missions, temporal logic [26] is an efficient tool to describe requirements for system tasks due to its expressiveness and similarity to natural languages. Compared to extensive studies in reward-based planning, very few results on formal methods based planning have been established for POMDP, which makes it an open problem [27]. Until recently, there are some advances in the controller synthesis of POMDP under temporal logics. In [28], the controller synthesis of POMDP with Linear Temporal Logic (LTL) specifications over infinite horizon is discussed and solved based on gradient search where fixed-size FSCs are used to maximize the probability of satisfying the given LTL specification. However, this method suffers from local maxima, and the initial choice of the FSC’s structure does not have a systematic guideline [28]. In [29], the authors use observation-stationary (memoryless) con-\ntroller to regulate POMDP to satisfy almost-sure reachability properties. Since the action is selected only depends on current observation, the satisfiability modulo theories (SMT) method is applied with similar idea shows in [30] where a state-based controller for MDP is learned. Compared to history-dependent controllers, memoryless controllers used in these work are not general enough for reasoning over finite horizons. In [31], a linear time invariant system with linear observation model for states is considered, which is equivalent to a discrete time continuous space POMDP. The system specification is given as Gaussian Distribution Temporal Logic (GDTL) as an extension of Boolean logic. Sampling-based algorithms are proposed to build a transition system to generate a finite abstraction for belief space. With the specification being converted to deterministic Rabin automaton, the synthesis is done on the product MDP following dynamic programming approach. However, the size of the product MDP still suffers under the curse of history for POMDPs. Similar to POMDPs, the deterministic systems with partial information have also been studied for synthesis problems in [32]–[34]. However, applying these methods to POMDP is hard due to the probabilistic transition nature of POMDP.\nSince POMDP is an extended model of MDP, there is also some related work for supervisor synthesis on MDPs. Especially for permissive controller design, in [40], state-based controllers without memory are proposed for infinite horizon planning and Mixed Integer Linear Programming (MILP) [41] is applied to find a permissive controller. Similarly, in [30], SMT is combined with reinforcement learning to learn a state-based controller. While these methods assume a memoryless controller, a history-dependent controller is necessary for POMDP planning over a finite horizon. Also due to the partial observability in POMDPs, applying these methods to POMDP are fundamentally difficult. Besides these works, using the L∗ algorithm to learn system supervisor has also been considered in our previous work for MDPs [42]. To apply the L∗ algorithm to POMDP supervisor synthesis, in this paper, we extensively discuss the supervisor synthesis framework and design new membership query and conjecture checking rules to overcome the difficulties brought by partial observability."
    }, {
      "heading" : "B. Our Contributions",
      "text" : "This paper is an extended and revised version of our preliminary conference paper [39]. Compared to [39], this paper makes the following new contributions. First, we formally build the POMDP supervisor control framework by proving the sufficiency of using za-DFA as the controller form, defining the probability space for POMDP, then establishing PCTL satisfaction over POMDP. Secondly, the model checking of POMDP over observation-based adversaries is intensively studied, and a modified POMCP algorithm is given to conquer the computational complexity. Thirdly, we develop new oracles for the L∗ learning algorithm to guarantee the completeness and allow permissiveness of the supervisor. Based on that, a new example is given to illustrate the learning process in detailed steps.\n3 The technical contributions are summarized in the order in which they appear in the paper as follows: • We propose a supervisory control framework for POMDP\nto satisfy PCTL specifications over finite horizons. As a special type of DFA, za-DFA is used as the supervisor form. Based on that, we further define the probability space and PCTL satisfaction over POMDP. Then the POMDP model checking is intensively discussed and a modified POMCP method is given to speed up the model checking process. • We design an L∗ learning based supervisor synthesis algorithm to learn a suitable supervisor automatically. With properly defined membership queries and conjectures, our learning algorithm is sound and complete. The returned za-DFA can be permissive, and the non-blocking feature is guaranteed."
    }, {
      "heading" : "C. Outline of the Paper",
      "text" : "The rest of this paper is organized as follows. In Section II, MDP-related preliminaries are given with definitions and notations. The supervisory control framework for POMDP is proposed in Section III. Following by that, Section IV presents L∗ learning based supervisor synthesis algorithm. The analysis and discussions are addressed in Section V. Section VI gives an example to illustrate the learning process. Finally, Section VII concludes this paper with the future work."
    }, {
      "heading" : "II. PRELIMINARIES",
      "text" : ""
    }, {
      "heading" : "A. MDP Modeling, Paths and Adversaries",
      "text" : "MDPs are probabilistic models for systems with discrete state spaces. With nondeterminisms from decision making and probabilistic behavior in system transitions, MDPs are widely used to model system uncertainties.\nDefinition 1. [43] An MDP is a tuple M = (S, s̄, A, T ) where • S is a finite set of states; • s̄ ∈ S is the initial state; • A is a finite set of actions; • T : S ×A× S → [0, 1] is a transition function.\nHere T (s, a, s′) describes the probability of making a transition from a state s ∈ S to another state s′ ∈ S after taking an action a ∈ A.\nIn MDPs, there are multiple actions defined for each state. If we limit the number of actions defined for each state to be 1, we have a discrete-time Markov chain (DTMC).\nDefinition 2. [43] A DTMC is a tuple M = (S, s̄, T ) where • S is a finite set of states; • s̄ ∈ S is the initial state; • T : S × S → [0, 1] is a transition function.\nTo analyze the behavior of MDP and DTMC with additional information, we can define a labeling function L : S → 2AP that assigns each state s ∈ S with a subset of atomic propositions AP . This helps to introduce system requirements in forms of temporal logics.\nIn MDP M = (S, s̄, A, T ), a path ρ is a nonempty sequence of states and actions in the form\nρ = s0a0s1a1s2 . . .\nwhere s0 = s̄, si ∈ S, ai ∈ A and T (si, ai, si+1) ≥ 0 for all i ≥ 0 [43]. Generally, we denote the ith state si of a path ρ as ρ(i) and the length of ρ (the number of transitions) as |ρ|. We use PathM to represent the set of all possible paths in M and PrefM for its set of corresponding prefixes.\nTo solve the nondeterminism in MDP, we need an adversary to build a map between system paths and actions. Depending on whether a deterministic action is selected or a probability distribution over all possible actions is given, there are two types of adversaries: pure adversary and randomized adversary. For the pure adversary, it is a function σ : PrefM → A, that maps every finite path of M onto an action in A. For the randomized adversary, it is a function σ : PrefM → Dist(A), which maps every finite path of M onto a distribution over A. With an adversary σ that solves the nondeterminism in MDP, the set of possible MDP paths is denoted as PathσM and the regulated system behavior can be represented as a DTMC."
    }, {
      "heading" : "B. PCTL and PCTL Model Checking over MDPs",
      "text" : "For a labeled MDP, we can use PCTL [43] to represent the system design requirements. PCTL is the probabilistic extension of the Computation Tree Logic (CTL) [44].\nDefinition 3. [43] The syntax of PCTL is defined as • State formula φ ::= true | α | ¬φ | φ ∧ φ |P./p[ψ], • Path formula ψ ::= Xφ | φ U≤kφ | φ U φ,\nwhere α ∈ AP , ./∈ {≤, <,≥, >}, p ∈ [0, 1] and k ∈ N.\nHere ¬ stands for \"negation\", ∧ for \"conjunction\", X for \"next\", U≤k for \"bounded until\" and U for \"until\". Specially, P./p[ψ] takes a path formula ψ as its parameter and describes the probabilistic constraint.\nGiven the syntaxes of POMDP, we can define PCTL satisfaction relation on MDP as follows.\nDefinition 4. [43] For an labeled MDPM = (S, s̄, A, T, L), the satisfaction relation for any states s ∈ S is defined inductively:\ns true, ∀s ∈ S; s α⇔ α ∈ L(s); s ¬φ⇔ s 2 φ; s φ1 ∧ φ2 ⇔ s φ1 ∧ s φ2; s P./p[ψ]⇔ Pr({ρ ∈ PathσM| ρ ψ}) ./ p, ∀σ ∈ ΣM,\nwhere ΣM is the set of all adversaries and for any path ρ ∈ PathM\nρ Xφ⇔ ρ(1) φ; ρ φ1 U≤kφ2 ⇔ ∃i ≤ k, ρ(i) φ2 ∧ ρ(j) φ1,∀j < i; ρ φ1 Uφ2 ⇔ ∃k ≥ 0, ρ φ1 U≤kφ2.\nThe model checking of PCTL specification has been extensively studied for MDPs [43]. PCTL specifications with\n4 probabilistic operators are considered. Depending on whether ./ in the specification gives lower or upper bound, PCTL model checking of MDPs solves an optimization problem by computing either the minimum or maximum probability over all adversaries [43]. Since the states are fully observable, the model checking for MDPs can be solved following dynamic programming techniques with polynomial time complexity [45]. Different software tools for MDP model checking are available, such as PRISM [46] and recently developed model checker Storm [47]."
    }, {
      "heading" : "III. POMDP MODELING AND SUPERVISORY CONTROL FRAMEWORK",
      "text" : "In this section, we propose a supervisory control framework to regular the close-loop behavior of POMDP to satisfy finite horizon PCTL specifications."
    }, {
      "heading" : "A. POMDP Modeling, Paths and Adversaries",
      "text" : "POMDPs are widely used to capture systems uncertainties from difference aspects. As an extension of MDP model, POMDP considers states with partial observability to model uncertainties from system sensing.\nDefinition 5. A POMDP is a tuple P = {M, Z,O} where • M is an MDP; • Z is a finite set of observations; • O : S × Z → [0, 1] is an observation function.\nIn POMDP, the observable information for each state s ∈ S is given by O as a probability distribution over Z. Here O(s, z) stands for the probability of observing z ∈ Z at state s ∈ S. Then MDP can also be viewed as a special case of POMDP where its Z = S and its observation function defined for each s ∈ S is a Dirac delta function with\nO(s, z) =\n{ 1, z = s;\n0, otherwise.\nRemark: Since states in POMDP are not directly observable, it may happen that we observe an observation z and decide to take action a while a is not defined for the current real state s. In this case, no state transitions will be triggered, as the system will ignore this command and stay in its current state.\nDue to the partial observability, paths in POMDP can not be directly observed then used as the information for POMDP planning. Instead, the observation sequence of a path ρ = s0a0s1a1s2 . . . can be defined as a unique sequence obs(ρ) = z0a0z1a1z2 . . . where zi ∈ Z and O(si, zi) > 0 for all i ≥ 0 (if obs(ρ1) 6= obs(ρ2), then ρ1 and ρ2 are considered as different paths). This observation sequence can be seen as history in traditional POMDP planning problems. While history is defined to start with an action, the initial observation z0 in the observation sequence can be seen as a special observation Init for the initial state s̄ with O(s̄, Init) = 1 since we assume s̄ is known. If the initial status of POMDP is given as a probability distribution over S, we can add a dummy initial state then define its transitions to other s ∈ S based on the initial probability distribution [39]. In the rest of\nthis paper, we will use the observation sequence and history for POMDP interchangeably if the meanings are clear.\nGiven histories as control inputs, the planning problem of POMDP needs to find an adversary as a mapping function that maps every finite history onto an action in A or a probability distribution over A. As in MDP, the former type of adversaries is called pure adversary, and the later is called randomized adversary. As a special case of the randomized adversary, the pure adversary is less powerful generally. But for the finite horizon PCTL specifications considered in our work, the pure adversaries and randomized adversaries have the same power in the sense that restricting the set of adversaries to pure strategies will not change the satisfaction relation of the considered PCTL fragments [48]. While the detailed analysis follows the fact that POMDP is a one-and-a-half player game [48], the intuitive justification for this claim is that if we are just interested in upper and lower bounds to the probability of some events to happen, any probabilistic combination of these events stays within the bounds. Moreover, pure adversaries are sufficient to observe the bounds [48]. Therefore, we consider the controller design of pure adversary in our supervisory control framework."
    }, {
      "heading" : "B. Supervisory Control with za-DFA",
      "text" : "We want to find a supervisor to provide pure adversaries for POMDP and regulate the closed-loop behavior to satisfy finite horizon PCTL specifications. To improve the permissiveness, we target to find a set of proper pure adversaries. Since the control objective is given by a finite horizon specification, history-dependent controller outperforms history-independent (memoryless or observation-stationary) one and its justification can be directly inherited from MDP cases [40]. Based on these facts, we propose za-DFA as the supervisor for POMDP with the alphabet being defined in a particular form.\nDefinition 6. [39] A supervisor for POMDP P={S, s̄, A, Z, T,O} is a za-DFA F={Q, q̄,Σ, δ, Qm}, where • Q is a finite set of states; • q̄ ∈ Q is the initial state; • Σ = {α = 〈z, a〉| z ∈ Z, a ∈ A} is the finite alphabet; • δ : Q× Σ→ Q is a transition function; • Qm is a finite set of accepting states.\nSince DFA is an equivalent representation of regular language [49], za-DFA represents a regular set of strings with the set of the observation-action pairs in POMDP as its alphabet. A path q0〈z0, a0〉...qn〈zn, an〉 in F is a string concatenation of these pairs, which encodes a history z0a0...zn with an action an. Then the accepted runs in za-DFA give the enabled actions for different histories and represent POMDP executions. Note that the prefixes of the accepted runs must also be accepted since we have to allow the prefixes to happen in POMDP execution first. This implies that the accepted language Lm(F) of za-DFA F as the supervisor for POMDP is prefix-closed, i.e., Pref(L(F)) = Lm(F) where Pref(L(F)) denotes all prefixes of the language of F [49].\n5 Proposition 1. A set of pure adversaries to regulate a finite horizon PCTL specification for POMDP P = {S, s̄, A, Z, T,O} can always be represented as a za-DFA.\nProof: A pure adversary in POMDP maps a history h to an action a ∈ A. Since we consider finite POMDP, the observation set Z and action set A are finite, which form a finite alphabet for za-DFA. Meanwhile, for a finite horizon PCTL specification, the pure adversaries give the action selection rules for finite length histories. Thus all possible concatenations of history h and action a enabled by this set of pure adversaries will form a finite set of strings U and each string y ∈ U has a finite length. Then we can define a nondeterministic finite automaton (NFA) FN such that its accepted language is exactly the set U . Here FN can be constructed by unifying the initial state for DFA representing each string y ∈ U . By applying the subset construction on NFA FN , we can get a DFA whose accepted language is U [49]. With the set of observation-action pairs as the alphabet, we have shown that we can always find a za-DFA to represent a set of pure adversaries to regulate a finite horizon PCTL specification for POMDP.\nGiven a za-DFA as the supervisor for POMDP, all histories that may be encountered during POMDP executions are mapped to a set of enabled actions. Then we can define a product MDP as the parallel composition between POMDP and za-DFA to describe the regulated behavior.\nDefinition 7. Given a POMDP P = {S, s̄, A, Z, T,O} and a za-DFA F = {Q, q̄,Σ, δ, Qm} as the supervisor, their parallel composition P||F is an MDP MF = (SF , s̄F , AF , TF ),\n• SF = {szq|O(s, z) > 0, s ∈ S, z ∈ Z, q ∈ Qm} ∪ {s̄q̄} is a finite set of states; • s̄F = s̄q̄ is the initial state; • AF = A is a finite set of actions; • TF (s̄q̄, a, s′z′q′) = O(s′, z′)T (s̄, a, s′), if δ(q̄, 〈z, a〉) = q′ with O(s̄, z) > 0, T (s̄, a, s′) > 0 and O(s′, z′) > 0; • TF (szq, a, s′z′q′) = O(s′, z′)T (s, a, s′), if δ(q, 〈z, a〉) = q′ with T (s, a, s′) > 0 and O(s′, z′) > 0.\nFor the labeling function, LF (s̄q̄) = L(s̄) and LF (szq) = L(s), ∀s ∈ S, z ∈ Z, q ∈ Qm. Remark: Compared to the global Markov chain defined in [28] describing the regulated behavior of POMDP under an FSC, the product MDP defined in Definition 7 is more general because za-DFA is permissive and it enables more than one action to be selected under a history.\nTo make za-DFA feasible for POMDP planning in practice, we require that a POMDP P should not get \"blocked\" under the supervision of F in the sense that there always exists at least one action being enabled given a history allowed in F .\nDefinition 8. A supervisor za-DFA F to regulate POMDP P for a finite horizon k is non-blocking, if there are outgoing transitions defined on all states that are reachable in k steps from s̄q̄ in MF = P||F .\nCompared to the feasibility constraint defined in our previous work [39], here we allow multiple actions being enabled given\nAlgorithm 1: Simulation run of POMDP P regulated by za-DFA F up to time k\n1 s(0)← s̄, q(0)← q̄ 2 for i = 0, 1, ..., k do 3 simulate z(i) based on O given s(i) 4 choose any a(i) ∈ A such that 〈z(i), a(i)〉 defines an outgoing transition from q(i) to any q ∈ Qm 5 q(i+ 1)← δ(q(i), 〈z(i), a(i)〉) 6 simulate s(i+ 1) based on T given s(i) and a(i) 7 end\na history to have permissiveness in the supervisory control framework using za-DFA.\nGiven a non-blocking za-DFA F = {Q, q̄,Σ, δ, Qm}, the simulation run of POMDP P = {S, s̄, A, Z, T,O} is shown in Algorithm 1. Starting from initial state s̄, P first generates an observation z(i) on state s(i) at each time instance i. Then F will search for an outgoing transition 〈z(i), a(i)〉 from q(i) to any q ∈ Qm with q(0) = q̄ and the corresponding action a(i) is selected to execute. After that, the state of F is updated and a new POMDP state is simulated following action a(i)."
    }, {
      "heading" : "C. Probability Space and PCTL Satisfaction over POMDP",
      "text" : "To formally address the PCTL satisfaction over POMDP, we first define the probability space in POMDP. With an observation-based adversary, the behavior of POMDP is purely probabilistic. Given a finite path ρfin and its corresponding observation sequence obs(ρfin), with an observation-based adversary, we can define the basic cylinder set in POMDP P as follows:\nC(ρfin) : = {ρ ∈ PathP | ρfin is prefix of ρ and obs(ρfin)(i) = obs(ρ)(i),∀i ≤ |ρfin|},\nwhich is the set of all infinite paths with the prefix ρfin and observation prefix obs(ρfin). Let Cyl contain all sets C(ρfin) where ρfin ranges over all paths with all possible observation sequences. Then the σ-algebra can be defined on the paths generated by Cyl and the corresponding probability measure can be defined as\nPr(C(ρfin)) = 1, |ρfin| = 0; O(s(0), z(0)) |ρfin|∏ i=1 T (s(i− 1), a(i), s(i))O(s(i), z(i)),\notherwise,\nwhere s(i) = ρfin(i), z(i) = obs(ρfin)(i), and a(i) is the selected action from the adversary given the observation sequence up to time instance i. Since we assume the initial state s̄ is given, the initial observation will be the special observation Init with O(s̄, Init) = 1. With the domain PathP , σ-algebra and the corresponding probability measure, we have defined the probability space for POMDP under an observation-based adversary. These results are modified based on [50] where the probability space for Hidden Markov Model (HMM) is defined.\n6 Since PCTL over MDP is well defined, the product MDP that describes the regulated behavior of POMDP under the supervision of za-DFA can be used to connect PCTL satisfaction over POMDP with its definition for MDP. Given a path ρ in MF , we can have its observation sequence of obs(ρ) by extracting the observation symbol z out of the szq tuple for the state in ρ (the observation symbol for s̄q̄ is the special observation Init). With an observation-based adversary, there is a one-to-one correspondence between the paths inMF and P . Then based on the general definition of the probability space on MDP [43], it is not hard to see that the probability spaces on MDPMF and POMDP P are equivalent. Therefore, given a POMDP P and a za-DFA F , the PCTL satisfaction with a finite horizon over the regulated system is equivalent to the PCTL satisfaction over the product MDPMF constraining to observation-based adversaries. We denote the model checking on MF constrained to the observation-based adversaries as MF |=obs φ where |=obs stands for satisfaction relation constrained to the observation-based adversaries.\nFor the sake of simplicity, we consider bounded until PCTL specification φ = PEp[φ1 U≤kφ2] with E ∈ {≤, <} in the rest of this paper. But for finite horizon PCTL, the generality is not lost since lots of finite horizon PCTL specifications can be transformed to bounded until form and the model checking mechanism is similar as shown in [51]."
    }, {
      "heading" : "D. POMDP Model Checking",
      "text" : "To verify the satisfaction relation over the regulated behavior, we need to solve the PCTL model checking problem on MF where most of the operators are handled in the same way as in MDP model checking. But for state formula P./p[ψ], we need to check whether the probability bound ./ p is satisfied given the observation-based adversaries instead of all adversaries. We can solve this by computing either the minimum or maximum probability depending on whether a lower or upper bound is defined by ./ [43]. This problem can be solved with EXPTIME-complete complexity for finite horizon specifications. But with the size of POMDP and the planning horizon increasing, this problem becomes much harder to solve. Another promising approach is to convert the model checking to an equivalent optimal policy computation problem on POMDP. Following this method, we can leverage recently developed POMDP solvers that can handle a larger problem size with high-efficiency [13]–[15]. For the finite horizon PCTL φ = PEp[φ1 U≤kφ2], the model checking of this type of specifications can be converted to an optimal policy computation problem by modifying the transition structure of POMDP to make all states s |= ¬φ1 and states s |= φ2 absorbing, and designing the reward scheme that assigns 0 to intermediate transitions and 1 to the final transitions on s |= φ2 when the planning depth k is reached [28], [38], [52].\nAmong different POMDP solvers, we modify a state-of-art POMDP optimal policy computation algorithm, Partially Observable Monte-Carlo Planning (POMCP) [38], that can well fit with our supervisory control framework. POMCP is proposed as an online POMDP planner to find the control policy and optimize a discounted accumulative reward in future. In-\nAlgorithm 2: Modified POMCP to check the PCTL specification φ = PEp[φ1 U≤kφ2]\n1 Function SEARCH(h): 2 do 3 if h = empty then 4 s = s̄ 5 else 6 s ∼ B(h) 7 end 8 SIMULATE(s, h, 0) 9 while not TIMEOUT()\n10 return arg max a V (ha) 11 return 12 13 Function ROLLOUT(s, h, depth): 14 if depth > k then 15 return 0 16 else if depth = k then 17 return ∑ s|=φ2 b(s, h) 18 end 19 a ∼ σrollout(h, ·) 20 (s′, z) ∼ G(s, a) 21 return ROLLOUT(s′,haz,depth+ 1) 22 return 23 24 Function SIMULATE(s, h, depth): 25 if depth > k then 26 return 0 27 else if depth = k then 28 return ∑ s|=φ2 b(s, h) 29 end 30 if h 6∈ T then 31 for a ∈ A(h,F) do 32 T (ha)← (Ninit(ha), Vinit(ha), ∅) 33 end 34 return ROLLOUT(s,h,depth) 35 end 36 a← arg max\na V (ha) + c\n√ logN(h) N(ha)\n37 (s′, z) ∼ G(s, a) 38 R←SIMULATE(s′,haz,depth+ 1) 39 B(h)← B(h) ⋃ {s} 40 N(h)← N(h) + 1 41 N(ha)← N(ha) + 1 42 V (ha)← V (ha) + R−V (ha)N(ha) 43 return R 44 return\nstead of explicitly solving a POMDP, POMCP applies MonteCarlo tree search [53] by running Monte-Carlo simulations to maintain a search tree of histories. Each node in the search tree represents history h as T (h) = 〈N(h), V (h), B(h)〉. Here N(h) counts the number of times that history h has been visited; V (h) is the value of history h; B(h) is a set of particles used to approximate the belief state for history h\n7\nto avoid exact belief state update for each step. Given the current history ht, each simulation starts in an initial state sampled from the belief state B(ht). There are two stages of simulation: when the child nodes exist for all children, the actions selection rule follows the Upper Confidence Bounds 1 (UCB1) [54] algorithm to maximize V (ha) + c √ logN(h) N(ha) where c is the exploration constant; at the second stage, the actions will be selected following an observation-based rollout policy σrollout(h, a) and normally it follows a uniform random action selection policy. One new node is added to the search tree after each simulation.\nTo modify POMCP for our model checking purpose for the PCTL specification φ = PEp[φ1 U≤kφ2], we use a constant planning depth k instead of a discount factor for the value function to guarantee the termination of each simulation. Meanwhile, without intermediate rewards, a termination reward will be assigned when planning depth k is reached and this reward is equal to ∑ s|=φ2 b(s, h) where b(·, h) is the exact belief state of h. For the action selection rules, we limit the selection been considered only on the enabled action set A(h,F) given the supervisor F and history h. While the main algorithm is the same with POMCP, our modified version is shown in Algorithm 2. Then by initializing the current history h0 to empty, we can estimate the optimal value V ∗(h0), which is equal to the maximum satisfaction probability V (h0) [52]. To find the minimum satisfaction probability, we just need to change the sign of the termination reward, and the estimation is −V (h0). From the search tree in POMCP, we can also get the selected action for each history node which together gives an observation-based adversary σ that can achieve the estimated satisfaction probability. Since our modification on POMCP does not change its main mechanism, the convergence and performance analysis for POMCP is still hold. With the convergence guarantee in probability, the bias of the value function E[V (h0) − V ∗(h0)] is O(log(N(h0))/N(h0) [38]. Given a fixed δ > 0, the probability of V (h) in the range of [V ∗(h) − ∆n/n, V ∗(h) + ∆n/n] is less or equal to δ with ∆n = 9 √ 2n ln(2/δ) for a sufficiently large number of simulations n [55]. In practice, we may need to run many simulations (for example, 106) to get a good estimation, but the simulation run can be very fast and the total cost time is still very small (for example, in 10 to 100 seconds) as reported in [38]."
    }, {
      "heading" : "IV. LEARNING BASED SUPERVISOR SYNTHESIS",
      "text" : "Within the supervisory control framework using za-DFA, our task of finding a supervisor for POMDP is converted to find a DFA, which is an equivalent representation of regular set [49]. This inspires us to use L∗ algorithm to learn a supervisor."
    }, {
      "heading" : "A. L∗ Learning Algorithm",
      "text" : "The L∗ learning algorithm [56] is proposed to learn an unknown regular set [49]. Starting from a fixed known size of alphabet Σ, L∗ learning defines an observation table (Y,E,G) to organized the knowledge acquired by the learning algorithm. The row index of the table contains two parts: Y and Y · Σ, where Y is a nonempty finite prefix-closed set\nq0start 0\nFigure 1. The acceptor DFA corresponding to the observation table in Table I\nof strings. The column index is given by a nonempty finite suffix-closed set of strings E. The function G maps a string y ∈ Σ∗ to {0, 1} where Σ∗ is the set of all finite length strings containing symbols from Σ. For a string y ∈ ((Y ∪Y ·Σ) ·E), G(y) = 1 if and only if y ∈ U . For each row entry of a string y, its row denotes the finite function f from E to {0, 1} defined by f(e) = G(y · e). Initializing the observation table with Y = E = { }, L∗ algorithm tries to make the table closed and consistent. For closeness, ∀y1 ∈ Y · Σ, it requires that ∃y2 ∈ Y , s.t. row(y1) = row(y2); for consistence, whenever y1, y2 ∈ Y with row(y1) = row(y2), it requires that ∀α ∈ Σ, row(y1 · α) = row(y2 · α) [56]. Given a closed and consistent observation table, a DFA F = {Q, q̄,Σ, δ, Qm} as the acceptor can be generated with its accepting language Lm(F) representing the learned regular set as follows:\n• Q = {row(y) : y ∈ Y }, • q̄ = row( ), • σ(row(y), α) = row(y · α). • Qm = {row(y) : y ∈ Y and G(y) = 1},\nFor the observation table shown in Table I, it is closed and consistent, and the corresponding DFA is shown in Fig. 1.\nTo generate a closed and consistent observation table, L∗ learning maintains a Questions & Answers mechanism. Given the alphabet Σ, two types of questions, membership query and conjecture, are asked by the Learner and answered by the Teacher. For the membership query, the Learner asks whether a string y ∈ Σ∗ is a member of U or not, and the Teacher answers true or false, respectively. For the conjecture, the Learner asks whether a learned regular set is equal to U or not, and the Teacher answers true, or false with a string yc showing the symmetric difference between the learned set and U . In the latter case, yc is called a counterexample. With the membership query, if the table is not closed, the algorithm finds y1 ∈ Y , α ∈ Σ s.t., row(y1 · α) 6= row(y2),∀y2 ∈ Y , then adds y1 ·α to Y and extends the table; if the table is not consistent, the algorithm finds y1, y2 ∈ Y, α ∈ Σ, e ∈ E, s.t., row(y1) = row(y2) but G(y1 ·α ·e) 6= G(y2 ·α ·e), then adds α · e to E and extends the table [56]. With the conjecture, if yc is given as a counterexample, yc and its prefixes will be added to Y and the table is extended using membership queries. With a Teacher being able to answer membership queries and conjectures, L∗ algorithm is proved to converge to the minimum DFA accepting U in polynomial time [56].\n8 Output: Super visor Conjecture\nConstruct a closed and consistent obser vation table Membership Quer y OracleS Refine and extend the obser vation table OracleP Ini tial ization za-DFA as the super visor\n\"Super visor was found\"\nPreprocessing Tr ivial za-DFA Empty za-DFA OracleB Yes YesNo True True False False False No True"
    }, {
      "heading" : "B. Learn za-DFA as the Supervisor",
      "text" : "Given a POMDP P = {S, s̄, A, Z, T,O} and a finite horizon PCTL specification φ, we use L∗ learning to learn a za-DFA F as the supervisor. To get a feasible za-DFA that can regulate POMDP to satisfy the specification φ, we develop algorithms to answer membership queries and conjectures. To simplify the analysis, we will take φ = PE[φ1Ukφ2] with E ∈ {≤, <} as the specification to illustrate the learning process. The overview of the learning process is shown in\nFig. 2 and we illustrate it as follows.\n1) Preprocessing: Before the initialization of L∗ learning, we first find the observation-based adversaries σmax and σmin that give the maximum and minimum satisfaction probabilities pmax and pmin for the path formula ψ = φ1Ukφ2, respectively. With the probability bound in φ given by E, we compare pmax and pmin with the threshold p: if pmaxEp then any observationbased adversaries can be applied and a trivial za-DFA with one state and self loop transitions under any 〈z, a〉 ∈ Σ will be returned as the supervisor; if pmin 5 p then no observationbased adversaries can be applied and an empty za-DFA that only accepts the empty string will be returned.\n2) Initialization: After the preprocessing stage to calculate σmax, σmin and their corresponding pmax, pmin, we can initialize the L∗ learning algorithm. Starting with the alphabet Σ defined in Definition 6, the observation table (Y,E,G) is initialized with Y = { }, E = { } and G( ) = 1. Then membership queries are generated by the Learner to extend the table.\nBeside the observable table, we initialize two string sets CB and CS to empty. Here CB and CS will contain strings of negative counterexamples returned from the OracleB and OracleS, respectively, and both oracles will be introduced in the conjecture answering section.\n3) Answering Membership Queries: For each string y = α0α1...αn, αi ∈ Σ, the membership query checks whether or not the corresponding observation-action sequence obs = 〈z0, a0〉〈z1, a1〉...〈zn, an〉 can be used as the control policy for histories as the prefix of obs. If there exists a prefix of y in\nCS∪CB , the membership query returns false. Otherwise, we will unfold the POMDP P given the control policy from y. This unfolding process follows the product MDP generation rules given in Definition 7. Basically y can be converted to a za-DFA with a unique action being selected for a history as the prefix of y. Then its product MDP MF turns into a DTMC. On DTMC MF , the model checking result of specification φ will answer the membership query with true if and only if MF |= φ. If |y| > k, we will take its prefix y′ : |y′| = k and apply the membership query for y′ since the specification only constrain the regulated behavior up to the depth k. Remark: In the original L∗ algorithm, G(y) = 1 implies that the unknown regular set U accepts y. But in our case, if membership query returns true, it only means the corresponding control policy will not cause the violation of the specification by itself. Here y may still need to be removed to get a correct supervisor because the satisfaction probability of the regulated behavior is determined based on the accumulative probability brought by different strings accepted in the supervisor.\nBased on the L∗ algorithm, the Learner will keep generating membership queries until a closed and consistent table (Y,E,G) is learned. Then a za-DFA F is generated as the acceptor of (Y,E,G).\n4) Answering Conjectures: Given a za-DFA F as the acceptor, the Learner asks a conjecture to check whether or not F is a non-blocking supervisor that can regulate POMDP P to satisfy PCTL φ. If the answer is true, the algorithm will terminate with the learned za-DFA as a non-blocking and permissive supervisor. Otherwise, counterexamples will be returned to guide the refinement and extension process of the observation table for the Learner. To answer conjectures, three oracles are defined to guarantee the soundness and completeness of our learning algorithm.\nSince we know σmin is a suitable adversary that will not violate the specification or cause blocking during the POMDP execution, we define OracleP to check whether or not there exists a string y ∈ Σ∗ such that y = obs(ρ) · a with σmin(obs(ρ)) = a but y 6∈ Lm(F). If yes, the conjecture will\n9 answer false with y being returned as a positive counterexample to make y ∈ Lm(F). With OracleP, we can guarantee that the learned supervisor F will accept any history-action pairs given by σmin.\nRemark: For any string y = obs(ρ)·a with σmin(obs(ρ)) = a, the membership query will return true. That is because if this single control policy could bring a probability violating the requirement in the specification, pmin brought by σ{min} will also violate the requirement, which will terminate the algorithm in the preprocessing stage. Therefore, there are no conflicts between the membership query and OracleP.\nIf OracleP does not find a positive counterexample, we use OracleB to check whether or not F is non-blocking. Here we checks all states szq ∈ SF that are k-step reachable from s̄q̄ on MF : if all such states have outgoing transitions being defined, OracleB returns true; otherwise, OracleB will check the causes of blocking. Assume there exists a k-step reachable state szq that does not have any outgoing transitions. Then q ∈ Qm and by applying a depth-first search on MF we can find the shortest observation-action sequence transits from s̄q̄ to szq. Denote this string as y. Depending on whether y ∈ Y or not, we have two possible causes for the blocking on szq. If y 6∈ Y , the blocking of the supervisor is because the Learner can generate a conjecture without adding y to Y and asking membership queries for y ·Σ. To know if there exists an action that can be enabled for szq to fix the blocking, OracleB will return false with y as the counterexample to enforce y · Σ appearing as rows in the observation table. If y has already been included in Y , y ·α for all α ∈ Σ will appear as rows in the observation table. Since q has no outgoing transitions under the observation z, all strings y · α with α = 〈z, ·〉 have been answered by membership queries with false. This means once the POMDP execution reaches state s and observes z while the za-DFA reaches state q, choosing any action will cause the violation of the specification under current za-DFA. Therefore, szq should be avoided during the system transition. To remove the strings that may lead to such states, all states szq ∈ SF with no outgoing transitions will be marked as dark states and the transitions to dark states will be removed in MF . This process keeps running until no new dark state appears. From traces starting from the initial state s̄q̄ to dark states, the observation-action sequences are extracted and form a string set Cb. Then CB is updated to CB = CB ∪ Cb. OracleB will return false together with the shortest string yc ∈ CB as the negative counterexample.\nIf OracleB does not find a counterexample, we use OracleS to check whether or notMF |=obs φ. If no, OracleS will return false with a negative counterexample yc as the evidence of specification violation. To find such a string yc as the counterexample, we first solve MF and find the observationbased adversary σc that gives the maximum satisfaction probability pc : pc 5 p. With σc, we generate a derived DTMC with histories as states: M̃ = {H ∪ {hd}, h0, T̃}. Here H is the state space of histories with h0 = empty and hd is a dummy state. For h ∈ H with |h| < k, T̃ (h, haz) =∑ s∈S ∑ s′∈S b(s, h)T ∗(s, a, s′)O(s′, z) with b(s, h) the belief\nstate function, σc(h) = a and\nT ∗(s, a, s′) =\n{ δs(s\n′), s |= ¬φ1 ∨ φ2; T (s, a, s′), otherwise,\nwhere δs is the standard Dirac delta function to make s |= ¬φ1 ∨ φ2 absorb. For h ∈ H with |h| = k, T̃ (h, hd) =∑ s|=φ2 b(s, h). Basically we are grouping up paths with the same observation sequences together in MF|σc and generate M̃. Therefore, a path in M̃ corresponds to a set of paths in MF|σc . For a path in M̃ that starts from h0 and ends in hd, its transition probability is equal to the accumulative transition probability of the corresponding set of paths inMF|σc ending in a state with label φ2 in k steps. Since σc witnesses the violation of the specification, M̃ 6|= PEp[true Uk+1hd]. Then we apply the DTMC counterexample generation algorithm in [51] to get the strongest evidence as a finite path with the maximum probability of violate. Denote its corresponding observation-action sequence as yc. If yc = obs(ρ) · a while σmin(obs(ρ)) = a, yc will be replaced by the observationaction sequence of the path with the second largest probability of violation. This process keeps going until yc does not conflict with σmin. Then yc will be returned as the negative counterexample and CS is updated with CS = CS ∪ {yc}.\nIf all three oracles return true, our algorithm will return the result za-DFA as the supervisor and terminate. If there exist counterexamples returned from either oracle, the observation table will be refined and extended.\n5) Refining and Extending the Observation Table: In the next iteration, given a counterexample y returned from conjectures and the updated CB and CS , we first refine the observation table by correcting G(y′) = 1 to G(y′) = 0 if Pref(y′) ∈ CB ∪ CS . Then y and all its prefixes are added to Y in (Y,E,G). After that, the observation table is extended using membership queries to generate a new closed and consistent table."
    }, {
      "heading" : "V. ANALYSIS AND DISCUSSIONS",
      "text" : "We analyze the L∗ learning based supervisor synthesis algorithm in this section regards to the termination, soundness, and completeness, as well as the computational complexity. Our analysis focuses on the cases where the algorithm is not terminated during the preprocessing stage since trivial statements can be followed otherwise."
    }, {
      "heading" : "A. Termination",
      "text" : "In the L∗ learning, we use membership queries and conjectures to collect information about whether or not an observation-action sequence can be used as part of a proper supervisor. Because we consider finite POMDP with a finite horizon specification, the number of all possible observationaction sequences are finite. So we only have a finite number of strings needed to be labeled in the observation table for the L∗ algorithm. Our algorithm requires a refinement process for the observation table if the returned negative counterexamples and their suffixes were answered with true by membership queries in previous iterations. However, for a\n10\nstring y ∈ (Y ∪ Y · Σ) · E, it will never happen that G(y) is changed from 0 to 1. Consider a string y with G(y) = 0. Then either the accumulative probability from y violates the threshold given by the specification, or Pref(y) ∈ CB ∪CS . In any of these cases, membership queries will always return false for y. While OracleP will return certain strings as positive counterexamples, y will never be returned by OracleP, i.e., y is not accepted by σmin. If the accumulative probability from y violates the threshold, it will never belong to σmin. If y ∈ CS , by definition of OracleS, y cannot be returned by OracleP. If y ∈ CB , y must be returned by OracleB which will only happen after OracleP returns true. But when OracleP returns true, the observation-action sequences from σmin are all accepted by the acceptor za-DFA, and none of them will cause blocking of the supervisor which is guaranteed by σmin as an observation-based adversary. Therefore, if y ∈ CB , y can never belong to σmin. As a result, G(y) will never be changed from 0 to 1. With the fact that the number of strings to be inquired is finite and at each iteration the algorithm must return counterexamples if any oracles return false, we can conclude that the termination of our supervisor synthesis algorithm is guaranteed. The upper bound of the number of iterations is equal to twice of the number of possible strings."
    }, {
      "heading" : "B. Soundness and Completeness",
      "text" : "Our L∗ learning based supervisor synthesis algorithm is sound and complete. If a za-DFA is returned as the supervisor, based on the definition of OracleP, OracleB, and OracleS, this za-DFA is non-blocking, and the model checking on the regulated behavior of POMDP proves the satisfaction of the specification. This shows the soundness of the algorithm.\nFor the completeness, if there exists a proper supervisor, our algorithm will return a za-DFA representing σmin in the worst cases. This is guaranteed by OracleP. But we cannot guarantee the permissiveness for the worse cases when OracleS returns \"good\" observation-action sequences as negative counterexamples. While OracleS will never misidentify a single string carrying enough probability mass of violation, if a set of paths is needed to witness the violation, how to select a proper counterexample from that set is still a research question, and it is possible that some paths accepted by the desired supervisor are returned as negative counterexamples. While now we will return the one with the maximum probability mass, newly developed counterexample selection algorithms for probabilistic systems can be applied and improve the performance of our learning framework."
    }, {
      "heading" : "C. Complexity",
      "text" : "Define the size of the POMDP SP as the product of the size of the underlying MDP SM and |Z|: SP = SM ∗ |Z| and denote the planning horizon of the specification as k. Then following the termination analysis, the number of iterations is at most O(|Σ|k) where Σ is the alphabet. In each iteration, denote the size of current acceptor DFA as SF . OracleP tries to find the difference between the current acceptor DFA and σmin. This can be achieved with time complexity O(SF ) by doing complement and interaction between two DFAs then applying\ndepth first search to check whether or not the initial state can be reached in k steps from the accepted state. OracleB mainly applies depth first search on the product MDP, so the time complexity is O(SP ∗SF ). OracleS replies on POMDP solving which generally have a time complexity exponential with k, linear with the length of the PCTL formula Lf (the number of logical and temporal operators in the formula). But with the modified POMCP method, the model checking result can be returned in seconds by running thousands of simulations and the running time will depend on the hardware. After that, the counterexample selection algorithm will take polynomial time with k and the number of transitions in the derived DTMC [51]. In the learning process, the maximum number of membership queries is at most O ( k|Σ|k ) . Then combining with the time analysis of L∗ in [56], we can see that our algorithm has a complexity exponential with k, polynomial with SP , and Lf . However, whenever we eliminate negative counterexamples, their suffixes are also removed. Therefore the SF and the number of iterations rarely assume large values in practice. So this complexity analysis is rather conservative."
    }, {
      "heading" : "VI. EXAMPLE",
      "text" : "Consider a POMDP P = {S, s̄, A, Z, T,O}, where • S = {s0, s1, s2, s3, s4}; • s̄ = s0; • A = {a1, a2, a3}; • Z = {z1, z2}.\nThe transition probabilities under different actions are given in the order of a1, a2, a3 in the square brackets shown in Fig. 3. The observation matrix is given in Table II. Among S, the state s4 represents a failure state with label fail and is colored by orange in Fig. 3. The specification is given by a finite horizon PCTL φ = P≤0.28[ψ] with ψ = true U≤3fail, which requires the probability of reaching failure within 3 steps should be less or equal to 0.28. Remark: This POMDP P is specially designed that the model checking problem can be solved quite straightforwardly. Then we can focus on the illustration of our supervisor synthesis algorithm.\nBased on the observation and action sets, we have the alphabet Σ = {1, 2, 3, 4, 5, 6} for the za-DFA as the supervisor where • 1 : 〈z1, a1〉, 2 : 〈z1, a2〉, 3 : 〈z1, a3〉,\n11\nTABLE IV THE CLOSED AND CONSISTENT OBSERVATION TABLE IN THE SECOND ITERATION\nG 3 13 1 0 1 2 0 0 0 1 1 1 0 13 1 1 1 11 1 0 0 3 0 0 0 4 1 1 0 5 0 0 0 6 0 0 0 2{1, ..., 6} 0 0 0 12 1 1 1 14 1 0 0 15 1 1 1 16 1 1 1 13{1..., 6} 1 1 1 11{1, 2, 3} 0 0 0 11{4, 5, 6} 1 1 1\nq0start q1\n1, 4\n2, 3, 5, 6\nFigure 5. The za-DFA F1 as the acceptor in the first iteration\n• 4 : 〈z2, a1〉, 5 : 〈z2, a2〉, 6 : 〈z2, a3〉. During the preprocessing stage, it is not hard to see that the minimum probability of satisfying ψ is 0 given by the adversary shown as the za-DFA Fmin in Fig. 4 while the maximum probability of satisfying ψ is 0.96. Since pmin < 0.28 < pmax, we are ready to initialize the L∗ algorithm.\nIn the first iteration, after answering membership queries, we construct a closed and consistent observation table shown in Table III with the corresponding acceptor za-DFA F1 shown in Fig. 5. Since G(3) = 0 and G(6) = 0, the acceptor is generated with 3, 6 6∈ Lm(F1). This makes OracleP return false with 13 as the positive counterexample.\nIn the second iteration, after adding 13 to Y and extending the table with membership queries, we have a new closed and consistent observation table shown in Table IV. While OracleP returns true, OracleB finds that at state s3z1q3 no outgoing transitions is defined since δ(q3, {1, 2, 3}) = q4 while q4 6∈ Qm in F2. Then string 11 is selected as the\nFigure 7. The za-DFA F3 as the acceptor in the third iteration\nq0start q1 q2 q3 1, 4 2, 5 1, 4\nFigure 8. The witness za-DFA Fc in the third iteration\nshortest observation-action sequence transiting to s3z1q3. Because 11 ∈ Y in the observation table, OracleB will mark dark states and disable transitions leading to the dark states. Then the string set Cb = {11, 14, 41, 44} is returned and CB = {11, 14, 41, 44}. With 11 as the negative counterexample and updated CB , we need to refine the table.\nIn the third iteration, we first correct and extend the observation table to a closed and consistent one shown in Table V. Now both OracleP and OracleB return true. But OracleS finds an adversary σc shown as the za-DFA Fc in Fig. 8 gives the maximum probability 0.919 of satisfying ψ. In the derived DTMC M̃, with the largest probability 0.2916 of satisfying true U≤4hd, 124 is returned as the negative counterexample and CS = {124}.\nIn the fourth iteration, after adding 124 to Y and answering membership queries, we have the new observation table in Table VI and the za-DFA in Fig. 9. While both OracleP and OracleB return true, OracleB finds the adversary shown in Fig. 10 that witnesses a probability of 0.3882 > 0.28. This time OracleS returns 121 as the negative counterexample which gives the satisfaction probability of 0.1179. The updated CS = {121, 124}.\nIn the next iteration, OracleS returns 123 as the negative counterexample and CS = {121, 123, 124}. After that, in the new iteration, we finally learn the za-DFA shown as Fig. 11 with the maximum satisfaction probability 0.271 < 0.28. Thus with six iterations, we have found the non-blocking and permissive supervisor that can regulate the closed-loop\n12\nbehavior to satisfy given PCTL specification."
    }, {
      "heading" : "VII. CONCLUSIONS AND FUTURE WORK",
      "text" : "In this paper, an L∗ learning based supervisor synthesis framework is proposed for POMDP to satisfy formal specifications. With finite horizon PCTL being considered as system specifications, we design the supervisory control framework with za-DFA. By modifying the membership queries and conjectures in the L∗ algorithm, our learning process can automatically synthesize a za-DFA with termination guarantee. The proposed algorithm is also sound and complete. However,\ndue to the challenges from the counterexample selection for probabilistic systems, OracleS may misidentify good control policies as counterexamples, in which cases the permissiveness of the supervisor cannot be guaranteed.\nIn the future, we will explore different counterexample selection algorithms for probabilistic systems to reduce the possibility of misidentification from OracleS. While currently each part of the learning process is running separately based on different packages, for example, COMICS for DTMC counterexample selection [57], libalf for L∗ algorithm [58], POMCP for POMDP solving [38], we will glue every part together to deliver a whole software package for the automatic synthesis purpose."
    } ],
    "references" : [ {
      "title" : "Exploration and mapping with autonomous robot teams",
      "author" : [ "E. Olson", "J. Strom", "R. Goeddel", "R. Morton", "P. Ranganathan", "A. Richardson" ],
      "venue" : "Communications of the ACM, vol. 56, no. 3, pp. 62–70, 2013.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Cooperative air and ground surveillance",
      "author" : [ "B. Grocholsky", "J. Keller", "V. Kumar", "G. Pappas" ],
      "venue" : "Robotics & Automation Magazine, IEEE, vol. 13, no. 3, pp. 16–25, 2006.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Decentralized cognitive mac for opportunistic spectrum access in ad hoc networks: A pomdp framework",
      "author" : [ "Q. Zhao", "L. Tong", "A. Swami", "Y. Chen" ],
      "venue" : "Selected Areas in Communications, IEEE Journal on, vol. 25, no. 3, pp. 589–600, 2007.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Datadriven probabilistic modeling and verification of human driver behavior",
      "author" : [ "D. Sadigh", "K. Driggs-Campbell", "A. Puggelli", "W. Li", "V. Shia", "R. Bajcsy", "A.L. Sangiovanni-Vincentelli", "S.S. Sastry", "S.A. Seshia" ],
      "venue" : "AAAI Spring Symposium-Technical Report, 2014.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A pomdp framework for human-in-theloop system",
      "author" : [ "C.-P. Lam", "S.S. Sastry" ],
      "venue" : "Decision and Control (CDC), 2014 IEEE 53rd Annual Conference on. IEEE, 2014, pp. 6031–6036.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Modeling and solving human-robot collaborative tasks using pomdps",
      "author" : [ "N. Gopalan", "S. Tellex" ],
      "venue" : "Proc. Robot., Sci. Syst.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Human and robot behavior modeling for probabilistic cognition of an autonomous service robot",
      "author" : [ "S.R. Schmidt-Rohr", "M. Losch", "R. Dillmann" ],
      "venue" : "Robot and Human Interactive Communication, 2008. RO-MAN 2008. The 17th IEEE International Symposium on. IEEE, 2008, pp. 635–640.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Performance guaranteed human-robot collaboration through correct-by-design",
      "author" : [ "X. Zhang", "Y. Zhu", "H. Lin" ],
      "venue" : "American Control Conference (ACC), 2016. American Automatic Control Council (AACC), 2016, pp. 6183–6188.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Pomdp-based long-term user intention prediction for wheelchair navigation",
      "author" : [ "T. Taha", "J.V. Miró", "G. Dissanayake" ],
      "venue" : "Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on. IEEE, 2008, pp. 3920–3925.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Optimal control of markov processes with incomplete state information",
      "author" : [ "K.J. Astrom" ],
      "venue" : "Journal of mathematical analysis and applications, vol. 10, no. 1, pp. 174–205, 1965.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1965
    }, {
      "title" : "The optimal control of partially observable markov processes over the infinite horizon: Discounted costs",
      "author" : [ "E.J. Sondik" ],
      "venue" : "Operations Research, vol. 26, no. 2, pp. 282–304, 1978.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1978
    }, {
      "title" : "Formal methods for control synthesis in partially observed environments: application to autonomous robotic manipulation",
      "author" : [ "H.-T. Cheng" ],
      "venue" : "Ph.D. dissertation, University of British Columbia, 1988.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Speeding up the convergence of value iteration in partially observable markov decision processes",
      "author" : [ "N.L. Zhang", "W. Zhang" ],
      "venue" : "Journal of Artificial Intelligence Research, vol. 14, no. 1, pp. 29–51, 2001.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Anytime point-based approximations for large pomdps",
      "author" : [ "J. Pineau", "G. Gordon", "S. Thrun" ],
      "venue" : "Journal of Artificial Intelligence Research, pp. 335–380, 2006.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Sarsop: Efficient point-based pomdp planning by approximating optimally reachable belief spaces.",
      "author" : [ "H. Kurniawati", "D. Hsu", "W.S. Lee" ],
      "venue" : "in Robotics: Science and systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Finite-state controllers based on mealy machines for centralized and decentralized pomdps.",
      "author" : [ "C. Amato", "B. Bonet", "S. Zilberstein" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Bounded finite state controllers.",
      "author" : [ "P. Poupart", "C. Boutilier" ],
      "venue" : "in NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Introduction to automata theory, languages, and computation",
      "author" : [ "III I. Hill" ],
      "venue" : "1979.  13",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 1979
    }, {
      "title" : "Scaling internal-state policy-gradient methods for pomdps",
      "author" : [ "D. Aberdeen", "J. Baxter" ],
      "venue" : "MACHINE LEARNING-INTERNATIONAL WORK- SHOP THEN CONFERENCE-, 2002, pp. 3–10.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Solving pomdps by searching the space of finite policies",
      "author" : [ "N. Meuleau", "K.-E. Kim", "L.P. Kaelbling", "A.R. Cassandra" ],
      "venue" : "Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers Inc., 1999, pp. 417–426.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Isomorph-free branch and bound search for finite state controllers.",
      "author" : [ "M. Grzes", "P. Poupart", "J. Hoey" ],
      "venue" : "in IJCAI. Citeseer,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Hybrid controllers for path planning: A temporal logic approach",
      "author" : [ "G.E. Fainekos", "H. Kress-Gazit", "G.J. Pappas" ],
      "venue" : "Decision and Control, 2005 and 2005 European Control Conference. CDC-ECC’05. 44th IEEE Conference on. IEEE, 2005, pp. 4885–4890.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Temporal-logic-based reactive mission and motion planning",
      "author" : [ "H. Kress-Gazit", "G.E. Fainekos", "G.J. Pappas" ],
      "venue" : "Robotics, IEEE Transactions on, vol. 25, no. 6, pp. 1370–1381, 2009.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Temporal logic motion planning for dynamic robots",
      "author" : [ "G.E. Fainekos", "A. Girard", "H. Kress-Gazit", "G.J. Pappas" ],
      "venue" : "Automatica, vol. 45, no. 2, pp. 343–352, 2009.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Synthesis of control protocols for autonomous systems",
      "author" : [ "T. Wongpiromsarn", "U. Topcu", "R.M. Murray" ],
      "venue" : "Unmanned Systems, vol. 1, no. 01, pp. 21–39, 2013.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "The temporal logic of reactive and concurrent systems: Specification",
      "author" : [ "Z. Manna", "A. Pnueli" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2012
    }, {
      "title" : "A probabilistic approach for control of a stochastic system from ltl specifications",
      "author" : [ "M. Lahijanian", "S.B. Andersson", "C. Belta" ],
      "venue" : "Decision and Control, 2009 held jointly with the 2009 28th Chinese Control Conference. CDC/CCC 2009. Proceedings of the 48th IEEE Conference on. IEEE, 2009, pp. 2236–2241.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Formal methods for control synthesis in partially observed environments: application to autonomous robotic manipulation",
      "author" : [ "R. Sharan" ],
      "venue" : "Ph.D. dissertation, California Institute of Technology, 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A symbolic sat-based algorithm for almost-sure reachability with small strategies in pomdps",
      "author" : [ "K. Chatterjee", "M. Chmelik", "J. Davies" ],
      "venue" : "arXiv preprint arXiv:1511.08456, 2015.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Safetyconstrained reinforcement learning for mdps",
      "author" : [ "S. Junges", "N. Jansen", "C. Dehnert", "U. Topcu", "J.-P. Katoen" ],
      "venue" : "International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, 2016, pp. 130–146.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Control in belief space with temporal logic specifications",
      "author" : [ "C.-I. Vasile", "K. Leahy", "E. Cristofalo", "A. Jones", "M. Schwager", "C. Belta" ],
      "venue" : "Decision and Control (CDC), 2016 IEEE 55th Conference on. IEEE, 2016, pp. 7419–7424.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Replanning in domains with partial information and sensing actions",
      "author" : [ "G. Shani", "R.I. Brafman" ],
      "venue" : "IJCAI, vol. 2011, 2011, pp. 2021– 2026.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Heuristics for planning under partial observability with sensing actions",
      "author" : [ "G. Shani", "R. Brafman", "S. Maliah", "E. Karpas" ],
      "venue" : "Proceedings of the 23rd ICAPS Workshop on Heuristics and Search for Domainindependent Planning (HSDIPąŕ13), 2013.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Synthesis of joint control and active sensing strategies under temporal logic constraints",
      "author" : [ "J. Fu", "U. Topcu" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 61, no. 11, pp. 3464–3476, 2016.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "What is decidable about partially observable markov decision processes with ω-regular objectives",
      "author" : [ "K. Chatterjee", "M. Chmelík", "M. Tracol" ],
      "venue" : "Journal of Computer and System Sciences, vol. 82, no. 5, pp. 878–911, 2016.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Bounded model checking",
      "author" : [ "A. Biere", "A. Cimatti", "E.M. Clarke", "O. Strichman", "Y. Zhu" ],
      "venue" : "Advances in computers, vol. 58, pp. 117–148, 2003.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Provably-correct stochastic motion planning with safety constraints",
      "author" : [ "C. Yoo", "R. Fitch", "S. Sukkarieh" ],
      "venue" : "Robotics and Automation (ICRA), 2013 IEEE International Conference on. IEEE, 2013, pp. 981–986.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Monte-carlo planning in large pomdps",
      "author" : [ "D. Silver", "J. Veness" ],
      "venue" : "Advances in neural information processing systems, 2010, pp. 2164– 2172.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Learning based supervisor synthesis of pomdp for pctl specifications",
      "author" : [ "X. Zhang", "B. Wu", "H. Lin" ],
      "venue" : "2015 54th IEEE Conference on Decision and Control (CDC). IEEE, 2015, pp. 7470–7475.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Permissive controller synthesis for probabilistic systems",
      "author" : [ "K. Dräger", "V. Forejt", "M. Kwiatkowska", "D. Parker", "M. Ujma" ],
      "venue" : "International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer, 2014, pp. 531–546.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A tutorial guide to mixed-integer programming models and solution techniques",
      "author" : [ "J.C. Smith", "Z.C. Taskin" ],
      "venue" : "Optimization in Medicine and Biology, pp. 521–548, 2008.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Counterexample-guided permissive supervisor synthesis for probabilistic systems through learning",
      "author" : [ "B. Wu", "H. Lin" ],
      "venue" : "American Control Conference (ACC), 2015. IEEE, 2015, pp. 2894–2899.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Mathematical techniques for analyzing concurrent and probabilistic systems",
      "author" : [ "J.J. Rutten", "M. Kwiatkowska", "G. Norman", "D. Parker" ],
      "venue" : "American Mathematical Soc.,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2004
    }, {
      "title" : "Design and synthesis of synchronization skeletons using branching time temporal logic",
      "author" : [ "E.M. Clarke", "E.A. Emerson" ],
      "venue" : null,
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 1982
    }, {
      "title" : "Principles of Model Checking",
      "author" : [ "C. Baier", "J. Katoen" ],
      "venue" : null,
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2008
    }, {
      "title" : "Prism 4.0: Verification of probabilistic real-time systems",
      "author" : [ "M. Kwiatkowska", "G. Norman", "D. Parker" ],
      "venue" : "Computer aided verification. Springer, 2011, pp. 585–591.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A storm is coming: A modern probabilistic model checker",
      "author" : [ "C. Dehnert", "S. Junges", "J.-P. Katoen", "M. Volk" ],
      "venue" : "arXiv preprint arXiv:1702.04311, 2017.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Randomness for free",
      "author" : [ "K. Chatterjee", "L. Doyen", "H. Gimbert", "T.A. Henzinger" ],
      "venue" : "International Symposium on Mathematical Foundations of Computer Science. Springer, 2010, pp. 246–257.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Modeling and control of logical discrete event systems",
      "author" : [ "R. Kumar", "V.K. Garg" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2012
    }, {
      "title" : "Logic and model checking for hidden markov models",
      "author" : [ "L. Zhang", "H. Hermanns", "D.N. Jansen" ],
      "venue" : "International Conference on Formal Techniques for Networked and Distributed Systems. Springer, 2005, pp. 98–112.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Counterexample generation in probabilistic model checking",
      "author" : [ "T. Han", "J.-P. Katoen", "D. Berteun" ],
      "venue" : "IEEE Transactions on Software Engineering, vol. 35, no. 2, pp. 241–257, 2009.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Counterexample-guided abstraction refinement for pomdps",
      "author" : [ "X. Zhang", "B. Wu", "H. Lin" ],
      "venue" : "arXiv preprint arXiv:1701.06209, 2017.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Efficient selectivity and backup operators in monte-carlo tree search",
      "author" : [ "R. Coulom" ],
      "venue" : "International Conference on Computers and Games. Springer, 2006, pp. 72–83.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine learning, vol. 47, no. 2-3, pp. 235–256, 2002.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Bandit based monte-carlo planning",
      "author" : [ "L. Kocsis", "C. Szepesvári" ],
      "venue" : "European conference on machine learning. Springer, 2006, pp. 282– 293.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Learning regular sets from queries and counterexamples",
      "author" : [ "D. Angluin" ],
      "venue" : "Information and computation, vol. 75, no. 2, pp. 87–106, 1987.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "The comics tool–computing minimal counterexamples for dtmcs",
      "author" : [ "N. Jansen", "E. Ábrahám", "M. Volk", "R. Wimmer", "J.-P. Katoen", "B. Becker" ],
      "venue" : "International Symposium on Automated Technology for Verification and Analysis. Springer, 2012, pp. 349–353.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "libalf: The automata learning framework",
      "author" : [ "B. Bollig", "J.-P. Katoen", "C. Kern", "M. Leucker", "D. Neider", "D.R. Piegdon" ],
      "venue" : "Computer Aided Verification. Springer, 2010, pp. 360–364.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],",
      "startOffset" : 137,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],",
      "startOffset" : 177,
      "endOffset" : 180
    }, {
      "referenceID" : 3,
      "context" : "The planning and control problem for such systems has become a hot research area in recent years with background varying from navigation [1], [2], communication protocol design [3], autonomous driving [4],",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "[5], and human-robot collaboration [6]–[8].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[5], and human-robot collaboration [6]–[8].",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "[5], and human-robot collaboration [6]–[8].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "This property is very useful in modeling autonomous systems with hidden states, such as advanced driver assistant system (ADAS) [5] and human-robot collaboration [7], [9] where human intention can not be directly observed.",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "This property is very useful in modeling autonomous systems with hidden states, such as advanced driver assistant system (ADAS) [5] and human-robot collaboration [7], [9] where human intention can not be directly observed.",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "This property is very useful in modeling autonomous systems with hidden states, such as advanced driver assistant system (ADAS) [5] and human-robot collaboration [7], [9] where human intention can not be directly observed.",
      "startOffset" : 167,
      "endOffset" : 170
    }, {
      "referenceID" : 34,
      "context" : "While most of the ω−regular properties are undecidable for POMDPs [35], we consider PCTL specifications with finite horizons which can bound the searching space with finite memory in POMDP model checking following the philosophy of Bounded Model Checking [36].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 35,
      "context" : "While most of the ω−regular properties are undecidable for POMDPs [35], we consider PCTL specifications with finite horizons which can bound the searching space with finite memory in POMDP model checking following the philosophy of Bounded Model Checking [36].",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 36,
      "context" : "Meanwhile, a lot of robotics applications require task completion with finite time, such as motion planning [37], which also makes PCTL specification with finite horizons suitable to describe our control tasks.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 37,
      "context" : "To check the satisfaction relation efficiently, we show the connection between the model checking and the optimal policy computation, then modify a state-ofart POMDP solving algorithm, Partially Observable MonteCarlo Planning (POMCP) [38], to reduce the computational complexity for POMDP model checking.",
      "startOffset" : 234,
      "endOffset" : 238
    }, {
      "referenceID" : 9,
      "context" : "statistics for history [10], POMDP can be viewed as MDP with a continuous state space formed by belief states.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "Exact planning of POMDP [11] can be intractable with the size of state space and planning horizon exploding quickly.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 11,
      "context" : "As one of the most popular approaches, pointbased value iteration (PBVI) optimizes the value function only over a selected finite set of belief states and provides the optimization result with a bounded error [12]–[15].",
      "startOffset" : 209,
      "endOffset" : 213
    }, {
      "referenceID" : 14,
      "context" : "As one of the most popular approaches, pointbased value iteration (PBVI) optimizes the value function only over a selected finite set of belief states and provides the optimization result with a bounded error [12]–[15].",
      "startOffset" : 214,
      "endOffset" : 218
    }, {
      "referenceID" : 15,
      "context" : "Compared to the point-based approach that solves POMDP on the continuous state space of belief states, the controllerbased approach [16] finds the optimal policy represented by a finite state controller (FSC) with finite memory.",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 16,
      "context" : "Each node has one outward edge per observation, and a policy can be executed by taking action associated with the node at current time instance and updating the current node by following the edge labeled by the observation made [17].",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 17,
      "context" : "This representation is equivalent to Moore machine [18] from automata theory [16].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 15,
      "context" : "This representation is equivalent to Moore machine [18] from automata theory [16].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 18,
      "context" : "The gradient search usually leads to a suboptimal solution that often traps in local optimum [19], [20].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "The gradient search usually leads to a suboptimal solution that often traps in local optimum [19], [20].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 16,
      "context" : "To combine the advantages from gradient ascent and policy iteration, bounded policy iteration (BPI) is proposed in [17] to limit the size of the controller and provide evidence to help escape local optimum.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "Besides direct graph as the controller form for FSC, DFA and Mealy machine have also been considered in [21] and [16], respectively.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 15,
      "context" : "Besides direct graph as the controller form for FSC, DFA and Mealy machine have also been considered in [21] and [16], respectively.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "used to generate controllers that can guarantee the system performance to satisfy high-level mission requirements [22]– [25].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 24,
      "context" : "used to generate controllers that can guarantee the system performance to satisfy high-level mission requirements [22]– [25].",
      "startOffset" : 120,
      "endOffset" : 124
    }, {
      "referenceID" : 25,
      "context" : "For complicated missions, temporal logic [26] is an efficient tool to describe requirements for system tasks due to its expressiveness and similarity to natural languages.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 26,
      "context" : "Compared to extensive studies in reward-based planning, very few results on formal methods based planning have been established for POMDP, which makes it an open problem [27].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 27,
      "context" : "In [28], the controller synthesis of POMDP with Linear Temporal Logic (LTL) specifications over infinite horizon is discussed and solved based on gradient",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 27,
      "context" : "However, this method suffers from local maxima, and the initial choice of the FSC’s structure does not have a systematic guideline [28].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 28,
      "context" : "In [29], the authors use observation-stationary (memoryless) controller to regulate POMDP to satisfy almost-sure reachability properties.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 29,
      "context" : "Since the action is selected only depends on current observation, the satisfiability modulo theories (SMT) method is applied with similar idea shows in [30] where a state-based controller for MDP is learned.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 30,
      "context" : "In [31], a linear time invariant system with linear observation model for states is considered, which is equivalent to a discrete time continuous space POMDP.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 31,
      "context" : "Similar to POMDPs, the deterministic systems with partial information have also been studied for synthesis problems in [32]–[34].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 33,
      "context" : "Similar to POMDPs, the deterministic systems with partial information have also been studied for synthesis problems in [32]–[34].",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 39,
      "context" : "Especially for permissive controller design, in [40], state-based controllers without memory are proposed for infinite horizon planning and Mixed Integer Linear Programming (MILP) [41] is applied to find a permissive controller.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 40,
      "context" : "Especially for permissive controller design, in [40], state-based controllers without memory are proposed for infinite horizon planning and Mixed Integer Linear Programming (MILP) [41] is applied to find a permissive controller.",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 29,
      "context" : "Similarly, in [30], SMT is combined with reinforcement learning to learn a state-based controller.",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 41,
      "context" : "Besides these works, using the L∗ algorithm to learn system supervisor has also been considered in our previous work for MDPs [42].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 38,
      "context" : "preliminary conference paper [39].",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 38,
      "context" : "Compared to [39], this paper makes the following new contributions.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 42,
      "context" : "[43] An MDP is a tuple M = (S, s̄, A, T ) where",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "• S is a finite set of states; • s̄ ∈ S is the initial state; • A is a finite set of actions; • T : S ×A× S → [0, 1] is a transition function.",
      "startOffset" : 110,
      "endOffset" : 116
    }, {
      "referenceID" : 42,
      "context" : "[43] A DTMC is a tuple M = (S, s̄, T ) where",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "• S is a finite set of states; • s̄ ∈ S is the initial state; • T : S × S → [0, 1] is a transition function.",
      "startOffset" : 76,
      "endOffset" : 82
    }, {
      "referenceID" : 42,
      "context" : "where s0 = s̄, si ∈ S, ai ∈ A and T (si, ai, si+1) ≥ 0 for all i ≥ 0 [43].",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 42,
      "context" : "For a labeled MDP, we can use PCTL [43] to represent the system design requirements.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 43,
      "context" : "PCTL is the probabilistic extension of the Computation Tree Logic (CTL) [44].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 42,
      "context" : "[43] The syntax of PCTL is defined as",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "/∈ {≤, <,≥, >}, p ∈ [0, 1] and k ∈ N.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 42,
      "context" : "[43] For an labeled MDPM = (S, s̄, A, T, L), the satisfaction relation for any states s ∈ S is defined inductively:",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 42,
      "context" : "The model checking of PCTL specification has been extensively studied for MDPs [43].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 42,
      "context" : "/ in the specification gives lower or upper bound, PCTL model checking of MDPs solves an optimization problem by computing either the minimum or maximum probability over all adversaries [43].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 44,
      "context" : "Since the states are fully observable, the model checking for MDPs can be solved following dynamic programming techniques with polynomial time complexity [45].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 45,
      "context" : "Different software tools for MDP model checking are available, such as PRISM [46] and recently developed model checker Storm [47].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 46,
      "context" : "Different software tools for MDP model checking are available, such as PRISM [46] and recently developed model checker Storm [47].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "• M is an MDP; • Z is a finite set of observations; • O : S × Z → [0, 1] is an observation function.",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 38,
      "context" : "If the initial status of POMDP is given as a probability distribution over S, we can add a dummy initial state then define its transitions to other s ∈ S based on the initial probability distribution [39].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 47,
      "context" : "But for the finite horizon PCTL specifications considered in our work, the pure adversaries and randomized adversaries have the same power in the sense that restricting the set of adversaries to pure strategies will not change the satisfaction relation of the considered PCTL fragments [48].",
      "startOffset" : 286,
      "endOffset" : 290
    }, {
      "referenceID" : 47,
      "context" : "While the detailed analysis follows the fact that POMDP is a one-and-a-half player game [48], the intuitive justification for this claim is that if we are just interested in upper and lower bounds to the probability of some events to happen, any probabilistic combination of these events stays within the bounds.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 47,
      "context" : "Moreover, pure adversaries are sufficient to observe the bounds [48].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 39,
      "context" : "Since the control objective is given by a finite horizon specification, history-dependent controller outperforms history-independent (memoryless or observation-stationary) one and its justification can be directly inherited from MDP cases [40].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 38,
      "context" : "[39] A supervisor for POMDP P={S, s̄, A, Z, T,O} is a za-DFA F={Q, q̄,Σ, δ, Qm}, where",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "Since DFA is an equivalent representation of regular language [49], za-DFA represents a regular set of strings with the set of the observation-action pairs in POMDP as its alphabet.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 48,
      "context" : ", Pref(L(F)) = Lm(F) where Pref(L(F)) denotes all prefixes of the language of F [49].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 48,
      "context" : "By applying the subset construction on NFA F , we can get a DFA whose accepted language is U [49].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "Remark: Compared to the global Markov chain defined in [28] describing the regulated behavior of POMDP under an FSC, the product MDP defined in Definition 7 is more general",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 38,
      "context" : "Compared to the feasibility constraint defined in our previous work [39], here we allow multiple actions being enabled given Algorithm 1: Simulation run of POMDP P regulated by za-DFA F up to time k",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 49,
      "context" : "These results are modified based on [50] where the probability space for Hidden Markov Model (HMM) is defined.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 42,
      "context" : "Then based on the general definition of the probability space on MDP [43], it is not hard to see that the probability spaces on MDPMF and POMDP P are equivalent.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 50,
      "context" : "But for finite horizon PCTL, the generality is not lost since lots of finite horizon PCTL specifications can be transformed to bounded until form and the model checking mechanism is similar as shown in [51].",
      "startOffset" : 202,
      "endOffset" : 206
    }, {
      "referenceID" : 42,
      "context" : "/ [43].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "Following this method, we can leverage recently developed POMDP solvers that can handle a larger problem size with high-efficiency [13]–[15].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "Following this method, we can leverage recently developed POMDP solvers that can handle a larger problem size with high-efficiency [13]–[15].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 27,
      "context" : "computation problem by modifying the transition structure of POMDP to make all states s |= ¬φ1 and states s |= φ2 absorbing, and designing the reward scheme that assigns 0 to intermediate transitions and 1 to the final transitions on s |= φ2 when the planning depth k is reached [28], [38], [52].",
      "startOffset" : 279,
      "endOffset" : 283
    }, {
      "referenceID" : 37,
      "context" : "computation problem by modifying the transition structure of POMDP to make all states s |= ¬φ1 and states s |= φ2 absorbing, and designing the reward scheme that assigns 0 to intermediate transitions and 1 to the final transitions on s |= φ2 when the planning depth k is reached [28], [38], [52].",
      "startOffset" : 285,
      "endOffset" : 289
    }, {
      "referenceID" : 51,
      "context" : "computation problem by modifying the transition structure of POMDP to make all states s |= ¬φ1 and states s |= φ2 absorbing, and designing the reward scheme that assigns 0 to intermediate transitions and 1 to the final transitions on s |= φ2 when the planning depth k is reached [28], [38], [52].",
      "startOffset" : 291,
      "endOffset" : 295
    }, {
      "referenceID" : 37,
      "context" : "POMDP optimal policy computation algorithm, Partially Observable Monte-Carlo Planning (POMCP) [38], that can well fit with our supervisory control framework.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 52,
      "context" : "stead of explicitly solving a POMDP, POMCP applies MonteCarlo tree search [53] by running Monte-Carlo simulations to maintain a search tree of histories.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 53,
      "context" : "1 (UCB1) [54] algorithm to maximize V (ha) + c √ logN(h)",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 51,
      "context" : "Then by initializing the current history h0 to empty, we can estimate the optimal value V (h0), which is equal to the maximum satisfaction probability V (h0) [52].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 37,
      "context" : "With the convergence guarantee in probability, the bias of the value function E[V (h0) − V (h0)] is O(log(N(h0))/N(h0) [38].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 54,
      "context" : "Given a fixed δ > 0, the probability of V (h) in the range of [V ∗(h) − ∆n/n, V ∗(h) + ∆n/n] is less or equal to δ with ∆n = 9 √ 2n ln(2/δ) for a sufficiently large number of simulations n [55].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 37,
      "context" : "in [38].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 48,
      "context" : "Within the supervisory control framework using za-DFA, our task of finding a supervisor for POMDP is converted to find a DFA, which is an equivalent representation of regular set [49].",
      "startOffset" : 179,
      "endOffset" : 183
    }, {
      "referenceID" : 55,
      "context" : "L∗ Learning Algorithm The L∗ learning algorithm [56] is proposed to learn an unknown regular set [49].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 48,
      "context" : "L∗ Learning Algorithm The L∗ learning algorithm [56] is proposed to learn an unknown regular set [49].",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 55,
      "context" : "row(y1) = row(y2); for consistence, whenever y1, y2 ∈ Y with row(y1) = row(y2), it requires that ∀α ∈ Σ, row(y1 · α) = row(y2 · α) [56].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 55,
      "context" : ", row(y1) = row(y2) but G(y1 ·α ·e) 6= G(y2 ·α ·e), then adds α · e to E and extends the table [56].",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 55,
      "context" : "With a Teacher being able to answer membership queries and conjectures, L∗ algorithm is proved to converge to the minimum DFA accepting U in polynomial time [56].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 50,
      "context" : "Then we apply the DTMC counterexample generation algorithm in [51] to get the strongest evidence as a finite path with the maximum probability of violate.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "1] [0, 0, 1]",
      "startOffset" : 3,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "[1, 1, 1] [0 .",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "[1, 1, 1] [0 .",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 0,
      "context" : "[1, 1, 1] [0 .",
      "startOffset" : 0,
      "endOffset" : 9
    }, {
      "referenceID" : 50,
      "context" : "After that, the counterexample selection algorithm will take polynomial time with k and the number of transitions in the derived DTMC [51].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 55,
      "context" : "Then combining with the time analysis of L∗ in [56], we can see that our algorithm has a complexity exponential with k, polynomial with SP , and Lf .",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 56,
      "context" : "While currently each part of the learning process is running separately based on different packages, for example, COMICS for DTMC counterexample selection [57], libalf for L∗ algorithm [58], POMCP for POMDP solving [38], we will glue every part together to deliver a whole software package for the automatic synthesis purpose.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 57,
      "context" : "While currently each part of the learning process is running separately based on different packages, for example, COMICS for DTMC counterexample selection [57], libalf for L∗ algorithm [58], POMCP for POMDP solving [38], we will glue every part together to deliver a whole software package for the automatic synthesis purpose.",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 37,
      "context" : "While currently each part of the learning process is running separately based on different packages, for example, COMICS for DTMC counterexample selection [57], libalf for L∗ algorithm [58], POMCP for POMDP solving [38], we will glue every part together to deliver a whole software package for the automatic synthesis purpose.",
      "startOffset" : 215,
      "endOffset" : 219
    } ],
    "year" : 2017,
    "abstractText" : "As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an L∗ learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}