{
  "name" : "1301.2288.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Inference in Hybrid Networks: Theoretical Limits and Practical Algorithms",
    "authors" : [ "Uri Lerner", "Ronald Parr" ],
    "emails" : [ "uri@cs.stanford.edu", "parr@cs.duke.edu" ],
    "sections" : null,
    "references" : [ {
      "title" : "Stable local compuation with con­ ditional Gaussian distributions",
      "author" : [ "F. Jensen" ],
      "venue" : "Technical Report R-99-2014,",
      "citeRegEx" : "Jensen.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jensen.",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2011,
    "abstractText" : "An important subclass of hybrid Bayesian networks are those that represent Conditional Linear Gaussian (CLG) distributionsa distribution with a multivari­ ate Gaussian component for each instantiation of the discrete variables. In this paper we explore the prob­ lem of inference in CLGs, and provide complexity re­ sults for an important class of CLGs, which includes Switching Kalman Filters. In particular, we prove that even if the CLG is restricted to an extremely simple structure of a polytree, the inference task is NP-hard. Furthermore, we show that, unless P=NP, even ap­ proximate inference on these simple networks is in­ tractable. Given the often prohibitive computational cost of even approximate inference, we must take advantage of spe­ cial domain properties which may enable efficient in­ ference. We concentrate on the fault diagnosis domain, and explore several approximate inference algorithms. These algorithms try to find a small subset of Gaus­ sians which are a good approximation to the full mix­ ture distribution. We consider two Monte Carlo ap­ proaches and a novel approach that enumerates mix­ ture components in order of prior probability. We com­ pare these methods on a variety of problems and show that our novel algorithm is very promising for large, hybrid diagnosis problems.",
    "creator" : "pdftk 1.41 - www.pdftk.com"
  }
}