{
  "name" : "1609.01995.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unifying task specification in reinforcement learning",
    "authors" : [ "Martha White" ],
    "emails" : [ "martha@indiana.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Reinforcement learning is a formalism for trial-and-error interaction between an agent and an unknown environment. This interaction is typically specified by a Markov decision process (MDP), which contains a transition model, reward model, and potentially discount parameters γ specifying a discount on the sum of future values in the return. Domains are typically separated into two cases: episodic problems (finite horizon) and continuing problems (infinite horizon). In episodic problems, the agent reaches some terminal state, and is teleported back to a start state. In continuing problems, the agent interaction is continual, with a discount to ensure a finite total reward (e.g., constant γ < 1).\nThis formalism has a long and successful tradition, but is limited in the problems that can be specified. Progressively there have been additions to specify a broader range of objectives, including options [Sutton et al., 1999], state-based discounting [Sutton, 1995, Sutton et al., 2011] and interest functions [Maei and Sutton, 2010, Sutton et al., 2016]. These generalizations have particularly been driven by off-policy learning and the introduction of general value functions for Horde [Sutton et al., 2011, White, 2015], where predictive knowledge can be encoded as more complex prediction and control tasks. Generalizations to problem specifications provide exciting learning opportunities, but can also reduce clarity and complicate algorithm development and theory. For example, options and general value functions have significant overlap, but because of different terminology and formalization, the connections are not transparent. Another example is the classic divide between episodic and continuing problems, which typically require different convergence proofs [Bertsekas and Tsitsiklis, 1996, Tsitsiklis and Van Roy, 1997, Sutton et al., 2009] and different algorithm specifications.\nIn this work, we propose a formalism for reinforcement learning task specification that unifies many of these generalizations. The focus of the formalism is to separate the specification of the dynamics of the environment and the specification of the objective within that environment. Though natural, this represents a significant change in the way tasks are currently specified in reinforcement learning and has important ramifications for simplifying implementation, algorithm development and theory.\nar X\niv :1\n60 9.\n01 99\n5v 1\n[ cs\n.A I]\n7 S\nep 2\n01 6\nThe paper consists of two main contributions. First, we demonstrate the utility of this formalism by showing unification of previous tasks specified in reinforcement learning, including options, general value functions and episodic and continuing, and further providing case studies of utility. We demonstrate how to specify episodic and continuing tasks with only modifications to the discount function, without the addition of states and modifications to the underlying Markov decision process. This enables a unification that significantly simplifies implementation and easily generalizes theory to cover both settings. Second, we prove novel contraction bounds on the Bellman operator for these generalized RL tasks, and show that previous bounds for both episodic and continuing tasks are subsumed by this more general result. Overall, our goal is to provide an RL task formalism that requires minimal modifications to previous task specification, with significant gains in simplicity and unification across common settings."
    }, {
      "heading" : "2 Generalized problem formulation",
      "text" : "We assume the agent interacts with an environment formalized by a Markov decision process (MDP): (S,A,Pr) where S is the set of states, n = |S|; A is the set of actions; and Pr : S ×A× S → [0, 1] is the transition probability function where Pr(s, a, s′) is the probability of transitioning from state s into state s′ when taking action a. A reinforcement learning task (RL task) is specified on top of these transition dynamics, as the tuple (P, r, γ, i) where\n1. P is a set of policies π : S ×A → [0, 1]; 2. the reward function r : S ×A× S → R specifies reward received from (s, a, s′); 3. γ : S ×A× S → [0, 1] is a transition-based discount function1; 4. i : S → [0,∞) is an interest function that specifies the user defined interest in a state.\nEach task could have different reward functions within the same environment. For example, in a navigation task within an office, one agent could have the goal to navigate to the kitchen and the other the conference room. For a reinforcement learning task, whether prediction or control, a set or class of policies is typically considered. For prediction (policy evaluation), we often select one policy and evaluate its long-term discounted reward. For control, where a policy is learned, the set of policies may consist of all policies parameterized by weights that specify the action-value from states, with the goal to find the weights that yield the optimal policy. For either prediction or control in an RL task, we often evaluate the return of a policy: the cumulative discounted reward obtained from following that policy\nGt = ∞∑ i=0 i−1∏ j=0 γ(st+j , at+j , st+1+j) Rt+1+i where ∏−1 j=0 γ(st+j , at+j , st+1+j) := 1. Note that this subsumes the setting with a constant discount\nγc ∈ [0, 1), by using γ(s, a, s′) = γc for every s, a, s′ and so giving ∏i−1 j=0 γ(st+j , at+j , st+1+j) = γic for i > 0 and γ 0 c = 1 for i = 0. As another example, the end of the episode, γ(s, a, s\n′) = 0, making the product of these discounts zero and so terminating the recursion. We further explain how transition-based discount enables specification of episodic tasks and discuss the utility of the generalization to transition-based discounting throughout this paper. Finally, the interest function i specifies the degree of importance of each state for the task. For example, if an agent is only interested in learning an optimal policy for a subset of the environment, the interest function could be set to one for those states and to zero otherwise.\nWe will first explain the specification and use of such tasks, and then define essential learning functions, including the Bellman operator and algorithmic extensions."
    }, {
      "heading" : "2.1 Unifying specification of episodic and continuing problems",
      "text" : "The RL task specification enables episodic and continuing problems to be easily encoded with only modification to the transition-based discount. Previous approaches, including the absorbing state\n1We describe a further probabilistic generalization in Appendix A; much of the treatment remains the same, but the notation becomes cumbersome and the utility obfuscated.\nformulation [Sutton and Barto, 1998] and state-based discounting [Sutton, 1995, Maei and Sutton, 2010, Sutton et al., 2011][van Hasselt, 2011, Section 2.1.1], require special cases or modifications to the set of states and underlying MDP, coupling task specification and the dynamics of the environment.\nWe demonstrate how transition-based discounting seamlessly enables episodic or continuing tasks to be specified in an MDP via a simple chain world. Consider the chain world with three states s1,s2 and s3 in Figure 1. The start state is s1 and the two actions are right and left. The reward is -1 per step, with termination occurring when taking action right from state s3, which causes a transition back to state s1. The discount is 1 for each step, unless specified otherwise. The interest is set to 1 in all states, which is the typical case, meaning performance from each state is equally important.\nFigure 1a depicts the classical approach to specifying episodic problems using an absorbing state, drawn as a square. The agent reaches the goal of transitioning right from state s3, and then forever stays in the absorbing state, receiving a reward of zero. This encapsulates the definition of the return, but does not allow the agent to start another episode. In practice, when this absorbing state is reached, the agent is “teleported\" to a start state to start another episode. This episodic interaction can instead be represented the same way as a continuing problem, by specifying a transition-based discount γ(s3, right, s1) = 0. This defines the same return, but now the agent simply transitions normally to a start state, and no hypothetical states are added.\nTo further understand the equivalence, consider the updates made by TD (see equation (2)). Assume linear function approximation with feature function x : S → Rd, and have current weights w ∈ Rd. When the agent takes action right from s3, the agent transition from s3 to s1 with probability equal to one and so γt+1 = γ(s3, right, s1) = 0. This correctly gives\nδt = rt+1 + γt+1x(s1) >w − x(s3)>w = rt+1 − x(s3)>w\nand correctly clears the eligibility trace for the next step\net+1 = λt+1γt+1et + x(s1) = x(s1).\nThe stationary distribution is also clearly equal to the original episodic task, since the absorbing state is not used in the computation of the stationary distribution.\nAnother strategy is to still introduce hypothetical states, but use state-based γ, as discussed in Figure 1c. Unlike absorbing states, the agent does not stay indefinitely in the hypothetical state. When the agent goes right from s3, it transitions to hypothetical state s4, and then transition deterministically to the start state s1, with γs(s4) = 0. As before, we get the correct update, because γt+1 = γs(s4) = 0. Because the stationary distribution has some non-zero probability in the hypothetical state s4, we\nmust set x(s4) = x(s1) (or x(s4) = 0). Otherwise, the value of the hypothetical state will be learned, wasting function approximation resources and potentially modifying the approximation quality of the value in other states. We could also have tried state-based discounting without adding an additional state s4. However, this leads to incorrect value estimates, as depicted in Figure 1d; the relationship between transition-based and state-based is further discussed in Appendix B.1. Overall, to keep the specification of the RL task and the MDP separate, transition-based discounting is necessary to enable the unified specification of episodic and continuing tasks."
    }, {
      "heading" : "2.2 Options as RL tasks",
      "text" : "The options framework [Sutton et al., 1999] generically covers a wide range of settings, with discussion about macro-actions, option models, interrupting options and intra-option value learning. These concepts at the time merited their own language, but with recent generalizations can now more conveniently be cast as RL subtasks. Proposition 1. An option, defined as the tuple [Sutton et al., 1999, Section 2] (π, β, I) with policy π : S ×A → [0, 1], termination function β : S → [0, 1] and an initiation set I ⊂ S from which the option can be run, can be equivalently cast as an RL task.\nProof: This proof is mainly definitional. The discount function γ(s, a, s′) = 1−β(s′) for all s, a, s′ specifies termination. The interest function, i(s) = 1 if s ∈ I and i(s) = 0 otherwise, focuses learning resources on the states of interest. If a value function for the policy is queried, it would only make sense to query it from these states of interest. If the policy for this option is optimized for this interest function, the policy should only be run starting from s ∈ I, as elsewhere will be poorly learned. The rewards for the RL task correspond to the rewards often associated with the MDP.\nRL tasks provide a natural generalization to options, by generalizing the termination conditions to transition-based discounting and by providing degrees of interest rather than binary interest.\nFurther, as before, the policies associated with RL subtasks can be used as macro-actions, to specify a semi-Markov decision process. A set of such macro-actions can be specified from each state, Os, where some macro-actions o can only be initiated from states of interest. The addition of options as macro-actions to the MDP has been shown to constitute a valid semi-Markov decision process [Sutton et al., 1999, Theorem 1]. This result easily extends to these more general RL tasks. This relation is important for practically using these subtasks as part of semi-Markov decision processes, for which we have standard learning and planning algorithms [Sutton et al., 1999]."
    }, {
      "heading" : "2.3 General value functions and predictive questions",
      "text" : "In a similar spirit of abstraction as options, general value functions were introduced for single predictive or goal-oriented questions about the world [Sutton et al., 2011]. The idea is to encode predictive knowledge in the form of value function predictions: with a collection or horde of prediction demons, this constitutes knowledge [Sutton et al., 2011, Modayil et al., 2014, White, 2015]. The work on Horde [Sutton et al., 2011] and nexting [Modayil et al., 2014] provide numerous examples of the utility of the types of questions that can be specified by general value functions, and so by RL tasks, because general value functions can naturally can be specified as an RL task.\nThe generalization to RL tasks provide additional benefits for predictive knowledge. The separation into underlying MDP dynamics and task specification is particularly useful in off-policy learning, with the Horde formalism, where many demons (value functions) are learned off-policy. These demons share the underlying dynamics, and even feature representation, but have separate prediction and control tasks; keeping these separate from the MDP is key for avoiding complications (see Appendix B.2). Transition-based discounts, over state-based discounts, additionally enable the prediction of a change, caused by transitioning between states. Consider the taxi domain, described more fully in Section 3, where the agent’s goal is to pick up and drop off passengers in a grid world with walls. The taxi agent may wish to predict the probability of hitting a wall, when following a given policy. This can be encoded by setting γ(s, a, s) = 0 if the action causes the agent to remain in the same state, which occurs since one cannot move through the wall. Finally, in addition to enabling episodic problems and hard termination, transition-based questions also enable soft termination for transitions. Hard termination uses γ(s, a, s′) = 0 and soft termination γ(s, a, s′) = for some small positive value . Soft terminations can be useful for incorporating some of the value of a policy right after the soft termination. If two policies are equivalent up to a transition, but have very different returns after\nthe transition, a soft termination will reflect that difference. We empirically demonstrate the utility of soft termination in the next section."
    }, {
      "heading" : "3 Demonstration in the taxi domain",
      "text" : "To better ground this generalized formalism and provide some intuition, we provide a demonstration of RL task specification. We explore different transition-based discounts in the taxi domain [Dietterich, 2000, Diuk et al., 2008]. The goal of the agent is to take a passenger from a source platform to a destination platform, depicted in Figure 2. The agent receives a negative reward of -1 on each step, except for successful pickup and drop-off, which gives a reward of 0. We modify the domain to include the orientation of the taxi, with additional cost for not continuing in the current orientation. This encodes that turning right, left or going backwards are more costly than going forwards, with additional negative rewards of -0.1, -0.2 and -0.2 respectively. This additional cost is further multiplied by a factor of 2 when there is a passenger in the vehicle. For grid size g and the number of pickup/dropoff locations l, the full state information is a 5-tuple: (x position of taxi ∈ {1, . . . , g}, y position of taxi ∈ {1, . . . , g}, location of passenger ∈ {1, . . . , l + 1}, location of destination ∈ {1, . . . , l}, orientation of car ∈ {N,E, S,W} ). The location of the passenger can be in one of the pickup/drop-off locations, or in the taxi. Optimal policies and value functions are computed iteratively, with an extensive number of iterations.\nFigure 2 illustrates three policies for one part of the taxi domain, obtained with three different discount functions. The optimal policy is learned using a soft-termination, which takes into consideration the importance of approaching the passenger location with the right orientation to minimize turns after picking up the passenger. A suboptimal policy is in fact learned with hard termination, as the policy\nprefers the more locally greedy choice of minimizing turns to get to the passenger. For further details, refer to the caption in Figure 2.\nWe also compare to a constant γ, which corresponds to an average reward goal, as demonstrated in Equation (6). Table 3 summarizes the results. Though in theory it should in fact recognize the relative values of orientation before and after picking up a passenger, and obtain the same solution as the soft-termination policy, in practice we find that numerical imprecision actually causes a suboptimal policy to be learned, as seen in Figure 3. Because most of the rewards are negative per step, small differences in orientation can be more difficult to distinguish amongst for an infinite discounted sum. This result actually suggests that having multiple subgoals, as one might have with RL subtasks (options), could enable better chaining of decisions and local evaluation of the optimal action."
    }, {
      "heading" : "4 Objectives and algorithms",
      "text" : "With an intuition for the utility of the specification of problems as RL tasks, we now turn to generalizing some key algorithmic concepts to enable learning for RL tasks. We first generalize the definition of the Bellman operator for the value function. For a policy π : S × A → [0, 1], define Pπ,Pπ,γ ∈ Rn×n and rπ,vπ ∈ Rn, indexed by states s, s′ ∈ S,\nPπ(s, s ′) := ∑ a∈A π(s, a)Pr(s, a, s′) Pπ,γ(s, s ′) := ∑ a∈A π(s, a)Pr(s, a, s′)γ(s, a, s′)\nrπ(s) := ∑ a∈A π(s, a) ∑ s′∈S Pr(s, a, s′)r(s, a, s′) vπ(s) := rπ(s) + ∑ s′∈S Pπ,γ(s, s ′)vπ(s ′).\nwhere vπ(s) is the expected return, starting from a state s ∈ S. To compute a value function that satisfies this recursion, we define a Bellman operator. The Bellman operator has been generalized to include state-based discounting and a state-based trace parameter2 [Sutton et al., 2016, Eq. 29]. We further generalize the definition to the transition-based setting. The trace parameter λ : S×S → [0, 1] influences the fixed point and provides a modified (biased) return, called the λ-return; this parameter is typically motivated as a bias-variance trade-off parameter [Kearns and Singh, 2000]. In this work, the focus of the discussion is on the impact of the generalization of the discount, so we opt for the simpler state-to-state generalization of the trace parameter.\nThe generalized Bellman operator T(λ) : Rn → Rn is\nT(λ)v := rλπ + P λ πv, ∀v ∈ Rn (1)\nwhere Pλπ := (I−Pπ,γ ◦Λ) −1 Pπ,γ ◦ (I − Λ) and rλπ := (I−Pπ,γ ◦Λ) −1\nrπ. The symbol ◦ denotes element-wise product (Hadamard product), with Λ ∈ Rn×n s.t. Λ(s, s′) = λ(s, s′). To see why this is the definition of the Bellman operator, we define the expected λ-return, vπ,λ ∈ Rn for a given approximate value function, given by a vector v ∈ Rn.\nvπ,λ(s) := rπ(s) + ∑ s′∈S Pπ,γ(s, s ′) [(1− λ(s, s′))v(s′) + λ(s, s′)vπ,λ(s′)]\n= rπ(s) + Pπ,γ(s, :) ◦ (1−Λ(s, :))v + Pπ,γ(s, :) ◦Λ(s, :)vπ,λ.\n2A generalization to state-based trace parameters has been considered in [Sutton, 1995, Sutton and Barto, 1998, Maei and Sutton, 2010, Sutton et al., 2014, Yu, 2012]\nContinuing the recursion, we obtain3\nvπ,λ = [ ∞∑ i=0 (Pπ,γ ◦Λ) i ] (rπ + Pπ,γ ◦ (I−Λ)v) = (I−Pπ,γ ◦Λ)−1 (rπ + Pπ,γ ◦ (I−Λ)v)\n= T(λ)v\nThe fixed point to this formula satisfies T(λ)v = v for the Bellman operator defined in Equation (1).\nThe next step is to obtain algorithms to find this fixed point. Many algorithms can be easily generalized by replacing γc or γs(st+1) with transition-based γ(st, at, st+1). For example, the TD algorithm is generalized by setting the discount on each step to γt+1 = γ(st, at, st+1), λt+1 = λ(st, st+1):\nwt+1 = wt + αtδtet . for some step-size αt (2)\nδt = rt+1 + γt+1x(st+1) >w − x(st)>w\net = t∑ i=0  i∏ j=1 γt−j+1λt−j+1 x(st−i) = γtλtet−1 + x(st). The generalized TD fixed-point, under linear function approximation, can be expressed as a linear system Aw = b where\nA = X>D(I−Pπ,γ ◦Λ)−1(I−Pπ,γ)X b = X>D(I−Pπ,γ ◦Λ)−1rπ\nwhere each row in X ∈ Rn×d corresponds to features for a state, and D ∈ Rn×n is a diagonal weighting matrix. Typically, D = diag(dµ), where dµ ∈ Rn is the stationary distribution for the behavior policy µ : S × A → [0, 1] generating the stream of interaction. In on-policy learning, dµ = dπ. With the addition of the interest function, this weighting changes to D = diag(dµ ◦ i). More recently, a new algorithm, emphatic TD (ETD) [Mahmood et al., 2015, Sutton et al., 2016] specified yet another weighting, D = M where M = diag(m) with m = (I − Pλπ)−1(dµ ◦ i). Importantly, even for off-policy sampling, with this weighting, A is guaranteed to be positive definite. Therefore, it constitutes the currently most general algorithm for learning RL tasks, and we put more focus on understanding its properties for this generalization in the next section."
    }, {
      "heading" : "5 Analysis of contraction properties and convergence for RL tasks",
      "text" : "In this section, we provide a general approach to incorporating transition-based discounting into approximation bounds. Most previous bounds have assumed a constant discount. For example, ETD was introduced with state-based γs; however, Hallak et al. [2015] analyzed approximation error bounds of ETD using a constant discount γc. By using matrix norms on Pπ,γ instead, we generalize previous approximation bounds to both the episodic and continuing case."
    }, {
      "heading" : "5.1 List of Assumptions",
      "text" : "Define the set of bounded vectors for the general space of value functions V = {v ∈ Rn : ‖v‖Dµ < ∞}. We assume throughout that we have some subspace Fv ⊂ V of possible solutions, such as is created by linear function approximation, e.g., Fv = {Xw|w ∈ Rd, ‖w‖2 <∞}.\nA1. The action space A and state space S are finite. A2. For given polices µ, π : S ×A → [0, 1], there exist unique invariant distributions dµ and dπ\nsuch that dπPπ = dπ and dµPµ = dµ. This assumption is typically satisfied by assuming an ergodic Markov chain for the policy.\nA3. There exist transition s, a, s′ such that γ(s, a, s′) < 1 and π(s, a)P (s, a, s′) > 0. This assumptions states that the policy reaches some part of the space where the discount is less than 1.\n3For a matrix M with maximum eigenvalue less than 1, ∑∞ i=0 M\ni = (I−M)−1. We show in Lemma 3 that Pπ,γ satisfies this condition, implying Pπ,γ ◦Λ satisfies this condition and so this infinite sum is well-defined.\nA4. Assume for any v ∈ Fv, if v(s) = 0 for all s ∈ S where i(s) > 0, then v(s) = 0 for all s ∈ S s.t. i(s) = 0. For linear function approximation, this requires F = span{x(s) : s ∈ S, i(s) 6= 0}.\nFor weighted norm ‖v‖D = √\nv>Dv, if we can take the square root of D, the induced matrix norm is ‖Pλπ‖D = ∥∥D1/2PλπD1/2∥∥sp, where the spectral norm ‖·‖sp is the largest singular value of the matrix. For simplicity of notation below, define sD := ‖Pλπ‖D. Further, for any diagonalizable, nonnegative matrix D, the projection ΠD : V → Fv onto Fv exists and is defined as ΠDz = argminv∈Fv ‖z− v‖D."
    }, {
      "heading" : "5.2 Approximation bound",
      "text" : "We first prove that the generalized Bellman operator in Equation 1 is a contraction and bound bias incurred due to λ. We extend the bound from [Tsitsiklis and Van Roy, 1997, Hallak et al., 2015] for constant discount and constant trace parameter, to the general transition-based setting. The normed difference to the true value function could be defined by multiple weightings. A well-known result is that for D = Dπ the Bellman operator is a contraction for constant γc and λc [Tsitsiklis and Van Roy, 1997]; recently, this has been generalized for a variant of ETD to M, still with constant parameters [Hallak et al., 2015]. We extend this result for transition-based γ and λ and for both Dπ and the transition-based emphasis matrix M.\nLemma 1. For D = Dπ or D = M,\nsD = ‖Pλπ‖D < 1.\nProof: For D = Dπ:\n‖Pλπ‖Dπ = ∥∥∥D1/2π PλπD1/2π ∥∥∥ sp ≤ ∥∥Pλπ∥∥sp\nbecause D1/2π can only decrease entries in Pλπ , and so the maximum singular value of D 1/2 π PλπD 1/2 π must be less than D1/2π PλπD 1/2 π , by the same argument as in Lemma 3. Since Pλπ has maximum eigenvalue strictly less than 1 by Lemma 4, we get that sD < 1.\nFor D = M:\nlet ξ ∈ Rn be the vector of row sums for Pλπ: Pλπ1 = ξ. Then for any v ∈ V , with v 6= 0,\n‖Pλπv‖2M = ∑ s∈S m(s) (∑ s′∈S Pλπ(s, s ′)v(s′) )2\n= ∑ s∈S m(s)ξ(s)2 (∑ s′∈S Pλπ(s, s ′) ξ(s) v(s′) )2\n≤ ∑ s∈S m(s)ξ(s)2 ∑ s′∈S Pλπ(s, s ′) ξ(s) v(s′)2\n= ∑ s′∈S v(s′)2 ∑ s∈S m(s)ξ(s)Pλπ(s, s ′)\n= ∑ s′∈S v(s′)2(m ◦ ξ)>Pλπ(:, s ′)\n= v> diag ( (m ◦ ξ)>Pλπ ) v\nwhere the first inequality follows from Jensen’s inequality, because Pλπ(s, :) is normalized. Now because ξ has entries that are less than 1, due to the fact that row sums of Pλπ are less than 1 as shown\nin Lemma 4, and because each of the values in the above product are nonnegative, v> diag ( (m ◦ ξ)>Pλπ ) v ≤ v> diag ( m>Pλπ ) v\n= v> diag ( m>(Pλπ − I) + m> ) v\n= v> diag ( −(dπ ◦ i)> + m> ) v\n= v> diag ( m> ) v − v> diag ( (dπ ◦ i) >)v < ‖v‖2M\nThe last inequality is a strict inequality because dπ ◦ i has at least one positive entry where v has a positive entry. Otherwise, if v(s) = 0 everywhere with i(s) > 0, then v = 0, which we assumed was not the case.\nTherefore, ‖Pλπv‖M < ‖v‖M for any v 6= 0, giving ‖Pλπ‖M := maxv∈Rn,v 6=0 ‖Pλπv‖M ‖v‖M < 1.\nLemma 2. Under assumptions A1-A3, the Bellman operator T(λ) in Equation (1) is a contraction under a norm weighted by D = Dπ or D = Mπ , i.e., for v ∈ V\n‖T(λ)v‖D < ‖v‖D.\nand therefore has a unique fixed point in V . Further, because the projection ΠD is a contraction, ΠDT (λ) is also a contraction and has a unique fixed point ΠDT(λ)v = v for v ∈ Fv .\nProof: Because any vector v can be written in terms of v = v1 − v2,\n‖T(λ)v‖Dπ = ‖T(λ)(v1 − v2)‖Dπ = ‖Pλπ(v1 − v2)‖Dπ ≤ ‖Pλπ‖Dπ‖v‖Dπ < ‖v‖Dπ\nwhere the last strict inequality follows from Lemma 1. By the Banach Fixed Point theorem, because the Bellman operator is a contraction under Dπ , it has a unique fixed point.\nTheorem 1. If D satisfies sD < 1, then there exists v ∈ Fv such that ΠDT(λ)v = v, and the error to the true value function is bounded as\n‖v − v∗‖D ≤ (1− sD)−1‖ΠDv∗ − v∗‖D. (3)\nFor constant discount γc ∈ [0, 1) and constant trace parameter λc ∈ [0, 1], this bound reduces to the original bound [Tsitsiklis and Van Roy, 1997, Lemma 6]: (1− sD)−1 ≤ 1−γcλc1−γc .\nProof: Let v be the unique fixed point of ΠDT(λ), which exists by Lemma 2.\n‖v − v∗‖D ≤ ‖v −ΠDv∗‖D + ‖ΠDv∗ − v∗‖D = ‖ΠT(λ)v −ΠDv∗‖D + ‖ΠDv∗ − v∗‖D ≤ ‖T(λ)v − v∗‖D + ‖ΠDv∗ − v∗‖D = ‖T(λ)(v − v∗)‖D + ‖ΠDv∗ − v∗‖D = ‖Pλπ(v − v∗)‖D + ‖ΠDv∗ − v∗‖D = ‖Pλπ‖D‖v − v∗‖D + ‖ΠDv∗ − v∗‖D = sD‖v − v∗‖D + ‖ΠDv∗ − v∗‖D\nwhere the second inequality is due to ‖ΠT(λ)v‖D ≤ ‖T(λ)v‖D, the second equality due to T(λ)v∗ = v∗ and the third equality due to T(λ)v−T(λ)v∗ = Pλπ(v−v∗) because the rπ cancels. By rearranging terms, we get\n(1− sD)‖v − v∗‖Dπ ≤ ‖Πv∗ − v∗‖Dπ\nand since sD < 1, we get the final result.\nFor constant γc < 1 and λc, because Pπ,γ = γPπ , we get\nsD = ‖Pλπ‖D\n= ‖D1/2 ∞∑ i=0 (Pπ,γ ◦Λ) iPπ,γ ◦ (I−Λ)D1/2‖2\n= ‖D1/2 ( ∞∑ i=0 γicλ i cP i π ) γc(1− λc)PπD1/2‖2\n= γc(1− λc) ∥∥∥∥∥ ∞∑ i=0 γicλ i cD 1/2PiπPπD 1/2 ∥∥∥∥∥ 2\n≤ γc(1− λc) ∞∑ i=0 γicλ i c‖D1/2Pi+1π D1/2‖2\n= γc(1− λc) ∞∑ i=0 γicλ i c‖Pi+1π ‖D\n≤ γc(1− λc) ∞∑ i=0 γicλ i c\n= γc(1− λc) 1− γcλc\nwhere ‖Pi+1π ‖D ≤ 1 using the same Jensen’s inequality argument as above."
    }, {
      "heading" : "5.3 Properties of learning algorithms",
      "text" : "Emphatic TD To be a practical formalism, a method for learning value functions and policies for RL tasks is required. For policy evaluation, ETD and ELSTD, the least-squares version of ETD that uses the above defined A and b with D = M, have both been shown to converge with probability one [Yu, 2015]. As an important component of this proof is convergence in expectation, which relies on A being positive definite. In particular, for appropriate step-sizes αt (see [Yu, 2015]), if A is positive definite, the iterative update is convergent wt+1 = wt + αt(b−Awt). We extend convergence in expectation for the emphatic algorithms to ELSTDQ, which learns action-value functions (see the pseudocode in Appendix D), with theorem statement and proof provided in Appendix F.\nConvergence rate of LSTD(λ) Tagorti and Scherrer [2015] recently provided convergence rates for LSTD(λ) for continuing tasks, for some γc < 1. These results can be extended to the episodic setting with the generic treatment of Pλπ. For example, in [Tagorti and Scherrer, 2015, Lemma 1], which describes the sensitivity of LSTD, the proof extends by replacing the matrix (1−λc)γcPπ(I− λcγcPπ)\n−1 (which they call M in their proof) with the generalization Pλπ, resulting instead in the constant 11−sD in the bound rather than 1−λcγc 1−γc . Further, this generalizes convergence rate results to emphatic LSTD, since M satisfies the required convergence properties, with rates dictated by sM rather than sDµ for standard LSTD."
    }, {
      "heading" : "6 Discussion and conclusion",
      "text" : "The goal of this paper is to provide intuition and examples of how to use the RL task formalism. Consequently, to avoid jarring the explanation, technical contributions were not emphasized, and in some cases included only in the appendix. For this reason, we would like to highlight and summarize the technical contributions, which include 1) the introduction of the RL task formalism, and of transition-based discounts; 2) generalized approximation bounds, applying to both episodic and continuing tasks; 3) an explicit characterization of the relationship between state-based and transitionbased discounting. This work, through intuition from simple examples and simple but fundamental theoretical extensions, provides a relatively complete characterization of the RL task formalism, as a foundation for use in practice and theory."
    }, {
      "heading" : "A More general formulation with probabilistic discounts",
      "text" : "In the introduction of transition-based discounting, we could have instead assumed that we had a more general probability model: Pr(r, γ|s, a, s′). Now, both the reward and discount are not just functions of states and action, but also are stochastic. This generalization in fact, does not much alter the treatment in this paper. This is because, when taking the expectations for value function, the Bellman operator and the A matrix, we are left again with γ(s, a, s′). To see why,\nvπ(s) = ∑ a,s′ π(s, a)Pr(s, a, s′)E[r + γvπ(s′)|s, a, s′]\n= ∑ a,s′ π(s, a)Pr(s, a, s′)E[r|s, a, s′] + ∑ a,s′ π(s, a)Pr(s, a, s′)E[γ|s, a, s′]vπ(s′)\n= rπ(s) + ∑ s′ Pπ,γ(s, s ′)vπ(s ′)\nfor γ(s, a, s′) = E[γ|s, a, s′]."
    }, {
      "heading" : "B Relationship between state-based and transition-based discounting",
      "text" : "In this section, we show that for any MDP with transition-based discounting, we can construct an equivalent MDP with state-based discounting. The MDPs are equivalent in the sense that learned policies and value functions learned in either MDP would have equal values when evaluated on the states in the original transition-based MDP. This equality ignores practicality of learning in the larger induced state-based MDP, and at the end of this section, we discuss advantages of the more compact transition-based MDP.\nB.1 Equivalence result\nThe equivalence is obtained by introducing hypothetical states for each transition. The key is then to prove that the stationary distribution for the state-based MDP, with additional hypothetical states, provides the same solution even with function approximation. For each triplet s, a, s′, add a new hypothetical state fsas′ , with set F comprised of these additional states. Each transition now goes through a hypothetical state, fsas′ , and allows the discount in the hypothetical state to be set to γ(s, a, s′). The induced state-based MDP has state set S̄ = S ∪ F with |S̄| = |A|n2 + n. We define the other models in the proof in Appendix B.3.\nThe choice of action in the hypothetical states is irrelevant. To extend the policy π, we arbitrarily choose that the policy uniformly selects actions when in the hypothetical states and define π̄(s, a) = π(s, a) for s ∈ S and π̄(s, a) = 1/|A| otherwise. For linear function approximation, we also need to assume x(fsas′) = x(s′) for fsas′ ∈ F . Theorem 2. For a given transition-based MDP (Pr, r,S,A, γ) and policy π, assume that the stationary distribution dπ exists. Define state-based MDP (P̄r, r̄, S̄,A, γ̄s) with extended π̄, all as above. Then the stationary distribution d̄π for π̄ exists and satisfies\nd̄π(s)∑ i∈S d̄π(i) = dπ(s). (4)\n∀s ∈ S, v̄π(s) = vπ(s) and π̄(s, a) = π(s, a) for all s ∈ S, a ∈ A with π = argminπ ∑ s∈S dπ(s)vπ(s); π̄ = argminπ ∑ i∈S̄ d̄π(s)v̄π(s)\nB.2 Advantages of transition-based discounting over state-based discounting\nThough the two have equal representational abilities, there are several disadvantages of state-based discounting that compound to make the more general transition-based discount strictly more desirable. The disadvantages of using an induced state-based MDP, rather than the original transition-based MDP, arises from the addition of states and include the following.\nCompactness. In the worst-case, for a transition-based MDP with n states, the induced state-based MDP can have |A|n2 + n states.\nProblem definition changes for different discounts. For the same underlying MDP, multiple learners with different discount functions would have different induced state-based MDPs. This complicates code and reduces opportunities for sharing variables and computation.\nOverhead. Additional states must be stored, with additional algorithmic updates in those non-states, or cases to avoid these updates, and the need to carefully set features for hypothetical states. This overhead is both computational as well as conceptual, as it complicates the code.\nStationary distribution. This distribution superfluously includes hypothetical states and requires renormalization to obtain the stationary distribution for the original transition-based MDP.\nOff-policy learning. In off-policy learning, one goal is to learn many value functions with different discounts [White, 2015]. As mentioned above, these learners may have different induced state-based MDPs, which complicates implementation and even theory. To theoretically characterize a set of off-policy learners, it would be necessary to consider different induced state-based MDPs. Further, sharing information, such as the features, is again complicated by using induced state-based MDP rather than a single transition-based MDP, with varying discount functions.\nB.3 Proof of Theorem 2\nThis prove illustrates the representability relationship between transition-based discounting and state-based discounting. This equivalence could be obtained more compactly if γ(s, a, s′) is not different for every s′; however, the proof becomes much more involved. Since our main goal is to simply show a representability result, we opt for interpretability. Note that, in addition, the result in Theorem 2 fills a gap in the previous theory, which indicated that state-based discounting could be used to represent episodic problems, but did not explicitly demonstrate that the stationary distribution would be equivalent (see [Yu, 2015]).\nDefine transition probabilities P̄r : S̄ × S̄ → [0, 1]\nP̄r(i, a, j) = { Pr(i, a, s′) i ∈ S, j = fias′ 1 j ∈ S, i = fsaj 0 otherwise\nrewards\nr̄(i, a, j) = { r(i, a, s′) i ∈ S, j = fias′ 0 otherwise\nand state-based discount function γ̄s : S̄ → [0, 1]\nγ̄s(i) = { γ(s, a, s′) i = fsas′ 1 otherwise\nTheorem 2 For a given transition-based MDP (Pr, r,S,A, γ) and policy π, assume that the stationary distribution dπ exists. Define state-based MDP (P̄r, r̄, S̄,A, γ̄s) with extended π̄, all as above. Then the stationary distribution d̄π for π̄ exists and satisfies\nd̄π(s)∑ i∈S d̄π(i) = dπ(s) (5)\nand for all s ∈ S, v̄π(s) = vπ(s).\nProof: Define matrix P̄π ∈ R(n+|A|n 2)×(n+|A|n2) where P̄π(i, j) = ∑ a∈A π̄(i, a)P̄r(i, a, j), giving\nP̄π(i, j) = { π(i, a) Pr(i, a, s′) i ∈ S, j = fias′ 1 i = fsaj , a ∈ A, j ∈ S 0 otherwise\nDefine\nd̄π(i) := 1\nc { dπ(i) i ∈ S dπ(s)π(s, a) Pr(s, a, s ′) i = fsas′\nwhere c > 0 is a normalizer to ensure that 1>d̄π = 1. Now we need to show that d̄πP̄π = d̄π. For any j ∈ S,\nd̄πP̄π(:, j) = 1\nc (∑ s∈S dπ(s)P̄π(s, j) + ∑ f∈F d̄π(f)P̄π(f, j) )\nCase 1: j ∈ S For the first component, because P̄π(s, j) = 0 by definition of P̄r, we get∑\ns∈S dπ(s)P̄π(s, j) = 0\nFor the second component, because P̄π(fsaj , j) = 1,∑ fsas′∈F d̄π(fsas′)P̄π(fsas′ , j) = ∑ fsaj∈F d̄π(fsaj)P̄π(fsaj , j)\n= ∑\nfsaj∈F\nd̄π(fsaj)\n= ∑ s∈S dπ(s) ∑ a∈A π(s, a)Pr(s, a, j)\n= ∑ s∈S dπ(s)Pπ(s, j)\n= dπ(j)\nwhere the last line follows from the definition of the stationary distribution. Therefore, for j ∈ S\nd̄πP̄π(:, j) = 1\nc dπ(j) = d̄π(j)\nCase 2: j = fsas′ ∈ F For the first component, because P̄π(i, fsas′) = 0 for all i 6= s and because P̄π(s, fsas′) = π(s, a)Pr(s, a, s′) by construction,∑\ni∈S dπ(i)P̄π(i, fsas′) = dπ(s)P̄π(s, fsas′)\n= dπ(s)π(s, a)Pr(s, a, s ′)\n= c d̄π(fsas′).\nFor the second component, because P̄π(f, j) = 0 for all f, j ∈ F , we get∑ f∈F d̄π(f)P̄π(f, j) = 0.\nTherefore, for j = fss′ ∈ F , d̄πP̄π(:, j) = d̄π(j). Finally, clearly by normalizing the first component of d̄π over s ∈ S, we get the same proportion across states as in dπ , satisfying (5).\nTo see why v̄π(s) = vπ(s) for all s ∈ S, first notice that\nr̄π(i) = { rπ(i) i ∈ S 0 otherwise\nand for any fsas′ ∈ F\nv̄π(fsas′) = 0 + ∑ j∈S̄ P̄π(fsas′ , j)γ̄s(j)v̄π(j)\n= v̄π(s ′).\nNow for any s ∈ S, v̄π(s) = r̄π(s) + ∑ j∈S̄ P̄π(s, j)γ̄s(j)v̄π(j)\n= rπ(s) + ∑\nfsas′∈F\nP̄π(s, fsas′)γ̄s(fsas′)v̄π(fsas′)\n= rπ(s) + ∑ s′∈S ∑ a∈A Pr(s, a, s′)γ(s, a, s′)v̄π(s ′)\nTherefore, because it satisfies the same fixed point equation, v̄π(s) = vπ(s) for all s ∈ S. With this equivalence, it is clear that∑\ni∈S̄\nd̄π(i)v̄π(i) = 1\nc ∑ s∈S dπ(s)vπ(s) + 1 c ∑ fss′∈F dπ(s)Pπ(s, s ′)vπ(s ′)\n= 1\nc ∑ s∈S dπ(s)vπ(s) + 1 c ∑ s∈S ∑ s′∈S dπ(s)Pπ(s, s ′)vπ(s ′)\n= 1\nc ∑ s∈S dπ(s)vπ(s) + 1 c ∑ s′∈S dπ(s ′)vπ(s ′)\n= 2\nc ∑ s∈S dπ(s)vπ(s)\nTherefore, optimizing either results in the same policy."
    }, {
      "heading" : "C Discounting and average reward for control",
      "text" : "The common wisdom is that discounting is useful for asking predictive questions, but for control, the end goal is average reward. One of the main reasons for this view is that it has been previously shown that, for a constant discount, optimizing the expected return is equivalent to optimizing average reward. This can be easily seen by expanding the expected return weighting according to the stationary distribution for a policy, given constant discount γc < 1,\ndπvπ = dπ(rπ + Pπ,γvπ) = dπrπ + γcdπPπvπ = dπrπ + γcdπvπ\n=⇒ dπvπ = 1\n1− γc dπrπ. (6)\nTherefore, the constant γc < 1 simply scales the average reward objective, so optimizing either provides the same policy. This argument, however, does not extend to transition-based discounting, because γ(s, a, s′) can significantly change weighting in returns in a non-uniform way, affecting the choice of the optimal policy. We demonstrate this in the case study for the taxi domain in Section 3."
    }, {
      "heading" : "D Algorithms",
      "text" : "We show how to write generalized pseudo-code for two algorithms: true-online TD (λ) and ELSTDQ(λ). We choose these two algorithms because they generally demonstrate how one would extend to transition-based γ, and further previously had a few unclear points in their implementation. For TO-TD, the pseudo-code has been given for episodic tasks van Seijen and Sutton [2014], rather than more generally, and has treated vold carefully at the beginning of episodes, which is not necessary. LSTDQ has typically only been written for a batch of data, without importance sampling; we provide an ELSTDQ variant here with importance sampling, where LSTDQ is a special case using M = 1.\nThere are a few other implementation details that merit clarification. We use the notation γt+1 for γ(st, at, st+1), and λt+1 for λ(st, st+1). Further, unlike previous pseudo-code, we do not reinitialize vold specially at the start of an episode (i.e., when γt+1 = 0). This is because the value of vold is not relevant for the next step after γt+1 = 0. The eligibility trace is zeroed, and so α(δ + v̂ − vold)e− α(v̂ − vold)x = αδx. Finally, for both algorithms, we stage the updates to the traces. This is to avoid saving both γt, λt and γt+1, λt+1 across timesteps.\nAlgorithm 1 True-online TD(λ) w← 0, e← 0, vold ← 0 Obtain initial x0 while agent interacting with environment, t = 0, 1, . . . do\nObtain next feature vector xt+1, reward rt+1 and discount γt+1 v̂ = w>xt v̂′ = w>xt+1 δ ← rt+1 + γt+1v̂′ − v̂ e← e + xt w← w + α(δ + v̂ − vold)e− α(v̂ − vold)xt vold ← v̂′ e← γt+1λt+1e− αγt+1λt+1(e>xt+1)xt+1\nreturn w\nAlgorithm 2 ELSTDQ(λ) A← 0, b← 0, e← 0 F ← 0, M ← 0 Obtain initial action-value feature vector x0 (implicitly x(s0, a0)) and action a0 while agent interacting with environment, t = 0, 1, . . . do\nObtain next action-value feature vector xt+1, action at+1 reward rt+1 and discount γt+1 ρt+1 ← π(xt+1,at+1)µ(xt+1,at+1) F ← γt+1F + i(xt) M ← λt+1i(xt) + (1− λt+1)F e← e +Mxt A← A + e (xt − ρt+1γt+1xt+1)> b← b + ert+1 e← γt+1λt+1ρt+1e F ← ρt+1F\nreturn A−1b // The solution w to the linear system"
    }, {
      "heading" : "E Lemmas",
      "text" : "To bound the maximum eigenvalues of the discount-weighted transition matrix, we first provide the following lemma. This lemma is independently interesting, in that it explicitly verifies the previous Assumption 2.1 [Yu, 2015]. Lemma 3. Under Assumption A3, the maximum eigenvalue of Pπ,γ is less than 1:\nr(Pπ,γ) < 1.\nProof:\nPart 1: We first show that r(M̃) < 1 for\nM̃ = { Mkl − δ if i = k, j = l Mkl otherwise.\nfor any i, j, and any 0 < δ < Mij .\nFor M = Pπ , we know that r(M) = 1 and that, by assumption, Pπ is irreducible. We know that M̃ is still irreducible, because the connectivity is not changed (since no additional entries are zeroed). By the Perron-Frobenius theorem, we know that the eigenvector x that corresponds to the maximum eigenvalue r(M̃) has strictly positive entries and so δxixj > 0. Therefore,\nx>M̃x = ∑\nk,l:(k,l) 6=(i,j)\nxkMklxl + xi(Mij − δ)xj\n= x>Mx− δxixj < x>Mx.\nWe know that x>Mx ≤ 1, because r(M) = 1 and x is a unit vector. Therefore, using the fact that r(M̃) = x>M̃x by Courant-Fischer-Weyl4, we get\nr(M̃) = x>M̃x < 1.\nPart 2: Next we show that further reducing entries, even to zero values, will not increase the maximum eigenvalue. This follows simply from the fact that non-negative matrices are guaranteed to have a non-negative eigenvector x that corresponds to the maximum eigenvalue.\nTo see why, for notational convenience, we now let M be the matrix where entry i, j in Pπ was reduced by δ. Let M̃ further reduce an entry by δ, now potentially to a minimum value of 0, so that M̃ is guaranteed to be non-negative (rather than strictly positive). Using the same argument as above, we obtain that for the non-negative eigenvector of M̃\nx>M̃x = x>Mx− δxixj ≤ r(M)\nbecause δxixj ≥ 0. Therefore, with further reduction, r(M̃) cannot increase and so r(M̃) ≤ r(M) < 1.\nPart 3: Finally, we can see that for any γ and Pπ as given under Assumptions 2 and 3, M = Pπ,γ satisfies the above construction.\nLemma 4. Under Assumption A3, I−Pπ,γ ◦Λ is non-singular and the matrix\nPλπ = (I−Pπ,γ ◦Λ) −1 Pπ,γ ◦ (I−Λ)\nis non-negative and has rows that sum to no greater than 1,\n0 ≤ Pλπ ≤ 1 and Pλπ1 ≤ 1\nwith maximum eigenvalue less than 1\nr(Pλπ) < 1.\nProof:\nBy Lemma 3, we know r(Pπ,γ) < 1. Because 0 ≤ Λ ≤ 1, this means that r(Pπ,γ ◦ Λ) < 1. Therefore I−Pπ,γ ◦Λ is non-singular and so Pλπ is well-defined. Further, r(Pλπ) < 1 because of the properties of the spectral norm:∥∥Pλπ∥∥sp ≤ ∥∥∥(I−Pπ,γ ◦Λ)−1∥∥∥sp ‖Pπ,γ ◦ (I−Λ)‖sp\n< 1\nbecause r(Pπ,γ ◦Λ) < 1 and so ‖Pπ,γ ◦Λ‖sp < 1.\nNotice that I−Pπ,γ ◦Λ is a non-singular M-matrix, since the maximum eigenvalue of Pπ,γ ◦Λ is less than one and entrywise Pπ,γ ◦Λ ≥ 0. Therefore, the inverse of I−Pπ,γ ◦Λ is positive, making (I−Pπ,γ ◦Λ)−1 Pπ,γ ◦ (I−Λ) a positive matrix. The fact that the matrix has entries that are less than or equal to 1 follows from showing Pλπ1 ≤ 1 below. To show that the matrix rows always sum to less than 1, we will use a simple inductive argument. Since\nPλπ = ∞∑ k=0 (Pπ,γ ◦Λ) kPπ,γ ◦ (I−Λ)\nwe simply need to show that for every t, ∑t k=0(Pπ,γ ◦Λ)\nkPπ,γ ◦ (I−Λ)1 ≤ 1. 4The Courant-Fischer-Weyl min-max principle states that the maximum eigenvalue r(M) of a matrix M corresponds to argmaxx:x 6=0 x >Mx/(x>x), where the corresponding x that gives the maximum is an eigenvector of M. Therefore, for this x, x>Mx = r(M) and ‖x‖22 = x>x = 1.\nFor the base case, t = 0: clearly Pπ,γ ◦ (I−Λ)1 ≤ 1\nFor t > 0: Assume that t∑\nk=0\n(Pπ,γ ◦Λ) kPπ,γ ◦ (I−Λ)1 ≤ 1.\nThen t+1∑ k=0 (Pπ,γ ◦Λ) kPπ,γ ◦ (I−Λ)1 = Pπ,γ ◦Λ [ t∑ k=0 (Pπ,γ ◦Λ) kPπ,γ ◦ (I−Λ) ] 1\n≤ Pπ,γ ◦Λ1 ≤ 1\ncompleting the proof."
    }, {
      "heading" : "F Convergence of emphatic algorithms for the RL task formalism",
      "text" : "We start with convergence in expectation of ETD for transition-based discounts. The results for statebased MDPs should automatically extend to transition-based MDPs, due to the equivalence proved in Section B.1. However, in an effort to similarly generalize the writing of the theoretical analysis to the more general transition-based MDP setting, as we did for algorithms and implementation, we explicitly extend the proof for transition-based MDPs. Theorem 3. Assume the value function is approximated using linear function approximation: v(s) = x(s)>w. For X with linearly independent columns (i.e. linearly independent features), with an interest function i : S → (0,∞) and M = diag(m) for m = (I − Pλπ)−1(d ◦ i), the matrix A is positive definite.\nA := X>M(I−Pπ,γ ◦Λ)−1(I−Pπ,γ)X\nis positive definite.\nProof:\nFirst, we write an equivalent definition for Pλπ ,\nPλπ = (I−Pπ,γ ◦Λ) −1\nPπ,γ ◦ (I−Λ) = (I−Pπ,γ ◦Λ)−1 (Pπ,γ −Pπ,γ ◦Λ) = (I−Pπ,γ ◦Λ)−1 (Pπ,γ − I + I−Pπ,γ ◦Λ) = I− (I−Pπ,γ ◦Λ)−1(I−PπΓ).\nSince X is a full rank matrix, to prove that\nA = X>M(I−Pλπ)X\nis positive definite, we need to prove that M(I−Pλπ) is positive definite. As in [Sutton et al., 2016, Theorem 1], [Yu, 2015, Proposition C.1], we need to show that for M(I−Pλπ), (a) the diagonal entries are nonnegative, (b) the off-diagonal entries are nonpositive (c) its row sums are nonnegative and (d) the columns sums are are positive. The requirements (a) - (c) follow from Lemma 4, because M is a non-negative diagonal weighting matrix. To show (d), first if i(s) > 0 for all s ∈ S, the vector of columns sums is\n1>M(I−Pλπ) = m>(I−Pλπ) = (dπ ◦ i)>\nwhich always has positive entries.\nOtherwise, if i(s) = 0 for some s ∈ S, we can prove that M(I − Pλπ) is positive definite using the same argument as in [Yu, 2015, Corollary C.1]. The proof nicely encapsulates Pλπ generically as a matrix Q. We simply have to ensure that the inverse of I−Pλπ exists and that Pλπ has entries\nless than or equal to 1, both of which were showed in Lemma 4. The first condition is to have well-defined matrices, and the second to ensure that Q has a block-diagonal structure. Therefore, under Assumption 4, we can follow the same proof as [Yu, 2015, Corollary C.1] to ensure that A is positive definite.\nFor the proofs for ELSTDQ, the main difference in is using action-value functions. Corollary 1. Assume the action-value function is approximated using linear function approximation: q(s) = x(s, a)>w. For X with linearly independent columns (i.e. linearly independent features), with an interest function i : S → (0,∞) and Mq = diag(mq) for mq = (I−Pλ,qπ )−1(dµ,q ◦ iq), the matrix\nAq := X > q Mq(I−Pπ,γ,q ◦Λq) −1 (I−Pπ,γ,q)Xq\nis positive definite.\nProof: We construct the augmented space, with states S̄ = S ×A and\nPπ,γ,q((s, a), (s, a ′)) = P (s, a, s′)γ(s, a, s′)π(s′, a′) Λq((s, a), (s, a ′)) = Λ(s, s′)\niq((s, a)) = i(s)\ndµ,q((s, a)) = dµ(s)µ(s, a) rq((s, a)) = ∑ s′∈S Pr(s, a, s′)r(s, a, s′)\nPλ,qπ = (I−Pπ,γ,qΛq)−1Pπ,γ,q(I−Λq) Mq = diag ( dµ,q ◦ iq(I−Pλ,qπ )−1 ) .\nThen\nAq := X > q Mq(I−Pλ,qπ )X\nbq := X > q Mq(I−Pπ,γ,q)rq.\nThe projected Bellman operator is defined as ΠMqT (λ,q)q = rq+P λ,q π q where ELSTD(λ) converges to the projected Bellman operator fixed point ΠMqT (λ,q)q = q.\nThe above MDP with the assumptions on the subspace produced by the state-action features satisfies the conditions of Theorem 3, and so Aq is also positive definite.\nSimilarly, the other properties of the Bellman operator and the weighted norm on Pλ,qπ extend, giving a unique fixed point for the action-value Bellman operator Pλ,qπ and ‖Pλ,qπ ‖Mq < 1."
    } ],
    "references" : [ {
      "title" : "Neuro-dynamic programming",
      "author" : [ "Dimitri P Bertsekas", "John N Tsitsiklis" ],
      "venue" : "Athena Scientific Press,",
      "citeRegEx" : "Bertsekas and Tsitsiklis.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis.",
      "year" : 1996
    }, {
      "title" : "Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition",
      "author" : [ "Thomas G Dietterich" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Dietterich.,? \\Q2000\\E",
      "shortCiteRegEx" : "Dietterich.",
      "year" : 2000
    }, {
      "title" : "An object-oriented representation for efficient reinforcement learning",
      "author" : [ "Carlos Diuk", "Andre Cohen", "Michael L Littman" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Diuk et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Diuk et al\\.",
      "year" : 2008
    }, {
      "title" : "Generalized Emphatic Temporal Difference Learning: Bias-Variance Analysis",
      "author" : [ "Assaf Hallak", "Aviv Tamar", "Rémi Munos", "Shie Mannor" ],
      "venue" : "CoRR abs/1509.05172,",
      "citeRegEx" : "Hallak et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hallak et al\\.",
      "year" : 2015
    }, {
      "title" : "Bias-Variance error bounds for temporal difference updates",
      "author" : [ "Michael J Kearns", "Satinder P Singh" ],
      "venue" : "In Annual Conference on Learning Theory,",
      "citeRegEx" : "Kearns and Singh.,? \\Q2000\\E",
      "shortCiteRegEx" : "Kearns and Singh.",
      "year" : 2000
    }, {
      "title" : "GQ (λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
      "author" : [ "H Maei", "R Sutton" ],
      "venue" : "In AGI,",
      "citeRegEx" : "Maei and Sutton.,? \\Q2010\\E",
      "shortCiteRegEx" : "Maei and Sutton.",
      "year" : 2010
    }, {
      "title" : "Emphatic temporal-difference learning",
      "author" : [ "A Rupam Mahmood", "Huizhen Yu", "Martha White", "Richard S Sutton" ],
      "venue" : "In European Workshop on Reinforcement Learning,",
      "citeRegEx" : "Mahmood et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2015
    }, {
      "title" : "Multi-timescale nexting in a reinforcement learning robot. Adaptive Behavior - Animals, Animats, Software Agents, Robots",
      "author" : [ "Joseph Modayil", "Adam White", "Richard S Sutton" ],
      "venue" : "Adaptive Systems,",
      "citeRegEx" : "Modayil et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Modayil et al\\.",
      "year" : 2014
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R Sutton", "H Maei", "D Precup", "S Bhatnagar" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "Richard S Sutton", "Doina Precup", "Satinder Singh" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    }, {
      "title" : "A new Q(lambda) with interim forward view and Monte Carlo equivalence",
      "author" : [ "Richard S Sutton", "Ashique Rupam Mahmood", "Doina Precup", "Hado van Hasselt" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2014
    }, {
      "title" : "An emphatic approach to the problem of off-policy temporal-difference learning",
      "author" : [ "Richard S Sutton", "Ashique Rupam Mahmood", "Martha White" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2016
    }, {
      "title" : "TD models: Modeling the world at a mixture of time scales",
      "author" : [ "R.S. Sutton" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Sutton.,? \\Q1995\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1995
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A G Barto" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
      "author" : [ "R.S. Sutton", "J Modayil", "M Delp", "T Degris", "P.M. Pilarski", "A White", "D Precup" ],
      "venue" : "In International Conference on Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2011
    }, {
      "title" : "On the Rate of Convergence and Error Bounds for LSTD(λ)",
      "author" : [ "Manel Tagorti", "Bruno Scherrer" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Tagorti and Scherrer.,? \\Q2015\\E",
      "shortCiteRegEx" : "Tagorti and Scherrer.",
      "year" : 2015
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "J.N. Tsitsiklis", "B Van Roy" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Tsitsiklis and Roy.,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy.",
      "year" : 1997
    }, {
      "title" : "Insights in Reinforcement Learning",
      "author" : [ "Hado Philip van Hasselt" ],
      "venue" : "PhD thesis, Hado van Hasselt,",
      "citeRegEx" : "Hasselt.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hasselt.",
      "year" : 2011
    }, {
      "title" : "True online TD(lambda)",
      "author" : [ "Harm van Seijen", "Rich Sutton" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Seijen and Sutton.,? \\Q2014\\E",
      "shortCiteRegEx" : "Seijen and Sutton.",
      "year" : 2014
    }, {
      "title" : "Developing a predictive approach to knowledge",
      "author" : [ "Adam White" ],
      "venue" : "PhD thesis, University of Alberta,",
      "citeRegEx" : "White.,? \\Q2015\\E",
      "shortCiteRegEx" : "White.",
      "year" : 2015
    }, {
      "title" : "Least Squares Temporal Difference Methods: An Analysis under General Conditions",
      "author" : [ "Huizhen Yu" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Yu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yu.",
      "year" : 2012
    }, {
      "title" : "On convergence of emphatic temporal-difference learning",
      "author" : [ "Huizhen Yu" ],
      "venue" : "In Annual Conference on Learning Theory,",
      "citeRegEx" : "Yu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yu.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Progressively there have been additions to specify a broader range of objectives, including options [Sutton et al., 1999], state-based discounting [Sutton, 1995, Sutton et al.",
      "startOffset" : 100,
      "endOffset" : 121
    }, {
      "referenceID" : 13,
      "context" : "formulation [Sutton and Barto, 1998] and state-based discounting [Sutton, 1995, Maei and Sutton, 2010, Sutton et al.",
      "startOffset" : 12,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "2 Options as RL tasks The options framework [Sutton et al., 1999] generically covers a wide range of settings, with discussion about macro-actions, option models, interrupting options and intra-option value learning.",
      "startOffset" : 44,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "This relation is important for practically using these subtasks as part of semi-Markov decision processes, for which we have standard learning and planning algorithms [Sutton et al., 1999].",
      "startOffset" : 167,
      "endOffset" : 188
    }, {
      "referenceID" : 14,
      "context" : "3 General value functions and predictive questions In a similar spirit of abstraction as options, general value functions were introduced for single predictive or goal-oriented questions about the world [Sutton et al., 2011].",
      "startOffset" : 203,
      "endOffset" : 224
    }, {
      "referenceID" : 14,
      "context" : "The work on Horde [Sutton et al., 2011] and nexting [Modayil et al.",
      "startOffset" : 18,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : ", 2011] and nexting [Modayil et al., 2014] provide numerous examples of the utility of the types of questions that can be specified by general value functions, and so by RL tasks, because general value functions can naturally can be specified as an RL task.",
      "startOffset" : 20,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "The trace parameter λ : S×S → [0, 1] influences the fixed point and provides a modified (biased) return, called the λ-return; this parameter is typically motivated as a bias-variance trade-off parameter [Kearns and Singh, 2000].",
      "startOffset" : 203,
      "endOffset" : 227
    }, {
      "referenceID" : 3,
      "context" : "For example, ETD was introduced with state-based γs; however, Hallak et al. [2015] analyzed approximation error bounds of ETD using a constant discount γc.",
      "startOffset" : 62,
      "endOffset" : 83
    }, {
      "referenceID" : 3,
      "context" : "A well-known result is that for D = Dπ the Bellman operator is a contraction for constant γc and λc [Tsitsiklis and Van Roy, 1997]; recently, this has been generalized for a variant of ETD to M, still with constant parameters [Hallak et al., 2015].",
      "startOffset" : 226,
      "endOffset" : 247
    }, {
      "referenceID" : 21,
      "context" : "For policy evaluation, ETD and ELSTD, the least-squares version of ETD that uses the above defined A and b with D = M, have both been shown to converge with probability one [Yu, 2015].",
      "startOffset" : 173,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "In particular, for appropriate step-sizes αt (see [Yu, 2015]), if A is positive definite, the iterative update is convergent wt+1 = wt + αt(b−Awt).",
      "startOffset" : 50,
      "endOffset" : 60
    }, {
      "referenceID" : 15,
      "context" : "Convergence rate of LSTD(λ) Tagorti and Scherrer [2015] recently provided convergence rates for LSTD(λ) for continuing tasks, for some γc < 1.",
      "startOffset" : 28,
      "endOffset" : 56
    } ],
    "year" : 2017,
    "abstractText" : "Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.",
    "creator" : "LaTeX with hyperref package"
  }
}