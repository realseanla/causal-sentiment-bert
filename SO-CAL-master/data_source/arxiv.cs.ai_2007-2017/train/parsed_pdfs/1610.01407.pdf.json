{
  "name" : "1610.01407.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Towards semi-episodic learning for robot damage recovery",
    "authors" : [ "Konstantinos Chatzilygeroudis", "Antoine Cully", "Jean-Baptiste Mouret" ],
    "emails" : [ "jean-baptiste.mouret@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n01 40\n7v 1\n[ cs\n.R O\n] 5\nO ct\n2 01\n6\nI. INTRODUCTION\nRecent research on autonomous systems and robotics has achieved important progress in increasing the autonomy of robots, which makes it possible to operate robots for long periods of time in real-world scenarios. Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].\nCurrent methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods — mostly Reinforcement Learning (RL) techniques [6], [7], [8]. Most of the techniques in the first category require to anticipate the situations that the robot may have to face; an issue can be diagnosed only if the right sensors are present in the right place. These requirements make diagnosis-based techniques difficult to use in complex robotics systems/scenarios — typically they are only used in the lowest levels of control. Nevertheless, the state-of-the-art RL approaches are also difficult to use for damage recovery because they require many iterations to converge. For example, many RL approaches require tens if not hundreds or thousands of iterations to learn problems with low-dimensional state spaces and fairly benign dynamics, like the mountain car [9]. The data efficiency of RL approaches is a critical aspect that limits their application in real-world robotics scenarios [10].\nA promising approach is the Intelligent Trial and Error algorithm (IT&E), a recently introduced algorithm [8]. The intuition behind IT&E is that, before the mission, an off-line and computationally expensive evolutionary algorithm can be used to create a behavior-performance map that predicts the performance of thousands of different behaviors. While in mission, this map, guides a fast and on-line search, based on\n*Corresponding author: jean-baptiste.mouret@inria.fr 1Inria, Villers-lès-Nancy, F-54600, France 2CNRS, Loria, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France 3Université de Lorraine, Loria, UMR 7503, Vandœuvre-lès-Nancy, F-54500, France 4Personal Robotics Lab, Department of Electrical and Electronic Engineering,\nImperial College London, UK\nBayesian Optimization [11], to find a compensatory behavior. An important idea is that the behavior-performance map is created using a simulated intact robot, but the algorithm is able to find a working behavior on the damaged real robot because some behaviors from the map perform similarly on the intact and the damaged robot (typically, the behaviors that do not rely on the broken part). The most recent results showed that IT&E can allow various types of robots (a 6- legged robot and an 8-DOF manipulator) to compensate for many different types of injuries in a matter of minutes [8], [12].\nAlthough the IT&E approach is promising, its main limitation is the pure episodic approach it has adopted: for each trial (episode), the robot has to begin in the same starting state (Figure 1a). This is limiting because learning a compensatory behavior has to be achieved in two steps, first learn a compensatory behavior, and then use it to complete the task. On the contrary, a wounded animal, for example, can perform trial and error “episodes” to learn how to walk again, while going back to its nest for protection.\nIn this paper, we extend the IT&E algorithm by (1) using a generic reward of the outcome of each atomic behavior of the robot in the adaptation part, and by (2) adding a specialized reward selection layer that selects a specialized reward function at each episode. These additions allow for a semi-episodic learning scheme that improves the robot’s long-term autonomy by allowing to recover while attempting to achieve its task(s) (Figure 1b)."
    }, {
      "heading" : "II. BACKGROUND",
      "text" : ""
    }, {
      "heading" : "A. Bayesian Optimization with Gaussian Processes",
      "text" : "Bayesian Optimization (BO) is a well-established strategy for finding the extrema of functions that are expensive to evaluate [11], [13]. It is applicable in cases where one does not have a closed-form expression for the objective function (the function is a “black-box”), but where one can obtain observations (possibly noisy) of this function. One of the distinctive features of BO is that it constructs a probabilistic model for the objective function and then exploits this model to make decisions about which point to evaluate next, while taking into account the uncertainty.\nThere are two major choices that must be made when performing BO. First, one must select a prior over functions that will express assumptions about the function being optimized. Second, one must choose an acquisition function, u(x|D1:t), which is used to construct a utility function from the model posterior, allowing us to determine the next point to evaluate.\nMany models could be used for the BO prior, but Gaussian Process (GP) priors are the most common choice [11]. A GP is an extension of the multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [11]. A GP is a distribution over functions, completely specified by its mean function, m(·) and covariance function, k(·, ·):\nf(x) ∼ GP (m(x), k(x,x′))\nAssuming D1:t = {(x1, f(x1)), · · · , (xt, f(xt))} is a set of observations and σ2noise the sampling noise, the GP is computed as follows:\np(f(x)|D1:t,x) = N (mt(x), σ 2 t (x))\nwhere: mt(x) = k ⊤ K −1D1:t\nσ2t (x) = k(x,x)− k ⊤ K −1 k\nK =\n\n  k(x1,x1) · · · k(x1,xt) ... . . . ...\nk(xt,x1) · · · k(xt,xt)\n\n  + σ2noiseI\nk = [ k(x,x1) · · · k(x,xt) ]\nWe used Upper Confidence Bound (UCB) as the acquisition function. We refer the reader to Brochu et al. [11] for a more detailed explanation.\nB. Intelligent Trial & Error Algorithm\nIT&E proposed a novel approach for robot damage recovery that consists of a 2-step process. An off-line evolutionary algorithm, MAP-Elites [14][8], that generates many thousands of potential good behaviors is followed by a trial and error on-line adaptation part, based on BO (M-BOA), in order to find a compensatory behavior.\nMAP-Elites is an evolutionary illumination algorithm: instead of searching for a single, best solution, like optimization algorithms, MAP-Elites searches for the highestperforming individual for each point in a user-defined space. This user-defined space is often called the behavior space, because the dimensions of variation (behavior descriptors) usually measure behavioral characteristics.\nIn IT&E, the authors made a slight modification to the classical BO scheme. Their BO variation, called Map-Based BO Algorithm (M-BOA), models the difference between a mean function and the actual performance, instead of directly modeling the objective function (P(·) is the mean function):\nmt(x) = P(x) + k ⊤ K −1(D1:t −P(x1:t))\nIn the original work, the mean function was the prediction of the performance in the map generated from MAP-Elites. Algorithm 1 shows the pseudo-code for M-BOA.\nAlgorithm 1 M-BOA (Map-Based BO Algorithm)\n1: procedure M-BOA 2: ∀x ∈ map : 3: p(f(x)|x) = N (P(x), k(x,x)) 4: while stopping criteria not met do 5: xt+1 = argmaxxu(x|D1:t) ⊲ Next test point 6: Yt+1 = performance(execute behavior(xt+1)) 7: D1:t+1 = {D1:t, (xt+1, Yt+1)} 8: Update GP"
    }, {
      "heading" : "III. APPROACH",
      "text" : ""
    }, {
      "heading" : "A. Generic Reward",
      "text" : "In the original IT&E paper, the GP modeled the performance of each atomic behavior given a task. In this paper, we suggest learning a mapping from the atomic behaviors to the resulting relative outcomes. We call it a Generic Reward (GR) of the outcome of each atomic behavior of the robot. We use one GP for each dimension of the GR.\nFor example, imagine we have a robot moving in 2D space using an 1D continuous atomic behavior (direction to move 0.1-step). A GR could be the relative position of the robot after executing a behavior - (x, y). Thus, we need 2 GPs: GPx(θ), GPy(θ). If we query the GPs at the point θ0, then we get a position, p1 = (GPx(θ0), GPy(θ0)), as the prediction. In that way, we can now compute specialized rewards for different locomotion tasks, like the distance to different target points.\nPut differently, the GR is a description of the outcome of each atomic behavior of the robot that it is generic-enough to be independent from one task to another, but specificenough so that the performance of the outcome of one atomic behavior given a task can be computed.\nThe changes for M-BOA to work are: • define a Reward function that takes the GPs’ prediction\nas input and returns the expected task performance; • define an Aggregator function (afun) that takes as input\nthe execution of an atomic behavior and returns the GR."
    }, {
      "heading" : "B. Specialized Reward Selection Layer",
      "text" : "We, also, augment the proposed algorithm, by adding a layer responsible for selecting the Reward function, defined above. We call it Specialized Reward Selection Layer (RSL). Since we are modeling a GR of the outcome of each behavior of the robot and not the actual performance (given a task), we can change the Reward function as often as needed. This is true, because only the acquisition function needs an actual reward to select a new test point. We suggest updating or selecting the Reward function at each iteration of M-BOA.\nFor instance, if we consider the previous mobile robot example, at each iteration a planner algorithm chooses the next best point to reach. This point can then be used by the RSL in order to update the Reward function so that it outputs the Euclidean distance between the point selected by the planner and the prediction of the GPs."
    }, {
      "heading" : "C. Semi-Episodic Learning Algorithm",
      "text" : "Using the two proposed additions, we can now have a non purely episodic version of the IT&E algorithm. We call it Semi-Episodic Learning Algorithm (SELA). The pseudocode is shown in Algorithm 2.\nAlgorithm 2 Semi-Episodic Learning Algorithm\n1: procedure SELA 2: Before mission (in simulation with intact robot): 3: Create Behavior-Performance Map using MAP-Elites 4: while in mission do 5: if significant performance drop then 6: Adaptation-Step (using SELA-ADAPT) 7: procedure SELA-ADAPT 8: ∀x ∈ map : 9: p(f(x)|x) = N (P(x), k(x,x))\n10: while stopping criteria not met do 11: Update Reward function 12: xt+1 = argmaxxu(Reward(GPs(x))|D1:t) 13: Yt+1 = afun(execute behavior(xt+1)) 14: D1:t+1 = {D1:t, (xt+1,Yt+1)} 15: Update GPs"
    }, {
      "heading" : "IV. PRELIMINARY EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "A. Toy Simulation",
      "text" : "As a toy example, we consider the mobile robot example introduced previously. This mobile robot is a point (no dimensions, no orientation) and can take a 0.1-long step in any direction. We represent each atomic behavior by a scalar value, θ: the direction of the corresponding move. This environment was inspired by Engel et al. [15]. The task of the robot is to reach a target point despite some damage.\nBecause the example is too simple, but also to show the effectiveness of our method without relying on simulated data, we did not generate any behavior-performance map. We used the exact model of the intact robot as the mean function. Also, for the GR, we used the (x, y) relative end position of each behavior, for the Reward function the Euclidean\ndistance between the next target and the prediction of the GPs and for the reward selection layer an A* path planner.\nTo evaluate our technique we used the following two control experiments:\n• learn the model of the robot (using GPs) via random babbling and then use it to complete the task; • solve the problem with the classic IT&E approach: we first learn with IT&E how to walk in 4 major directions (up, down, right, left) and then use these behaviors to reach the target.\nWe ran 50 replicates of each approach for the scenario: “Reach the target point (2.0, 2.0) starting from the origin despite a 0.5 radians angle offset in the range direction θ > 0”. To make the task a little more realistic we added a small Gaussian noise (µ = 0, σ2 = 0.01) to the position observations. Figure 2 shows the resulting performance (number of atomic behaviors taken to reach the target) for the different approaches. Our algorithm is able to reach the target with almost the optimal number of steps (i.e. if we perfectly knew the model), that is in much fewer steps than the other approaches.\nB. 6-Legged Simulated Robot locomotion task\nAs a more realistic example, we consider a simulated 6- legged (hexapod) robot moving in space with the same task as in the toy simulation. See Figure 1b for the scenario and [8] for more details on the simulated hexapod. We evolved different atomic behaviors using the MAP-Elites algorithm with an 8D behavior descriptor (2 dimensions for space diversity + 6 dimensions for walking diversity), inspired by [16], [12]. The number of atomic behaviors evolved were approximately 1 million. We used this behavior-performance map as the mean function. All the other parameters were the same as in the toy simulation experiment.\nTo evaluate our technique we used similar control experiments as in the toy simulation experiment:\n• IT&E variant #1: we learn the outcome of the atomic behaviors (using GPs) via selecting the most uncertain behavior for N = 15 iterations. This can be considered as a uniform sampling of the behavior space. We then use what we learned to reach the target. • IT&E variant #2: we first learn with IT&E how to walk in 4 major directions (forward, backward, turn cw, turn ccw) and then use these behaviors to reach the target.\nWe ran 50 replicates of each approach for the scenario: “Reach the target point (2.0, 2.0) despite the middle right leg being removed”. We, also, added a small Gaussian noise (µ = 0, σ2 = 0.01) to the position observations. Figure 3 shows the resulting performance (number of atomic behaviors taken to reach the target) for the different approaches. Our algorithm is able to find solutions in fewer steps than the other approaches.\nC. 6-Legged Robot locomotion task\nWe, also, applied our technique on a real 6-legged robot. Preliminary experiments show promising results1."
    }, {
      "heading" : "V. CONCLUSION AND FUTURE WORK",
      "text" : "We have introduced a semi-episodic learning scheme for robot damage recovery and a novel algorithm in this direction: Semi-Episodic Learning Algorithm. The intuition behind this scheme is that the robot can learn in a data-efficient way how to compensate for damages while completing its task(s). This is achieved by (1) shrinking the search space, using simulated or computed data as prior knowledge, and by (2) using a generic reward of the outcome of the atomic behaviors of the robot instead of their performance given a task.\n1https://www.youtube.com/watch?v=Gpf5h07pJFA\nFuture work includes performing more experiments with the real robot as well as experiments with different robots. In addition, BO can be replaced by other techniques that scale better. What is more, we used a naive reward selection layer, but more efficient/sophisticated methods can be used. We are currently investigating in this direction. Additionally, theoretical guarantees and analysis should be investigated in detail. Overall, this work is a first step towards semi-episodic and life-long learning for robot damage recovery.\nAPPENDIX For all experiments the following parameters were used: Error threshold for reaching goal: ǫgoal = 0.1"
    }, {
      "heading" : "A. BO with GPs",
      "text" : "Acquistion function: UCB with α = 0.05 Kernel: Exponential kernel with σ = 0.1 GP noise: σ2noise = 0.001 Max iterations: N = 10 (Toy), N = 15 (Hexapod)"
    }, {
      "heading" : "B. Learning with random babbling",
      "text" : "Error threshold: ǫmodel = 0.01 Max iterations: N = 15"
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "This work was supported by the ERC project “ResiBots” (grant agreement No 637972), funded by the European Research Council."
    } ],
    "references" : [ {
      "title" : "Emergency response to the nuclear accident at the Fukushima Daiichi Nuclear Power Plants using mobile rescue robots",
      "author" : [ "K. Nagatani", "S. Kiribayashi", "Y. Okada", "K. Otake", "K. Yoshida", "S. Tadokoro", "T. Nishimura", "T. Yoshida", "E. Koyanagi", "M. Fukushima" ],
      "venue" : "Journal of Field Robotics, vol. 30, no. 1, pp. 44–63, 2013.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On autonomous navigation in a natural environment",
      "author" : [ "M. Devy", "R. Chatila", "P. Fillatreau", "S. Lacroix", "F. Nashashibi" ],
      "venue" : "Robotics and Autonomous Systems, vol. 16, no. 1, pp. 5 – 16, 1995.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Follow-up analysis of mobile robot failures",
      "author" : [ "J. Carlson", "R.R. Murphy", "A. Nelson" ],
      "venue" : "ICRA. IEEE, 2004.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "How UGVs physically fail in the field",
      "author" : [ "J. Carlson", "R.R. Murphy" ],
      "venue" : "IEEE Transactions on Robotics, vol. 21, no. 3, pp. 423–437, 2005.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Real-time fault diagnosis",
      "author" : [ "V. Verma", "G. Gordon", "R. Simmons", "S. Thrun" ],
      "venue" : "Robotics & Automation Magazine, IEEE, vol. 11, no. 2, pp. 56–66, 2004.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Online discovery of AUV control policies to overcome thruster failures",
      "author" : [ "S.R. Ahmadzadeh", "M. Leonetti", "A. Carrera", "M. Carreras", "P. Kormushev", "D.G. Caldwell" ],
      "venue" : "ICRA. IEEE, 2014.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Free gait generation with reinforcement learning for a six-legged robot",
      "author" : [ "M.S. Erden", "K. Leblebicioğlu" ],
      "venue" : "Robotics and Autonomous Systems, vol. 56, no. 3, pp. 199–212, 2008.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Robots that can adapt like animals",
      "author" : [ "A. Cully", "J. Clune", "D. Tarapore", "J.-B. Mouret" ],
      "venue" : "Nature, vol. 521, no. 7553, pp. 503–507, 2015.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "MIT press,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "PILCO: A model-based and data-efficient approach to policy search",
      "author" : [ "M.P. Deisenroth", "C.E. Rasmussen" ],
      "venue" : "ICML, 2011.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning",
      "author" : [ "E. Brochu", "V.M. Cora", "N. De Freitas" ],
      "venue" : "arXiv preprint arXiv:1012.2599, 2010.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Creative adaptation through learning",
      "author" : [ "A. Cully" ],
      "venue" : "Ph.D. dissertation, Université Pierre et Marie Curie, 2015.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Bayesian approach to global optimization: theory and applications",
      "author" : [ "J. Mockus" ],
      "venue" : "Kluwer Academic,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "Illuminating search spaces by mapping elites",
      "author" : [ "J.-B. Mouret", "J. Clune" ],
      "venue" : "arXiv preprint arXiv:1504.04909, 2015.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Reinforcement learning with Gaussian processes",
      "author" : [ "Y. Engel", "S. Mannor", "R. Meir" ],
      "venue" : "ICML. ACM, 2005.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Behavioral repertoire learning in robotics",
      "author" : [ "A. Cully", "J.-B. Mouret" ],
      "venue" : "GECCO. ACM, 2013.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 2,
      "context" : "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].",
      "startOffset" : 257,
      "endOffset" : 260
    }, {
      "referenceID" : 3,
      "context" : "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].",
      "startOffset" : 262,
      "endOffset" : 265
    }, {
      "referenceID" : 4,
      "context" : "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods — mostly Reinforcement Learning (RL) techniques [6], [7], [8].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods — mostly Reinforcement Learning (RL) techniques [6], [7], [8].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods — mostly Reinforcement Learning (RL) techniques [6], [7], [8].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 7,
      "context" : "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods — mostly Reinforcement Learning (RL) techniques [6], [7], [8].",
      "startOffset" : 197,
      "endOffset" : 200
    }, {
      "referenceID" : 8,
      "context" : "For example, many RL approaches require tens if not hundreds or thousands of iterations to learn problems with low-dimensional state spaces and fairly benign dynamics, like the mountain car [9].",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 9,
      "context" : "The data efficiency of RL approaches is a critical aspect that limits their application in real-world robotics scenarios [10].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 7,
      "context" : "A promising approach is the Intelligent Trial and Error algorithm (IT&E), a recently introduced algorithm [8].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 10,
      "context" : "Bayesian Optimization [11], to find a compensatory behavior.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "The most recent results showed that IT&E can allow various types of robots (a 6legged robot and an 8-DOF manipulator) to compensate for many different types of injuries in a matter of minutes [8], [12].",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 11,
      "context" : "The most recent results showed that IT&E can allow various types of robots (a 6legged robot and an 8-DOF manipulator) to compensate for many different types of injuries in a matter of minutes [8], [12].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 10,
      "context" : "Bayesian Optimization (BO) is a well-established strategy for finding the extrema of functions that are expensive to evaluate [11], [13].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 12,
      "context" : "Bayesian Optimization (BO) is a well-established strategy for finding the extrema of functions that are expensive to evaluate [11], [13].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "Many models could be used for the BO prior, but Gaussian Process (GP) priors are the most common choice [11].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 10,
      "context" : "A GP is an extension of the multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [11].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 10,
      "context" : "[11] for a more detailed explanation.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "An off-line evolutionary algorithm, MAP-Elites [14][8], that generates many thousands of potential good behaviors is followed by a trial and error on-line adaptation part, based on BO (M-BOA), in order to find a compensatory behavior.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "An off-line evolutionary algorithm, MAP-Elites [14][8], that generates many thousands of potential good behaviors is followed by a trial and error on-line adaptation part, based on BO (M-BOA), in order to find a compensatory behavior.",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 14,
      "context" : "[15].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "See Figure 1b for the scenario and [8] for more details on the simulated hexapod.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "We evolved different atomic behaviors using the MAP-Elites algorithm with an 8D behavior descriptor (2 dimensions for space diversity + 6 dimensions for walking diversity), inspired by [16], [12].",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 11,
      "context" : "We evolved different atomic behaviors using the MAP-Elites algorithm with an 8D behavior descriptor (2 dimensions for space diversity + 6 dimensions for walking diversity), inspired by [16], [12].",
      "startOffset" : 191,
      "endOffset" : 195
    } ],
    "year" : 2016,
    "abstractText" : "The recently introduced Intelligent Trial and Error algorithm (IT&E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT&E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot’s life-time autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results.",
    "creator" : "LaTeX with hyperref package"
  }
}