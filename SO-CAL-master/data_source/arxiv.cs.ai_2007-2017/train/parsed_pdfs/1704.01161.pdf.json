{
  "name" : "1704.01161.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Finite Sample Analysis for TD(0) with Linear Function Approximation",
    "authors" : [ "Gal Dalal", "Balázs Szörényi", "Gugan Thoppe", "Shie Mannor" ],
    "emails" : [ "gald@tx.technion.ac.il", "szorenyi.balazs@gmail.com", "gugan.thoppe@gmail.com", "shie@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Temporal Difference (TD) algorithms lie at the core of Reinforcement Learning (RL), dominated by the celebrated TD(0) algorithm. The term has been coined in [Sutton and Barto, 1998], describing an iterative process of updating an estimate of a value function V π(s) with respect to a given policy π based on temporally-successive samples. The classical version of the algorithm uses a tabular representation, i.e., entry-wise storage of the value estimate per each state s ∈ S. However, in many problems the state-space S is too large for such a vanilla approach. The common practice to mitigate this caveat is to approximate the value function using some parameterized family. Often, linear regression is used, i.e., V π(s) ≈ θ>φ(s). This allows for an efficient implementation of TD(0) even on large state-spaces and has shown to perform well in a variety of problems Tesauro [1995], Powell [2007]. More recently, TD(0) has become prominent in many state-of-the-art RL solutions when combined with deep neural network architectures, as an integral part of fitted value iteration [Mnih et al., 2015, Silver et al., 2016]. In this work we focus on the former case of linear Function Approximation (FA); nevertheless, we consider this work as a preliminary milestone in route to achieving theoretical guarantees for deep RL approaches.\nTwo types of convergence rate results exist in literature: with high probability and in expectation. We stress that no results of the first type exist for the actual, commonly used, TD(0) algorithm with linear FA; our work is the first to provide such a result. In fact, it is the first work to give a concentration bound for an unaltered online TD algorithm of any type. To emphasize, TD(0) with linear FA is formulated and used with non-problem-specific step-sizes. Also, it does not require a projection step to keep θ in a ‘nice’ set. In contrast, the few recent works that managed to provide concentration bounds for TD(0) analyzed only altered versions of them, carefully crafted for the analyses to hold. These modifications include a projection step and eigenvalue-dependent step-sizes; we expand on this in the coming section. As for the second type of results, i.e., expectation bounds, existing results\n∗Equal contribution\nar X\niv :1\n70 4.\n01 16\n1v 2\n[ cs\n.A I]\n2 J\nul 2\n01 7\neither apply only to altered versions of TD(0) as described above, or applies to average of iterates. In this work, we obtain the first expectation bound directly on the iterates for the unaltered TD(0)."
    }, {
      "heading" : "1.1 Existing Literature",
      "text" : "The first TD(0) convergence result was obtained by Tsitsiklis et al. [1997] for both finite and infinite state-spaces. Following that, a key result by Borkar and Meyn [2000] paved the path to a unified and convenient tool for convergence analyses of Stochastic Approximation (SA), and hence of TD algorithms. This tool is based on the Ordinary Differential Equation (ODE) method. Essentially, that work showed that under the right conditions, the SA trajectory follows the solution of a suitable ODE, often referred to as its limiting ODE; thus, it eventually converges to the solution of the limiting ODE. Several usages of this tool in RL literature can be found in [Sutton et al., 2009a,b, 2015].\nAs opposed to the case of asymptotic convergence analysis of TD algorithms, very little is known on their finite sample behavior. We now briefly discuss the few existing results on this topic. In [Borkar, 2008], a concentration bound is given for generic SA algorithms. Recent works [Kamal, 2010, Thoppe and Borkar, 2015] obtain better concentration bounds via tighter analyses. The results in these works are conditioned on the event that the n0−th iterate lies in some a-priori chosen bounded region containing the desired equilibria; this, therefore, is the caveat in applying them to TD(0).\nIn [Korda and Prashanth, 2015], concentration bounds for TD(0) with mixing-time consideration have been given. However, unlike in our work, a strong requirement for all their high probability bounds is that the iterates need to lie in some a-priori chosen bounded set; this is ensured there via projections (personal communication). Additionally, their results require the learning rate to be set based on prior knowledge about system dynamics, which, as argued in the paper, is problematic; alternatively, they apply to average of iterates. An additional work by Liu et al. [2015] considered the gradient TD algorithms GTD(0) and GTD2, which were first introduced in [Sutton et al., 2009b,a]. That work interpreted the algorithms as gradient methods to some saddle-point optimization problem. This enabled them to obtain concentration bounds on altered versions of these algorithms using results from the convex optimization literature. Despite the alternate approach, in similar fashion to the results above, a projection step that keeps the parameter vectors in a convex set is needed there.\nBounds similar in flavor to ours are also given in [Frikha and Menozzi, 2012, Fathi and Frikha, 2013]. However, they apply only to a class of SA methods satisfying strong assumptions, which do not hold for TD(0). In particular, neither the uniformly Lipschitz assumption nor its weakened version, the Lyapunov Stability-Domination criteria, hold for TD(0) when formulated in their iid noise setup.\nTwo additional works [Yu and Bertsekas, 2009, Lazaric et al., 2010] provide sample complexity bounds on the batch LSTD algorithms. However, in the context of finite sample analysis, these belong to a different class of algorithms. The case of online TD learning has proved to be more practical, at the expense of increased analysis difficulty compared to LSTD methods."
    }, {
      "heading" : "1.2 Our Contribution",
      "text" : "Our work is the first to give a bound on the convergence rate of TD(0) in its original, unaltered form. In fact, it is the first to obtain a concentration bound for an unaltered online TD algorithm of any type. Indeed, as discussed earlier, existing convergence rates apply only to online TD algorithms with alterations such as projections and step-sizes dependent on unknown problem parameters; alternatively, they only apply to average of iterates. The key ingredients in our approach to obviate these alterations are i) show that the n-th iterate at worst is only O(n) away from the solution θ∗; and ii) based on that, show that after some additional steps all subsequent iterates are -close to the solution w.h.p. We believe this approach is not limited to TD(0) alone.\nMoreover, we provide the first expectation decay rate of the actual TD(0) iterates. It applies for a general family of step-sizes that is not restricted to square-summable sequences, as is assumed in most works."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "We consider the problem of policy evaluation for a Markov Decision Process (MDP). A MDP is defined by the 5-tuple (S,A , P,R, γ) [Sutton, 1988], where S is the set of states, A is the set of\nactions, P = P (s′|s, a) is the transition kernel,R(s, a, s′) is the reward function, and γ ∈ (0, 1) is the discount factor. In each time-step, the process is in some state s ∈ S, an action a ∈ A is taken, the system transitions to a next state s′ ∈ S according to the transition kernel P , and an immediate reward r is received according toR(s, a, s′). Let policy π : S → A be a stationary mapping from states to actions. Assuming the associated Markov chain is ergodic and uni-chain, let ν be the induced stationary distribution. Moreover, let V π(s) be the value function at state s w.r.t. π defined via the Bellman equation V π(s) = Eν [r + γV π(s′)]. In our policy evaluation setting, the goal is to estimate V π(s) using linear regression, i.e., V π(s) ≈ θ>φ(s), where φ(s) ∈ Rd is a feature vector at state s, and θ ∈ Rd is a weight vector. For brevity, we omit the notation π and denote φ(s), φ(s′) by φ, φ′. Let {(φn, φ′n, rn)}n be iid samples of (φ, φ′, r). Then the TD(0) algorithm has the update rule\nθn+1 = θn + αn[rn + γφ ′> n θn − φ>n θn]φn, (1)\nwhere αn is the step-size. For analysis, we can rewrite the above as\nθn+1 = θn + αn[h(θn) +Mn+1] , (2)\nwhere h(θ) = b−Aθ and Mn+1 = ( rn + γφ ′> n θn − φ>n θn ) φn − [b−Aθn] , (3)\nwith A = Eν [φ(φ− γφ′)>] and b = Eν [rφ]. It is known that A is positive definite [Bertsekas, 2012] and that (2) converges to θ∗ := A−1b [Borkar, 2008]. Note that\nh(θ) = −A[θ − θ∗] . (4)\nWe make the following assumption:\nA1 . All rewards r(s, a, s′) and feature vectors φ(s) are uniformly bounded, i.e., ‖φ(s)‖ ≤ 1/2, ∀s ∈ S, and |r(s, a, s′)| ≤ 1, ∀s, s′ ∈ S, a ∈ A ."
    }, {
      "heading" : "3 Main Result",
      "text" : "Our main result is the following. The Õ notation hides problem dependent constants and polylogarithmic terms.\nTheorem 1 (TD(0) Concentration Bound). Let λ ∈ (0,mini∈[d]{real(λi(A))}), where λi(A) is the i-th eigenvalue of A. Let αn = (n+ 1)−1. Then for > 0 and δ ∈ (0, 1), there exists a function\nN( , δ) = Õ ( max {[ 1 ]1+ 1λ [ ln 1\nδ\n]1+ 1λ , [ 1 ]2 [ ln 1\nδ ]3}) such that\nPr {‖θn − θ∗‖ ≤ ∀n ≥ N( , δ)} ≥ 1− δ .\nThe proof of Theorem 1 also gives the following result; instead of fixed , we have a decreasing rate.\nTheorem 2. Let λ, αn be as in Theorem 1. Fix δ ∈ (0, 1). Then there exists some function N0(δ) = O(ln(1/δ)) such that for all n ≥ N0(δ),\nPr { ‖θn − θ∗‖ = Õ ( n−min{1/2,λ/(λ+1)} )} ≥ 1− δ.\nRemark 1. Theorem 1, Korda and Prashanth [2015] requires the TD(0) step-sizes to satisfy: αn = fn(λ) for some function fn, where λ is as above. Further, Theorem 2 there applies to average of iterates. Also, concentration bounds in these results require projecting the iterates to some bounded set (personal communication). In contrast, our result applies directly to the original TD(0) algorithm and we obviate all the above modifications. However, our result is weaker than Theorem 1 there when λ < 1.\nOur other main result is a bound on the expected decay rate of the TD(0) iterates.\nTheorem 3 (Expected Decay Rate for TD(0)). Fix σ ∈ (0, 1) and let αn = (n + 1)−σ. Fix λ ∈ (0, λmin(A+A>)). Then, for n ≥ 0,\nE‖θn+1 − θ∗‖2 ≤ K1e−(λ/2)(n+2) 1−σ + K2\n(n+ 1)σ ,\nwhere K1,K2 ≥ 0 are some constants that depend on both λ and σ; see Theorem 12 for the exact expression. Remark 2. The exponentially decaying term in Theorem 3 corresponds to the convergence rate of the noiseless TD(0) algorithm, while the inverse polynomial term appears due to the martingale noise Mn. The inverse impact of σ on these two terms introduces the following tradeoff:\n1. For σ close to 0, the first term converges faster and corresponds to slowly decaying stepsizes, which, in turn, speed up the noiseless TD(0) convergence.\n2. For σ close to 1, the second term decays quickly and corresponds to small step-sizes that better mitigate the effect of martingale noise; this originates in the term αnMn+1.\nWhile this insight is folklore, a formal estimate of the tradeoff, to the best of our knowledge, has been obtained here for the first time. Remark 3. The expectation bound in Theorem 1, Korda and Prashanth [2015] again requires the stepsize sequence be scaled as in Remark 1. Theorem 2 there obviates this, but it applies to average of iterates. In contrast, our expectation bound applies directly to the TD(0) iterates and does not need any scaling of the above kind. Moreover, our result applies to a broader family of stepsizes; see Remark 4. Our expectation bound when compared to that of Theorem 2, Korda and Prashanth [2015] is of the same order (even though theirs is for average of iterates). Remark 4. In Theorem 3, unlike most works, ∑ n≥0 α 2 n need not be finite. Thus this result is applicable for a wider class of stepsizes; e.g., 1/nκ with κ ∈ (0, 1/2]. In [Borkar, 2008], on which much of the existing RL literature is based on, the square summability assumption is due to the Gronwall inequality. In contrast, in our work, we use the Variation of Parameters Formula [Lakshmikantham and Deo, 1998] for comparing the SA trajectory to appropriate trajectories of the limiting ODE; it is a stronger tool than Gronwall inequality. Remark 5. In the proof of Theorem 3, one can see that the requirement on the martingale noise is of the form E[||Mn+1||2|Fn] ≤ C(1 + ||θn||2) where C is a constant, in correspondence to Remark 6. It is in fact a weaker requirement than the one obtained for TD(0), as is given in Lemma 4."
    }, {
      "heading" : "4 Proof of Theorem 1",
      "text" : "This section outlines the analysis conducted for Theorem 1. All proofs are given in Appendix B."
    }, {
      "heading" : "4.1 Outline of Approach",
      "text" : "We compare the TD(0) iterates {θn} with suitable solutions of its limiting ODE using the Variation of Parameters (VoP) method [Lakshmikantham and Deo, 1998]. As the solutions of the ODE are continuous functions of time, we first define a linear interpolation {θ̄(t)} of {θn}. Let t0 = 0. For n ≥ 0, let tn+1 = tn + αn and let\nθ̄(τ)= { θn if τ = tn , θn +\nτ−tn αn\n[θn+1 − θn] if τ ∈ (tn, tn+1) . (5)\nThe limiting ODE for (2) is\nθ̇(t) = h(θ(t)) = b−Aθ(t) = −A(θ(t)− θ∗) . (6)\nLet θ(t, s, u0), t ≥ s, denote the solution to the above ODE starting at u0 at time t = s. When the starting point and time are unimportant, we will denote this solution by θ(t) .\nInitially, θ̄(t) could stray away from θ∗ when the step-sizes may not be small enough to tame the noise. However, we show that ‖θ̄(tn)− θ∗‖ = O(n), i.e., θn does not stray away from θ∗ too fast. Later, we show that we can fix some n0 so that first the TD(0) iterates for n ≥ n0 stay within a O(n0) distance from θ∗; then after for some additional time when the stepsizes decay enough the TD(0) iterates start behaving almost like a noiseless version. These three different behaviours are summarized in Table 1 and illustrated in Figure 1."
    }, {
      "heading" : "4.2 Preliminaries",
      "text" : "We establish some preliminary results here that will be used throughout this section. Let s ∈ R, and u0 ∈ Rd. Using results from Chapter 6, [Hirsch et al., 2012], it follows that the solution θ(t, s, u0), t ≥ s, of (6) satisfies the relation\nθ(t, s, u0) = θ ∗ + e−A(t−s)(u0 − θ∗) . (7)\nAs the matrix A is positive definite, for θ(t) ≡ θ(t, s, u0), d\ndt ‖θ(t)− θ∗‖2 = −2(θ(t)− θ∗)>A(θ(t)− θ∗) < 0 .\nHence ‖θ(t′, s, u0)− θ∗‖ ≤ ‖θ(t, s, u0)− θ∗‖ , (8) for all t′ ≥ t ≥ s and u0. Let λ be as in Theorem 1. From Corollary 3.6, p71, [Teschl, 2012], ∃Kλ ≥ 1 so that ∀t ≥ s\n‖e−A(t−s)‖ ≤ Kλ e−λ(t−s) . (9) Separately, as tn+1 − tk+1 = ∑n `=k+1 α` = ∑n `=k+1 1 `+1 ,\n(k + 1)λ\n(n+ 1)λ ≤ e−λ(tn+1−tk+1) ≤ (k + 2)\nλ\n(n+ 2)λ . (10)\nThe following result gives a bound on the martingale difference noise as a function of the iterates. We emphasize that this bound is significant in our work and that this strong behavior of TD(0) is usually overlooked in existing literature.\nLemma 4 (Martingale Noise Behavior). For all n ≥ 0,\n‖Mn+1‖ ≤ Km [1 + ‖θn − θ∗‖] ,\nwhere\nKm := 1\n4 max\n{ 2 + [1 + γ]‖A−1‖‖b‖, 1 + γ + 4‖A‖ } .\nRemark 6. The noise behavior usually used in the literature (e.g., [Sutton et al., 2009a]) is\nE[||Mn+1||2|Fn] ≤ C(1 + ||θn||2) ,\nfor some constant C ≥ 0. The result on the noise behavior in Lemma 4 is in fact stronger than that. For easier comparison, we also provide following result (the proof technique is similar to that in Lemma 4). For all n ≥ 0,\n‖|Mn+1||2 ≤ 3[1 + γ + max(‖A‖, ‖b‖)]2(1 + ||θn||2).\nThe remaining parts of the analysis rely on the comparison of the discrete TD(0) trajectory {θn} to the continuous solution θ(t) of the limiting ODE. For this, we first switch from directly treating {θn} to treating their linear interpolation {θ̄(t)} as defined in (5). The key idea then is to use the VoP method [Lakshmikantham and Deo, 1998] and express θ̄(t) as a perturbation of θ(t) due to two factors: the discretization error and the martingale difference noise. This is discussed further in Lemma 13 in Appendix A.\nFor the interval [t`1 , t`2 ], let E d [`1,`2] := ∑`2−1 k=`1 ∫ tk+1 tk\ne−A(tn+1−τ)A[θ̄(τ)− θk]dτ , and Em[`1,`2] :=∑`2−1 k=`1 [∫ tk+1 tk e−A(tn+1−τ)dτ ] Mk+1 . Corollary 5 below shows that θ̄(t`2) − θ∗ differs from θ(t`2 , t`1 , θ̄(t`1))−θ∗ byEd[`1,`2]+E m [`1,`2]\n.We highlight that both the paths, θ̄(t) and θ(t, t`1 , θ̄(t`1)), t ≥ t`1 , start at the same point θ̄(t`1) at time t`1 . As mentioned above, Ed[`1,`2] and E m [`1,`2]\nrespectively denote the cumulative discretization error and martingale difference noise over the interval [t`1 , t`2 ].\nCorollary 5 (Comparison of SA Trajectory and ODE Solution). For every `2 ≥ `1,\nθ̄(t`2)− θ∗ = θ(t`2 , t`1 , θ̄(t`1))− θ∗ + Ed[`1,`2] + E m [`1,`2] .\nWe shall use this result later in Lemmas 14 and 15, in Appendix B."
    }, {
      "heading" : "4.3 Part I – Initial Possible Divergence",
      "text" : "In this section we show that the TD(0) iterates lie in a O(n)-ball around θ∗. We emphasize that this is one of the results that enables us to accomplish more than existing literature. Previously, the distance of the initial iterates from θ∗ was bounded using various assumptions, often justified with an artificial projection step which we are able to avoid.\nLet R0 := 1 + ‖θ0 − θ∗‖. Lemma 6 (Worst-case Iterates Bound). For n ≥ 0,\n‖θn − θ∗‖ ≤ Rwc(n) ,\nwhere Rwc(n) := [n+ 1]C∗R0\nand C∗ := 1 + ‖θ∗‖ ≤ 1 + ‖A−1‖ ‖b‖\nNext, since ‖Mn+1‖ is linearly bounded by ‖θn − θ∗‖, the following result shows that ‖Mn+1‖ is O(n) as well. It follows from Lemmas 4 and 6.\nCorollary 7 (Worst-case Noise Bound). For n ≥ 0,\n‖Mn+1‖ ≤ Km [1 + C∗R0][n+ 1] ."
    }, {
      "heading" : "4.4 Part II – Rate of Convergence",
      "text" : "Our formal aim here is to obtain an estimate on the probability of the event\nE(n0, n1) := {‖θn − θ∗‖ ≤ ∀n > n0 + n1}\nfor sufficiently large n0, n1 ≥ 1; how large they ought to be will be elaborated later. We do this by comparing the TD(0) trajectory θn+1 with the ODE solution θ(tn+1, tn0 , θ̄(tn0)) ∀n ≥ n0; for that we use Corollary 5 along with Lemma 6. In this section we show that if n0 is sufficiently large, or equivalently the stepsizes {αn}n≥n0 are small enough, then after a finite number of iterations from n0, the TD(0) iterates are −close to θ∗ w.h.p. This holds as the small stepsize and sufficiently long waiting time ensure that the ODE solution θ(tn+1, tn0 , θ̄(tn0)) is −close to θ∗, the discretization error Ed[n0,n+1] is small and martingale difference noise E m [n0,n+1] is small w.h.p.\nLet δ ∈ (0, 1), and let be such that > 0. Also, for an event E , let Ec denote its complement and let {E1, E2} denote E1 ∩ E2. We begin with a careful decomposition of Ec(n0, n1), the complement of the event of interest. The idea is to break it down into an incremental union of events. Each such event has an inductive structure: good up to iterate n (denoted by Gn0,n below) and the (n+ 1)−th iterate is bad. The good event Gn0,n holds when all the iterates up to n remain in an O(n0) ball around θ∗. For n < n0 + n1, the bad event means that θn+1 is outside the O(n0) ball around θ∗, while for n ≥ n0 + n1, the bad event means that θn+1 is outside the ball around θ∗. Formally, for n1 ≥ 1, define the events\nEmidn0,n1 := n0+n1−1⋃ n=n0 {Gn0,n, ‖θn+1 − θ∗‖>2Rwc(n0)} ,\nEaftern0,n1 := ∞⋃\nn=n0+n1\n{Gn0,n, ‖θn+1 − θ∗‖ > min{ , 2Rwc(n0)}} ,\nand, ∀n ≥ n0, let\nGn0,n :=\n{ n⋂\nk=n0\n{‖θk − θ∗‖≤2Rwc(n0)} } .\nUsing the above definitions, the decomposition of Ec(n0, n1) is the following relation. Lemma 8 (Decomposition of Event of Interest). For n0, n1 ≥ 1,\nEc(n0, n1) ⊆ Emidn0,n1 ∪ E after n0,n1 .\nFor the following results, define the constants\nCm2 :=\n{ 6KmKλ 2\nλ−0.5 √ 2λ−1 if λ > 0.5\n6KmKλ√ 1−2λ if λ < 0.5 .\nNext we show that on the “good” event Gn0,n, the discretization error is small for all sufficiently large n.\nLemma 9 (Part II Discretization Error Bound). For any n ≥ n0 ≥ Kλ6‖A‖(‖A‖+2Km )λ ,\n‖Ed[n0,n+1]‖ ≤ 1 3 [n0 + 1]C∗R0 = 1 3Rwc(n0).\nFurthermore, for n ≥ nc ≥ (\n1 + Kλ6‖A‖(‖A‖+2Km )C∗R0λmin{ ,Rwc(n0)} ) (n0 + 1) it thus also holds on Gn0,n that\n‖Ed[nc,n+1]‖ ≤ 1 3 min{ , [n0 + 1]C∗R0} = 1 3 min{ , Rwc(n0)} .\nThe next result gives a bound on the probability that, on the “good” event Gn0,n, the martingale difference noise is small when n is large. The bound has two forms for the different values of λ.\nLemma 10 (Part II Martingale Difference Noise Concentration). Let n0 ≥ 1 and R ≥ 0. Let n ≥ n′ ≥ n0. For λ > 1/2,\nPr{Gn0,n, ‖Em[n′,n+1]‖ ≥ R} ≤ 2d 2 exp\n[ − (n+ 1)R 2\n2d3C2m2R 2 wc(n0)\n] .\nFor λ < 1/2,\nPr{Gn0,n, ‖Em[n′,n+1]‖ ≥ R} ≤ 2d 2 exp\n[ − [n ′ + 1]1−2λ(n+ 1)2λR2\n2d3C2m2R 2 wc(n0)\n] .\nLemmas 8, 9 and 10 are the key ingredients for proving Theorem 1. The detailed proof is given in Appendix B. However, we now outline the underlying idea.\nFrom Lemma 8, by a union bound,\nPr{Ec(n0, n1)} = Pr{Ec(n0, n1)} ≤ Pr{Emidn0,n1}+ Pr{E after n0,n1} .\nNext, we use Lemmas 9 and 10 to set n0 and n1 in the following way to bound the terms on the RHS. The behavior of Emidn0,n1 is dictated by n0, while the behavior of E after n0,n1 by n1. We set n0 so that E mid n0,n1\nis less than δ/2 by substituting Rwc(n0)2 in r from Lemma 10, resulting in the condition n0 = O ( ln 1δ ) . Next, we set r = 3 for bounding E after n0,n1 by δ/2, resulting in n1 = Õ ([ (1/ ) ln (1/δ)\n]max{1+1/λ,2}) for λ > 1/2, and n1 = Õ ([ (1/ ) ln (1/δ) ]1+1/λ) for λ < 1/2."
    }, {
      "heading" : "5 Proof of Theorem 3",
      "text" : "The expectation bound is due to an inductive argument and an application of a subtle trick from Kamal [2010]. Building on the approach there, our key steps are: identifying a “nice\" Liapunov function V of the TD(0) method’s limiting ODE; and then using conditional expectation suitably to get rid of the linear noise terms in the relation between V (θn) and V (θn+1). Induction then leads to the desired result.\nThroughout this section only, {αn} is a stepsize sequence satisfying ∑ n≥0 αn =∞, limn→∞ αn = 0 and supn≥0 αn ≤ 1. The proofs in this section are provided in Appendix C.\nRecall that all eigenvalues of a symmetric matrix are real. For a symmetric matrix X, let λmin(X) and λmax(X) be its minimum and maximum eigenvalues, respectively.\nTheorem 11 (Technical Result: Expectation Bound). Fix λ ∈ (0, λmin(A+A>)).\nE‖θn+1 − θ∗‖2 ≤ Kp [ e−λ ∑n k=0 αk ] E‖θ0 − θ∗‖2 + 4K2mKp n∑ i=0 [ e−λ ∑n k=i+1 αk ] α2i ,\nwhere Kp,Km ≥ 0 are constants as defined in Lemmas 16 and 4, respectively.\nThe next result provides closed form estimates of the expectation bound given in Theorem 11 for the specific stepsize sequence αn = 1/(n+ 1)σ, with σ ∈ (0, 1). Theorem 12. Fix σ ∈ (0, 1) and let αn = 1/(n+ 1)σ. Then\nE‖θn+1 − θ∗‖2 ≤ [ Kpe λE‖θ0 − θ∗‖2e−(λ/2)(n+2) 1−σ + 8K2mKpKbe λ\nλ\n] e−(λ/2)(n+2) 1−σ\n+ 8K2mKpe λ/2\nλ\n1\n(n+ 1)σ ,\nwhere Kb = e[(λ/2) ∑i0 k=0 αk] with i0 denoting a number larger than (2σ/λ)1/(1−σ)."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this work we obtained the first concentration bound for an unaltered version of the celebrated TD(0); it is, in fact, the first to show the convergence rate of an unaltered online TD algorithm of any type. Our proof technique is general and can be used to provide convergence rates for additional TD methods. Specifically, using the non-linear analysis presented in [Thoppe and Borkar, 2015], we believe it can be extended to a broader family of function approximators, e.g., neural networks. Furthermore, future work can extend to a more general family learning rates, including the commonly used adaptive ones. Building upon Remark 5, we believe that a stronger expectation bound may hold for TD(0). This may enable obtaining tighter concentration bounds for TD(0) even with generic stepsizes."
    }, {
      "heading" : "B Supplementary Material for Proof of Theorem 1",
      "text" : "Proof of Lemma 4. We have\n‖Mn+1‖ = ‖rnφn + (γφ′n − φn)>θnφn − [b−Aθn]‖ = ‖rnφn + (γφ′n − φn)>(θn − θ∗)φn\n+(γφ′n − φn)>θ∗φn +A(θn − θ∗)‖\n≤ 1 2 + [1 + γ] 4 ‖A−1‖ ‖b‖+ [1 + γ + 4‖A‖] 4 ‖θn − θ∗‖,\nwhere the first relation follows from (3), the second holds as b = Aθ∗, while the third follows since A1 holds and θ∗ = A−1b. The desired result is now easy to see.\nProof of Corollary 5. The result follows by using Lemma 13 from Appendix A, with i = `1, t = t`2 , and subtracting θ∗ from both sides.\nProof of Lemma 6. The proof is by induction. The claim holds trivially for n = 0. Assume the claim for n. Then from (1),\n‖θn+1 − θ∗‖ ≤ ‖θn − θ∗‖+ αn‖[γφ′n − φn]>θ∗φn‖ + αn‖rnφn‖+ αn‖[γφ′n − φn]>[θn − θ∗]φn‖ .\nApplying the Cauchy-Schwarz inequality, and usingA1 and the fact that γ ≤ 1, we have\n‖θn+1 − θ∗‖ ≤ ‖θn − θ∗‖+ αn 2 C∗ + αn 2 ‖θn − θ∗‖.\nNow as 1 ≤ R0, we have ‖θn+1 − θ∗‖ ≤ [ 1 +\nαn 2\n] ‖θn − θ∗‖+\nαn 2 C∗R0.\nUsing the induction hypothesis and the stepsize choice, the claim for n+ 1 is now easy to see. The desired result thus follows.\nProof of Lemma 8. For any two events E1 and E2, note that E1 = [Ec2 ∩ E1] ∪ [E2 ∩ E1] ⊆ Ec2 ∪ [E2 ∩ E1] . (13)\nSeparately, for any sequence of events {Ek}, observe that m⋃ k=1 Ek = [ m⋃ k=1 ([ k−1⋃ i=1 Ei ]c ∩ Ek )] , (14)\nwhere ⋃i2 i=i1 Ei = ∅ whenever i1 > i2. Using (13), we have\nEc(n0, n1) ⊆ Gcn0,n0+n1 ∪ [Gn0,n0+n1 ∩ E c(n0, n1)] . (15)\nFrom Lemma 6, {‖θn0 − θ∗‖ ≤ Rwc(n0)} is a certain event. Hence it follows from (14) that\nGcn0,n0+n1 = E mid n0,n1 . (16)\nSimilarly, from (14) and the fact that ≤ R0,\nGn0,n0+n1 ∩ Ec(n0, n1) ⊆ Eaftern0,n1 . (17) Substituting (16) and (17) in (15) gives\nEc(n0, n1) ⊆ Emidn0,n1 ∪ E after n0,n1 .\nThe claimed result follows.\nProof of Lemma 9. For n ≥ n′ ≥ n0 ≥ 0, by its definition and the triangle inequality,\n‖Ed[n′,n+1]‖≤ n∑\nk=n′\n∫ tk+1 tk ‖e−A(tn+1−τ)‖‖A‖‖θ̄(τ)− θk‖dτ.\nFix a k ∈ {n′, . . . , n} and τ ∈ [tk, tk+1). Then using (5), (2), (4), and the fact that (τ − tk) ≤ αk, we have ‖θ̄(τ)− θk‖ ≤ αk[‖A‖‖θk − θ∗‖+ ‖Mk+1‖] . Combining this with Lemma 4, we get\n‖θ̄(τ)− θk‖ ≤ αk[Km + (‖A‖+Km )‖θk − θ∗‖] . As the event Gn0,n holds, and since αk ≤ αn′ and Rwc(n0) ≥ 1, we have\n‖θ̄(τ)− θk‖ ≤ 2[‖A‖+ 2Km ]αn′ [n0 + 1]C∗R0 . From the above discussion, (9), the step-size choice, and the facts that\nn∑ k=n′ ∫ tk+1 tk e−λ(tn+1−τ)dτ = ∫ tn+1 tn′ e−λ(tn+1−τ)dτ ≤ 1 λ ,\nand αk ≤ αn′ ≤ αn0 , we get\n‖Ed[n′,n+1]‖ ≤ Kλ2‖A‖(‖A‖+2Km )(n0+1)C∗R0 λ(n′+1) .\nThe desired results now follow by substituting n′ first with n0 and then with nc. Proof of Lemma 10. Let Qk,n = ∫ tk+1 tk e−A(tn+1−τ)dτ. Then, for any n0 ≤ n′ ≤ n,\nEm[n′,n+1] = n∑ k=n′ Qk,nMk+1 ,\na sum of martingale differences. When the event Gn0,n holds, it follows that the indicator 1Gn0,k = 1 ∀k ∈ {n0, . . . , n′, . . . n}. Hence, for any R ≥ 0,\nPr{Gn0,n, ‖Em[n′,n+1]‖ ≥ R} = Pr { Gn0,n, ∥∥∥∥∥ n∑\nk=n′\nQk,nMk+11Gn0,k ∥∥∥∥∥ ≥ R }\n≤ Pr {∥∥∥∥∥ n∑\nk=n′\nQk,nMk+11Gn0,k ∥∥∥∥∥ ≥ R } .\nLet Qijk,n be the i, j−th entry of the matrix Qk,n and let M j k+1 be the j−th coordinate of Mk+1. Then using the union bound twice on the above relation, we have\nPr{Gn0,n, ‖Em[n′,n+1]‖ ≥ R} ≤ d∑ i=1 d∑ j=1 Pr {∣∣∣∣∣ n∑ k=n′ Qijk,nM j k+11Gn0,k ∣∣∣∣∣ ≥ Rd√d } .\nAs |Qijk,nM j k+1|1Gn0,k ≤ ‖Qk,n‖‖Mk+1‖1Gn0,k =: βk,n, Azuma-Hoeffding inequality now gives\nPr{Gn0,n, ‖Em[n′,n+1]‖ ≥ R} ≤ 2d 2 exp\n[ − R 2\n2d3 ∑n k=n′ β 2 k,n\n] . (18)\nOn the event Gn0,k, ‖θk − θ∗‖ ≤ 2Rwc(n0) by definition. Hence from Lemma 4, we have\n‖Mk+1‖1Gk ≤ 3KmRwc(n0) . (19)\nAlso from (9), ‖Qk,n‖ ≤ Kλ e−λ(tn+1−tk+1)αk. Combining the two inequalities, and using (10) along with the fact that 1/(k + 1) ≤ 2/(k + 2), we get\nβk,n ≤ 3KmKλRwc(n0)e−λ(tn+1−tk+1)αk\n≤ 6KmKλRwc(n0) (k + 2)λ−1\n(n+ 2)λ .\nConsider the case λ > 1/2. By treating the sum as a right Riemann sum, we have n∑\nk=n′\n(k + 2)2λ−2 ≤ (n+ 3)2λ−1/(2λ− 1) .\nAs (n+ 3) ≤ 2(n+ 2) and (n+ 2) ≥ (n+ 1), we have n∑\nk=n′\nβ2k,n ≤ C2m2 R2wc(n0)\nn+ 1 .\nNow consider the case λ < 1/2. Again treating the sum as a right Riemann sum, we have n∑\nk=n′\n(k + 2)2λ−2 ≤ 1 (1− 2λ)[n′ + 1]1−2λ .\nAs (n+ 2) ≥ (n+ 1), it follows that n∑\nk=n′\nβ2k,n ≤ C2m2 R2wc(n0)\n[n′ + 1]1−2λ(n+ 1)2λ .\nSubstituting ∑n k=n0 β2k,n bounds in (18), the desired result is easy to see.\nB.1 Conditional Results on the Bad Events\nOn the first “bad” event Emidn0,n1 , the TD(0) iterate θn for at least one n between n0 + 1 and n0 + n1 leaves the 2Rwc(n0) ball around θ∗. The next lemma shows that this event has low probability.\nLemma 14 (Bound on Probability of Emidn0,n1 ). Let n0 ≥ max { Kλ6‖A‖(‖A‖+2Km ) λ , 2 1 λ } and n1 ≥ 1.\n• If λ > 1/2, then\nPr{Emidn0,n1} ≤ 16d 5C2m2 exp\n[ − n0\n8d3C2m2\n] .\n• If λ < 1/2, then\nPr{Emidn0,n1} ≤ 2d 2\n[ 8d3C2m2\nλ\n] 1 2λ exp[− n0\n64d3C2m2 ]\n(n0 + 1) 1−2λ 2λ\n.\nProof. From Corollary 5, we have\n‖θn+1 − θ∗‖ ≤ ‖θ(tn+1, tn0 , θn0)− θ∗‖+ ‖Ed[n0,n+1]‖+ ‖E m [n0,n+1] ‖ .\nSuppose the event Gn0,n holds. Then from (8),\n‖θ(tn+1, tn0 , θn0)− θ∗‖ ≤ ‖θn0 − θ∗‖ ≤ Rwc(n0) .\nAlso, as n0 ≥ Kλ6‖A‖(‖A‖+2Km )λ , by Lemma 9, ‖E d [n0,n+1]\n‖ ≤ Rwc(n0)/3. From all of the above, we have\n{Gn0,n, ‖θn+1 − θ∗‖ > 2Rwc(n0)} ⊆ {Gn0,n, ‖Em[n0,n+1]‖ > Rwc(n0)/2} .\nFrom this, we get\nEmidn0,n1 ⊆ n0+n1−1⋃ n=n0 { Gn0,n, ‖Em[n0,n+1]‖ > Rwc(n0) 2 } ⊆ ∞⋃\nn=n0\n{ Gn0,n, ‖Em[n0,n+1]‖ > Rwc(n0) 2 } .\nConsequently,\nPr{Emidn0,n1} ≤ ∞∑\nn=n0\nPr { Gn0,n, ‖Em[n0,n+1]‖> Rwc(n0) 2 } . (20)\nConsider the case λ > 1/2. Lemma 10 shows that\nPr { Gn0,n, ‖Em[n0,n+1]‖ > Rwc(n0) 2 } ≤ 2d2 exp [ − n+ 1\n8d3C2m2\n] .\nSubstituting this in (20) and treating the resulting expression as a right Riemann sum, the desired result is easy to see.\nNow consider the case λ < 1/2. From Lemma 10, we get\nPr { Gn0,n, ‖Em[n0,n+1]‖ > Rwc(n0) 2 } ≤ 2d2 exp [ − (n0 + 1) 1−2λ(n+ 1)2λ\n8d3C2m2\n] .\nLet `n0 := (n0 + 1) 1−2λ/8d3C2m2. Observe that ∞∑\nn=n0\nexp[−`n0(n+ 1)2λ]\n≤ ∞∑\ni=b(n0+1)2λc\ne−i`n0 |{n : b(n+ 1)2λc = i}|\n≤ 1 2λ ∞∑ i=b(n0+1)2λc e−i`n0 (i+ 1) 1−2λ 2λ (21)\n≤ 1 2λ ∞∑ i=b(n0+n1)2λc e−i`n0/2e−i`n0/2 (i+ 1) 1−2λ 2λ\n≤ 1 2λ [ (1− 2λ) `n0λ ] 1−2λ 2λ e 1 2 [`n0− 1−2λ λ ] ∞∑ i=b(n0+1)2λc e−i`n0/2 (22)\n≤ 1 `n0λ [ (1− 2λ) `n0λ ] 1−2λ 2λ e 1 2 [`n0− 1−2λ λ ]e− `n0 n0 2λ 4 (23)\n≤ [\n1− 2λ e\n] 1−2λ 2λ [\n8d3C2m2 λ\n] 1 2λ exp[− n0\n64d3C2m2 ]\n(n0 + 1) 1−2λ 2λ\n(24)\n≤ [\n8d3C2m2 λ\n] 1 2λ exp[− n0\n64d3C2m2 ]\n(n0 + 1) 1−2λ 2λ\n. (25)\nThe relation (21) follows, as by calculus,\n|{n : b(n+ 1)2λc = i}| ≤ 1 2λ (i+ 1) 1−2λ 2λ ,\n(22) holds since, again by calculus,\nmax i≥0\ne−i`n0/2(i+ 1) 1−2λ 2λ ≤ [ (1− 2λ) `n0λ ] 1−2λ 2λ e 1 2 [`n0− 1−2λ λ ] ,\n(23) follows by treating the sum as a right Riemann sum, (24) follows by substituting the value of `n0 and using the fact that n 2λ 0 ≥ 4 and (25) holds since 1− 2λ ≤ 1. Substituting (25) in (20), the desired result follows.\nOn the second “bad” event Eaftern0,n1 , the TD(0) iterate θn for at least one n > n0 + n1 lies outside the min{ , 2Rwc(n0)} radius ball around θ∗. The next result shows that this event also has low probability.\nLemma 15 (Bound on Probability of Eaftern0,n1 ). Let n0 ≥ max { Kλ6‖A‖(‖A‖+2Km ) λ , 2 1 λ } and\nnc ≥ (\n1 + Kλ6‖A‖(‖A‖+2Km )λmin{ ,Rwc(n0)}\n) Rwc(n0),\nLet n1 ≡ n1( , nc, n0) ≥ (nc + 1) [ 6Kλ Rwc(n0) ]1/λ − n0.\nIf λ > 1/2, then\nPr{Eaftern0,n1} ≤ 36d 5C2m2\n[ Rwc(n0) ]2 exp − (6Kλ )1/λ 18d3C2m2 (nc + 1) [ Rwc(n0) ]2− 1λ . If λ < 1/2, then\nPr{Eaftern0,n1} ≤ 2d 2\n[ 18d3C2m2[Rwc(n0)] 2\n2λ\n] 1 2λ\nexp [ − K 2 λ\n4d3C2m2 (nc + 1)\n] .\nProof. Assume the event Gn0,n holds for some n ≥ nc. Then\n‖θnc − θ∗‖ ≤ 2Rwc(n0).\nHence from (7) and (9), for t ≥ tnc , we have\n‖θ(t, tnc , θnc)− θ∗‖ ≤ Kλ e−λ(t−tnc )2Rwc(n0) . (26)\nNow as n1 ≥ (nc + 1) [ 6Kλ Rwc(n0) ]1/λ − n0, it follows that ∀n ≥ n0 + n1,\n‖θ(tn+1, tnc , θnc)− θ∗‖ ≤\n3 . Also, as nc ≥ (\n1 + Kλ6‖A‖(‖A‖+2Km )C∗R0λmin{ ,Rwc(n0)} ) (n0 + 1), from Lemma 9, we have ‖Ed[nc,n+1]‖ ≤ /3\nfor all n ≥ nc. Combining these with Corollary 5, it follows that ∀n ≥ n0 + n1,\n{Gn0,n, ‖θn+1 − θ∗‖ > min{ , 2Rwc(n0)}} ⊆ {Gn0,n, ‖θn+1 − θ∗‖ > }\n⊆ {Gn0,n, ‖Em[nc,n+1]‖ ≥ 3} .\nHence from the definition of Eaftern0,n1 ,\nPr{Eaftern0,n1} ≤ ∞∑\nn=n0+n1\nPr { Gn0,n, ‖Em[nc,n+1]‖ ≥ 3 } . (27)\nConsider the case λ > 1/2. Lemma 10 and the definition of Rwc(n0) in Theorem 6 shows that Pr { Gn0,n, ‖Em[nc,n+1]‖ ≥ 3 } ≤ 2d2 exp [ − (n0 + 1) −2(n+ 1) 2\n18d3C2m2C 2 ∗R 2 0\n] .\nUsing this in (27) and treating the resulting expression as a right Riemann sum, we get\nPr{Eaftern0,n1} ≤ 36d 5C2m2\n[ Rwc(n0) ]2 exp [ − (n0 + n1) 2\n18d3C2m2[Rwc(n0)] 2\n] .\nSubstituting the given relation between n1 and nc, the desired result is easy to see.\nConsider the case λ < 1/2. From Lemma 10 and the definition of Rwc(n0) in Theorem 6, we have Pr { Gn0,n, ‖Em[nc,n+1]‖ ≥ 3 } ≤ 2d2 exp [ − (nc + 1) 1−2λ(n+ 1)2λ 2\n18d3C2m2[Rwc(n0)] 2\n] .\nLet knc := 2(nc + 1) 1−2λ/(18d3C2m2[Rwc(n0)] 2). Pr { Gn0,n, ‖Em[nc,n+1]‖ ≥ 3 } ≤ 2d2 exp [ −knc (n+ 1)2λ ] .\nThen by the same technique that we use to obtain (23) in the proof for Lemma 14, we have ∞∑\nn=n0+n1\nexp[−knc(n+ 1)2λ]\n≤ 1 kncλ [ (1− 2λ) kncλ ] 1−2λ 2λ e 1 2 [knc− 1−2λ λ ]e− knc (n0+n1) 2λ 4\n≤ [ 1\nkncλ\n] 1 2λ\ne− knc (n0+n1)\n2λ\n8\n=\n[ 18d3C2m2[Rwc(n0)] 2\n2λ(nc + 1)1−2λ\n] 1 2λ\nexp [ − 2(nc + 1) 1−2λ(n0 + n1) 2λ\n144d3C2m2[Rwc(n0)] 2 ] where the second inequality is obtained using the facts that (n0 + n1)2λ ≥ n2λ0 ≥ 4 and 1− 2λ ≤ 1 and the last equality is obtained by substituting the value of knc . From this, after substituting the given relation between nc and n1, the desired result is easy to see.\nProof of Theorem 1. From Lemma 8, by a union bound,\nPr{Ec(n0, n1)} ≤ Pr{Emidn0,n1}+ Pr{E after n0,n1} .\nWe now show how to set n0 and n1 so that each of the two terms above is less than δ/2.\nConsider the case λ > 1/2. Let N0(δ) = max { Kλ6‖A‖(‖A‖+2Km ) λ , 2 1 λ , 8d3C2m2 ln [ 32d5C2m2 δ ]} =O ( ln 1δ ) , (28)\nNc( , δ, n0) = max {[( 1 + Kλ6‖A‖(‖A‖+2Km )λmin{ ,Rwc(n0)} ) Rwc(n0) ] ,\n18d3C2m2 (6Kλ )1/λ\n[ Rwc(n0) ]2− 1λ ln [ 72d5C2m2 [ 1\nδ\n] [ Rwc(n0) ]2]} ,\nso that Nc( , δ,N0(δ)) = Õ ( max { 1 ln [ 1 δ ] , [ 1 ]2− 1λ [ln 1δ ]3− 1λ}) , and let N1( , nc, n0) = (nc + 1) [ 6KλRwc(n0) ]1/λ − n0,\nso that\nN1( ,Nc( , δ,N0(δ)), N0(δ)) = Õ\n( max {[ 1 ]1+ 1λ [ ln 1\nδ\n]1+ 1λ , [ 1 ]2 [ ln 1\nδ\n]3}) . (29)\nLet n0 ≥ N0(δ), nc ≥ Nc( , δ, n0) and n1 ≥ N1( , nc, n0). Then from Lemma 14, Pr{Emidn0,n1} ≤ δ/2 and from Lemma 15, Pr{Eaftern0,n1} ≤ δ/2. Hence Pr{E\nc(n0, n1)} ≤ δ. Consequently, N( , δ) = N1( ,Nc( , δ,N0(δ)), N0(δ)) satisfies the desired properties, which completes the proof for λ > 1/2.\nNow consider the case λ < 1/2. The same exact proof can be repeated, with the following N0, Nc and N1.\nN0(δ) = max { Kλ6‖A‖(‖A‖+2Km ) λ , 2 1 λ , 64d3C2m2 2λ ln ( 32d5C2m2 δλ )} =O ( ln 1δ ) , (30)\nNc( , δ, n0) = max {[( 1 + Kλ6‖A‖(‖A‖+2Km )λmin{ ,Rwc(n0)} ) Rwc(n0) ] ,\n4d3C2m2 2λK2λ ln\n( 72d5C2m2\nλ\n[ 1\nδ\n] [Rwc(n0)] 2\n2\n)} ,\nso that Nc( , δ,N0(δ)) = Õ ( 1 ln 1 δ ) and let\nN1( , nc, n0) = (nc + 1)\n[ 6KλRwc(n0) ]1/λ − n0, (31)\nso that N1( ,Nc( , δ,N0(δ)), N0(δ)) = Õ ([ (1/ ) ln (1/δ) ]1+1/λ)\n. Thus N( , δ) = N1( ,Nc( , δ,N0(δ)), N0(δ)) satisfies the desired properties for the case λ < 1/2.\nFor λ = 1/2, the same process can be repeated, resulting in the same O and Õ results as in (30) and (31)."
    }, {
      "heading" : "C Supplementary Material for Proof of Theorem 3",
      "text" : "Notice that the matrices (A>+A) and (A>A+KmI) are symmetric. Further, asA is positive definite, the above matrices are also positive definite. Hence their minimum and maximum eigenvalues are strictly positive. Lemma 16. For n ≥ 0, let λn := λmax(Λn), where\nΛn := I− αn(A+A>) + α2n(A>A+ 4K2m I).\nFix λ ∈ (0, λmin(A+A>)). Let m be so that ∀k ≥ m, αk ≤ λmin(A+A >)−λ\nλmax(A>A+4K2m I) . Then for any k, n\nsuch that n ≥ k ≥ 0, n∏ i=k λk ≤ Kpe−λ[ ∑n i=k α`] ,\nwhere\nKp := max `1≤`2≤m `2∏ `=`1 eα`(µ+λ) ,\nwith µ = −λmin(A+A>) + λmax(A>A+ 4K2m I).\nRemark 7. Such m exists since αk → 0 as k →∞.\nProof. Using Weyl’s inequality, we have\nλn ≤ λmax(I− αn(A+A>)) + α2nλmax(A>A+ 4K2m I). (32)\nSince λmax(I− αn(A+A>)) ≤ (1− αnλmin(A+A>)), we have\nλn ≤ e[−αnλmin(A >+A)+α2nλmax(A >A+4K2m I)].\nFor n < m, using αn ≤ 1 and hence α2n ≤ αn, we have the following weak bound:\nλn ≤ eαnµ. (33)\nOn the other hand, for n ≥ m, we have\nλn ≤ e−λαne−αn[(λmin(A >+A)−λ)−αnλmax(A>A+4K2m I)] ≤ e−λαn . (34)\nTo prove the desired result, we consider three cases: k ≤ n ≤ m, m ≤ k ≤ n and k ≤ m ≤ n. For the last case, using (33) and (34), we have\nn∏ `=k λ` ≤ [ m∏ `=k λ` ] e−λ( ∑n `=m+1 α`) = [ m∏ `=k λ` ] eλ( ∑m `=k α`)e−λ( ∑n `=k α`) ≤ Kpe−λ( ∑n `=k α`) ,\nas desired. Similarly, it can be shown that bound holds in other cases as well. The desired result thus follows.\nProof of Theorem 11. Let V (θ) = ‖θ − θ∗‖2. Using (2) and (4), we have\nθn+1 − θ∗ = (I − αnA)(θn − θ∗) + αnMn+1.\nHence\nV (θn+1) =(θn+1 − θ∗)>(θn+1 − θ∗) =[(I − αnA)(θn − θ∗) + αnMn+1]>[(I − αnA)(θn − θ∗) + αnMn+1] =(θn − θ∗)>[I − αn(A> +A) + α2nA>A](θn − θ∗)\n+ αn(θn − θ∗)>(I − αnA)>Mn+1 + αnM>n+1(I − αnA)(θn − θ∗) + α2n‖Mn+1‖2.\nTaking conditional expectation and using E[Mn+1|Fn] = 0, we get\nE[V (θn+1)|Fn] = (θn − θ∗)>[I − αn(A> +A) + α2nA>A](θn − θ∗) + α2nE[‖Mn+1‖2|Fn].\nFrom Lemma 4 and as |x| ≤ 1 + x2, we have ‖Mn+1‖2 ≤ 4K2m [1 + ‖θn− θ∗‖2]. This immediately shows that E[‖Mn+1‖2|Fn] ≤ 4K2m [1 + ‖θn − θ∗‖2]. Hence\nE[V (θn+1)|Fn] ≤ (θn − θ∗)>Λn(θn − θ∗) + 4K2m α2n,\nwhere Λn = [I − αn(A> + A) + α2n(A>A + 4K2m I)]. Since Λn is a symmetric matrix, all its eigenvalues are real. With λn := λmax(Λn), we have\nE[V (θn+1)|Fn] ≤ λnV (θn) + 4K2m α2n.\nTaking expectation on both sides and letting wn = E[V (θn)], we have\nwn+1 ≤ λnwn + 4K2m α2n.\nSequentially using the above inequality, we have\nwn+1 ≤ [ n∏ k=1 λk ] w0 + 4K 2 m n∑ i=0 [ n∏ k=i+1 λk ] α2i .\nUsing Lemma 16 and using the constant Kp defined there, the desired result follows.\nProof of Theorem 12. Let {tn} be as defined in Subsection 4.1. Observe that n∑ i=0 [ e−(λ/2) ∑n k=i+1 αk ] αi ≤ ( sup i≥0 e(λ/2)αi ) n∑ i=0 [ e−(λ/2) ∑n k=i αk ] αi\n= ( sup i≥0 e(λ/2)αi ) n∑ i=0 [ e−(λ/2)(tn+1−tk) ] αi\n≤ (\nsup i≥0\ne(λ/2)αi )∫ tn+1\n0\ne−(λ/2)(tn+1−s)ds\n≤ (\nsup i≥0\ne(λ/2)αi ) 2\nλ\n≤ 2e λ/2\nλ ,\nwhere the third relation follows by treating the sum as right Riemann sum, and the last inequality follows since supi≥0 αi ≤ 1. Hence it follows that\nn∑ i=0 [ e−λ ∑n k=i+1 αk ] α2i ≤ ( sup 0≤i≤n [ αie −(λ/2) ∑n k=i+1 αk ]) n∑ i=0 [ e−(λ/2) ∑n k=i+1 αk ] αi\n≤ 2e λ/2\nλ\n( sup\n0≤i≤n\n[ αie −(λ/2) ∑n k=i+1 αk ]) . (35)\nWe claim that for all n ≥ i0,\nsup i0≤i≤n\n[ αie −(λ/2) ∑n k=i+1 αk ] ≤ 1\n(n+ 1)σ . (36)\nTo establish this, we show that for any n ≥ i0, αie−(λ/2)[ ∑n k=i+1 αk] monotonically increases as i is varied from i0 to n. To prove the latter, it suffices to show that αie−(λ/2)αi+1 ≤ αi+1, or equivalently (i+ 2)σ/(i+ 1)σ ≤ eλ/[2(i+2)σ ] for all i ≥ i0. But the latter is indeed true. Thus (36) holds. From (35) and (36), we then have\nn∑ i=0 [ e−λ ∑n k=i+1 αk ] α2i\n≤ 2e λ/2\nλ\n[( sup\n0≤i≤i0\n[ αie −(λ/2) ∑n k=i+1 αk ]) + ( sup\ni0≤i≤n\n[ αie −(λ/2) ∑n k=i+1 αk ])] ≤ 2e λ/2\nλ\n[( sup\n0≤i≤i0\n[ αie −(λ/2) ∑n k=i+1 αk ]) +\n1\n(n+ 1)σ ] ≤ 2e λ/2\nλ\n[ e−[(λ/2) ∑n k=0 αk] ( sup\n0≤i≤i0\n[ αie (λ/2) ∑i k=0 αk ]) +\n1\n(n+ 1)σ ] ≤ 2e λ/2\nλ\n[ Kbe −[(λ/2) ∑n k=0 αk] +\n1\n(n+ 1)σ\n] ,\nwhere the first relation holds as sup{a0, . . . , an} ≤ sup{a0, . . . , ai0} + sup{ai0 , . . . , an} for any positive sequence {a0, . . . , an} with 0 ≤ i0 ≤ n, and the last relation follows as αi ≤ 1 and sup0≤i≤i0 e (λ/2) ∑i k=0 αk ≤ Kb. Combining the above inequality with the relation from Theorem 11, we have\nE‖θn+1−θ∗‖2 ≤ Kp [ e −λ n∑ k=0 αk ] E‖θ0−θ∗‖2+ 8K2mKpe λ/2\nλ\n[ Kbe −[(λ/2) n∑ k=0 αk] +\n1\n(n+ 1)σ\n] ,\nSince n∑ k=0 αk ≥ ∫ n+1 0\n1\n(x+ 1)σ dx = (n+ 2)1−σ − 1,\nthe desired result follows."
    } ],
    "references" : [ {
      "title" : "Dynamic Programming and Optimal Control",
      "author" : [ "D.P. Bertsekas" ],
      "venue" : "Vol II. Athena Scientific, fourth edition,",
      "citeRegEx" : "Bertsekas.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bertsekas.",
      "year" : 2012
    }, {
      "title" : "Stochastic approximation: a dynamical systems viewpoint",
      "author" : [ "Vivek S Borkar" ],
      "venue" : null,
      "citeRegEx" : "Borkar.,? \\Q2008\\E",
      "shortCiteRegEx" : "Borkar.",
      "year" : 2008
    }, {
      "title" : "The ode method for convergence of stochastic approximation and reinforcement learning",
      "author" : [ "Vivek S Borkar", "Sean P Meyn" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Borkar and Meyn.,? \\Q2000\\E",
      "shortCiteRegEx" : "Borkar and Meyn.",
      "year" : 2000
    }, {
      "title" : "Transport-entropy inequalities and deviation estimates for stochastic approximation schemes",
      "author" : [ "Max Fathi", "Noufel Frikha" ],
      "venue" : "Electron. J. Probab.,",
      "citeRegEx" : "Fathi and Frikha.,? \\Q2013\\E",
      "shortCiteRegEx" : "Fathi and Frikha.",
      "year" : 2013
    }, {
      "title" : "Concentration bounds for stochastic approximations",
      "author" : [ "Noufel Frikha", "Stéphane Menozzi" ],
      "venue" : "Electron. Commun. Probab.,",
      "citeRegEx" : "Frikha and Menozzi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Frikha and Menozzi.",
      "year" : 2012
    }, {
      "title" : "Differential equations, dynamical systems, and an introduction to chaos",
      "author" : [ "Morris W Hirsch", "Stephen Smale", "Robert L Devaney" ],
      "venue" : "Academic press,",
      "citeRegEx" : "Hirsch et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hirsch et al\\.",
      "year" : 2012
    }, {
      "title" : "On the convergence, lock-in probability, and sample complexity of stochastic approximation",
      "author" : [ "Sameer Kamal" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Kamal.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kamal.",
      "year" : 2010
    }, {
      "title" : "On td (0) with function approximation: Concentration bounds and a centered variant with exponential convergence",
      "author" : [ "Nathaniel Korda", "LA Prashanth" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Korda and Prashanth.,? \\Q2015\\E",
      "shortCiteRegEx" : "Korda and Prashanth.",
      "year" : 2015
    }, {
      "title" : "Method of variation of parameters for dynamic systems",
      "author" : [ "Vangipuram Lakshmikantham", "Sadashiv Deo" ],
      "venue" : null,
      "citeRegEx" : "Lakshmikantham and Deo.,? \\Q1998\\E",
      "shortCiteRegEx" : "Lakshmikantham and Deo.",
      "year" : 1998
    }, {
      "title" : "Finite-sample analysis of lstd",
      "author" : [ "Alessandro Lazaric", "Mohammad Ghavamzadeh", "Rémi Munos" ],
      "venue" : "In ICML-27th International Conference on Machine Learning,",
      "citeRegEx" : "Lazaric et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lazaric et al\\.",
      "year" : 2010
    }, {
      "title" : "Finite-sample analysis of proximal gradient td algorithms",
      "author" : [ "Bo Liu", "Ji Liu", "Mohammad Ghavamzadeh", "Sridhar Mahadevan", "Marek Petrik" ],
      "venue" : "In UAI,",
      "citeRegEx" : "Liu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximate Dynamic Programming: Solving the curses of dimensionality, volume 703",
      "author" : [ "Warren B Powell" ],
      "venue" : null,
      "citeRegEx" : "Powell.,? \\Q2007\\E",
      "shortCiteRegEx" : "Powell.",
      "year" : 2007
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree",
      "author" : [ "David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot" ],
      "venue" : "search. Nature,",
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "Richard S Sutton" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Sutton.,? \\Q1988\\E",
      "shortCiteRegEx" : "Sutton.",
      "year" : 1988
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "A convergent o (n) temporal-difference algorithm for off-policy learning with linear function approximation",
      "author" : [ "Richard S Sutton", "Hamid R Maei", "Csaba Szepesvári" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "Richard S Sutton", "Hamid Reza Maei", "Doina Precup", "Shalabh Bhatnagar", "David Silver", "Csaba Szepesvári", "Eric Wiewiora" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "An emphatic approach to the problem of off-policy temporal-difference learning",
      "author" : [ "Richard S Sutton", "A Rupam Mahmood", "Martha White" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2015
    }, {
      "title" : "Temporal difference learning and td-gammon",
      "author" : [ "Gerald Tesauro" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Tesauro.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro.",
      "year" : 1995
    }, {
      "title" : "A concentration bound for stochastic approximation via alekseev’s formula",
      "author" : [ "Gugan Thoppe", "Vivek S Borkar" ],
      "venue" : null,
      "citeRegEx" : "Thoppe and Borkar.,? \\Q2015\\E",
      "shortCiteRegEx" : "Thoppe and Borkar.",
      "year" : 2015
    }, {
      "title" : "An analysis of temporal-difference learning with function approximation",
      "author" : [ "John N Tsitsiklis", "Benjamin Van Roy" ],
      "venue" : "IEEE transactions on automatic control,",
      "citeRegEx" : "Tsitsiklis and Roy,? \\Q1997\\E",
      "shortCiteRegEx" : "Tsitsiklis and Roy",
      "year" : 1997
    }, {
      "title" : "Convergence results for some temporal difference methods based on least squares",
      "author" : [ "Huizhen Yu", "Dimitri P Bertsekas" ],
      "venue" : "IEEE Transactions on Automatic Control,",
      "citeRegEx" : "Yu and Bertsekas.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yu and Bertsekas.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "The term has been coined in [Sutton and Barto, 1998], describing an iterative process of updating an estimate of a value function V (s) with respect to a given policy π based on temporally-successive samples.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "The term has been coined in [Sutton and Barto, 1998], describing an iterative process of updating an estimate of a value function V (s) with respect to a given policy π based on temporally-successive samples. The classical version of the algorithm uses a tabular representation, i.e., entry-wise storage of the value estimate per each state s ∈ S. However, in many problems the state-space S is too large for such a vanilla approach. The common practice to mitigate this caveat is to approximate the value function using some parameterized family. Often, linear regression is used, i.e., V (s) ≈ θ>φ(s). This allows for an efficient implementation of TD(0) even on large state-spaces and has shown to perform well in a variety of problems Tesauro [1995], Powell [2007].",
      "startOffset" : 29,
      "endOffset" : 754
    }, {
      "referenceID" : 11,
      "context" : "This allows for an efficient implementation of TD(0) even on large state-spaces and has shown to perform well in a variety of problems Tesauro [1995], Powell [2007]. More recently, TD(0) has become prominent in many state-of-the-art RL solutions when combined with deep neural network architectures, as an integral part of fitted value iteration [Mnih et al.",
      "startOffset" : 151,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "In [Borkar, 2008], a concentration bound is given for generic SA algorithms.",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "In [Korda and Prashanth, 2015], concentration bounds for TD(0) with mixing-time consideration have been given.",
      "startOffset" : 3,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "Following that, a key result by Borkar and Meyn [2000] paved the path to a unified and convenient tool for convergence analyses of Stochastic Approximation (SA), and hence of TD algorithms.",
      "startOffset" : 32,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Following that, a key result by Borkar and Meyn [2000] paved the path to a unified and convenient tool for convergence analyses of Stochastic Approximation (SA), and hence of TD algorithms. This tool is based on the Ordinary Differential Equation (ODE) method. Essentially, that work showed that under the right conditions, the SA trajectory follows the solution of a suitable ODE, often referred to as its limiting ODE; thus, it eventually converges to the solution of the limiting ODE. Several usages of this tool in RL literature can be found in [Sutton et al., 2009a,b, 2015]. As opposed to the case of asymptotic convergence analysis of TD algorithms, very little is known on their finite sample behavior. We now briefly discuss the few existing results on this topic. In [Borkar, 2008], a concentration bound is given for generic SA algorithms. Recent works [Kamal, 2010, Thoppe and Borkar, 2015] obtain better concentration bounds via tighter analyses. The results in these works are conditioned on the event that the n0−th iterate lies in some a-priori chosen bounded region containing the desired equilibria; this, therefore, is the caveat in applying them to TD(0). In [Korda and Prashanth, 2015], concentration bounds for TD(0) with mixing-time consideration have been given. However, unlike in our work, a strong requirement for all their high probability bounds is that the iterates need to lie in some a-priori chosen bounded set; this is ensured there via projections (personal communication). Additionally, their results require the learning rate to be set based on prior knowledge about system dynamics, which, as argued in the paper, is problematic; alternatively, they apply to average of iterates. An additional work by Liu et al. [2015] considered the gradient TD algorithms GTD(0) and GTD2, which were first introduced in [Sutton et al.",
      "startOffset" : 32,
      "endOffset" : 1758
    }, {
      "referenceID" : 14,
      "context" : "A MDP is defined by the 5-tuple (S,A , P,R, γ) [Sutton, 1988], where S is the set of states, A is the set of",
      "startOffset" : 47,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "It is known that A is positive definite [Bertsekas, 2012] and that (2) converges to θ∗ := A−1b [Borkar, 2008].",
      "startOffset" : 40,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "It is known that A is positive definite [Bertsekas, 2012] and that (2) converges to θ∗ := A−1b [Borkar, 2008].",
      "startOffset" : 95,
      "endOffset" : 109
    }, {
      "referenceID" : 7,
      "context" : "Theorem 1, Korda and Prashanth [2015] requires the TD(0) step-sizes to satisfy: αn = fn(λ) for some function fn, where λ is as above.",
      "startOffset" : 11,
      "endOffset" : 38
    }, {
      "referenceID" : 1,
      "context" : "In [Borkar, 2008], on which much of the existing RL literature is based on, the square summability assumption is due to the Gronwall inequality.",
      "startOffset" : 3,
      "endOffset" : 17
    }, {
      "referenceID" : 8,
      "context" : "In contrast, in our work, we use the Variation of Parameters Formula [Lakshmikantham and Deo, 1998] for comparing the SA trajectory to appropriate trajectories of the limiting ODE; it is a stronger tool than Gronwall inequality.",
      "startOffset" : 69,
      "endOffset" : 99
    }, {
      "referenceID" : 6,
      "context" : "The expectation bound in Theorem 1, Korda and Prashanth [2015] again requires the stepsize sequence be scaled as in Remark 1.",
      "startOffset" : 36,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "The expectation bound in Theorem 1, Korda and Prashanth [2015] again requires the stepsize sequence be scaled as in Remark 1. Theorem 2 there obviates this, but it applies to average of iterates. In contrast, our expectation bound applies directly to the TD(0) iterates and does not need any scaling of the above kind. Moreover, our result applies to a broader family of stepsizes; see Remark 4. Our expectation bound when compared to that of Theorem 2, Korda and Prashanth [2015] is of the same order (even though theirs is for average of iterates).",
      "startOffset" : 36,
      "endOffset" : 481
    }, {
      "referenceID" : 8,
      "context" : "1 Outline of Approach We compare the TD(0) iterates {θn} with suitable solutions of its limiting ODE using the Variation of Parameters (VoP) method [Lakshmikantham and Deo, 1998].",
      "startOffset" : 148,
      "endOffset" : 178
    }, {
      "referenceID" : 5,
      "context" : "Using results from Chapter 6, [Hirsch et al., 2012], it follows that the solution θ(t, s, u0), t ≥ s, of (6) satisfies the relation θ(t, s, u0) = θ ∗ + e(u0 − θ∗) .",
      "startOffset" : 30,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "The key idea then is to use the VoP method [Lakshmikantham and Deo, 1998] and express θ̄(t) as a perturbation of θ(t) due to two factors: the discretization error and the martingale difference noise.",
      "startOffset" : 43,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "The expectation bound is due to an inductive argument and an application of a subtle trick from Kamal [2010]. Building on the approach there, our key steps are: identifying a “nice\" Liapunov function V of the TD(0) method’s limiting ODE; and then using conditional expectation suitably to get rid of the linear noise terms in the relation between V (θn) and V (θn+1).",
      "startOffset" : 96,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "Specifically, using the non-linear analysis presented in [Thoppe and Borkar, 2015], we believe it can be extended to a broader family of function approximators, e.",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 8,
      "context" : "From the above two relations and the VoP formula [Lakshmikantham and Deo, 1998], the desired result follows.",
      "startOffset" : 49,
      "endOffset" : 79
    } ],
    "year" : 2017,
    "abstractText" : "TD(0) is one of the most commonly used algorithms in reinforcement learning. Despite this, there is no existing finite sample analysis for TD(0) with function approximation, even for the linear case. Our work is the first to provide such a result. Works that managed to obtain concentration bounds for online Temporal Difference (TD) methods analyzed modified versions of them, carefully crafted for the analyses to hold. These modifications include projections and step-sizes dependent on unknown problem parameters. Our analysis obviates these artificial alterations by exploiting strong properties of TD(0) and tailor-made stochastic approximation tools.",
    "creator" : "LaTeX with hyperref package"
  }
}