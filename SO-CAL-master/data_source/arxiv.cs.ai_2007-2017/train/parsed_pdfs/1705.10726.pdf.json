{
  "name" : "1705.10726.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Strength Factors: An Uncertainty System for a Quantified Modal Logic",
    "authors" : [ "Naveen Sundar Govindarajulu", "Selmer Bringsjord" ],
    "emails" : [ "naveensundarg@gmail.com", "selmer.bringsjord@gmail.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We introduce a new system S for talking about uncertainty of iterated beliefs in a quantified modal logic with belief operators. The quantified modal logic we use is based on the deontic cognitive event calculus (DCEC ), which belongs to the\nfamily of cognitive calculi that have been used in modeling complex cognition. Here, we use a subset of DCEC that we term micro cognitive calculus (µC ). Specifically, we add a system of uncertainty derived from Chisholm’s epistemology [Chisholm, 1987].1 The system S is a work in progress and hence the presentation here will be abstract in nature.\nOne of our primary motivations is to design a system of uncertainty that is easy to use in end-user facing systems. There have been many studies that show that laypeople have difficulty understanding raw probability values (e.g. see [Kaye and Koehler, 1991]); and we believe that our approach borrowed from philosophy can pave the way for systems that can present uncertain statements in a more understandable format to lay users."
    }, {
      "heading" : "2 Prior Work",
      "text" : "Members in the family of cognitive calculi have been used to formalize and automate highly intensional reasoning processes.2 More recently, using DCEC we have presented an automation of the doctrine of double effect in [Govindarajulu and Bringsjord, 2017].3 The doctrine of double effect is an ethical principle that has been shown to be used by both untrained laypeople and experts when faced with moral dilemmas; and it plays a central role in many legal systems.\nOther tasks automated by cognitive calculi include the false-belief task [Arkoudas and Bringsjord, 2008] and akrasia (succumbing to temptation to violate moral principles) [Bringsjord et al., 2014].4 Each cognitive calculus is a sorted (i.e. typed) quantified modal logic (also known as sorted firstorder modal logic). Each calculus has a well-defined syntax and proof calculus. The proof calculus is based on natural\n1See the SEP entry on Chisholm for a quick overview of Chisholm’s epistemology: https://plato.stanford.edu/entries/ chisholm/#EpiIEpiTerPriFou.\n2By “intensional processes”, we roughly mean processes that take into account knowledge, beliefs, desires, intentions, etc. of agents. This is opposed to extensional systems such as pure firstorder systems that do not take into account states of minds of other agents. This is not be confused with “intentional” systems which would be modeled with intensional systems. See [Zalta, 1988] for a detailed treatment of intensionality.\n3This work will be presented at IJCAI 2017. 4Arkoudas and Bringsjord [2008] introduced the general family\nof cognitive event calculi to which DCEC belongs.\nar X\niv :1\n70 5.\n10 72\n6v 1\n[ cs\n.A I]\n3 0\nM ay\n2 01\n7\ndeduction [Gentzen, 1935], and includes all the introduction and elimination rules for first-order logic, as well as inference schemata for the modal operators and related structures.\nOn the uncertainty and probability front, there have been many logics of probability, see [Demey et al., 2016] for an overview. Since our system builds upon probabilities, our approach could use a variety of such systems. As we noted above, there has been very little work in uncertainty systems for first-order modal logics. Among first-order systems, the seminal work in [Halpern, 1990] presents a first-order logic with modified semantics to handle probabilistic statements. We can use such a system as the foundation for our work, and use it to define the base probability function Pr used below. (Note that we leave Pr unspecified for now.)"
    }, {
      "heading" : "3 The Formal System",
      "text" : "The formal system µC is a modal extension of the the event calculus. The event calculus is a multi-sorted first-order logic with a family of axiom sets. The exact axiom set is not important. The primary sorts in the system are shown below.\nSort Description\nAgent Human and non-human actors. Moment or Time Time points and intervals. E.g. simple, such as ti, or complex, such as birthday(son( jack)). Event Used for events in the domain. ActionType Action types are abstract actions. They are instantiated at particular times by actors. E.g.: “eating” vs. “jack eats.” Action A subtype of Event for events that occur as actions by agents. Fluent Used for representing states of the world in the event calcu-\nlus.\nFull DCEC has a suite of modal operators and inference schemata. Here we focus on just two: an operator for belief B and an operator for perception P. The syntax of and inference schemata of the system are shown below. S is the set of all sorts, f are the core function symbols, t shows the set of terms, and φ is the syntax for the formulae.\nSyntax\nS ::=\n{ Object ∣∣ Agent ∣∣ Self @ Agent ∣∣ ActionType ∣∣ Actionv Event ∣∣ Moment ∣∣ Formula ∣∣ Fluent ∣∣ Numeric\nf ::=  action : Agent×ActionType→ Action initially : Fluent→ Formula holds : Fluent×Moment→ Formula happens : Event×Moment→ Formula clipped : Moment×Fluent×Moment→ Formula initiates : Event×Fluent×Moment→ Formula terminates : Event×Fluent×Moment→ Formula prior : Moment×Moment→ Formula\nt ::= x : S ∣∣ c : S ∣∣ f (t1, . . . , tn)\nφ ::=\n{ t :Formula ∣∣ ¬φ ∣∣ φ∧ψ ∣∣ φ∨ψ ∣∣ P(a, t,φ)\n∣∣ B(a, t,φ) The above calculus lets us formalize statements of the form “John believes now that Mary perceived that it was raining yesterday.” One formalization could be:\n∃t < now : B ( john,now,P ( mary, t,holds(raining, t) ))\nThe figure below shows the inference schemata for DCEC . RP captures that perceptions get turned into beliefs. RB is an inference schema that lets us model idealized agents that have their beliefs closed under the µC proof theory. While normal humans are not deductively closed, this lets us model more closely how deliberate agents such as organizations and more strategic actors reason. Assume that there is a background set of axioms Γ we are working with.\nInference Schemata\nP(a, t1,φ1), Γ ` t1 < t2 B(a, t2,φ) [RP]\nB(a, t1,φ1), . . . ,B(a, tm,φm), {φ1, . . . ,φm} ` φ, Γ ` ti < t B(a, t,φ) [RB]"
    }, {
      "heading" : "4 The Uncertainty System S",
      "text" : "In the uncertainty system, we augment the belief modal operators with a discrete set of uncertainty factors termed as strength factors. The factors are not arbitrary and are based on how derivable a proposition is for a given agent.\nChisholm’s epistemology has a primitive undefined binary relation that he terms reasonableness with which he defines a scale of strengths for beliefs one might have in a proposition. Note that Chisholm’s system is agent free while ours is agentbased. Let φ at ψ denote that φ is more reasonable than ψ to an agent a at time t. For convenience, we define a new operator, the withholding operator W, defined below:\nW(a, t,φ)≡ ¬B(a, t,φ)∧¬B(a, t,¬φ)\nWe now reproduce Chisholm’s system below. Note the formula used in the definitions below are meta-formula and not strictly in µC .\nStrength Factor Definitions\nAcceptable An agent a at time t finds φ acceptable iff withholding φ is not more reasonable than believing in φ.\nB1(a, t,φ)⇔ W(a, t,φ) 6 a t B(a, t,φ); or( ¬B(a, t,φ)∧¬B(a, t,¬φ) ) 6 at B(a, t,φ)\nSome Presumption in Favor An agent a at time t has some presumption in favor of φ iff believing φ at t is more reasonable than believing ¬φ at time t:\nB2(a, t,φ)⇔ ( B(a, t,φ) ta B(a, t,¬φ) )\nBeyond Reasonable Doubt An agent a at time t has beyond reasonable doubt in φ iff believing φ at t is more reasonable than withholding φ at time t:\nB3(a, t,φ)⇔  B(a, t,φ) t a W(a, t,φ); or( B(a, t,φ) at ( ¬B(a, t,φ)∧¬B(a, t,¬φ)\n) Evident A formula φ is evident to an agent a at time t iff φ is\nbeyond reasonable doubt and if there is a ψ such that believing ψ is more reasonable for a at time t than believing φ, then a is certain about ψ at time t.\nB4(a, t,φ)⇔  B3(a, t,φ)∧ ∃ψ : [ B(a, t,ψ) at B(a, t,φ)\n⇒B5(a, t,ψ)\n]\nCertain An agent a at time t is certain about φ iff φ is beyond reasonable doubt and there is no ψ such that believing ψ is more reasonable for a at time t than believing φ.\nB5(a, t,φ)⇔\n{ B3(a, t,φ)∧\n¬∃ψ : B(a, t,ψ) at B(a, t,φ)\nThe above definitions are from Chisholm but more rigorously formalized in µC . The definitions almost give us S except for the fact that at is undefined. While Chisholm gives a careful and informal analysis of the relation, he does not provide a more precise definition. Such a definition is needed for automation. We provide a three clause defintion that is based on both probabilities and proof theory.\nThere are many probability logics that allow us to define probabilities over formulae. They are well studied and understood for propositional and first-order logics. Let L be the set of all formulae in µC . Let Lp be a pure first-order subset of L . Assume that we have the following partial probability function defined over Lp5:\nPr : Agent×Moment×Formula 7→ R\nThen we have the first clause of our definition for at . Clause I. Defining\nIf Pr(a, t,φ) and Pr(a, t,ψ) are defined then:( B ( a, t,φ ) at B ( a, t,φ )) ⇔ ( Pr(a, t,φ)> Pr(a, t,ψ) )\nWe might not always have meaningful probabilities for all propositions. For example, consider propositions of the form “I believe that Jack believes that φ.” It is hard to get precise numbers for such statements. In such situations, we might look at the ease of derivation of such statements given a knowledge base Γ. Given two competing statements φ and ψ, we can say one is more reasonable than the other if we can easily derive one more than the other from Γ. This assumes that we can derive φ and ψ from Γ. We assume we have a cost function ρ : Proof 7→R+ that lets us compute costs of proofs.\n5Something similar to the system in [Halpern, 1990] that accounts for probabilities as statistical information or degrees of belief can work.\nThere are many ways of specifying such functions. Possible candidates are length of the proof, time for computing the proof, depth vs breadth of the proof, unique symbols used in the proof etc. We leave this choice unspecified but any such function could work here. Let `a,t denote provability w.r.t. to agent a at time t.\nClause II. Defining\nIf one of Pr(a, t,φ) and Pr(a, t,ψ) is not defined, but if Γ `a,t φ and Γ `a,t ψ:(\nφ at ψ ) ⇔ ( ρ ( Γ `a,t φ ) < ρ ( Γ `a,t ψ )) Clauses I and II might not always hold. A more common case could be when we cannot derive the propositions of interest from our background set of axioms Γ. For example, if we are interested in the uncertainty values for statements that we know are false, then it should be the case that they be not derivable from our background set of axioms. In this situation, we look at Γ and see what minimal changes we can make to it to let us derive the proposition of interest. Trivially, if we cannot derive φ from Γ, we can add it to Γ to derive it, as Γ+φ ` φ. This is not desirable for two reasons.\nFirst, simply adding to Γ might result in a contradiction. In such cases we would be looking at removing a minimal set of statements Λ from Γ. Second, we might prefer to add a more simpler set of propositions Θ to Γ rather than φ itself to derive φ. Recapping, we go from (1) to (2) below:\nΓ 6` φ (1) Γ∪Θ−Λ ` φ (2)\nWhen we go from (1) to (2) we would like to modify the background axioms as minimally as possible. Assume that we have a similarity function π for sets of formulae. We then choose Θ and Λ as given below (Con[S] denotes that S is consistent):\n〈Θ,Λ〉= argmin 〈Θ,Λ〉 π(Γ,Γ∪Θ−Λ); such that { Γ∪Θ−Λ ` φ; and Con [ Γ∪Θ−Λ\n] Consider a statement such as “It rained last week” when it did not actually rain last week, and another statement such as “The moon is made of cheese.” Both statements denote things that did not happen, but intuitively it seems that former should be more easier to accept from what we know than the latter. There are many similarity measures which can help convey this. Analogical reasoning is one such possible measure of similarity. If the new formulae are structurally similar to existing formulae, then we might be more justified in accepting such formulae. For example, one such measure could be the analogical measure used by us in [Licato et al., 2013].\nNow we have the formal mechanism in place for defining the final clause in our definition for our reasonableness. Let δat (Γ,φ) be the distance between Γ and closest consistent set under π that lets us prove φ:\nδat (Γ,φ)≡ min〈Θ,Λ〉\n{ π ( Γ,Γ∪Θ−Λ )∣∣∣∣∣ ( Γ∪Θ−Λ ) `at ψ; and Con [ Γ∪Θ−Λ ] }\nClause III. Defining\nIf one of Pr(a, t,φ) and Pr(a, t,ψ) is not defined, and one of Γ `a,t φ and Γ `a,t ψ does not hold, then(\nφ at ψ ) ⇔ [ δat (Γ,φ)< δ a t (Γ,ψ) ]\nThe final piece of S is inference rules for belief propagation with uncertainty values. This is quite straightforward. Inferences propagate uncertainty values from the premises with the lowest strength factor; and inferences happen only with beliefs that are close in their uncertainty values, with maximum difference being parametrized by u, with default u = 2.\nInference Schemata for S\nP(a, t1,φ1), Γ ` t1 < t2 B5(a, t2,φ) [RsP]\nBs1(a, t1,φ1), . . . ,Bsm(a, tm,φm),{φ1, . . . ,φm} ` φ,Γ ` ti < t Bmin(s1,...,sm)(a, t,φ) [RsB]\nwith max({s1, . . . ,sm})−min({s1, . . . ,sm})≤ u"
    }, {
      "heading" : "5 Usage",
      "text" : "In this section, we illustrate S by applying it solve problems of foundational interest such as the lottery paradox [Kyburg Jr, 1961, p. 197] and a toy version of a more real life example, a murder mystery example (following in the traditions of logic pedagogy). Finally, we very briefly sketch abstract scenarios in which S can be used to generate uncertainty values for counterfactual statements and to generate explanations for actions."
    }, {
      "heading" : "5.1 Paradoxes: Lottery Paradox",
      "text" : "In the lottery paradox, we have a situation in which an agent a comes to believe φ and ¬φ from a seemingly consistent set of premises ΓL describing a lottery. Our solution to the paradox is that the agent simply has different strengths of beliefs in the proposition and its negation. We first go over the paradox formalized in µC and then present the solution.\nLet ΓL be a meticulous and perfectly accurate description of a 1,000,000,000,000-ticket lottery, of which rational agent a is fully apprised. Assume that from ΓL it can be proved that either ticket 1 will win or ticket 2 will win or . . . or ticket 1,000,000,000,000 will win. Lets write this (exclusive) disjunction as follows (here ⊕ is an exclusive disjunction):\nΓL ` win(t1)⊕win(t2)⊕ . . .⊕win(t1,000,000,000,000)\nThe paradox has two strands of reasoning. The first strand yields B(a,now,φ) and the second strand yields B(a,now,¬φ) with φ≡ ∃t : win(t). Strand 1: Since a believes all propositions in ΓL, a can then deduce from this the belief that there is at least one ticket that will win, a proposition represented as:\nS1 B ( a,now,∃t : win ( t ))\nStrand 2: From ΓL it can be proved that the probability of a particular ticket ti winning is 10−12.[\nPr ( a,now,win ( t1 )) = 10−12 ] ∧ [ Pr ( a,now,win ( t2 )) = 10−12 ]\n∧ . . .∧ [ Pr ( a,now,win ( t1T )) = 10−12 ]\nFor the next step, note that the probability of ticket t1 winning is lower than, say, the probability that if you walk outside a minute from now, you will be flattened on the spot by a door from a 747 that falls from a jet of that type cruising at 35,000 feet. Since you, the reader, have the rational belief that death won’t ensue if you go outside (and have this belief precisely because you believe that the odds of your sudden demise in this manner are vanishingly small), the inference to the rational belief on the part of a that t1 won’t win sails through — and this of course works for each ticket. Hence we have as a valid belief (though not derivable in µC from ΓL):\nB ( a,now,¬win ( t1) ) ∧B ( a,now,¬win ( t2) ) ∧ . . .\n∧B ( a,now,¬win ( t1T ) ) From RB and above, we get:\nB ( a,now,¬win ( t1)∧¬win ( t2)∧ . . .∧¬win ( t1T ) ) Applying RB to the above and ΓL, we get:\nS2 B ( a,now,¬∃t : win(t) )\nThe two strands are complete, and we have derived contradictory beliefs labeled S1 and S2. Our solution consists of two new uncertainty infused strands that result in beliefs of sufficiently varying strengths that block inferences that could combine them. Strand 3: Assume that a is certain of all propositions in ΓL, then using RsB, we have:\nS3 B5 ( a,now,∃t : win(t) ) Strand 4: Since Pr(a,now,win(ti)) < Pr(a,now,¬win(ti)), using Clause I and the strength factor definitions, we have now that for all ti\nB2 ( a,now,¬win(ti) ) Using the reasoning similar to that in Strand 2, we get:\nS4 B2 ( a,now,¬∃t : win(t) )\nStrands 3 and 4 resolve the paradox. Note that RsB cannot be applied to S3 and S4 and churn out arbitrary propositions, as the default value of the u parameter in RsB requires beliefs to be no more than 2 levels apart."
    }, {
      "heading" : "5.2 Application: Solving a Murder",
      "text" : "We look at a toy example in which an agent s has to solve a murder that happened at time t3. s believes that either Alice or Bob is the murderer. The agent knows that there is a gun gun involved in the murder and that the owner of the gun at t3 committed the murder. s also knows that Alice is the owner of the gun initially at time t0.\nPresumption in Favor of Alice Being the Murderer\nFrom just these facts, the agent has some presumption for believing that Alice is the murderer.\nProof Sketch: All the above statements can be taken as certain beliefs B5 of s. For convenience, we consider the formulae directly without the belief operators.\nIn order to prove the above, we need to prove that it is easier for the agent to derive that Alice is the murderer than to derive that Alice is not the murderer. First, to prove the former, the agent just has to assume that Alice’s ownership of the gun did not change from t0 to t3. Second, in order for the agent to believe that Alice did not commit the murder but Bob committed it, the agent must be willing to admit that something happened to change Alice’s ownership of the gun from time t0 to t3 that results in Bob owning the gun. One possibility is that Alice simply sold the gun to Bob. Both the scenarios are shown as proofs in the Slate theorem proving workspace [Bringsjord et al., 2008] in the Appendix. Figure 1 shows a proof modulo belief operators of B(s,now,Murderer(Alice)) from Γ∪Θ1 and Figure 2 shows a proof of B(s,now,¬Murderer(Alice)) from Γ∪Θ2.\nIf we assume that Θ1 and Θ2 exhaust the space of allowed additions, then it easy to see how syntactic measures of complexity will yield that δat ( Γ,Γ∪Θ1 ) < δat ( Γ,Γ∪Θ2 ) as Θ2 is more complex than Θ1. This lets us derive that s has some presumption in favor of Murderer(Alice).\nWhat happens if the agent knows or has a belief with certainty that Alice’s ownership of the gun did not change from t0 to t3?\nBeyond Reasonable Doubt that Alice is the Murderer\nIf the agent is certain that Alice’s ownership of the gun did not change from t0 till t3, the agent has beyond reasonable doubt that she is the murderer.\nProof Sketch: In this case we directly have that: Γ ` B(s,now,Murderer(Alice)) Γ 6 ` ¬B(s,now,Murderer(Alice)) Γ 6 ` ¬B(s,now,¬Murderer(Alice))\nIn order to flip the last two statements above, we need to modify Γ, but we can derive that Alice is the murderer without any modifications, and since δat (Γ,Γ) = 0, it easier to believe Alice is the murderer than to withhold that Alice is the murderer."
    }, {
      "heading" : "5.3 Counterfactuals",
      "text" : "At time t, assume that an agent a believes in a set of propositions Γ and is interested in propositions holds( f , t ′) and holds(g, t ′) with t ′ < t and:\nΓ ` ¬holds( f , t ′)∧¬holds(g, t ′)\nWe may need non-trivial uncertainty values, but in this case, Pr will assign a trivial value of 0 to both the propositions. We can then look at closest consistent sets to Γ under δ:\nΓ1 ` holds( f , t ′) Γ2 ` holds(g, t ′)\nClause III from the definition for reasonableness gives us: B ( a, t,holds( f , t ′) ) at B ( a, t,holds(g, t ′) ) ⇔\nδat (Γ,Γ1)< δ a t (Γ,Γ2)"
    }, {
      "heading" : "5.4 Explanations",
      "text" : "The definitions of the strength factors and reasonableness above can be used to generate high-level schemas for explanations. These schemas can be used instead of simply presenting raw probability values to end-users. While we have not fleshed out such explanation schemas, we illustrate one possible schema. Say an agent performs an action α on the basis of φ. In this case, the agent could generate an explanation that at the highest level simply says that it is more reasonable for the agent to believe φ than for the agent to believe in ¬φ. The agent could then further explain why it was reasonable for it by using one of the three clauses in the reasonableness definition."
    }, {
      "heading" : "6 Inference Algorithm Sketch",
      "text" : "Describing the inference algorithm in detail is beyond the scope of this paper, but we provide a high-level sketch here.6 Our proof calculus is simply an extension of standard firstorder proof calculus under different modal contexts. For example, if a believes that b believes in a set of propositions Γ and Γ `FOL ψ, then a believes that b believes ψ. We convert B(a, ta,B(b, tb,Q)) into the pure first-order formula Q(context(a, ta,b, tb)) and use a first-order prover. The conversion process is a bit more nuanced as we have to handle negations, properly handle substitutions of equalities, uncertainties and transform compound formulae within iterated beliefs."
    }, {
      "heading" : "7 Conclusion and Future Work",
      "text" : "We have presented initial steps in building a system of uncertainty that is both probability and proof theory based that could lend itself to (1) solving foundational problems; (2) being useful in applications; (3) generating uncertainty values for counterfactuals; and (4) building understandable explanations.\nMany challenges remain, some relatively easy and some quite hard. Among the easy challenges are defining and experimenting with different candidates for Pr, ρ, π and δ. On the more difficult side, we have to come up with tractable computational mechanisms for computing the min〈Θ,Λ〉 in the definition for δ. Also on the difficult side, is the challenge of coming up efficient reasoning schemes. While we have an exact inference algorithm, we believe that an approximate algorithm that selectively discards beliefs in a large knowledge base during reasoning will be more useful.\n6More details can be found here: https://goo.gl/2Vz2nJ"
    }, {
      "heading" : "8 Acknowledgements",
      "text" : "We are grateful to the Office of Naval Research for their funding of research that enabled the research presented in this paper."
    }, {
      "heading" : "A Appendix: Slate Proofs",
      "text" : "The figures below are vector graphics and can be zoomed to more easily read the contents.\nFigure 1: Alice is the murder: B ( s, t,Murderer(Alice) )\nBeliefs of s\n⇥1 FOL ⊢ ✓\nΓ 6. ∀x (Murderer(x) ↔ Holds(Owns(x,gun),t3)) {Γ 6} Assume ✓\nΓ 4. ∀x,t ((Holds(Owns(x,gun),t) ∧ holds(Owns(y,gun),t)) → (x = y)) {Γ 4} Assume ✓\nΓ 3. Murderer(Bob) → ¬Murderer(Alice) {Γ 3} Assume ✓\nΓ 1. Murderer(Alice) ∨ Murderer(Bob) {Γ 1} Assume ✓\nΓ 5. Alice ≠ Bob {Γ 5} Assume ✓\nΓ 2. Murderer(Alice) → ¬Murderer(Bob) {Γ 2} Assume ✓\nΓ 8. ∀f,t ((Initially(f) ∧ ¬Clipped(t0,f,t)) → Holds(f,t)) {Γ 8} Assume ✓\nΓ 7. Initially(Owns(Alice,gun)) {Γ 7} Assume ✓\nΘ 1. ¬Clipped(t0,Owns(Alice,gun),t3) {Θ 1} Assume ✓\n10. Murderer(Alice) {Γ 1,Γ 2,Γ 3,Γ 4,Γ 5,Γ 6,Γ 7,Γ 8,Θ 1}\n( s, t,¬Murderer(Alice) )"
    } ],
    "references" : [ {
      "title" : "Toward Formalizing Common-Sense Psychology: An Analysis of the False-Belief Task",
      "author" : [ "Arkoudas", "Bringsjord", "2008] Konstantine Arkoudas", "Selmer Bringsjord" ],
      "venue" : "Proceedings of the Tenth Pacific Rim International Conference on Artificial Intelligence (PRICAI",
      "citeRegEx" : "Arkoudas et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Arkoudas et al\\.",
      "year" : 2008
    }, {
      "title" : "Slate: An Argument-Centered Intelligent Assistant to Human Reasoners",
      "author" : [ "Bringsjord et al", "2008] Selmer Bringsjord", "Joshua Taylor", "Andrew Shilliday", "Micah Clark", "Konstantine Arkoudas" ],
      "venue" : "Proceedings of the 8th International Workshop",
      "citeRegEx" : "al. et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2008
    }, {
      "title" : "In Proceedings of ETHICS • 2014 (2014 IEEE Symposium on Ethics in Engineering",
      "author" : [ "Selmer Bringsjord", "Naveen Sundar Govindarajulu", "Daniel Thero", "Mei Si. Akratic Robots", "the Computational Logic Thereof" ],
      "venue" : "Science, and Technology), pages 22–29, Chicago, IL,",
      "citeRegEx" : "Bringsjord et al.. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Theory of Knowledge 3rd ed",
      "author" : [ "Roderick Chisholm" ],
      "venue" : "Prentice-Hall, Englewood Cliffs, NJ,",
      "citeRegEx" : "Chisholm. 1987",
      "shortCiteRegEx" : null,
      "year" : 1987
    }, {
      "title" : "editor",
      "author" : [ "Lorenz Demey", "Barteld Kooi", "Joshua Sack. Logic", "Probability. In Edward N. Zalta" ],
      "venue" : "The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, winter 2016 edition,",
      "citeRegEx" : "Demey et al.. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "editor",
      "author" : [ "Gerhard Gentzen. Investigations into Logical Deduction. In M.E. Szabo" ],
      "venue" : "The Collected Papers of Gerhard Gentzen, pages 68–131. North-Holland, Amsterdam, The Netherlands,",
      "citeRegEx" : "Gentzen. 1935",
      "shortCiteRegEx" : null,
      "year" : 1935
    }, {
      "title" : "On Automating the Doctrine of Double Effect",
      "author" : [ "Naveen Sundar Govindarajulu", "Selmer Bringsjord" ],
      "venue" : "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI 2017),",
      "citeRegEx" : "Govindarajulu and Bringsjord. 2017",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Artificial intelligence",
      "author" : [ "Joseph Y Halpern. An Analysis of First-order Logics of Probability" ],
      "venue" : "46(3):311–350,",
      "citeRegEx" : "Halpern. 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Can Jurors Understand Probabilistic Evidence? Journal of the Royal Statistical Society",
      "author" : [ "D.H. Kaye", "Jonathan J. Koehler" ],
      "venue" : "Series A (Statistics in Society), 154(1):75–81,",
      "citeRegEx" : "Kaye and Koehler. 1991",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "Middletown",
      "author" : [ "Henry E Kyburg Jr. Probability", "the Logic of Rational Belief. Wesleyan University Press" ],
      "venue" : "CT,",
      "citeRegEx" : "Kyburg Jr. 1961",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Analogico-Deductive Generation of Gödel’s First Incompleteness Theorem from the Liar Paradox",
      "author" : [ "Licato et al", "2013] John Licato", "Naveen Sundar Govindarajulu", "Selmer Bringsjord", "Michael Pomeranz", "Logan Gittelson" ],
      "venue" : "Proceedings of the 23rd International Joint Conference on Arti-",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "Cambridge",
      "author" : [ "Edward N Zalta. Intensional Logic", "the Metaphysics of Intentionality. MIT Press" ],
      "venue" : "MA,",
      "citeRegEx" : "Zalta. 1988",
      "shortCiteRegEx" : null,
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Specifically, we add a system of uncertainty derived from Chisholm’s epistemology [Chisholm, 1987].",
      "startOffset" : 82,
      "endOffset" : 98
    }, {
      "referenceID" : 8,
      "context" : "see [Kaye and Koehler, 1991]); and we believe that our approach borrowed from philosophy can pave the way for systems that can present uncertain statements in a more understandable format to lay users.",
      "startOffset" : 4,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "2 More recently, using DCEC we have presented an automation of the doctrine of double effect in [Govindarajulu and Bringsjord, 2017].",
      "startOffset" : 96,
      "endOffset" : 132
    }, {
      "referenceID" : 2,
      "context" : "Other tasks automated by cognitive calculi include the false-belief task [Arkoudas and Bringsjord, 2008] and akrasia (succumbing to temptation to violate moral principles) [Bringsjord et al., 2014].",
      "startOffset" : 172,
      "endOffset" : 197
    }, {
      "referenceID" : 11,
      "context" : "See [Zalta, 1988] for a detailed treatment of intensionality.",
      "startOffset" : 4,
      "endOffset" : 17
    }, {
      "referenceID" : 5,
      "context" : "deduction [Gentzen, 1935], and includes all the introduction and elimination rules for first-order logic, as well as inference schemata for the modal operators and related structures.",
      "startOffset" : 10,
      "endOffset" : 25
    }, {
      "referenceID" : 4,
      "context" : "On the uncertainty and probability front, there have been many logics of probability, see [Demey et al., 2016] for an overview.",
      "startOffset" : 90,
      "endOffset" : 110
    }, {
      "referenceID" : 7,
      "context" : "Among first-order systems, the seminal work in [Halpern, 1990] presents a first-order logic with modified semantics to handle probabilistic statements.",
      "startOffset" : 47,
      "endOffset" : 62
    }, {
      "referenceID" : 7,
      "context" : "5Something similar to the system in [Halpern, 1990] that accounts for probabilities as statistical information or degrees of belief can work.",
      "startOffset" : 36,
      "endOffset" : 51
    } ],
    "year" : 2017,
    "abstractText" : "We present a new system S for handling uncertainty in a quantified modal logic (first-order modal logic). The system is based on both probability theory and proof theory. The system is derived from Chisholm’s epistemology. We concretize Chisholm’s system by grounding his undefined and primitive (i.e. foundational) concept of reasonableness in probability and proof theory. S can be useful in systems that have to interact with humans and provide justifications for their uncertainty. As a demonstration of the system, we apply the system to provide a solution to the lottery paradox. Another advantage of the system is that it can be used to provide uncertainty values for counterfactual statements. Counterfactuals are statements that an agent knows for sure are false. Among other cases, counterfactuals are useful when systems have to explain their actions to users (If I had not done α, then φ would have happened). Uncertainties for counterfactuals fall out naturally from our system. Efficient reasoning in just simple first-order logic is a hard problem. Resolution-based first-order reasoning systems have made significant progress over the last several decades in building systems that have solved non-trivial tasks (even unsolved conjectures in mathematics). We present a sketch of a novel algorithm for reasoning that extends firstorder resolution. Finally, while there have been many systems of uncertainty for propositional logics, first-order logics and propositional modal logics, there has been very little work in building systems of uncertainty for first-order modal logics. The work described below is in progress; and once finished will address this lack.",
    "creator" : "LaTeX with hyperref package"
  }
}