{
  "name" : "1705.07615.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "AIXIjs: A Software Demo for General Reinforcement Learning",
    "authors" : [ "John Stewart Aslanides" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "AIXIjs: A Software Demo for General Reinforcement Learning\nJohn Stewart Aslanides"
    }, {
      "heading" : "A thesis submitted in partial fulfillment of the degree of",
      "text" : "Master of Computing (Advanced)\nat the Australian National University\nOctober 2016\nar X\niv :1\n70 5.\n07 61\n5v 1\n[ cs\n.A I]\n2 2\nM ay\n2 01\n7\nii\nDeclaration\nThis thesis is an account of research undertaken between March 2016 and October 2016 at The Research School of Computer Science, The Australian National University, Canberra, Australia.\nExcept where acknowledged in the customary manner, the material presented in this thesis is, to the best of my knowledge, original and has not been submitted in whole or part for a degree in any university.\nJohn Stewart Aslanides 27 October, 2016\nSupervisors:\n• Dr. Jan Leike (Future of Humanity Institute, University of Oxford)\n• Prof. Marcus Hutter (Australian National University)\nConvenor:\n• Prof. John Slaney (Australian National University)\niii\niv"
    }, {
      "heading" : "Acknowledgements",
      "text" : "And so, my formal education comes to an end, at least for the time being. Naturally, one cannot take credit for one’s successes any more than one can take credit for one’s genes and environment. I owe everything to having been fortunate enough to grow up in a wealthy and peaceful country (Australia), with loving and well-educated parents (Jenny and Timoshenko), and to having been exposed to the quality of tuition and mentorship that I’ve received over the years at the Australian National University. In my time at the ANU, I’ve met many smart people who have, to varying degrees, spurred my intellectual development and shaped how I think. They are (in order of appearance): Craig Savage, Paul Francis, John Close, Joe Hope, Ra Inta, Bob Williamson, Justin Domke, Christfried Webers, Jan Leike, and Marcus Hutter. To these mentors and teachers, past and present: thank you.\n• To Jan, my supervisor: thank you for agreeing to supervise me from across the world, and for always being easy-going and genial, despite having to wake up so early for\nall of those Skype meetings. I hope that it’s obvious that I’m extremely glad to have gotten to know you over the course of this year. I really hope that we see each other again soon.\n• To Marcus: although we didn’t collaborate directly, it has been an honour to follow your work, and to pick your brain at group meetings. I love your sense of humour.\nAlso, good job discovering AIXI; overall, I think it’s a pretty neat idea.\n• To Jarryd: getting to know you has been one of the highlights of this year for me. A more true, honest, intelligent, and kind friend and lab partner I cannot conceive\nof. I’m going to miss you when you’re the CEO of DeepMind. :)\n• Thanks to my other friends and colleagues in the Intelligent Agents research group: Suraj Narayanan, Tom Everitt, Boris Repasky, Manlio Valenti, Sean Lamont, and\nSultan Javed. Before I met you guys, I didn’t know the meaning of the phrase ‘hard work’. Ours is the lab that never sleeps! In particular, I’d like to thank Tom for going to the trouble of introducing me via email to many of his connections in the Bay area. Meeting these people added immense value to my trip, and I now have valuable connections with members of the AI community in the US because of this.\n• Thanks also to the Future of Life Institute for sponsoring my travel to Berkeley for a week-long workshop run by the Center for Applied Rationality. It was immensely\ngratifying to be selected to such an elite gathering of young AI students and researchers, and hugely fun spending a week hanging out with smart people thinking about rationality. I will never forget the experience.\nFinally, there are three people to whom I am especially indebted:\n• Lulu, my partner of five years: I owe you so much gratitude for your unconditional love and support. Being my partner, you often have to experience the worst of me.\nv\nvi\nThank you for putting up with it, for being there when I needed you most, and for showing me the right path. I love you so much.\n• And of course, my parents, Jenny and Timoshenko: thank you for your never-ending love and support, and for being so forbearing and understanding. Seeing you every\nsecond Sunday has been a balm. I love you, and I miss you.\nI used to always groan when I was told this, but it’s finally starting to ring true: the older you get, the wiser your parents become.\nAbstract\nReinforcement learning (RL; Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1995) is a general and powerful framework with which to study and implement artificial intelligence (AI; Russell and Norvig, 2010). Recent advances in deep learning (Schmidhuber, 2015) have enabled RL algorithms to achieve impressive performance in restricted domains such as playing Atari video games (Mnih et al., 2015) and, recently, the board game Go (Silver et al., 2016). However, we are still far from constructing a generally intelligent agent. Many of the obstacles and open questions are conceptual: What does it mean to be intelligent? How does one explore and learn optimally in general, unknown environments? What, in fact, does it mean to be optimal in the general sense?\nThe universal Bayesian agent AIXI (Hutter, 2000, 2003, 2005) is a model of a maximally intelligent agent, and plays a central role in the sub-field of general reinforcement learning (GRL). Recently, AIXI has been shown to be flawed in important ways; it doesn’t explore enough to be asymptotically optimal (Orseau, 2010), and it can perform poorly with certain priors (Leike and Hutter, 2015). Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al., 2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike, 2016a), Thompson sampling (Leike et al., 2016), and optimism (Sunehag and Hutter, 2015).\nWe present AIXIjs, a JavaScript implementation of these GRL agents. This implementation is accompanied by a framework for running experiments against various environments, similar to OpenAI Gym (Brockman et al., 2016), and a suite of interactive demos that explore different properties of the agents, similar to REINFORCEjs (Karpathy, 2015). We use AIXIjs to present numerous experiments illustrating fundamental properties of, and differences between, these agents. As far we are aware, these are the first experiments comparing the behavior of GRL agents in non-trivial settings.\nOur aim is for this software and accompanying documentation to serve several pur-\nposes:\n1. to help introduce newcomers to the field of general reinforcement learning,\n2. to provide researchers with the means to demonstrate new theoretical results relating\nto universal AI at conferences and workshops,\n3. to serve as a platform with which to run empirical studies on AIXI variants in small\nenvironments, and\n4. to serve as an open-source reference implementation of these agents.\nKeywords: Reinforcement learning, AIXI, Knowledge-seeking agents, Thompson sampling.\nvii\nviii\nContents\nTitle Page i\nDeclaration iii\nAcknowledgements v\nAbstract vii\nContents x\nList of Figures xiv\nList of Algorithms xvi"
    }, {
      "heading" : "1 Introduction 1",
      "text" : ""
    }, {
      "heading" : "2 Background 7",
      "text" : "2.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.1.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.2 Probability theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.1.3 Information theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.2.1 Agent-environment interaction . . . . . . . . . . . . . . . . . . . . . 12 2.2.2 Discounting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.2.3 Value functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2.4 Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.3 General Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.3.1 Bayesian agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3.2 Knowledge-seeking agents . . . . . . . . . . . . . . . . . . . . . . . . 21 2.3.3 BayesExp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.3.4 MDL Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.3.5 Thompson Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n2.4 Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n2.4.1 Value iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 2.4.2 MCTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n2.5 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
    }, {
      "heading" : "3 Implementation 31",
      "text" : "3.1 JavaScript web demo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.2 Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.2.1 Approximations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n3.3 Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n3.3.1 Gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\nix\nx Contents\n3.3.2 Chain environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.4 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.4.1 Mixture model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.4.2 Factorized Dirichlet model . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Planners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.6 Visualization and user interface . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.7 Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48"
    }, {
      "heading" : "4 Experiments 55",
      "text" : "4.1 Knowledge-seeking agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n4.1.1 Hooked on noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.1.2 Stochastic gridworld . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.2 AIµ and AIξ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.2.1 Model classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 4.2.2 Dependence on priors . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.3 Thompson Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n4.3.1 Random exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.4 MDL Agent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.4.1 Stochastic environments . . . . . . . . . . . . . . . . . . . . . . . . . 66 4.4.2 Deterministic environments . . . . . . . . . . . . . . . . . . . . . . . 66\n4.5 Wireheading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.6 Planning with MCTS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68"
    }, {
      "heading" : "5 Conclusion 73",
      "text" : "Bibliography 75\nList of Figures\n1.1 Open-source examples of real-time GPU-accelerated particle simulations\nrun natively in Google Chrome, using JavaScript and WebGL. Left: ParticulateJS. Right: Polygon Shredder. . . . . . . . . . . . . . . . . . . . . . 4\n1.2 Open-source examples of visualizations made with d3js. Left: Chord\nplot for data visualization (Bostock, 2016). Right: Visualization of the WaterWorld reinforcement learning environment (Karpathy, 2015). . . . 4\n2.1 Cybernetic model of agent-environment interaction. . . . . . . . . . . . . . 12\n2.2 A generic finite-state Markov Decision Process with two states and two\nactions: S = {?, ◦}, A = {→, 99K}. The transition matrix P (s′|s, a) is a 2× 2× 2 stochastic matrix, and the reward matrix R (s, a) is 2× 2. . . . . 14\n2.3 A two-armed Gaussian Bandit. A = {→, 99K}, |S| = 1, and O = ∅. Rewards are sampled from the distribution of the respective arm. . . . . . . . 15\n2.4 Square-KSA utility function plotted against that of Shannon-KSA. . . . . . 24\n3.1 BayesAgent UML. discount is the agent’s discount function, γtk.\nhorizon is the agent’s MCTS planning horizon, m. ucb is the MCTS UCB exploration parameter C. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.2 Agent class inheritance tree. Note that the BayesAgent is simply AIξ. . . 34\n3.3 Environment UML. state is the environment’s current state, it is simply\nof type Object, since we are agnostic as to how the environment’s state is represented. If JavaScript supported privated attributes, this would be private to the environment, to enforce the fact that the state is hidden in general. In contrast, minReward (α), maxReward (β), and numActions (|A|) are public attributes: it is necessary that the agent know these properties so that the agent-environment interaction can take place. . . . . . . . . . . 37\n3.4 Visualization of a 10×10 Gridworld with one Dispenser. The agent starts in the top left corner. Wall tiles are in dark grey. Empty tiles are in\nwhite. The Dispenser tile is represented by an orange disc on a white background. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.5 Chain environment. There are two actions: A = {→, 99K}, the environment is fully observable: O = S, and R = {r0, ri, rb} with rb ri > r0. For N < rbri , the optimal policy is to continually take action 99K, and periodically receive a large reward rb. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3.6 BayesMixture UML diagram. Internally, the BayesMixture contains a\nmodelClassM, which is an array of environments, and weights w, which are a normalized array of floating-point numbers. . . . . . . . . . . . . . . . 40\nxi\nxii LIST OF FIGURES\n3.7 Gridworld visualization with the agent’s posterior w over Mloc superimposed. Green tiles represent probability mass of the posterior wν , with\nhigher values correspond to darker green color. The true dispenser’s location is represented by the orange disc. As the Bayesian agent walks around the gridworld, it will move probability mass in its posterior from tiles that it has visited to ones that it hasn’t. . . . . . . . . . . . . . . . . . . . . . . . 42\n3.8 Visualization of a Gridworld, overlayed with the factorized Dirichlet model.\nWhite tiles are yet unexplored by the agent. Pale blue tiles are known to be walls. Different shades of purple/green represent different probabilities of being Empty or a Dispenser. . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.9 Demo user interface. In the top left, there is a visualization of the agent\nand environment, including a visualization of the agent’s beliefs about the environment. Below the visualization are playback controls, so that the user can re-watch interesting events in the simulation. On the right are several plots: average reward per cycle, cumulative information gain, and exploration progress. In the bottom left are agent and environment parameters that can be tweaked by the user. . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.10 Demo picker interface. Each thumbnail corresponds to a separate demo,\nand is accompanied by a title and short description. . . . . . . . . . . . . . 50\n4.1 10× 10 Gridworld environment used for the experiments. There is a single Dispenser, with dispense probability θ = 0.75. See the caption to 3.7 for\na description of each of the visual elements in the graphic. Unless stated otherwise in this chapter, µ refers to this Gridworld. . . . . . . . . . . . . . 56\n4.2 Hooked on noise: The entropy seeking agents (Shannon in red, and\nSquare in blue, obscured behind Shannon) get hooked on noise and do not explore. In contrast, the Kullback-Leibler agent explores normally and achieves a respectable exploration score. . . . . . . . . . . . . . . . . . . . . 57\n4.3 Exploration progress of the Kullback-Leibler, Shannon, and Square KSA\nusing the mixture model Mloc. . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.4 Exploration progress of the Kullback-Leibler, Shannon, and Square KSA\nusing the factorized model MDirichlet. Note the remarkable difference in performance between the Kullback-Leibler and entropy-seeking agents. . . 61\n4.5 KL-KSA-Dirichlet is highly motivated to explore every reachable tile in the\nGridworld. Left (t = 14): The agent begins to explore the Gridworld by venturing deep into the maze. Center (t = 72): The agent visits the dispenser tile for the first time, but is still yet to explore several tiles. Right (t = 200): The agent is still motivated to explore, and has long ago visited every reachable tile in the Gridworld. Key: Unknown tiles are white, and walls are pale blue. Tiles that are colored grey are as yet unvisited, but known to not be walls; that is, the agent has been adjacent to them and seen the ‘0’ percept. Purple tiles have been visited. The shade of purple represents the agent’s posterior belief in there being a dispenser on that tile; the deeper the purple, the lower the probability. Notice the subtle non-uniformity in the agent’s posterior in the right-hand image: even at t = 200, there is still some knowledge about the environment to be gained. 62\nLIST OF FIGURES xiii\n4.6 AIξ vs Square vs Shannon KSA, using the average reward metric on a\nstochastic Gridworld with the Mloc model class. Notice that AIξ significantly underperforms compared to the Square and Shannon KSAs. At the moment, we do not have a good hypothesis for why this is the case. . . . . 63\n4.7 AIµ vs AIξ vs the optimal policy. . . . . . . . . . . . . . . . . . . . . . . . . 64 4.8 MC-AIXI vs MC-AIXI-Dirichlet: average reward. MC-AIXI-Dirichlet per-\nforms worse, since its model MDirichlet has less prior knowledge than Mloc, and incentivizes AIXI to continue to explore even after it has found the (only) dispenser. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n4.9 MC-AIXI vs MC-AIXI-Dirichlet: exploration. The MDirichlet model assigns high a priori probability to any given tile being a dispenser. Because\neach tile is modelled independently, discovering a dispenser does not influence the agent’s beliefs about other tiles; hence, it is motivated to keep\nexploring, unlike MC-AIXI using the Mloc model. . . . . . . . . . . . . . . 66 4.10 Thompson sampling vs MC-AIXI on the stochastic Gridworld from Figure\n4.1. Notice that Thompson sampling takes many more cycles than AIξ to ‘get off the ground’; within 50 runs of Thompson sampling with identical initial conditions (not including the random seed), not a single one finds the dispenser before t = 50. . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.11 Thompson sampling vs Q-learning with random exploration. Even though\nThompson sampling performs badly compared to the Bayes-optimal policy due to its tendency to overcommit to irrelevant or suboptimal policies, it still dominates -greedy exploration, which is still commonly used in modelfree reinforcement learning (Bellemare et al., 2016). . . . . . . . . . . . . . . 68\n4.12 The MDL agent fails in a stochastic environment class. . . . . . . . . . . . 69 4.16 Average reward for AIµ for varying MCTS samples budget κ on the stan-\ndard Gridworld of Figure 4.1. For very low values of κ, the agent is unable to find the dispenser at all. . . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n4.13 MDL agent vs AIξ on a deterministic Gridworld, in which one of the ‘sim-\nplest’ environment models in M happens to be true. Since in this case AIξ uses a uniform prior over M, it over-estimates the likelihood of more complex environments, in which the dispenser is tucked away in some deep crevice of the maze. Of course, AIXI (Definition 13) combines the benefits of both by being Bayes-optimal with respect to the Solomonoff prior wν = 2 −K(ν). It is in this way that AIXI incorporates both the famous principles of Epicurus and Ockham (Hutter, 2005). . . . . . . . . . . . . . . 70\n4.14 Left: AIξ with a uniform prior and finite horizon is not far-sighted enough\nto explore the beginning of the maze systematically. After exploring most of the beginning of the maze, it greedily moves deeper into the maze, where ξ assigns significant value. Right: In contrast, the MDL agent systematically visits each tile in lexicographical (row-major) order; we use ‘closeness to starting position’ as a surrogate for ‘simplicity’. . . . . . . . . . . . . . . . . 71\n4.17 AIµ’s performance on the chain environment, varying the UCT parameter.\nNote the ‘zig-zag’ behavior of the average reward of the optimal policy. These discontinuities are simply caused by the fact that, when on the optimal policy π99K, the agent receives a large reward every N cycles and 0 reward otherwise. Asymptotically, these jumps will smooth out, and the average reward r̄t will converge to the dashed curve, r̄ ∗ t . . . . . . . . . . . . 71\nxiv LIST OF FIGURES\n4.15 Left: AIξ initially explores normally, looking for the dispenser tile. Once\nit reaches the point above, the blue ‘self-modification’ tile is now within its planning horizon (m = 6), and so it stops looking for the dispenser and makes a bee-line for it. Right: After self-modifying, the agent’s percepts are all maximally rewarding; we visualize this by representing the gridworld starkly in yellow and black. The agent now loses interest in doing anything useful, as every action is bliss. . . . . . . . . . . . . . . . . . . . . . . . . . . 72\nList of Algorithms\n2.1 BayesExp (Lattimore, 2013) . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.2 MDL Agent (Lattimore and Hutter, 2011) . . . . . . . . . . . . . . . . . . . 25 2.3 Thompson Sampling (Leike et al., 2016) . . . . . . . . . . . . . . . . . . . . 26 2.4 ρUCT (Veness et al., 2011). . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.2 Constructing the dispenser-parametrized model class. . . . . . . . . . . . . 42 3.1 BayesMixture model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.3 Agent-environment simulation. . . . . . . . . . . . . . . . . . . . . . . . . . 53\nxv\nxvi LIST OF ALGORITHMS\nChapter 1\nIntroduction\nWho could have imagined, ever so long ago, what minds would someday do? 1\nWhat a time to be alive! The field of artificial intelligence (AI) seems to be coming of age, with many predicting that the field is set to revolutionize science and industry (Holdren et al., 2016), and some predicting that it may soon usher in a posthuman civilization (Vinge, 1993; Kurzweil, 2005; Bostrom, 2014). While the field has notoriously over-promised and under-delivered in the past (Moravec, 1988; Miller et al., 2009), there now seems to be a growing body of evidence in favor of optimism. Algorithms and ideas that have been developed over the past thirty years or so are being applied with significant success in numerous domains; natural language processing, image recognition, medical diagonosis, robotics, and many more (Russell and Norvig, 2010). This recent success can be largely attributed to the availability of large datasets, cheaper computing power2, and the development of open-source scientific software3. As a result, the gradient of scientific and engineering progress in these fields is very steep, and seemingly steepening every year. The past half-decade in particular has seen an acceleration in funding and interest, primarily driven by advances in the field of statistical machine learning (SML; Bishop, 2006; Hastie et al., 2009), and in particular, the growing sub-field of deep learning with neural networks (Schmidhuber, 2015; LeCun et al., 2015).\nMachine learning\nMachine learning (ML) can be thought of as a process of automated hypothesis generation and testing. ML is typically framed in terms of passive tasks such as regression, classification, prediction, and clustering. In the most common supervised learning setup, a system observes data sampled i.i.d. from some generative process ρ (x, y), where x is some object, for example an image, audio signal, or document, and y is (in the context of classification) a label. A typical machine learning task is to correctly predict y, given a (in general, previously unseen) datum x sampled from ρ (x). This often involves constructing a model p (y|x, θ) parametrized by θ. The system is said to learn from data by tuning the model parameters θ so as to minimize the risk, which is the ρ-expectation of some loss function L Eρ [ L ( x, y, y′ )] , (1.1)\n1This, and all subsequent chapter quotes, are taken from Rationality: From AI to Zombies (Yudkowsky, 2015).\n2In particular, and of particular relevance to machine learning with neural networks, the hardware acceleration due to Graphical Processing Units (GPUs).\n3For example, scikit− learn (Pedregosa et al., 2011), Theano (Al-Rfou et al., 2016), Caffe (Jia et al., 2014), and TensorFlow (Abadi et al., 2015), along with many others.\n1"
    }, {
      "heading" : "2 Introduction",
      "text" : "where L is constructed in such a way as to penalize prediction error Bishop (2006); Hastie et al. (2009); Murphy (2012). High profile breakthroughs of statistical machine learning include image recognition (Szegedy et al., 2015), voice recognition (Sak et al., 2015), synthesis (van den Oord et al., 2016), and machine translation (Al-Rfou et al., 2016). Many more examples exist, in diverse fields such as fraud detection (Phua et al., 2010) and bioinformatics (Libbrecht and Noble, 2015). Informally, these systems might be called ‘intelligent’, insofar as they learn an accurate model of (some part of) the world that generalizes well to unseen data. We refer to these learning systems as narrow AI; they are narrow in two senses:\n1. They are typically applicable only within a narrow domain; a neural network trained\nto recognize cats cannot play chess or reason about climate data.\n2. They are able to solve only passive tasks, or active tasks in a restricted setting.\nIn contrast, the goal of general artificial intelligence can be described (informally) as designing and implementing an agent that learns from, and interacts with, its environment, and eventually learns to (vastly) outperform humans in any given task (Legg, 2008; Müller and Bostrom, 2016).\nArtificial intelligence\nConstructing an artificial general intelligence (AGI) has been one of the central goals of computer science, since the beginnings of the discipline (McCarthy et al., 1955). The field of hard, or general AI has infamously had a history of overpromising and under-delivering, virtually since its birth (Moor, 2006; Miller et al., 2009). Despite this, the recent success of machine learning has inspired a new generation of researchers to approach the problem, and there is a considerable amount of investment being made in the field, most notably by large technology companies: Facebook AI Research, Google Brain, OpenAI and DeepMind are some high profile examples; the latter has made the scope of its ambitions explicit by stating that its goal is to ‘solve intelligence’.\nThe framework of choice for most researchers working in pursuit of AGI is called reinforcement learning (RL; Sutton and Barto, 1998). The current state-of-the-art algorithms combine the relatively simple Q-learning (Watkins and Dayan, 1992) with deep convolutional neural networks to form the so-called deep Q-networks (DQN) algorithm (Mnih et al., 2013) to learn effective policies on large state-space Markov decision processes. This combination has seen significant success at autonomously learning to play games, which are widely considered to be a rich testing ground for developing and testing AI algorithms.\nSome recent successes using systems based on this technique include achieving humanlevel performance at numerous Atari-2600 video games (Mnih et al., 2015), super-human performance at the board game Go (Silver et al., 2016; Google, 2016), and super-human performance at the first-person shooter Doom (Lample and Chaplot, 2016). This has inspired a whole sub-field called deep reinforcement learning, which is moving quickly and generating many publications and software implementations.\nWhile this is all very impressive, these are primarily engineering successes, rather than scientific ones. The fundamental ideas and algorithms used in DQN date from the early nineties; Q-learning is due to Watkins and Dayan (1992), and convolutional neural networks and deep learning are usually attributed to LeCun and Bengio (1995). Arguably, the scientific breakthroughs necessary for AGI are yet to be made, and are still some way\n3 off. In fact, when one considers the problem of learning and acting in general environments, there are still many open foundational problems (Hutter, 2009): What is a good formal definition of intelligent or rational behavior? What is a good notion of optimality with which to compare different algorithms? These are conceptual and theoretical questions which must be addressed by any useful theory of AGI.\nGeneral reinforcement learning\nOne proposed answer to the first of these questions is the famous AIXI model, which is a parameter-free (up to a choice of prior) and general model of unbounded rationality in unknown environments (Hutter, 2000, 2002, 2005). AIXI is formulated as a Bayesian reinforcement learner, and makes few assumptions about the nature of its environment; notably, when studying AIXI we lift the ubiquitous Markov assumption on which algorithms like Q-learning depend for convergence (Sutton and Barto, 1998). Because of this important distinction, we refer to AIXI as a general reinforcement learning4 (GRL) agent (Lattimore et al., 2013).\nRecently, there have been a number of key negative results proven about AIXI; namely that it isn’t asymptotically optimal (Orseau, 2010, 2013) – a concept we will formally introduce in Chapter 2 – and it can be made to perform poorly with certain priors (Leike and Hutter, 2015). These results have motivated, in part, the development of alternative GRL agents: entropy-seeking agents (Orseau, 2011), optimistic AIXI Sunehag and Hutter (2012), knowledge-seeking agents (Orseau et al., 2013), minimum description length agents (Lattimore, 2013), Bayes with exploration (Lattimore, 2013; Lattimore and Hutter, 2014b), and Thompson sampling (Leike et al., 2016).\nNumerous results (positive and negative) have been proven about this family of universal Bayesian agents; together they form a corpus that is of considerable significance to the AGI problem. With the exception of AIXI, many of these agents (and their associated properties) are relatively obscure. We argue that as AI research continues, the theoretical underpinnings of GRL will rise in importance, and these ideas and models will serve as useful guiding principles for practical algorithms. This motivates us to create an open-source web demo of AIXI and its variants, to help in the presentation of these agents to the AI community generally, and to serve as a platform for experimentation and demonstration of deep results relating to rationality and intelligence.\nWeb demos\nWith increasing computing power, and the maturation of the JavaScript programming language, web browsers have become a feasible platform on which to run increasingly complex and computationally intensive software. JavaScript, in its modern incarnations, is stable, portable, expressive, and, with engines like WebGL and V8, highly performant; see Figure 1.1 and Figure 1.2 for examples. Thanks to this, and the popular d3js visualization library, there are now a growing number of excellent open source machine learning web demos available online. Representative examples include Keras-js, a demo of very large convolutional neural networks (Chen, 2016); TensorFlow Playground, a highly interactive demo designed to give intuition for how neural networks classify data (Smilkov\n4Elsewhere in the literature – most prominently by Hutter (2005) and Orseau (2011) – the term universal AI is used."
    }, {
      "heading" : "4 Introduction",
      "text" : "and Carter, 2016); a demo to illustrate the pitfalls and common misunderstandings when using the t-SNE dimensionality reduction technique (Wattenberg et al., 2016), and Andrej Karpathy’s excellent reinforcement learning demo REINFORCEjs, that demonstrates the DQN algorithm (Karpathy, 2015).\nArguably, these demos have immense value to the community, as they serve at once as reviews of recent research, pedagogic aides, and as accessible reference implementations for developers. They are also effective marketing for the techniques or approaches being demonstrated, and the people producing them. We now describe the objectives of this project.\nObjective\nThis thesis is about understanding existing theoretical results relating to GRL agents, implementing these agents, and communicating these properties via an interactive software demo. In particular, the demo should:\n• be portable, i.e. runnable on any computer with a modern web browser and internet connection,\n• be general and extensible, so as to support a wide range of agents and environments,\n5 • be modular, so as to facillitate future development and improvement, and\n• be performant, so that users can run non-trivial simulations in a reasonable amount of time.\nThe demo will consist of:\n• implementations of the agents and their associated modules (planners, environment models),\n• a suite of small environments on which to demonstrate properties of the agents,\n• a user interface (UI) that provides the user control over agent and environment parameters,\n• a visualization interface that allows the user to playback the agent-environment simulation, and\n• a suite of explanations, one accompanying each demo, to explain what the user is seeing.\nIn particular, the demo should serve three purposes:\n• as a helpful introduction to the theory of general reinforcement learning, for both students and researchers; in this regard, we follow the model of REINFORCEjs\n(Karpathy, 2015);\n• as a platform for researchers in this area to develop and run experiments to accompany their theoretical results, and to help present their findings to the community;\nin this aspect, we follow the model of OpenAI Gym (Brockman et al., 2016);\n• and as an open-source reference implementation for many of the general reinforcement learning agents.\nContribution\nIn this work, we present:\n• a review of the general reinforcement learning literature of Hutter, Lattimore, Sunehag, Orseau, Legg, Leike, Ring, Everitt, and others. We present the agents and\nresults under a unified notation and with added conceptual clarifications. As far as we are aware, this is the only document in which agents and algorithms from the GRL literature are presented as a collection.\n• an applied perspective on Bayesian agents and mixture models with insights into MCTS planning and modelling errors,\n• an open-source JavaScript reference implementation of many of the agents,\n• experimental data that validates and illustrates several theoretical results, and\n• an extensible and general framework with which researchers can run experiments and demos on reinforcement learning agents in the browser."
    }, {
      "heading" : "6 Introduction",
      "text" : "The software itself is found at http://aslanides.github.io/aixijs, and can be run in the browser on any operating system. Note that different browsers have differing implementations of the JavaScript specification; we strongly recommend running the demo on Google Chrome5, as we didn’t test the implementation on other browsers.\nThesis Outline\nIn Chapter 2 (Background) we present the theoretical framework for general reinforcement learning, introduce the agent zoo, and present the basic optimality results. In Chapter 3 (Implementation) we document the design and implementation of the software itself. In Chapter 4 (Experiments) we outline the experimental results we obtained using the software. Chapter 5 (Conclusion) makes some concluding remarks, and points out potential directions for further work.\nWe expect that this thesis will typically be read in soft copy, i.e. digitally, through a PDF viewer. For this reason, we augment this thesis throughout with hyperlinks, for the reader’s convenience. These are used in three ways:\n• on citations, so as to link to the corresponding bibliography entry,\n• on cross-references, so as to link to the appropriate page in this thesis,6 or\n• as external hyperlinks, to link the interested reader to an internet web page.\nIn particular, we encourage the reader to use the cross-references to jump around the text.\n5https://www.google.com.au/chrome/browser/desktop/ 6After following a link, the reader can return to where they were previously, using (usually) Alt +\non Windows or Linux, and + [ (in Preview) or + (in Acrobat) on Mac OS.\nChapter 2\nBackground\n“Where will an Artificial Intelligence get money?” they ask, as if the first Homo sapiens had found dollar bills fluttering down from the sky, and used them at convenience stores already in the forest.\nIn this Chapter we present a brief background on reinforcement learning, with a focus on the problem of general reinforcement learning (GRL). Our objective is for this chapter to be relatively accessible. To this end, we try to aim for conceptual clarity and conciseness over technical details and mathematical rigor. For a more complete and rigorous treatment of GRL, we refer the reader to the excellent PhD theses of Leike (2016a) and Lattimore (2013), and of course to the seminal book, Universal Artificial Intelligence by Hutter (2005).\nThe Chapter is laid out as follows: In Section 2.1 (Preliminaries), we introduce some notation and basic concepts. In Section 2.2 (Reinforcement Learning), we introduce the reinforcement learning problem in its most general setting. In Section 2.3 (General Reinforcement Learning) we introduce the Bayesian general reinforcement learner AIXI and its relatives, the implementation and experimental study of which forms the bulk of this thesis. We draw the GRL literature together and present these agents under a unified notation. In Section 2.4 (Planning) we discuss approaches to the problem of planning in general environments. We conclude with some remarks and a short summary in Section 2.5 (Remarks)."
    }, {
      "heading" : "2.1 Preliminaries",
      "text" : "We briefly introduce some of the tools and concepts that are used to reason about the general reinforcement learning (GRL) problem. We assume that the reader has a basic familiarity with the concepts of probability, information theory, and statistics, and ideally some exposure to standard concepts in artificial intelligence (e.g. breadth-first search, expectimax, minimax), and reinforcement learning (e.g. Q-learning, bandits). For some general background, we refer the reader to MacKay (2002) for probability and information theory, Bishop (2006) for machine learning and statistics, Russell and Norvig (2010) for artificial intelligence, and Sutton and Barto (1998) for reinforcement learning."
    }, {
      "heading" : "2.1.1 Notation",
      "text" : "Numbers and vectors. The set N .= {1, 2, 3, . . . } is the set of natural numbers, and R denotes the reals. We use R+ = [0,∞) and R++ = (0,∞). A set is countable if it can be brought into bijection with a subset (finite or otherwise) of N, and is uncountable\n7"
    }, {
      "heading" : "8 Background",
      "text" : "otherwise. We use RK to denote the K-dimensional vector space over R. We represent vectors with bold face: x is a vector, and xi is its i th component. We (reluctantly1) represent inner products with the standard notation for engineering and computer science:\ngiven x,y ∈ RK , xTy = ∑K\ni=1 xiyi.\nStrings and sequences. Define a finite, nonempty set of symbols X , which we call an alphabet. The set X n with n ∈ N is the set of all strings over X with length n, and X ∗ = ∪n∈NX n is the set of all finite strings over X . X∞ is the set of infinite strings over X , and X# = X ∗ ∪X∞ is their union. The empty string is denoted by ; this is not to be confused with the small positive number ε. For any string x ∈ X#, we denote its length by |x|.\nFor any string x with |x| ≥ k, xk is the kth symbol of x, x1:k is the first k symbols of x, and x<k is the first k − 1 symbols of x. We often make use of the binary alphabet B = {0, 1}. For two finite strings x, y ∈ X ∗ we denote their concatenation by xy. For two finite strings a, e ∈ X n of length n, it will be convenient to write æ to indicate the riffled string a1e1a2e2 . . . anen; we slightly overload our indexing notation by stipulating that for k ≤ n, æ1:k = a1e1, . . . , akek, and similarly for æ<k. Miscellaneous. We use . = to mean ‘is defined as’, and we use the convention that log is the logarithm base two and ln is the natural logarithm. We usually, but not always, refer to random variables in upper case. The indicator function I [P ] returns 1 if the predicate P is true and 0 otherwise. We use→ and to denote deterministic and stochastic mappings respectively."
    }, {
      "heading" : "2.1.2 Probability theory",
      "text" : "For our purposes, we will only be working with discrete event spaces, and so we will omit the machinery of measure theory, which is needed to treat probability theory over continuous spaces. Given a sample space Ω, we construct an event space F as a σ-algebra on Ω: a set of subsets of Ω that is closed under countable unions and complements; for discrete distributions, this is simply the power set 2Ω. A random variable X is discrete if its associated sample space ΩX is countable; we associate with it a probability mass function p : ΩX → [0, 1]. If X is continuous, provided ΩX is measurable, we can associate with it a probability density function R→ R+. For a countable set Ω, we use ∆Ω to represent the set of all probability distributions over Ω. We use E [X] .= ∑ x∈ΩX xp (x) (or, in\nthe continuous setting, E [X] = ∫ X xp (x) dx) to represent the expectation of the random variable X. In many cases we will emphasize for clarity that X is distributed according to p by writing the expectation as Ep [X]. We say x ∼ ρ (·) to mean that x is sampled from the distribution ρ.\nThe two fundamental results of probability theory are the sum and product rules: p (a) = ∑ b∈ΩB p (a, b) (2.1)\np (a, b) = p (a|b) p (b) , (2.2)\nfrom which we immediately get Bayes’ rule, which plays a central role in the theory of\nrationality and intelligence (Hutter, 2000).\n1The author greatly favors using the Einstein notation for its power and clarity.\n§2.1 Preliminaries 9\nTheorem 1 (Bayes’ rule). Bayes’ rule is given by the following identity:\nposterior︷ ︸︸ ︷ Pr (A|B) =\nlikelihood︷ ︸︸ ︷ Pr (B|A) prior︷ ︸︸ ︷ Pr (A)\nPr (B)︸ ︷︷ ︸ predictive distribution\n(2.3)\n= Pr (B|A) Pr (A)∑ a∈ΩA Pr (B|a) Pr (a) .\nNote that Bayes’ rule follows from the fact that the product rule is symmetric in its arguments: p (a|b) p (b) = p (b|a) p (a). Its power and significance comes through its interpretation as a sequential updating scheme for subjective beliefs about hypotheses; we annotate Equation (2.3) with this interpretation, which we discuss below. The most distinguishing feature of being Bayesian is of interpreting your probabilites subjectively, in the sense that they represent your credence in some outcome, or some model. Updating beliefs using Bayes’ rule is a (conceptually) trivial step, since it just says that your beliefs are constrained by the rules of probability theory; if they weren’t, you would be vulnerable to Dutch book arguments (Jaynes, 2003).\nIn our context, typically A is some model or hypothesis, and B is some observation. Pr (A) is our prior belief in the correctness of hypothesis A, and Pr (A|B) is our posterior belief in A after taking in some observation, B. Effectively, Bayes’ rule defines the mechanism with which we move probability mass between competing hypotheses. Note that once we assign zero probability (or credence) to some hypothesis A, then there is no observation B that will change our mind about the impossibility of A. This is not such a problem if it so happens that A is false; the situation in which A is true, and has been prematurely (and incorrectly) falsified, is sometimes known informally as Bayes Hell. For this reason, we try to avoid using priors that assign zero probability to events; this is known more formally as Cromwell’s rule.\nNotice that in general the sample spaces ΩA and ΩB are different; B is a random variable on some set of possible observations, ΩB, while A is a random variable over a set of hypotheses, which aren’t observed, but constructed. To emphasize this distinction, we use a separate notation: M represents a set (or, in the uncountable case, space) of hypotheses, which we will call a model class. We implicitly assume that in all cases M contains at least two elements. Sequential Bayesian updating in this way is an inductive process; we refine our models based on observation. As we will see in Section 2.3, the predictive distribution Pr (B) will play an important role for our reinforcement learning agents.\nWe formalize Cromwell’s rule with the concept of a universal prior.\nDefinition 1 (Universal prior). A prior over a countable class of objects M is a probability mass function p ∈ ∆M, such that p (ν) is defined for each ν ∈M, with p (ν) ∈ [0, 1] and ∑ ν∈M p (ν) = 1. A universal prior assigns non-zero mass to every hypothesis such that p (ν) ∈ (0, 1) for all ν ∈M.\nWe often make use of the following distributions:\nBernoulli. We use Bern (θ) to represent the Bernoulli process on x ∈ {0, 1}, with probability mass function given by p (x|θ) = θx (1− θ)x.\nBinomial. We use Binom (n, p) to represent the Binomial distribution on k ∈ {0, . . . , n} with mass function given by p (k|n, p) = ( n k ) pk (1− p)k, where ( n k ) = n!k!(n−k)! is"
    }, {
      "heading" : "10 Background",
      "text" : "the known as the binomial coefficient.\nUniform. We use U (a, b) to represent the measure that assigns uniform density to the closed interval [a, b], with b > a; its density is given by p (x) = 1b−aI [a ≤ x ≤ b]. We overload our notation (and nomenclature) and also use U (A) to represent the uniform distribution over the finite set A; its mass function is given by p (a) = 1|A| .\nNormal. We use N ( µ, σ2 ) to represent the univariate Gaussian distribution on R\nwith density given by\np ( x|µ, σ2 ) = ( 2πσ2 )− 1 2 exp ( −(x− µ) 2\n2σ2\n) .\nBeta. We use Beta (α, β) to represent the Beta distribution on [0, 1] with density given\nby\np (x|α, β) = Γ (α+ β) Γ (α) Γ (β) xα−1 (1− x)β−1 ,\nwhere Γ is the Gamma function that interpolates the factorials.\nThe beta distribution is conjugate to the Bernoulli and Binomial distributions; this means that a Bayesian updating scheme can use a Beta distribution as a prior p (θ) over the parameter of some Bernoulli process, whose likelihood is given by p (x|θ). Since the Beta and Bernoulli are conjugate, the posterior p (θ|x) will also take the form of a Beta distribution. Conjugate pairs of distributions such as this allow us to analytically compute the posterior resulting from a Bayesian update, and are essential for tractable Bayesian learning.\nDirichlet. We use Dirichlet (α1, . . . , αK) to represent the Dirichlet distribution on the\n1-simplex\nSK . = { x ∈ RK+ ∣∣ 1Tx = 1} , with density given by\np (x|α) = Γ (∑K i=1 αi ) ∏K i=1 Γ (αi) K∏ i=1 xαi−1i .\nThis is the multidimensional generalization of the Beta distribution, and is conjugate to the Categorical and Multinomial distributions. The categorical distribution over some discrete set X is simply a vector on the 1-simplex, p ∈ SK , where K = |X |. The multinomial simply generalizes the binomial distribution.\nAs we shall see in Section 2.3, a significant aspect of intelligence is sequence prediction. For this reason, we introduce measures over sequences. A distribution over finite sequences ρ ∈ ∆X ∗ can be written as ρ (x1:n) for some finite n. Analogously to the sum and product rules, we have\nρ (xn|x<n) = ρ (x1:n)\nρ (x<n) ρ (x<n) = ∑ y∈X ρ (x<ny) .\nThere are two important properties that sequences can have which are relevant to\nreinforcement learning: the Markov and ergodic properties:\n§2.2 Reinforcement Learning 11\nMarkov property. A generative process ρ is nth-order Markov if it has the property\nρ (xt|x<t) = ρ ( xt|x(t−n):(t−1) ) .\nTypically, when we invoke the Markov property, we mean that the process is 1st-order Markov. A Markov chain is simply a first-order Markov process over a finite alphabet X , in which the conditional distribution is stationary, and can thus be represent as a |X |×|X | transition matrix P (x′|x) ≡ ρ (x′t|xt−1). This matrix is said to be stochastic, to emphasise that it represents a distribution over x′, so that P (x′|x) ∈ [0, 1] and ∑ s′ P (x\n′|x) = 1. In this context, we often identify the symbols x ∈ X with states.\nErgodicity. In a Markov chain, a state i is said to be ergodic if there is non-zero probability of leaving the state, and the probability of eventually returning is unity. If all states are ergodic, then the Markov chain is ergodic. Informally, this means that the Markov chain has no traps: at all times, we can freely move around the MDP without ever making unrecoverable mistakes. Ergodicity is an important assumption in the theory of Markov Decision Processes, which we will see later."
    }, {
      "heading" : "2.1.3 Information theory",
      "text" : "For a distribution p ∈ ∆X over a countable set X , the entropy of p is\nEnt (p) . = − ∑\nx∈X : p(x)>0\np (x) log p (x) . (2.4)\nAbsent additional constraints, the maximum entropy distribution is U , our generalized uniform distribution. We also define the conditional entropy\nEnt (p (·|y)) .= ∑\nx∈X : p(x)>0\np (x|y) log p (x|y) .\nGiven two distributions p, q ∈ ∆X , the Kullback-Leibler divergence (KL-divergence, also known as relative entropy) is defined by\nKL (p‖q) .= ∑\nx∈X : p(x)>0,q(x)>0\np (x) log p (x)\nq (x) .\nWe use the ‖ symbol to separate the arguments so as to emphasise that the KLdivergence is not symmetric, and hence not a distance measure. It is non-negative, by Gibbs’ inequality. If p and q are measures over sequences, then we can define the conditional d-step KL-divergence\nKLd (p, q|x<t) = ∑ xt:t+d∈X d p ( x1:(t+d)|x<t ) log\np ( x1:(t+d)|x<t ) q ( x1:(t+d)|x<t\n) ."
    }, {
      "heading" : "2.2 Reinforcement Learning",
      "text" : "In contrast to machine learning, in the reinforcement learning setting, the training data that the system receives is now dependent on its actions; we thus introduce agency to the learning problem (Sutton and Barto, 1998). What observations the agent can make, and therefore what it can learn, now depend not only on the environment (as in machine"
    }, {
      "heading" : "12 Background",
      "text" : "learning), but also on the agent’s own policy, which determines how it will behave (Barto and Dietterich, 2004). In this way, reinforcement learning considerably generalizes machine learning; we replace the loss function of Equation (1.1) with a reward signal. Now, instead of minimizing risk, the agent must seek to maximize future expected rewards. In this way, reinforcement learning generalizes machine learning to the active setting, so that the agent can now influence its environment with the actions that it takes.\nWe distinguish this from the related set-up known as inverse reinforcement learning or imitation learning (Abbeel and Ng, 2004), in which the agent is given training data consisting of a history of actions and percepts from which it must infer a policy. In contrast, reinforcement learners must take their own actions and learn through trial and error – they are only supervised to the extent that their extrinsic reward signal gives them feedback on their policy.\nBecause we are motivated by the general reinforcement learning problem, we introduce a more general and pedantic setup than is common in the reinforcement learning literature. This set-up has been honed by (for example) Lattimore and Hutter (2011) and Leike et al. (2016)."
    }, {
      "heading" : "2.2.1 Agent-environment interaction",
      "text" : "In the standard cybernetic model (see Figure 2.1), the agent and environment are separate entities that play a turn-based two-player game. At time t, the agent produces an action at, which is passed as an input to the environment, which performs some computation that (in general) changes its internal state, and then returns a percept et to the agent. We often refer to the time t as the number of agent-environment cycles that have elapsed. Together, the agent and environment generate a history æ1:t = a1e1 . . . atet. In general, it is consequential to the behavior of the agent whether this interaction runs indefinitely or finishes after some finite lifetime T (Martin et al., 2016); we discuss this to an extent when we introduce discount functions in section 2.2.2.\nDefinition 2 (Environment). An environment is a tuple (A,S, E , D, ν), where\n• A is the action space,\n• S is the state space of the environment, which is in general hidden from the agent.\n• E is the percept space, which is itself composed of observations o ∈ O and rewards r ∈ R with E = O ×R.\n§2.2 Reinforcement Learning 13\n• D : S × A S is the (in general stochastic) dynamics/transition function on the environment’s state space. Note that, without loss of generality, we can allow D to be first-order Markov.\n• ρ : S → ∆E is the percept function, by analogy to a hidden Markov model (HMM) in the context of statistical machine learning2.\nNote that for the purposes of General reinforcement learning (GRL), we make no Markov assumption on the percepts, and we make no ergodicity assumption on the state or percept spaces.\nSince we typically take the agent’s perspective, we don’t have access to the environment’s state s ∈ S, nor its dynamics D. For this reason, we typically talk about the environment in terms of the measure\nν : (A× E)∗ ×A → ∆E ,\nwhich we write\nν (et|æ<tat) .\nNote that the vertical bar | is an abuse of notation here: ν is not conditioned on the actions, since it is not derived from a joint distribution over actions and percepts; the sequence of actions a1:t are inputs to the environment. A more pedantic (but ugly) notation would be to write ν (et|e<t‖a1:t) or ν (et|e<t; a1:t), which emphasizes that ν is a conditional distribution with respect to percepts, but not with respect to actions. We typically refer to the environment itself with the symbol ν, for convenience.\nIt is worth pausing to make some remarks about this setup here:\n1. In the general setting, environments are partially-observable Markov decision pro-\ncesses (POMDPs). We can always model an environment as Markovian with respect to some hidden state, since if it depends on some history of states, we incorporate sufficient history into the state until the Markov property is restored.\n2. For our purposes, we assume that A, E , and S are all finite.\n3. No matter what state the agent is in, it always has the full action space available\nto it. This simplifies the setup, and means that when implementing a simulated environment, we have to specify dynamics for every action in every state – ‘illegal’ or not. For an example of this, see Example 1.\n4. Stochastic environments are sufficiently general to model everything, including Na-\nture, adversaries, and naturally, deterministic environments.\n5. This is an implicitly dualistic model, in the sense that the agent is separate from the\nenvironment; in reality the agent will be embedded within the environment.\n6. As with all simulations run on computers, time is of course discretized.\n7. We stipulate that our environments have the chronological property, which simply\nmeans that percepts at time t do not depend on future actions, i.e. ν (e1:t‖a1:∞) = ν (e1:t‖a1:t).\n2POMDPs are to MDPs as Hidden Markov Models are to Markov chains.\n14 Background\nThe agent-environment interaction is thus modelled as a stochastic, imperfect-information, two-player game. The environment specifies both the percept space E and action space A. The agent ‘plugs in’ to the environment (which, without loss of generality, can be thought of as a game simulation) and plays its moves in turns.\nWe now present some definitions of common classes of environments. For a more\ncomprehensive taxonomy, see, for example, Legg (2008).\nDefinition 3 (Markov Decision Process). A finite-state Markov decision process (MDP) is a tuple (S,A,P,R) where\n• S is a finite state space, labelled by indices s1, . . . , s|S|.\n• A is a finite action space, labelled by indices a1, . . . , a|A|.\n• P is the set of transition probabilities P (s′|s, a), which can be thought of as a stochastic rank-3 tensor of dimensions |S| × |S| × |A|\n• R is the set of rewards R (s, a).\nDefinition 4 (Bandit). An N -armed bandit is a Markovian environment with one state S = {s}, N actions A = {a1, . . . , aN} and N corresponding reward distributions {ρ1, . . . , ρn} with ρi ∈ ∆R. There are no observations, only a reward signal which is sampled from the distribution ρi corresponding to the agent’s last action, ai. Typical choices for ρ, R are Bernoulli distributions over {0, 1}, or Gaussians over R; see Figure 2.2.1.\nExample 1 (Go). Go is a two-player, deterministic, and fully-observable3 board game with a large, finite state-action space. Played on a 19× 19 board, there are (naively) 3192\n3Here we are referring to the game state being fully observable. The opponent’s strategy can of course be modelled as some hidden variable; for simplicity assume that we model them as minimax, so that there is no hidden state.\n§2.2 Reinforcement Learning 15\npossibly game states, with an action space of 192 (though many of these moves will be illegal). It is notoriously hard to evaluate who is winning in any given game state (Silver et al., 2016); the reward signal is 0 for all game states for which a winner has not been declared, and ±1 otherwise (depending on which player won).\nDefinition 5 (Policy). A policy is, in the most general setting, a probability distribution over actions, conditioned on a history: π (at|æ<t) : (A× E)∗ → ∆A .\nNote the symmetry between Definition 5 with ν (et|æ<tat) from Definition 2.\nDefinition 6 (Agent). Let ΠA,E be the set of all policies on the (A, E)-space. An agent is fully characterized by a policy π, and a learning algorithm, which is as a mapping from experience (histories) to policies (A× E)∗ → ΠA,E .\nThe agent and environment, combined, induce a distribution over histories. We denote this by νπ ∈ ∆ (A× E)∗. This is equivalent to the state-action visit distribution in the standard reinforcement learning literature (Sutton et al., 1999).\nνπ (æ<t) = t∏ k=1 π (ak|æ<t) ν (ek|æ<k) (2.5)\nThe distribution νπ plays an important role in the theory of GRL, since we will use it to compute the expected sum of future rewards, which is what our reinforcement learners will seek to maximize."
    }, {
      "heading" : "2.2.2 Discounting",
      "text" : "In the context of reinforcement learning, we wish our agent to act according to a policy that maximizes reward accumulated over its lifetime. In general it is not good enough to greedily maximize the reward obtained in the next time-step, since in many cases this will lead to reduced total reward. Thus we define the return resulting from executing policy π in environment ν from time t as the sum of future rewards ri.\nRπν (æ<t) = ∞∑ k=t rk,\nwhere each of the rk are sampled from ν (·|π,æ<k); thus the return is a random variable that depends on the agent policy π, the environment ν, and the history æ<t. In general this sum will diverge, so in practice we concern ourselves with either the average reward\nr̄πν = limn→∞\n1\nn n∑ k=t rk,"
    }, {
      "heading" : "16 Background",
      "text" : "or the discounted return\nRπνγ (æ<t) = ∞∑ k=1 γtkrk,\nwhere γtk ≤ 1 is some generalized discount function γt : N→ [0, 1] such that\nΓtγ . = ∞∑ k=t γtk <∞.\nHere we interpret t as the current age of the agent, and k is the agent’s planning\nlook-ahead.\nDefinition 7 (ε-Effective horizon; Lattimore and Hutter (2014a)). Given a discount function γ, the ε-effective horizon is given by\nHtγ (ε) . = min\n{ H :\nΓt+Hγ Γtγ ≤ ε\n} . (2.6)\nThe ε-effective horizon represents the distance ahead in the future that the agent can plan while still taking into account a proportion of the available return equal to (1− ε). The choice of discount function is relevant to how the agent plans; some discount functions will make the agent far-sighted, and others will make it near-sighted. We discuss planning more in Section 2.4, and present experiments relating to this in Chapter 4. A common choice of discount function is the geometric discount function, which is ubiquitous in RL due to its simplicity:\nγtk = β k,\nfor some β ∈ [0, 1]. That is, it is β raised to the number of cycles that we look ahead in planning."
    }, {
      "heading" : "2.2.3 Value functions",
      "text" : "In general, the environment ν is noisy and stochastic, and the agent’s policy will often be stochastic. As a result, we can’t maximize the discounted return directly; we must instead maximize it in expectation. This follows from the Von Neumann-Morgenstern utility theorem (Morgenstern and von Neumann, 1944).\nDefinition 8 (Value function). The value V πνγ : (A× E) ∗ → R of a history æ<t in environment ν under policy π with discount function γ is the expected sum of discounted future rewards\nV πνγ (æ<t) . = Eπν [ ∞∑ k=t γtkrk ∣∣∣∣∣æ<t ] , (2.7)\nwhere we use Eπν above to mean the expectation with respect to νπ, defined in Equation (2.5). Equation (2.7) above expresses the value function in iterative form. We can also express it recursively (Leike, 2016a) using the mutually recursive relations\n§2.2 Reinforcement Learning 17\nV πνγ (æ<t) = ∑ at∈A π (at|æ<t)V πνγ (æ<tat)\nV πνγ (æ<tat) = 1\nΓt ∑ et∈E ν (et|æ<tat) [ γtrt + Γt+1V π νγ (æ1:t) ] .\nFor simplicity, from here on we will often omit the γ subscript and make the dependence on the discount function implicit. We will also suppress the normalization 1Γt , as it clutters the notation and is introduced for technical reasons (so that value is normalized). Finally, we often also suppress the history æ<t for clarity. Definition 9 (Optimal value & policy). The optimal value V ∗ν achievable in environment ν given a history ae<t is\nV ∗ν .\n= max π\nV πν , (2.8)\nand the corresponding optimal policy π∗ is\nπ∗ν = arg maxπ V πν .\nAssuming bounded rewards and finite action spaces, these maxima exist for all ν (Lattimore and Hutter, 2014b), though they are not unique in general. For our purposes, we allow arg max to break ties at random. At this point it is elucidatory to unroll Equation (2.8) into the expectimax expression\nV ∗ν (æ<t) = limm→∞ max at ∑ et · · ·max at+m ∑ et+m t+m∑ k=t γtkrk k∏ j=t ν (ej |æ<jaj) . (2.9)\nNote that we can do this by using the distributive property of max over +. In Section 2.4, we will discuss how to approximate this expectimax calculation for general environments, up to a finite horizon m."
    }, {
      "heading" : "2.2.4 Optimality",
      "text" : "Informally, it makes sense to evaluate an agent’s performance against that of the optimal policy, were it put in the same situation. We can only sensibly talk about this performance asymptotically in general, that is, in the limit t→∞, since the agent needs time to learn the environment, and we can’t evaluate the agent after some finite time t, since this time would in general be environment-dependent.\nDefinition 10 (Asymptotic optimality; Lattimore and Hutter, 2011). A policy π is strongly asymptotically optimal in environment class M if ∀µ ∈M\nµπ (\nlim t→∞\n{ V ∗µγ (æ<t)− V πµγ (æ<t) } = 0 ) = 1,\nwhere µπ is the measure induced by the interaction of environment µ with policy π.\nThe policy π is weakly asymptotically optimal in M if ∀µ ∈M\nµπ ( lim n→∞ 1 n n∑ t=1 { V ∗µγ (æ<t)− V πµγ (æ<t) } = 0 ) = 1."
    }, {
      "heading" : "18 Background",
      "text" : "Finally, we say π is asymptotically optimal in mean over M if ∀µ ∈M\nlim t→∞\nEπµ [ V ∗µγ (æ<t)− V πµγ (æ<t) ] = 0.\nAsymptotic optimality is objective and general, but unfortunately doesn’t capture everything we want in an agent. For example, in environments with traps – that is, an accepting state with no transitions leaving it and very low reward – every policy will be asymptotically optimal after falling into the trap, since no policy will outperform any other, conditioned on being trapped. Moreover, in uncertain environments with traps, an agent cannot be asymptotically optimal unless it is sufficiently gung-ho in its exploration that it eventually falls into traps (Leike, 2016a). Therefore, we should take asymptotic optimality with a grain of salt; it is not a particularly good measure of optimality in general environments. The quest for good notions of optimality is currently an open problem in the theory of GRL (Leike and Hutter, 2015; Leike, 2016a)."
    }, {
      "heading" : "2.3 General Reinforcement Learning",
      "text" : "We now introduce the agents that are central to the theory of general reinforcement learning (GRL). We begin with AIµ, which is simply the policy of the informed agent that has a perfect model of the environment µ:\nDefinition 11 (AIµ). AIµ corresponds to the policy in which the true environment µ is known to the agent, and so no learning is required. Behaving optimally reduces to the planning problem of computing the µ-optimal policy\nπAIµ = π∗µ .\n= arg max π\nV πµ .\nThe astute reader will notice that πAIµ is simply the optimal policy for environment µ; we introduce it here as a separate agent so as to have a benchmark against which to compare our other reinforcement learners.\nIn general the environment will be unknown, and so our agents will have to learn it. For the purpose of studying the general reinforcement learning problem, we consider primarily Bayesian agents, as they are the most general and principled way to think about the problem of induction (Hutter, 2005)."
    }, {
      "heading" : "2.3.1 Bayesian agents",
      "text" : "Our Bayesian agents maintains a Bayesian mixture or predictive distribution ξ over a countable model class M, given by\nξ (et|æ<tat) = ∑ ν∈M wνν (et|æ<tat) (2.10)\n= ∑ ν∈M Pr (ν|æ<tat) Pr (et|ν,æ<tat) .\nNote that ξ is equivalent to the normalization term in Theorem 1; ξ (e|·) represents the probability that the agent’s model assigns to e; in other words, it is the agent’s predictive\n§2.3 General Reinforcement Learning 19\ndistribution. The weights wν ≡ w (ν) ≡ Pr (ν) represent the agent’s credence in hypothesis ν ∈ M. ν (e|æ<tat) is the probability that model ν assigns to percept e, given history æ<tat. One can think of the hypothesis/model ν as a latent variable in the model, which is marginalized out to get the predictive distribution. The only strong assumption we make in this setup is that the true environment µ is contained inM. Given a new percept e = (o, r), the Bayesian updates its weights model using Bayes rule:\nw (ν|e) = w (ν) ν (e) ξ (e) .\nThis gives us the very natural updating scheme\nwν ← wν ν (e)\nξ (e) .\nNote that above we have suppressed the history æ<tat for clarity. That is, given a new percept e, we compute our posterior by multiplying the prior by\nthe likelihood ratio. Let us pause and make some remarks:\n1. We see that Bayesian induction generalizes the Popperian idea of conjecture\nand refutation. An environment/model/hypothesis ν is falsified by observation/perception/experience/experiment e iff ν (e) = 0. Clearly, the posterior goes to zero for these environments.\n2. Bayesian induction is parameter free up to a choice of model class M and prior w (ν| ).\n3. We use the words environment, model, and hypothesis interchangeably.\n4. We can see by its ‘type signature’ that ξ itself is an environment.\nItem 4 above underpins the Bayes-optimal agent AIξ.\nDefinition 12 (AIξ). AIξ computes the ξ-optimal policy, i.e.\nπAIξ .\n= arg max π\nV πξ . (2.11)\nThat is, AIξ uses the policy that is optimal in the mixture environment ξ, which we\nupdate with percepts from the true environment µ using Bayes’ rule.\nFor the purpose of reasoning about general artificial intelligence, we use the largest model class we can, which isMcomp, the class of all computable environments4. This is what the famous agent AIXI does:\nDefinition 13 (AIXI). AIXI is AIξ with the model class given by Mcomp and the Solomonoff prior\nwν = 2 −K(ν),\nwhere K (ν) is the Kolmogorov complexity of ν. For a string x, the Kolmogorov\ncomplexity is given by\nK (ν) .\n= min {|p| | U (p) = x} , 4For technical reasons, the literature typically uses MLSCCCS , the class of lower semi-computable chrono-\nlogical conditional semimeasures. This distinction is a technical one and of little consequence to us."
    }, {
      "heading" : "20 Background",
      "text" : "where U is a universal Turing machine (Li and Vitányi, 2008). For every computable environment ν, there is a corresponding Turing machine T , so we can define the K (ν) as the Kolmogorov complexity of its index in the enumeration ν1, ν2, . . . of all environments. The Kolmogorov complexity is, of course, incomputable.\nThis gives rise to the famous equation describing the AIXI policy, unrolled in all its\nincomputable glory:\naAIXIt = arg maxat lim m→∞ ∑ et · · ·max at+m ∑ et+m t+m∑ k=t γtkrk ∑\np : U(p,a<t)=e1:j\n2−|p|.\nOne can derive computable approximations of Solomonoff induction, most notably by using a generalization of the Context-Tree Weighting algorithm, which is a mixture over Markov models up to some finite order n, weighted by their complexity; this is used in the well-known MC-AIXI-CTW implementation due to Veness et al. (2011).\nAIXI achieves on-policy value convergence (Leike, 2016a): µπ (\nlim t→∞\n[ V πξ − V πµ ] = 0 ) = 1,\nwhich means that it asymptotically learns the true value of its policy π in environment\nµ. It however, doesn’t achieve asymptotic optimality.\nTheorem 2 (AIXI is not asymptotic optimal; Orseau, 2010; Leike, 2016a). For any class M ⊇ Mcomp no Bayes optimal policy π∗ξ is asymptotically optimal: there is an environment µ ∈M and a time step t0 ∈ N such that for all time steps t ≥ t0\nµπ ∗ ξ ( V ∗µ (æ<t)− V π∗ξ µ (æ<t) = 1\n2\n) = 1.\nThis theorem effectively means that the Bayes agent will eventually decide that its current policy is good enough, and that any additional exploration is not worth its Bayesexpected payoff. Moreover, AIξ can be made to perform badly with a so-called dogmatic prior:\nTheorem 3 (Dogmatic prior; Leike and Hutter, 2015). Let π be some computable policy, ξ some universal mixture, and let ε > 0. There exists a universal mixture ξ′ such that for any history h consistent with πand V πξ (h) > ε, the action π (h) is the unique ξ ′-optimal action.\nThis theorem says that, even using a universal prior that assigns non-zero mass to every hypothesis in the model class, we can construct a prior in such a way that the agent never overcomes the bias in its prior. This is in contrast to Bayesian learners in the passive setting, which can overcome (given sufficient data) any biases in their (universal) prior. We demonstrate in Chapter 4 an example of a dogmatic prior that prevents the Bayesian agent from exploring.\n§2.3 General Reinforcement Learning 21"
    }, {
      "heading" : "2.3.2 Knowledge-seeking agents",
      "text" : "We now come to our first exhibit in the GRL agent zoo: knowledge-seeking agents (KSA). There are several motivations for defining and studying KSA:\n• They represent a way to construct a purely ‘exploratory’ policy. A principled solution to exploration by intrinsic motivation is one of the central problems in reinforcement\nlearning (Thrun, 1992).\n• They remove the dependence on arbitrary reward signals or utility functions; up to a choice of model class and prior, ‘knowledge’ is an objective quantity (Orseau, 2011).\n• They collapse the exploration-exploitation trade-off to just exploration.\nBefore formally defining knowledge-seeking agents, it is necessary to introduce the concept of a utility agent, which generalizes the concept of a reinforcement learning agent.\nDefinition 14 (Utility Agent; Orseau, 2011). A utility agent is a reinforcement learner equipped with a bounded utility function u : (A× E)∗×A → R which replaces the notion of reward. The corresponding value function5 is given by\nV πνγ (æ<t) = Eπν [ ∞∑ k=t γtku (æ1:k) ∣∣∣∣∣æ<tat ] . (2.12)\nOne can easily verify that this definition generalizes RL agents by setting uRL (æ1:t) = r (et) , where r (·) returns the second component of the percept tuple et = (ot, rt). Utility agents are fully autonomous, in the sense that they are not dependent on being ‘supervised’ by an extrinsic reward signal to learn. They are equipped with a utility function at birth and from then on seek to maximize the discounted sum of future utility.\nKnowledge-seeking agents (KSA) are Bayesian utility agents whose utility function is constructed in such a way as to motivate them to ‘seek knowledge’ and learn about their environment (Orseau, 2011, 2014). There are several distinct ways in which one can define knowledge for a Bayesian agent. We will start by defining an agent that gets utility from lowering the entropy (i.e., reducing uncertainty) in its beliefs. We can define the information gain resulting from some percept e as the difference in entropy between the agent’s prior and posterior:\nIG (e) . = Ent (w (·))− Ent (w (·|e)) , (2.13)\nNow, following Lattimore (2013), we consider the ξ-expected information gain. Informally, this is the information that the agent expects to obtain were the percepts distributed according to its mixture model ξ. It is also the agent’s expected utility from seeing percept\n5Compare with Equation (2.7)."
    }, {
      "heading" : "22 Background",
      "text" : "e. For clarity, we suppress the history æ<tat and time subscripts. Eξ [IG (e)] = ∑ e∈E ξ (e) [Ent (w (·))− Ent (w (·|e))]\n= ∑ e∈E ξ (e) ∑ ν∈M [w (ν|e) logw (ν|e)− w (ν) logw (ν)]\n= ∑ e∈E ξ (e) ∑ ν∈M [ w (ν) ν (e) ξ (e) logw (ν|e)− w (ν) logw (ν) ] =\n∑ ν∈M w (ν) ∑ e∈E ξ (e) [ ν (e) ξ (e) logw (ν|e)− logw (ν) ]\n= ∑ ν∈M w (ν) [∑ e∈E ν (e) logw (ν|e)− logw (ν) ]\n= ∑ ν∈M w (ν) [∑ e∈E ν (e) logw (ν|e)− ∑ e∈E ν (e) logw (ν) ]\n= ∑ ν∈M w (ν) [∑ e∈E ν (e) log w (ν|e) w (ν) ]\n= ∑ ν∈M w (ν) [∑ e∈E ν (e) log ν (e) ξ (e) ] =\n∑ ν∈M w (ν) KL (ν‖ξ) .\nThus, by maximizing the ξ-expected information gain, one maximizes the belief-\nweighted Kullback-Leibler divergence between ν and ξ.\nDefinition 15 (Kullback-Leibler-KSA; Orseau, 2014). The KL-KSA is the Bayesian agent with\nuKL (æ1:t) = Ent (w (·|æ<t))− Ent (w (·|æ1:t)) . (2.14)\nNotice that the first term doesn’t depend on et, so at any given time step it is fixed by the agent’s past history. The term that matters is the second one, which is the negative entropy of the posterior beliefs, after updating on percept et. Intuitively, the agent gets reward from reducing the entropy (uncertainty) in its beliefs; it seeks out experiences et that will make it more certain about the world, and won’t be satisfied until entropy is minimal – that is, when its beliefs converge to the truth such that wν = I [ν = µ] and Ent (w) = 0. In the most general environment classes, this convergenge won’t be possible, as there are many environments that are indistinguishable on-policy; in other words, there will always be hypotheses that the agent can’t falsify. An example of this is the so-called blue emeralds hypothesis: ‘Emeralds are green, but after next Tuesday, they will become blue’.\nTheorem 4 (Orseau, 2014). KL-KSA is asymptotically optimal with respect to uKL in general environments.\nSo much for Kullback-Leibler KSA. There are other ways to construct a knowledgeseeker. To elucidate this, notice that the entropy in the Bayesian mixture ξ can be decomposed into contributions from uncertainty in the agent’s beliefs wν and noise in the\n§2.3 General Reinforcement Learning 23\nenvironment ν. That is, given a mixture ξ and for some percept e such that 0 < ξ (e) < 1, and suppressing the history æ<tat for clarity,\nξ (e) = ∑ ν∈M uncertainty︷︸︸︷ wν ν (e)︸︷︷︸\nnoise\n.\nThat is, if 0 < wν < 1, we say the agent is uncertain about whether hypothesis ν is true (assuming there is exactly one µ ∈ M that is the truth). On the other hand, if 0 < ν (e) < 1 we say that the environment ν is noisy or stochastic. If we restrict ourselves to deterministic environments such that ν (e) ∈ {0, 1} ∀ν ∀e, then ξ (·) ∈ (0, 1) implies that wν ∈ (0, 1) for at least one ν ∈ M. This motivates us to define two agents that seek out percepts to which the mixture ξ assigns low probability; in deterministic environments, these will behave like knowledge-seekers.\nDefinition 16 (Square-KSA; Orseau, 2011). The Square-KSA is the Bayesian agent with utility function given by\nuSquare (et|æ<t) = −ξ (et|æ<t) . (2.15)\nDefinition 17 (Shannon-KSA; Orseau, 2011). The Shannon-KSA is the Bayesian agent with utility function given by\nuShannon (et|æ<t) = − log (ξ (et|æ<t)) . (2.16)\nTheorem 5 (Orseau, 2014). Square-KSA and Shannon-KSA are strongly asymptotically optimal with respect to uSquare and uShannon respectively, in deterministic environments.\nThey are named ‘Square’ and ‘Shannon’, since in taking ξ-expectation of the utility\nfunctions we get Eξ [uSquare (·|æ<t)] = − ∑ et∈E [ξ (et|æ<t)]2\nEξ [uShannon (·|æ<t)] = − ∑ et∈E ξ (et|æ<t) log ξ (et|æ<t)\n= Ent (ξ) .\nThese are entropy-seeking agents, since they seek to maximize the discounted sum of expected utilities, which in both cases are entropies. Note from Figure 2.4 that uSquare and uShannon are approximately the same (up to an irrelevant additive constant) over the range [0.5, 1]. Their behaviors become significantly different for ξ → 0: Shannon-KSA loves rare events, and the rarer the better; uShannon is unbounded from above on the interval as ξ → 0. The Shannon-KSA, with its expected utility being measured in bits, is closely related to Schmidhuber’s ‘curiosity learning’, which gets utility from making compression progress (Schmidhuber, 1991).\nSquare- and Shannon-KSA both fail in general for stochastic environments. We can see this by constructing an environment adversarially to ‘trap’ these agents and stop them from exploring: just introduce a noise generator that is sufficiently rich (i.e. is sampled from a uniform distribution over a sufficiently large alphabet of percepts) so that the probability of any single percept is low enough that it swamps the utility gained from"
    }, {
      "heading" : "24 Background",
      "text" : "exploring the rest of the world and gaining information. Thus, we can get the Square and Shannon KSAs ‘hooked on noise’ – they would be endlessly fascinated with a white noise generator such as a detuned television, and would never get sick of watching the random, low-probability events. We construct an experiment to explore this property of KSA agents in Chapter 4."
    }, {
      "heading" : "2.3.3 BayesExp",
      "text" : "The idea behind the BayesExp agent is simple. Given that KL-KSA is effective at exploring, and AIξ is effective (by construction) at exploiting the agent’s beliefs as they stand: why not combine the two in some way? The algorithm for running BayesExp is simple: run AIξ by computing the ξ-optimal policy as normal, but at all times compute the value of the information-seeking policy πKSA. If the expected information gain (up to some horizon) exceeds some threshold , run the knowledge-seeking policy for an effective horizon. This combines the best of AIξ and KSA, by going on bursts of exploration when the agent’s beliefs suggest that the time is right to do so; thus, BayesExp breaks out of the sub-optimal exploration strategy of Bayes, but without resorting to ugly heuristics such as -greedy. Crucially, it explores infinitely often, which is necessary for asymptotic optimality (Leike, 2016b).\nEssentially, the BayesExp agent keeps track of two value functions: the Bayes-optimal value V ∗ξ , and the ξ-expected information gain value V ∗ ξ,IG, which we obtain by substituting Equation (2.14) into Equation (2.12). It then checks whether V ∗ ξ,IG exceeds some threshold, εt. If it does, then it will explore for an effective horizon Ht(εt), and otherwise it will exploit using the Bayes-optimal policy π∗ξ . See Algorithm 2.1 for the formal algorithm.\nTheorem 6 (Lattimore, 2013). With a finite prior w and a non-increasing exploration schedule ε1, ε2, . . . , with limt→∞ εt = 0, BayesExp is asymptotically optimal in general environments."
    }, {
      "heading" : "2.3.4 MDL Agent",
      "text" : "While AIXI uses the principle of Epicurus to mix over all consistent environments, the minimum description length (MDL) agent greedily picks the simplest unfalsified environ-\n§2.3 General Reinforcement Learning 25\nAlgorithm 2.1 BayesExp (Lattimore, 2013) Inputs: Model class M .= {ν1, . . . , νK}; w : M → (0, 1); exploration schedule {ε1, ε2, . . . }.\n1: t← 1 2: loop 3: d← Ht (εt) 4: if V ∗ξ,IG (æ<t) > εt then 5: for i = 1→ d do 6: act ( π?,IGξ\n) 7: end for 8: else 9: act ( π?ξ\n) 10: end if 11: end loop\nment in its model class and behaves optimally with respect to that environment until it falsifies it. In other words, the policy is given by\nπMDL = arg max a\nV ∗ρ ,\nwhere\nρ = arg min ν∈M : wν>0 K (ν) .\nHere, the Kolmogorov complexity K plays the role of a strongly weighted regularizer. That is, MDL chooses the policy that is optimal with respect to the simplest unfalsified environment. This algorithm will fail in stochastic environments, since there will exist environments which cannot be falsified (in the strict sense, i.e. wν = 0) by any percept – for example, an environment in which the agent receives a video feed which is (even slightly) noisy.\nAlgorithm 2.2 MDL Agent (Lattimore and Hutter, 2011) Inputs: Model class M; prior w : M→ (0, 1]; a total ordering over M.\n1: loop 2: Select ρ← min M 3: repeat 4: act ( π?ρ ) 5: until ρ (e<t) = 0 6: end loop"
    }, {
      "heading" : "2.3.5 Thompson Sampling",
      "text" : "Thompson sampling is a very common Bayesian sampling technique, named for Thompson (1933). In the context of general reinforcement learning, it can be used as another attempt at solving the exploration problems of AIξ. Informally, the idea is to use the ρ-optimal policy for an effective horizon, before re-sampling from the posterior ρ ∼ w (·|æ<t) and repeating – at all times, the agent updates it posterior as usual. This commits the agent to a single hypothesis for a significant amount of time; one can think of it as testing likely"
    }, {
      "heading" : "26 Background",
      "text" : "hypothesis one at a time. See Algorithm 2.3 for the formal description of the Thompson sampling policy πT .\nTheorem 7 (Leike et al., 2016). Thompson sampling is asymptotically optimal in mean in general environments.\nAlgorithm 2.3 Thompson Sampling (Leike et al., 2016) Inputs: Model class M; prior w : M→ (0, 1]; exploration schedule { 1, 2, . . . }.\n1: t← 1 2: loop 3: Sample ρ ∼ w (·|æ<t) 4: d← Ht ( t) 5: for i = 1→ d do 6: act ( π?ρ ) 7: end for 8: end loop\nSo much for our GRL agents. We now discuss how to compute the policy π∗ρ for general environments ρ, discount functions γ, and utility functions u. This general-purpose planning algorithm will be used to select actions for all of the agents above."
    }, {
      "heading" : "2.4 Planning",
      "text" : "We have discussed how Bayesian agents maintain a model of their environment, and update their models based on the percepts they receive. Of course, the other major aspect of artificial intelligence, distinct from learning, is acting. Recall that, if the environment is known, computing the optimal policy becomes a planning problem. In general (stochastic) environments, this involves computing the optimal value V ∗µ , which is the expectimax expression from Equation (2.9). In practice, with finite compute power, we must of course approximate this expectimax calculation up to some finite horizon m. For finite-state Markov decision processes with known transitions and rewards, and under geometric discounting, we can compute this by a simple dynamic programming algorithm called Value Iteration (section 2.4.1). For more general environments, we must approximate it by ‘brute force’, with Monte Carlo sampling (section 2.4.2)."
    }, {
      "heading" : "2.4.1 Value iteration",
      "text" : "In a finite-state MDP, if the state transitions P (s′|s, a) and reward function R (s, a) are known, then we can plan ahead by value iteration:\nVn+1 (s) = max a∈A Qn (s, a) , (2.17)\nwith\nQn (s, a) = R (s, a) + γ ∑ s′∈S P ( s′|s, a ) Vn ( s′ ) . (2.18)\nThis is as a dynamic programming algorithm, and is known that value iteration con-\n§2.4 Planning 27\nverges to the value of the optimal policy (Sutton and Barto, 1998):\nlim n→∞\nVn (s) = V ∗ (s) ∀s ∈ S.\nPlanning by value iteration relies heavily on two strong assumptions: the finite-state MDP assumption, and geometric discounting. We wish to be able to lift these assumptions for the purpose of our experiments in GRL, so we move our attention now to planning by Monte Carlo techniques."
    }, {
      "heading" : "2.4.2 MCTS",
      "text" : "Monte Carlo tree search (MCTS) is a general technique for approximating an expectimax calculation in stochastic games and deterministic games with uncertainty. Its use dates back several decades, but was popularized and formalized in the last decade or so in the context of planning for computer Go (Browne et al., 2012). Analogously to minimax (Russell and Norvig, 2010), we construct a game tree, with Max (the agent) playing one turn, and Environment (some distribution over percepts) playing the other turn. The branching factor of Max nodes is of course |A|, while the branching factor of Environment nodes is upper bounded by |E|. In contrast to minimax, which is used for deterministic games, we must collect sufficient samples from Environment nodes to get a good estimator V̂ of the expected value for this node. Needless to say, we wish to avoid expanding the tree out by naively visiting every history æt:m.\nAnalogously to α-β pruning in the context of minimax, UCT is a MCTS algorithm due to Kocsis and Szepesvári (2006) that avoids expanding the whole tree, by only investigating ‘promising’-looking histories. These choices must be made under uncertainty, since the environment is stochastic; hence, we have an instance of the classic exploration-exploitation dilemma. The UCT algorithm adapts and generalizes the famous UCB1 algorithm used in the context of bandits (Auer et al., 2002), to balance exploration and exploitation in the search tree.\nUCB stands for ‘upper confidence-bound’, and is a formal version of the principle of optimism under uncertainty. The general idea is to add an ‘exploration-bonus’ term to the action selection objective which prefers actions that haven’t been tried much. In the context of bandits, the UCB action selection is given by\naUCB = arg max a∈A\n( R̂ (a) + C √ log T\nN (a)\n) , (2.19)\nwhere R̂ (a) is the current estimator of the mean reward that results taking action a, T is the lifetime of the agent, N (a) is the number of times that a has been taken, and C > 0 is a tunable parameter. This exploration bonus allows us to make a good trade-off between exploration and exploitation. Consider the exploration bonus term in Equation (2.19) above: by the central limit theorem, we can use the fact that the variance in our estimate of the mean will be approximately bounded by 1√ N . The log T term in the numerator ensures that, asymptotically, we continue to visit every state-action pair infinitely often; this is necessary to establish regret bounds (Auer et al., 2009). Thus, Equation (2.19) captures the concept of ‘exploration under uncertainty’ in a principled way; UCT adapts this to the Monte Carlo tree search planning setting, in Markov decision processes (MDPs).\nWhile UCT is sufficient for planning in unknown MDPs, we need to generalize to histo-"
    }, {
      "heading" : "28 Background",
      "text" : "ries for planning in general environments. Veness et al. (2011) present this generalization, ρUCT, in their famous MC-AIXI-CTW implementation paper, based on earlier work in Monte Carlo planning on partially-observable MDPs (Silver and Veness, 2010). Using this algorithm, we don’t need to know the state transitions as is required for value iteration (Equation (2.18)); we instead only need some black-box environment model ρ. The ρUCT action-selection within each decision node of the tree search is given by\naUCT = arg max a∈A\n( 1\nm (β − α) V̂ (æ<ta) + C\n√ log T (æ<t)\nT (æ<ta)\n) , (2.20)\nwhere β − α is the reward range, and m is the planning horizon; together they are used to normalize the mean value estimate V̂ for the history under consideration, æ<ta. As in Equation (2.19), C is a positive parameter which controls how much we weight the exploration bonus. The exploration bonus itself is of a similar form, although note that we use log T (æ<t) in the numerator, i.e. the logarithm of the number of times we’ve visited the current history node.\nNotice that, in contrast to so-called ‘model-free’ methods such as Q-learning, our GRL agents can’t memorize or cache the value function in general; this is because we can only compute the value of a history and not of states, because of the weakness of our modelling assumptions. Clearly we can never visit any history æ1:t more than once, so memorization is useless. For this reason, in general our agent has to re-compute the value at each time step t, so as to plan its next action at. Hence, all of our model-based (Bayesian) agents must plan at each time step by forward simulation with ρUCT Monte Carlo tree search. As we will see in Section 3.7, this is the major computational bottleneck for our GRL agents. Moreover, planning with MCTS requires us to have finite (and, ideally, small) action and percept spaces. In Section 3.5, we discuss our implementation of ρUCT, along with some subtle emergent issues.\nIn Algorithm 2.4, we present a (slightly expanded, for clarity) version of the ρUCT\nalgorithm due to Veness et al. (2011).\n§2.4 Planning 29\nAlgorithm 2.4 ρUCT (Veness et al., 2011).\nInputs: History h; Search horizon m; Samples budget κ; Model ρ 1: Initialize (Ψ) 2: nsamples ← 0 3: repeat 4: ρ′ ← ρ.Copy() 5: Sample (Ψ, h,m) 6: ρ← ρ′ 7: until nsamples = κ\n8: return arg maxa∈A V̂Ψ(a)\n9: function Sample(Ψ, h,m) 10: if m = 0 then 11: return 0 12: else if Ψ(h) is a chance node then 13: ρ.Perform(a) 14: e = (o, r)← ρ.GeneratePercept() 15: ρ.Update(a, e) 16: if T (he) = 0 then 17: Create chance node Ψ(he) 18: end if 19: reward ← e.reward + Sample (Ψ, he,m− 1) 20: else if T (h) = 0 then 21: reward ← Rollout(h,m) 22: else 23: a← SelectAction (Ψ, h) 24: end if 25: V̂ (h)← 1T (h)+1 ( reward + T (h)V̂ (h)\n) 26: T (h)← T (h) + 1 27: end function\n28: function SelectAction(Ψ, h) 29: U = {a ∈ A : T (ha) = 0} 30: if U 6= ∅ then 31: Pick a ∈ U uniformly at random 32: Create node Ψ(ha) 33: return a 34: else 35: return arg maxa∈A { 1 m(β−α) V̂ (ha) + C √ log(T (h)) T (ha)\n} 36: end if 37: end function\n38: function Rollout(h,m) 39: reward ← 0 40: for i = 1 to m do 41: a ∼ πrollout(h) 42: e = (o, r) ∼ ρ(e|ha) 43: reward ← reward +r 44: h← hae 45: end for 46: return reward 47: end function"
    }, {
      "heading" : "30 Background",
      "text" : ""
    }, {
      "heading" : "2.5 Remarks",
      "text" : "We now conclude with a short summary, and some remarks.\nIn this chapter, we presented the problem of general reinforcement learning, in which the goal is to construct an agent that is able to learn an optimal policy in a broad class of (partially observable and non-ergodic) environments. We have presented the current state-of-the-art GRL agents and algorithms, namely AIξ, Thompson sampling, MDL, Square-, Shannon-, and Kullback-Leibler-KSA, and BayesExp, under a unified notation, and we have discussed the ideas and algorithms that allow these agents to learn and plan. These agents, and the analysis and formalism around them, represent our best theoretical understanding of rationality and intelligence in this general setting. In the subsequent two chapters, we present our software implementation of these agents, and some experiments we run on them.\nChapter 3\nImplementation1\nThere are no surprising facts, only models that are surprised by facts; if a\nmodel is surprised by the facts, it is no credit to that model.\nWe now present the design and implementation of the open-source software demo, AIXIjs.2 Our implementation can be decomposed into roughly five major components, or modules, which we discuss in this chapter:\n• Agents. We implement the agents specified in Chapter 2. Some of them differ by one line of code; for example, the KSA agents can be built from AIξ by simply\nreplacing its utility function. We document the agent implementation in Section 3.2.\n• Environments. We design and implement environments to showcase the various agents, including a partially observable Gridworld, and a ‘chain’ MDP environment;\nboth are documented in Section 3.3.3\n• Models. For our Bayesian agents, we design and implement two model classes with which they can learn the Gridworld environment,Mloc andMDirichlet. These are presented in Section 3.4.\n• Planners. We implement value iteration and ρUCT Monte Carlo tree search, which were presented in Section 2.4. We make some implementation-specific remarks in\nSection 3.5.\n• Visualization and user interface. We design and implement a user interface that allows the user to choose demos, read background and demo-specific information,\ntune parameters, and run experiments. We also present a graphic visualization for showing the agent-environment interaction, and for plotting the agent’s performance; this is presented in Section 3.6.\nFirst, we briefly discuss the software tools we used to implement the project.\n1AIXIjs was implemented in collaboration with Sean Lamont, a second-year undergraduate student at the ANU. Sean wrote many of the visualizations under my supervision; the rest of the implementation is my own work. More detailed contribution information (including commit history) can be found at https://github.com/aslanides/aixijs/graphs/contributors.\n2The demo can be run at http://aslanides.github.io/aixijs; all supporting source code can be found at http://github.com/aslanides/aixijs. We encourage the reader to interact with the demo, though again, we strongly recommend using Google Chrome, as the software was not tested on other browsers, for reasons detailed in Section 3.1.\n3We also implement multi-armed bandits, generic finite-state MDPs, and iterated prisoner’s dilemma, but we don’t document them here as they don’t play a prominent role in the demos or experiments.\n31"
    }, {
      "heading" : "32 Implementation",
      "text" : ""
    }, {
      "heading" : "3.1 JavaScript web demo",
      "text" : "We implement AIXIjs as a static web site. That is, apart from web hosting for the .html and .js source code and other site assets, there is no back-end server required to run the software; the demo runs natively, and locally, in the user’s web browser. All of the agent-environment simulations are implemented in modern JavaScript (ECMAScript 2015 specification), with minimal use of external libraries. This allows us to effectively build a lightweight4 and portable software suite, which a modern web browser can run without the need for specialized dependencies such as compilers or scientific libraries.\nJavaScript (JS) is a high-level, dynamic, and weakly-typed language typically used to create dynamic content on websites. Google’s V8 JS engine, implemented in their Chrome web browser, provides a fast JS runtime; in many benchmarks, it is significantly faster than Python 3.5 As we discussed in the Introduction, this allows for computationally intensive and visually impressive software. JavaScript, however, does have several shortcomings. The ones that are relevant to us are:\n• JavaScript is a notoriously6 weakly-typed language, which comes with all the programming pitfalls and runtime errors one would expect. For example, functions will\nsilently accept arguments that are null, and attempt to perform computations on them. In this way, subtle bugs can cause catastrophic runtime errors that can propagate quite far without being caught. We mitigate this to some extent by writing tests using the QUnit testing framework, and by frequently using the built-in debugger in Google Chrome.\n• JavaScript implementations differ between browsers. For example, some features of the ECMAScript 2015 specification (for example, anonymous functions) were not\nyet implemented by the latest version of the Safari web browser as of September 2016. Worse still, behaviors can differ subtly and in undocumented ways between browser implementations. We (unfortunately) are forced to work around this by only supporting recent versions of Google Chrome,7 and discouraging usage on other web browsers.\nWe use standard web frameworks and libraries: jQuery and Bootstrap for presentation; d3js for graphics and visualizations; marked for MarkDown parsing, and MathJax for rendering mathematics in the browser. Our implementation totals roughly 6000 lines of JavaScript.\nWe make use of a modular design, and use class inheritance frequently, so as to minimize code duplication and to leverage the conceptual connections between objects. In the sections that follow, we occasionally use simple UML diagrams to document these classes. Note that in these diagrams we use type annotations, for expository purposes.\n4Including all source code, external libraries, fonts, text, and image assets, the software totals less than 2 megabytes in size, uncompressed.\n5For inter-language comparisons on common benchmarks, see, for example, http://benchmarksgame.alioth.debian.org/u64q/compare.php?lang=node&lang2=python3.\n6The author highly recommends a brilliant four-minute video by Gary Bernhardt about the nonsense that comes from JavaScript’s (lack of) type system: https://www.destroyallsoftware.com/talks/wat.\n7The software was last tested on Google Chrome version 54.0.\n§3.2 Agents 33"
    }, {
      "heading" : "3.2 Agents",
      "text" : "All agents inherit from the base Agent class. Every agent’s constructor takes an Options object as input, which allows us to pass in default and user-specified options. See Figure 3.2 for the full agent class inheritance tree. The Agent base class specifies the methods\n• Update (a, e). Update the agent’s model of the environment, given that it just performed action a ∈ A and received percept e ∈ E from the environment.\n• SelectAction(). Compute, and sample from, the agent’s (in general, stochastic) policy π (a|æ<t), returning an action a ∈ A.\n• Utility (e). This is the agent’s utility function, as defined in Definition 14. For reward-based reinforcement learners it simply extracts the reward component from\npercept.\nEvery agent is further equipped with a Discount function, as defined in section 2.2.2.\nEvery Bayesian agent (i.e. of class BayesAgent or one of its descendants) is further composed of a Model and a Planner, which are both central to its operation. When we call Update (a, e) on BayesAgent, it saves its model’s state8, calls the model’s Update (a, e) method, and then computes and stores the information gain (defined in Equation (2.13)) between the old and new model states. When we call SelectAction, the agent passes its model to the Planner, and waits for it to compute a best action. If the information gain from the previous action was non-zero, the planner’s internal state is reset; otherwise, we prune the search tree but keep the partial result; see Section 3.5 for more discussion regarding the planner.\nThe other agents inherit from BayesAgent, and differ from it in straightforward and transparent ways specified by their respective definitions (Definition 16, Definition 17, Definition 15, Algorithm 2.1, Algorithm 2.2, and Algorithm 2.3). We won’t reproduce their source code here; the interested reader can find the code in the src/agents/ directory in the GitHub repository.\n8As we shall see in Section 3.4, models must behave like environments. Definition 2 implies that they must therefore in general maintain some internal state s that is changed by actions a ∈ A."
    }, {
      "heading" : "34 Implementation",
      "text" : ""
    }, {
      "heading" : "3.2.1 Approximations",
      "text" : "We now enumerate and justify our use of several approximations and simplifications in our agent implementations. The first two approximations are motivated by computational considerations. In both cases, we argue that our use of these simplifications leaves the agent’s policy invariant. The third simplification is in fact forced upon us; it is inconvenient and potentially highly consequential to the performance of Shannon-KSA.\n• Information gain. Recall from Chapter 2 that the information gain for a Bayesian agent given a history æ<t is\nIG (e|æ<tat) . = Ent (w (·|æ<t))− Ent (w (·|æ1:t)) ,\nand recall that this is the utility function of the Kullback-Leibler knowledge-seeking agent (KL-KSA). Now, when computing the KL-KSA policy at time t – that is, in calls to SelectAction – we compute the value V π,IGξ of various potential histories æ<tatetat+1et+1 . . . at+met+m, and select the action that maximizes this value. Note that our action-selection doesn’t depend on the absolute value of different histories, but only on their relative value. Note also that the quantity Ent (w (·|æ<t)) does not depend on future actions or percepts, as it is determined by events in the agent’s past. Hence it is a constant that we can ignore when comparing the relative value of future actions. From the definition of entropy (Equation (2.4)), we see that computing the entropy of the posterior w (ν|·) requires O (|M|) operations. For this reason, in our implementation of the KL-KSA, we achieve a 2× speedup by replacing uKL with the surrogate utility function\nu′KL (æ1:t) = −Ent (w (·|æ1:t)) .\n• Effective horizon. Recall from Algorithm 2.3 and Algorithm 2.1 that the Thompson sampling and BayesExp agents both explore for an effective horizon Htγ (ε)\n(Equation (2.6)); this requirement is, in fact, essential to the proofs of their asymptotic optimality. However, computing the effective horizon exactly for general discount functions is not possible in general, although approximate effective horizons have been derived for some common choices of γ (Lattimore, 2013; Table 2.1). Moreover, in practice, due to the computational demands of planning with MCTS (Al-\n§3.2 Agents 35\ngorithm 2.4), we are forced to plan only with a relatively short horizon m; for most discount functions γ and realistic ε,9 the true effective horizon Htγ (ε) is significantly greater than m. For this reason, and for simplicity and ease of computation, we use the MCTS planning horizon m as a surrogate for Htγ . Naturally, this choice affects the agent’s policy, but no more so than we already have by using MCTS to plan up to some (finite, time-constrained, and pragmaticaly chosen) horizon m rather than to infinity, as the agents do in the theoretical formalism.\n• Utility bounds. Recall from the ρUCT action selection algorithm (Equation (2.20)) that the value estimator V̂ (æ1:t) is normalized by a factor of m (β − α), where m is the MCTS planning horizon, and α and β are the minimum and maximum\nrewards that the agent can receive in any given percept. In the case of reward-based reinforcement learners, α and β are essentially metadata provided to the agent, along with the size of the action space |A|, at the beginning of the agent-environment interaction. For utility-based agents, however, the rewards are generated internally, and so the agent must calculate for itself what range of utilities it expects to see, so as to correctly normalize its value function for the purposes of planning.\nThankfully, for the Square and Kullback-Leibler KSAs, this is relatively easy to do. Since uSquare (e) = −ξ (e), we can immediately bound its utilities in the range [−1, 0]. In general this won’t be a tight bound, since there exist environment mixtures in which every percept is in some smaller range, i.e. ξ (·) ∈ [a, b] with a > −1 and b < 0,10 but in practice, and in particular for our model classes, it is effectively a tight bound.\nIn the case of the Kullback-Leibler KSA, recall that uKL (e) = Ent (w (·)) − Ent (w (·|e)) . If we assume that we are given the maximum-entropy (i.e. uniform) prior w (·| ), then clearly uKL (e) ≤ Ent (w (·| )) ∀e ∈ E , since entropy is always non-negative. Hence we have 0 ≤ uKL ≤ Ent (w (·| )), i.e. the KL-KSA’s rewards are bounded from above by the entropy of its prior (assuming a uniform prior), and from below by zero.\nFinally, we come to the problematic case: from Figure 2.4, we know that uShannon (e) = − log ξ (e) is unbounded from above as ξ → 0. This means that unless the agent can a priori place lower bounds on the probability that its model ξ will assign to an arbitrary percept e ∈ E , it cannot upper bound its utility function and therefore cannot normalize its value function correctly. This is problematic for us, especially as our environments and models are constructed in such a way as to allow arbitrarily small probabilities, as we will see in Section 3.3 and Section 3.4.\nUnfortunately, it seems we’re stuck here. We’re forced to make an ugly, arbitrary choice to upper bound the Shannon agent’s utility function with, so as to normalize its value function. If we choose the upper bound β too high, then the V̂ term in Equation (2.20) will be artificially, but consistently small; this is equivalent to inflating the exploration bonus constant C by roughly a constant multiplicative factor (which is itself upper bounded by some function of β). If β is chosen too small, however, we can run into much bigger problems, since now V̂ can be overinflated by an unboundedly large multiplicative factor. If Shannon KSA sees a very\n9Recall that in the case of BayesExp, ε is compared to the value of the knowledge-seeking policy, V ∗,IGξ . 10For example, a coinflip environment in which the agent is trying to falsify one of two hypotheses:\nwhether a coin is fair (ν (·) = 0.5) or bent (ν (·) 6= 0.5)."
    }, {
      "heading" : "36 Implementation",
      "text" : "improbable percept, its value estimates will blow up, which will cause suboptimal plan selection, since the V̂ will overwhelm the exploration bonus term in Equation (2.20). We are forced to choose a β, so we use a very large upper bound, β = 103 in an attempt to balance this trade-off, but bias it in favor of overestimating β. For us to exceed −103 in log2 probability requires us to assign a probability of ξ (e) ≤ 2−103 = 10−301, which is approaching the limits of numerical precision in JavaScript. With this setting of β we are unlikely to blow up our value estimate, although we will be severely inflating the UCB constant. As we will see in Chapter 4, this is quite possibly the cause of some suboptimal behavior in the Shannon KSA."
    }, {
      "heading" : "3.3 Environments",
      "text" : "Recall that AIXI and its variants are theoretical models of unbounded rationality, not practical algorithms. Bayesian learning and planning by forward simulation with Monte Carlo tree search are both very computationally demanding, so we restrict ourselves to demonstrating their properties on small-scale POMDPs and MDPs.\nAnalogously to the case of agents, all environments inherit from the base Environment class. Every environment’s constructor takes an Options object as input, which allows us to pass in default and user-specified options. The Environment base class specifies the methods\n• ConditionalDistribution(e). Returns the probability ν (et|ae<tat) that the environment assigns to percept e given its current state resulting from the history\næ<tat.\n• GeneratePercept(). Sample from ν (e) and returns a percept e ∈ E .\n• Perform(a). Take in action a ∈ A and mutate the environment’s (in general, hidden) state according to its dynamics.\n• Save() and Load(). These functions save and load the environment’s internal state. This is a convenience for our Bayesian agents; it allows them to reset the envi-\nronments ν ∈ M that make up their mixture model, after running counterfactual simulations in a planner.\nWe now introduce the Gridworld and Chain environments. The interested reader can find the source code to these, and other, environments in the src/environments/directory in the GitHub repository."
    }, {
      "heading" : "3.3.1 Gridworld",
      "text" : "Our gridworld consists of an N ×N array of tiles. There are four types of tiles: Empty, Wall, Dispenser, and Trap, with the following properties:\n• Empty tiles allow the agent to pass, albiet while incurring a small movement penalty rEmpty.\n• Wall tiles are not traversable. If the agent walks into a wall, it incurs a negative penalty rWall < rEmpty.\n§3.3 Environments 37\n• Dispenser tiles behave like Empty tiles as far as movement and observations are concerned, but they dispense some large reward rCake rEmpty with probability θ, and rEmpty otherwise; that is, all Dispensers are (scaled) Bernouilli (θ) processes. 11\n• Trap tiles, as the name suggests, don’t allow you to leave. Moreover, once stuck in a trap, the agent will receive rWall reward constantly.\nThe gridworld we construct is a POMDP; the environment’s hidden state is the agent’s grid position s = (i, j) and the positions of all walls, traps, and dispensers. Observations consist of a bitstring telling the agent whether the adjacent squares in the {←,→, ↑, ↓} directions are Walls or not; the edges of the Gridworld are treated implicitly as walls. The agent can move in these four cardinal directions, or stand still (this is the so-called ‘no-op’, which we denote by ©). The only way to distinguish a Dispenser from an Empty tile is to walk onto it and observe the (in general, stochastic) reward signal; for low values of θ, it may take some time for a Dispenser to reveal itself. The only way to distinguish a Trap from an Empty tile is to walk onto it and see if you get trapped or not. Hence, we can characterize the action and percept spaces as\nA = {←,→, ↑, ↓,©} E = B4 × {rWall, rEmpty, rCake} .\nMovement and observations are all deterministic; the only stochasticity in this envi-\nronment arises from the reward signal from the dispenser(s).\nFor the purposes of our demos and experiments, we generate random gridworlds by independently and randomly assigning each tile to one of the four classes, with a strong bias towards being Empty, and a slighter weaker bias towards being a Wall. The agent’s\n11The AIXIjs agent mascot is Roger the Robot . Roger likes Cake, and will do anything it takes to get near a Cake Dispenser."
    }, {
      "heading" : "38 Implementation",
      "text" : "starting position is always the top left corner, at tile (0, 0). We ensure that the gridworld is solvable by ensuring there is at least one dispenser, and by running a breadth-first-search to check whether there is a viable path from the agent’s starting position to the dispenser with the highest pay-out frequency, θ.\nThis gridworld environment is sufficiently rich and interesting to demonstrate most of what we seek to show: the agents have to reason under uncertainty to navigate the maze and find the (best) dispenser, while avoiding traps. We report on numerous experiments using this environment in Chapter 4."
    }, {
      "heading" : "3.3.2 Chain environment",
      "text" : "We present a deterministic version of the chain environment of Strens (2000). The chain environment is a deterministic finite-state Markov decision process. The action space is A = {→, 99K}, and the state space is |S| = N + 1, for some integer N ≥ 1. The reward space is {r0, ri, rb} with r0 < ri rb; example values are (r0, ri, rb) = (0, 5, 100), with N = 6. From Figure 3.5, we can see that at all times, the agent is tempted to reap immediate reward of ri by taking the → action, which puts it in the initial state, losing whatever progress it was making towards getting to sN , from which state it can take 99K, which isn’t immediately as rewarding as →, but eventually leads to a very large payoff rb ri. For N < rbri , the optimal policy is to always take 99K so as to perform the circuit s1 → s2 → · · · → sN → s1 → . . . and accumulate an average reward of rbN . Otherwise, the optimal policy is to always take → and remain in the initial state. We denote these two policies as π99K and π→.\nThe (deterministic) state transition matrix is given by\nP ( s′|s,→ ) = I [ s′ = 0 ] P ( s′|s, ) = I [ s′ = (s+ 1) mod (N + 1) ] ,\nand the rewards are given by\nR (s, a) = riI [a =→] + rbI [a =99K] I [s = N + 1] .\n§3.4 Models 39\nWe construct this environment with N < rbri , so as to present a test of an agent’s far-sightedness. To stay on the optimal policy π99K, the agent must at all times resist the temptation to take the greedy action → which results in the instant gratification ri, as this causes it to lose its progress towards the ‘goal’ state sN . This simple environment models a classic situation from economics and decision theory in which humans have been known to be time-inconsistent – that is, informally, an agent acts impulsively on desires that don’t agree with its long-term preferences (Hoch and Loewenstein, 1991). We report on experiments using this environment in Chapter 4."
    }, {
      "heading" : "3.4 Models",
      "text" : "As we have seen in Chapter 2, the GRL agents we are concerned with are model-based and Bayesian. In this section we describe the generic BayesMixture model, which provides a wrapper around any model class M, represented as an array of Environments, and allows us to compute the Bayes mixture of Equation (2.10). We then describe a model class for Gridworlds that we plug in to this BayesMixture, and a separate Dirichlet model.\nThe BayesMixture model provides us with a mechanism with which to use any array of hypotheses ( ν1, ν2, . . . , ν|M| ) and a prior ( w1, . . . , w|M| ) ∈ [0, 1]|M| as a Bayesian environment model. Note that all environment models must implement the environment interface: namely, they must have Perform, GeneratePercept, and ConditionalDistribution methods. In addition, Bayesian models must have an Update method, to update them with observations (either simulated or real), and Save and Load methods to restore their state after planning simulations. We document these methods in Algorithm 3.1:\n• GeneratePercept: To generate percepts from the mixture model ξ, we sample an environment ρ from the posterior w (·), then generate a percept from ρ; in the context of probabilistic graphical models, this is known as ancestral sampling (Bishop, 2006).\n• Perform(a): We simply perform action a on each member ν of M. • ConditionalDistribution(e): We return ξ (et|æ<tat) = ∑\nν∈Mwνν (et|æ<tat), where the conditioning on the history æ<tat is implicitly taken care of by conditioning on the environment’s internal state s."
    }, {
      "heading" : "40 Implementation",
      "text" : "• Update (a, e): We update our posterior given percept e using Bayes rule: w (ν|e) = w (ν) ν(e)ξ(e) , for each ν ∈M.\nOur objective is to construct a Gridworld model that is sufficiently informed, or constrained, so as to make it possible for the agent to learn to solve the environments we give it within a hundred or so cycles of agent-environment interaction, but that is also sufficiently rich and general so that it is interesting to watch the agent learn. For this reason, we eschew very general and flexible models such as the famous context-tree weighting data compressor used by Veness et al. (2011), since they will take too long to learn the environments for a practical demo. Instead, we construct two models, with varying degrees of domain knowledge built-in:\n1. A mixture model parametrized by dispenser location, which we call Mloc.\n2. A factorized Dirichlet model, in which each tile is represented as an independent\nDirichlet distribution. We call this model MDirichlet.\nThe interested reader can find the source code for these and other models in the src/models/ directory in the GitHub repository."
    }, {
      "heading" : "3.4.1 Mixture model",
      "text" : "Before we present the mixture model Mloc, we consider the problem of constructing a model classM. That is, we want a simple and principled method with which to construct a finite but non-trivial set of hypotheses about the nature of the true Gridworld environment\nµ. We do this by chosing some discrete parametrization D = { d1, . . . , d|M| } such that a model class M is constructed by sweeping through values of d ∈ D:\nξ (e) = ∑ d∈D wdνd (e) .\nOne can think ofD as describing a set of parameters about which the agent is uncertain; all other parameters are held constant, and the agent is fully informed of their value. We now consider and implement three different choices for the parametrization D, and enumerate some of the pros and cons for each.\n§3.4 Models 41\n1. Dispenser location. We construct M by sweeping through all legal (that is, not already occupied by a Wall) dispenser locations, given a fixed maze layout, and fixed\ndispenser frequencies. In other words, we hold constant the layout of all Empty, Wall, and Trap tiles, and vary the location of the dispensers. The agent’s beliefs w (νij) are now interpreted as the agent’s credence that the dispenser is at location (i, j) in the Gridworld.\nThe benefit of this choice of D is that it is straightforward and intuitive: the agent knows the layout of the gridworld and knows its dynamics, but is uncertain about the location of the dispensers, and must explore the world to figure out where they are. This also has the benefit of lending itself easily to visualization of the agent’s beliefs: see Figure 3.7. Moreover, since dispensers are stochastic, it may take several observations to falsify any given hypothesis ν; the model class allows for ‘soft’ falsification. Another advantage of this model class is that it incentivizes the agent to explore, since the agent will initially assign non-zero probability mass to there being a dispenser at every empty tile.\nA significant downside of this model class is that we get a combinatorial explosion if we want to model environments with more than one dispenser. That is, given a maze\nlayout with L legal positions, a model class with M dispensers will have |M| = ( L M ) elements. Another downside is that the agent knows the maze layout ahead of time, which detracts from some of the interest in having a maze on the Gridworld. We present the procedure for generating this model class in Algorithm 3.2.\n2. Agent starting location. We use a similar procedure as described in Algorithm\n3.2 to construct the model class, except this time by parametrizing by the agent’s starting location. In this case, D is given by the set of legal starting positions. This corresponds nicely to the (noise-free) localization problem given a known environment which shows up often in the field of robotics (Thrun et al., 1999). Since observations are deterministic, it is possible to discard many hypotheses at once, and so the agent is able to narrow down its true location very quickly. The Gridworlds we simulate aren’t large or repetitive enough to have sufficiently ambiguous percepts for the agent to be uncertain about its location for more than a few cycles. Thus, after a short time, the agent is certain of its position, and is longer incentivized to explore; if the dispenser isn’t within its planning horizon by this stage, it will not be able to find it, and will perform very badly. We discuss the quirks and limitations of planning more in Section 3.5.\n3. Maze configuration. Perhaps the most general, and hence most interesting, model\nparametrization is by maze configuration: the agent is initially uncertain about the identity of every tile in the Gridworld. Thus, the agent is thrown into a truly unknown gridworld, and must learn the environment layout from scratch. In a sense this is the most natural parametrization, since each gridworld layout gives rise to a truly different environment. Another benefit is that this is a very rich environment class; unfortunately, this is also the downside, as it is prohibitive to naively enumerate every possible maze configuration. Given just two tile classes, Empty and Wall, there are 2N 2\npossible N × N mazes. Using this naive enumeration, we would run out of memory even on a modest 6 × 6 Gridworld, as |M| = 236 ≈ 7 × 1010, and most laptop computers have only of the order of eight gigabytes, or 6.4 × 1010 bits of memory. We can alleviate this somewhat by simply downsampling, say by"
    }, {
      "heading" : "42 Implementation",
      "text" : "discarding at random most of the elements of this gargantuan model class. We find in practice that this runs into similar problems to the second parametrization, and produces demos that are slow (due to the size of the model class; see Section 3.7 for a discussion of time complexity) and uninteresting, because the agent is able to falsify so many hypotheses at once.\nAlgorithm 3.2 Constructing the dispenser-parametrized model class. Inputs: Environment class E and parameters Φ; Gridworld dimensions N . Outputs: Model class M and uniform prior w\n1: w ← Zeros(N2) 2: M← {} 3: k ← 1 4: for i = 1 to N do 5: for j = 1 to N do 6: ν ← Initialize (E,Φ) 7: if ν.Grid[i][j] = Wall then continue 8: end if 9: ν.Grid[i][j]← Dispenser(θ)\n10: M.Push(ν) 11: w[k]← 1 12: k ← k + 1 13: end for 14: end for 15: Normalize(w)\nWe find empirically that Item 1 above makes for the most interesting demos, and so our canonical model class used in many of the Gridworld demos is the dispenser-parametrized\n§3.4 Models 43\nmodel classMloc. We conclude with some remarks about the properties of learning with Mloc that will be consequential to our experiments in Chapter 4:\n• As mentioned above, usingMloc gives the agent complete knowledge a priori of the maze layout. The agent’s task becomes to search the maze for the dispenser. This\ntask incorporates both subjective uncertainty (we typically initialize the agent with a uniform prior over dispenser location) and noise (for θ ∈ (0, 1), the dispensers are stochastic processes).\n• Using Mloc, the agent knows that there is only one dispenser. This means that, regardless of θ, once it does find the dispenser – by experiencing the relevant reward\npercept – it is able to immediately falsify every other hypothesis regarding the location of the dispenser. In other words, its posterior w (·|æ<t) will collapse to the indicator function I [ν = µ], and the agent will have learned everything there is to know about the environment.\nNow, motivated by the limitations of Mloc that we discussed in Item 1, and inspired by the notion of a model that is uncertain about the maze layout (Item 3), we set out to design and implement an alternative Bayesian Gridworld model, MDirichlet."
    }, {
      "heading" : "3.4.2 Factorized Dirichlet model",
      "text" : "We now describe an alternative Gridworld model, which has several desirable properties. In contrast to the naive mixture model, it allows us to efficiently represent uncertainty over the maze layout, as well as the dispenser locations and payout frequencies θ. This means that MDirichlet is, in comparison to Mloc, a relatively unconstrained, and thus harder to learn, model.\nThe basic idea is to model each tile in the Gridworld independently with a categorical distribution over the four possible types of tile: Empty, Wall, Dispenser, and Trap. For an N × N Gridworld, label each of the tiles sij where i, j ∈ {1, . . . , N}. The joint distribution over all Gridworlds s11, . . . , sNN is then given by the product\np (s11, . . . , sNN ) = (N,N)∏ (i,j)=(1,1) p (sij) , (3.1)\nwhere sij ∈ {Empty,Dispenser,Wall,Trap}. Note that here, by Dispenser, we mean a dispenser with θ = 1. This allows us to model dispensers with θ ∈ (0, 1) as a stochastic mixture over an Empty tile and a Dispenser with θ = 1. For example, a dispenser with θ = 0.5 would be represented12 by the distribution p = (0.5, 0.5, 0, 0); a tile known to be a Wall would be represented by the distribution p = (0, 0, 1, 0). We initialize our model with the uniform prior; that is, for each tile sij we have p (sij) = 0.25 ∀sij ∈ {Empty,Dispenser,Wall,Trap}. Now, recall from section 2.1.2 that the Dirichlet distribution is conjugate to the categorical distribution. So, to represent our uncertainty about the relative probabilities of each of the classes, and to enable us to update our beliefs in a Bayesian way, we make use of a Dirichlet distribution over the four-dimensional probability simplex. That is, for each\n12Recall that the categorical distribution is just a distribution over a set of K categories. We represent the distribution with a length-K vector p ∈ [0, 1]K . We use the notation Pr (S = s) ≡ p (s) ≡ ps interchangeably."
    }, {
      "heading" : "44 Implementation",
      "text" : "tile s, the probability vector\np . =  Pr (s = Empty) Pr (s = Dispenser)\nPr (s = Wall) Pr (s = Trap)  is distributed according to\np ∼ Dirichlet (p|α) , where α = [ αEmpty αDispenser αWall αTrap ]T are the empirical counts of each class, and 1Tp = 1. Updates to the posterior are trivial: just increment the corresponding count, i.e. upon seeing one instance of class N , we update with\nDirichlet (p|α1, . . . , αK , N) = Dirichlet (p|α1, . . . , αN + 1, . . . αK) . (3.2)\nNow, given that the agent is at some tile st, the conditional distribution over percepts\net is drawn from the product over the neighbouring Dirichlet tiles: ρ (et|æ<tat) ∼ ∏\ns′∈ne(st)∪{st}\nDirichlet (p|αs′) . (3.3)\nThe astute reader will notice that though the joint distribution over tile states factorizes, percepts will be locally correlated, since percepts are sampled from neighboring tiles, and we have a four-connected grid topology.\nFor the purposes of computational efficiency we make two approximations:\n1. We don’t sample ρ from the Dirichlet distributions Equation (3.3), but instead simply\nuse their mean; recall that the mean of Dirichlet (p|α) is given by\nµ = α∑K k=1 αk .\nWe do this because sampling correctly from the Dirichlet distribution is non-trivial, and this sampling would need to occur whenever we wish to generate a percept, either real and simulated; this is a far too large computational cost to bear for the purposes of our demo. This approximation will effectively reduce the variance in percepts generated by the model, but in mean, over many simulations, will have negligible effect.\n2. When computing the entropy of the agent’s beliefs for the purposes of calculating\nthe information gain (Equation (2.13)), computing the joint entropy over all tiles becomes computationally very expensive, as neighboring tiles are correlated with respect to percepts, and so the entropy of the joint does not decompose nicely into a sum of entropies. We compute a surrogate for the entropy by associating with each tile the mean probability that it assigns to its being a dispenser; that is, for each tile sij we compute\nq (sij) = µ ij Dispenser.\nThat is, for each tile sij we compute its mean µ ij , which is a categorical distribution over {Empty,Dispenser,Wall,Trap}; we then take the Dispenser component.\n§3.4 Models 45\nWe concatenate all the q (sij) together into a vector q̃ of length N 2 and normalize. Thus, the components of q̃ are given by\nq̃ij . = q (sij)∑(N,N)\n(i,j)=(1,1) q (sij) .\nNow, q̃ij is effectively the model’s mean estimate of the probability that the (i, j) th tile is a dispenser; this is now directly analogous to the posterior belief w (ν|. . . ) in the Mloc mixture model, since each environment ν asserts that some unique tile (i, j) is the dispenser. Now, when computing the entropy of the Dirichlet model, we simply return Ent (q̃). This approximation is reasonable, since percepts relating to Walls and Traps are deterministic, and so, once the agent has visited any given tile, the only uncertainty (entropy) its model has is with respect to whether a tile is a Dispenser or Empty. Moreover, for a one-dispenser environment, if the agent visits every tile infinitely often, q̃ij will asymptotically converge to I [(i, j) = (iµ, jµ)] with Ent (q̃) = 0, where (iµ, jµ) is the true dispenser location in environment µ.\nWe emphasize that each tile has its own empirical counts αs′ ; these are learned separately, through observations. Now, in general, as soon as the agent is unsure whether an adjacent tile is a wall or not, it will become uncertain of its position; its posterior over its position will diffuse over the Gridworld as time progresses. This corresponds to the difficult problem known as simultaneous localization and mapping (SLAM), which shows up in robotics (Leonard and Durrant-Whyte, 1991); it is necessary to use a version of the Expectation Maximization (EM) algorithm to simultaneously solve the two inference problems. This is far too difficult a problem to solve in the demo.\nInstead, we choose our prior over each of the α so as to allow the agent to learn immediately whether an adjacent tile is a wall or not. We use the Haldane prior, αk = 0 ∀ k. This has the nice property that it behaves like a uniform prior over the classes {Empty,Wall,Dispenser,Trap}, but in contrast to the more common Laplace prior ak = 1 ∀ k, it also has the property that it allows us to do ‘hard’ updates, in which we move all of the probability mass onto one class in the categorical distribution. That is, given that observations are deterministic and the maze layout doesn’t change, we know that if we see a Wall tile adjacent, then our model should represent the fact that this tile is a Wall with probability one:\nαk = I [k = Wall] =⇒ µk = I [k = Wall] .\nNote that we avoid ‘hard’ updates with respect to whether a tile is Empty or a Dispenser by effectively using a Laplace prior over tiles that we know with certainty aren’t walls; these ‘Laplace’ tiles are easily identifiable as the grey tiles in Figure 3.8; they are tiles that the agent has been adjacent to, but which it hasn’t stepped onto yet:\nα (k|¬Wall) = I [k = Dispenser] + I [k = Empty] . (3.4)\nNote that above we use the shorthand αk ≡ α (k) so as to more conveniently represent conditioning; this is analogous to our writing wν ≡ w (ν) in the case of the mixture model. Using the Laplace prior, and updating with Bayes’ rule normally, yields the famous Laplace rule for binary events. Consider some Gridworld tile s that happens to be Empty. If the agent starts with the Laplace prior given by Equation (3.4) and subsequently visits this"
    }, {
      "heading" : "46 Implementation",
      "text" : "tile n times, then the agent’s posterior belief that s is in fact Empty is simply\nPr (s = Empty) = n+ 1\nn+ 2 , (3.5)\nwhich can easily be seen by applying the Dirichlet posterior update (Equation (3.2)) n times. Thus, the agent asymptotically learns the truth as n → ∞, but for any finite n the model still has some degree of uncertainty.\nThis Dirichlet model has numerous distinct advantages: it allows the agent to discover the grid layout as it explores, represent multiple dispensers, and learn online the Bernoulli parameter θd for any dispenser d, by virtue of maintaining a simple Laplace estimator of the probabilities Pr (d = Empty) and Pr (d = Dispenser). It also makes for an interesting visualization, as we can reveal the Gridworld to the user as the agent discovers it; see Figure 3.8. These advantages essentially stem from modelling each tile independently, and come at the cost of no longer being able to represent our model explicitly as a mixture in the form of Equation (2.10). This precludes the use of MDirichlet in some algorithms, for example Thompson sampling, which requires mixing coefficients wν to sample from. It also comes at a considerable computational cost: as we will see in Section 3.7, this model is more costly to compute than the (much simpler) Bayes mixture ξ. In Chapter 4, we perform numerous experiments using this model class, and contrast it (with respect to agent performance) with the dispenser model class Mloc."
    }, {
      "heading" : "3.5 Planners",
      "text" : "We implement both the value iteration and ρUCT MCTS algorithms that were introduced in Section 2.4. The interested reader can read the source code for our implementation of these algorithms in the src/planners/ directory in the GitHub repository. In this section, we discuss some subtle differences between our implementation and the referencee implementation by Veness et al. (2011), and we make some remarks about planning by\n§3.5 Planners 47\nsimulation generally, and planning in partially observable, history based environments in particular.\nRecall that the objective of ‘planning’ here is to compute, at each time step, the estimator V̂ ∗ρ , which is a sampling approximation of the expectimax calculation in Equation (2.9). The agent’s policy is then to take the action that maximizes this value. This is essentially planning by forward simulation. That is, we use our black-box environment model ρ to predict how the world will respond to future hypothetical actions. Informally, we run Monte Carlo simulations of numerous potential histories13, and collect statistics on which ones lead to the best outcomes. With each sample, we simulate a playout up to some fixed horizon m.\nDue to the stochasticity in general environments (and especially in the mixture model ξ), typically many samples are needed to converge to a good estimate of V ∗ρ . Note that, not only do we update the state of our model with each simulated time step, but we also update the agent’s beliefs. This is an important point that we feel is perhaps not emphasized enough: a rational agent, while planning under uncertainty, should simulate changes to its beliefs and the effects such changes will have on its subsequent actions. After each sample of a forward trajectory, we reset the agent’s model state and beliefs to what they were before simulating the play-out. MCTS is an anytime algorithm, in the sense that we can stop collecting samples early, and still have a valid (though perhaps inaccurate) estimate V̂ ∗ρ .\nNotice that we compute V̂ ∗ρ at each time step. Doing this naively, from scratch (i.e. resetting the search tree) seems wasteful. This prompts us to discuss the issue of caching partial results. Consider a generic scenario, in which our agent has experienced some history æ<t, and now computes V̂ ∗ ρ (æ<t) using MCTS, so as to plan which action at to take next. Say its tree search finds, after κ samples, some a∗t = arg maxat V̂ ∗ ρ (æ<tat) which is its best guess as to the most appropriate next action. Since a∗t is the planner’s preferred action, we surmise that ρUCT has spent a good number of samples simulating scenarios in the sub-tree that follows from a∗t . For any given percept et that is returned from the true environment following a∗t , the planner has (with high probability) collected numerous samples in the subtree corresponding to the history æ<ta ∗ t et, and so has done some of the work towards calculating V̂ ∗ρ (æ<ta ∗ t et). Thus, we keep the subtree V̂ ∗ ρ (æ<tatet) for future computations.\nWe now make a few more miscellaneous remarks about Monte Carlo tree search, and\nρUCT in particular:\n• Recall that ρUCT makes no assumptions about the environment; it treats ρ as a history-generating black box. Because ρUCT makes such weak assumptions, this\nmakes it very inefficient; it will spend a lot of time considering plans that continually revisit states in the POMDP, since the planner has no notion of state. In other words, many of the trajectories that it samples are cyclic and look like random walks through the state space. This is unfortunately unavoidable when planning by simulation on general POMDPs.\n• In the reference implementation, a clock timeout is used to limit the number of Monte Carlo samples to use. We use a fixed number of samples κ, to ensure consistency\nacross our experiments.\n13Also known as trajectories or play-outs."
    }, {
      "heading" : "48 Implementation",
      "text" : "• Being a Monte Carlo algorithm, its output is stochastic, which means that the resulting policy is stochastic. With a limited number of samples κ, the agent’s policy\nmay vary greatly, and be inconsistent. Clearly, in the limit κ→ 0 the agent’s policy becomes a random walk, and as κ → ∞ the agent’s policy converges to π∗ρ (Veness et al., 2011).\n• The choice of the UCT parameter C is consequential; recall from Equation (2.20) that it controls how much to weight the exploration bonus in the action-selection routine\nof the tree search. Low values of C correspond to low exploration in-simulation, and will result in deep trees and long-sighted plans. Conversely, high values of C will result in short, bushy trees, and greedier (but more statistically informed) plans (Veness et al., 2011). We experiment with the performance’s sensitivity to C in Chapter 4.\n• It goes without saying that planning by forward simulation is very computationally intensive, and makes up the bulk of the computation involved in running AIξ and\nits variants."
    }, {
      "heading" : "3.6 Visualization and user interface",
      "text" : "We now describe the design and implementation of the front-end of the web demo.\nThe user is initially presented with the About page, which provides an overview and introduction to the background of general reinforcement learning, including the definitions of each of the agents; we essentially present a less formal and abridged version of Chapter 2. Using the buttons at the top of the page, the user can navigate to the Demos page, which presents them with a selection of demos to choose from; see Figure 3.10. When the user clicks on one of the demos, the web app will open an interface similar to the one shown in Figure 3.9. This interface allows the user to choose agent and environment parameters in the Setup section of the UI, or simply use the defaults provided.\nOnce parameters have been selected, the agent-environment simulation is started by clicking Run. At this point, the agent-environment interaction loop (Algorithm 3.3) will begin, and depending on the choice of parameters, and CPU speed, will take a few seconds to a minute to complete. Three plots will appear on the right hand side: Average reward (Equation (4.1)), Information gain (Equation (2.13)), and fraction of the environment explored (for Gridworlds). These plots are updated in real time as the simulation progresses, so that the user has feedback on the rate of progress. The user can stop the simulation at any time by clicking Stop. Once the simulation is finished (or stopped prematurely), the user can watch a visualization of the agent-environment interaction using the Playback controls.\nBeneath each demo is a brief explanation of each of the elements of the visualization,\nand of the properties of the agent(s) being demonstrated."
    }, {
      "heading" : "3.7 Performance",
      "text" : "We conclude the chapter by making some remarks about the time and space complexity of these algorithms.\n§3.7 Performance 49"
    }, {
      "heading" : "50 Implementation",
      "text" : "§3.7 Performance 51\nTime complexity\nFrom Algorithm 3.3, we see that the main simulation loop consists of four function calls. The environment methods µ.Perform and µ.GeneratePercept both have constant time complexity, O (1). Unsurprisingly, most of the computation is done by the agent. The Bayesian agent AIξ has to, for each t, update its model, and compute its policy by approximating the value function through Monte-Carlo tree search. From Algorithm 3.1, updating the Bayes mixture requires |M| calls to ν.ConditionalDistribution and ν.Perform, which are both O (1) operations, and so the worst-case time-complexity of ρ.Update is O (|M|).\nFrom Algorithm 2.4, we see that the worst-case time complexity for a call to ρUCT is O (mκ |M| |A|), where recall that m is the agent’s planning horizon, κ is the number of Monte Carlo samples. This is because each Monte Carlo simulation requires playing through to the horizon m, and for each simulated time-step k ∈ {1, . . . ,m}, performing action selection (O (|A|), due to the arg max) and model updates (O (|M|), from above). Hence, the runtime for our Bayesian agents is dominated by planning; for typical values κ ≈ 103 and m ≈ 10 we see that well over 99% of the runtime is spent in agent action selection, performing forward simulations.\nIn contrast, for Thompson sampling (Algorithm 2.3) and the MDL agent (Algorithm 2.2), the time complexity of ρUCT is merely O (mκ |A|), since these agents compute a ρ-optimal policy (for some ρ ∈ M), rather than a ξ-optimal policy. Also, recall that for reward-based agents, computing the utility function is O (1), since the reward signal is provided by the environment. Recall that the knowledge-seeking agent is simply AIξ, but with utility function given in Definition 15. Since this involves computing the entropy of the posterior, which is a distribution overM, we incur an additional (worst-case) runtime cost of |M| for each simulated timestep, bringing the time complexity of ρUCT for the KL-KSA agent to O ( mκ |M|2 |A| ) . This is a nasty runtime: quadratic in the size of the\nhypothesis space!\nIn the Gridworld scenarios, and using the naive mixture model, we have |A| = 5 and |M| = N2, where N is the dimensions of the grid – see section 3.4.1. The total worst-case runtime of the demo is therefore O ( mκTN2 ) ; from Figure 3.9 we can see that the user has control of these parameters: T (Agent.Cycles), N (Env.N), m (Agent.Horizon), and κ (Agent.Samples). In practice, on a 3 GHz i7 desktop machine running the latest version of Google Chrome, values of m = 6, κ = 600, T = 200, and N = 10 yield runtimes of around 10 seconds, or 20 frames per second (fps). This runtime is while maintaining realtime plot updates on the frontend, which adds a considerable overhead to each iteration; if we run the simulations with the visualizations disabled, we get approximately a 2× speed-up.\nUsing the Dirichlet model class (section 3.4.2), we no longer have an explicit mixture, but instead use the factorized model. This means that the time complexity of model queries and updates doesn’t scale with the gridworld size, but instead scales with the size of the observation space. Although on paper this is a better scaling because the observation space is constrained by the four-connected topology of the gridworld, in practice, for the sizes of gridworlds that we simulate, the Dirichlet model runs significantly slower, because of the large constant overhead of sampling for each percept. Thus we see that the complexity of the environment affects the agent two-fold, in that it raises the difficulty of learning a model, and raises the difficulty of planning, given an accurate (and therefore usually at least as complex as the environment) model."
    }, {
      "heading" : "52 Implementation",
      "text" : "Space complexity\nAt any given time t, the Bayesian agent’s mixture model takes up O (|M|) space, and its Monte Carlo search tree takes up in the worst case O (mκ) space. The demo infrastructure itself is a significant memory consumer: at each time step t ∈ {1, . . . , T}, we log the state of the agent’s model ξ, the state of the environment µ, along with miscellaneous other information (actions, percepts, etc.). Therefore the total memory consumption of the demo is O (|M|T +mκ). For typical values, neither of these terms dominates the other: the products |M|T and mκ are usually of the order of 104. On modern machines, and for the parameter settings and constraints we typically use (see Table 3.1), memory consumption is not an issue. In practice, we find that physical memory usage rarely exceeds 100-200 megabytes.\n§3.7 Performance 53\nAlgorithm 3.1 BayesMixture model. Inputs: Model class M, a list of Environment objects; prior w, a normalized vector of probabilities.\n1: function GeneratePercept 2: Sample ρ from the posterior w(·|æ<t) 3: return ρ.GeneratePercept() 4: end function\n5: function Perform(a) 6: for ν in M do 7: ν.Perform(a) 8: end for 9: end function\n10: function ConditionalDistribution(e) 11: return ∑ ν∈Mwνν.ConditionalDistribution(e) 12: end function\n13: function update(a, e) 14: ξ ← ∑ ν∈Mwνν.ConditionalDistribution(e) 15: for ν in M do 16: wν ← 1ξν.ConditionalDistribution(e) 17: end for 18: end function\n19: function save 20: for ν in M do 21: ν.Save() 22: end for 23: end function\n24: function load 25: for ν in M do 26: ν.Load() 27: end for 28: end function\nAlgorithm 3.3 Agent-environment simulation. Inputs: Agent π; Environment µ; Timeout T 1: t← 0 2: for t = 1 to T do 3: e← µ.GeneratePercept() 4: π.Update(e) 5: a← π.selectAction() 6: µ.Perform(a) 7: end for\n54 Implementation\nChapter 4\nExperiments\nThe strength of a theory is not what it allows, but what it prohibits; if you can invent an equally persuasive explanation for any outcome, you have zero knowledge.\nIn this chapter we report on experiments that we performed using the AIXIjs software. In particular, we make several illuminating comparisons between various agents; as far as we are aware, these results represent the first empirical comparison of these agents.\nExcept where otherwise stated, all of the following experiments were run on 10 × 10 gridworlds with a single dispenser, with θ = 0.75 (see Section 3.3 for the definition of our Gridworld). The experiments were averaged over 50 simulations for each agent configuration. We run each simulation against the same gridworld (see Figure 4.1) for consistency. We typically run each simulation for 200 cycles, as this is usually sufficient to distinguish the behavior of different agents. We also typically (though not always) use κ = 600 MCTS samples and a planning horizon of m = 6. In all cases, discounting is geometric with γ = 0.99.\nThere are two metrics with respect to which we evaluate the agents – one for reinforce-\nment learners, and one for knowledge-seeking agents, respectively:\n• Average reward, which at any cycle t > 0 is given by\nr̄t = 1\nt t∑ i=1 ri, (4.1)\nwhere the ri are the rewards accumulated by the agent during the simulation. In the case of our Gridworlds, all dispensers have the same ‘pay-out’ rc, and differ only in the Bernoulli parameter θ which governs how frequently they dispense reward. In our dispenser gridworlds, the optimal policy is usually1 to walk from the starting location to the dispenser with the highest frequency, and then stay there. If this dispenser is D tiles away from the starting tile and has frequency θ, then the optimal\n1There are of course pathological cases that break this rule-of-thumb. For any simulation lifetime T and gridworld dimension N , there exists an ∈ (0, 1) such that we can put a dispenser with θ = 1 at the end of a long and circuitous maze, and put another dispenser right next to the agent’s starting position with θ = 1 − , such that walking to the best dispenser has a high enough opportunity cost to make it not worthwhile given a finite lifetime T . In practice, most of our demos only use one dispenser, and the frequencies of the dispensers differ sufficiently so that it is always better to take the time to seek out the better dispenser.\n55"
    }, {
      "heading" : "56 Experiments",
      "text" : "policy will, in µ-expectation, achieve an average reward of\nr̄?t . = E∗µ [r̄t] = D\nt rw + θ rc,\nwhere rw is the penalty for walking between tiles. In our set-up, rw = −1 and rc = 100.\n• Fraction of the environment explored. We simply count the number of tiles the agent visits nv (t), and divide by the number of reachable tiles nr:\nft . = 100× nv (t) nr .\nThe optimal ‘exploratory’ policy will achieve a perfect exploration score of f = 100% in O (nr) time steps.\nIn the plots that follow, the solid lines represent the mean value, and the shaded region corresponds to one standard deviation from the mean."
    }, {
      "heading" : "4.1 Knowledge-seeking agents",
      "text" : "We begin by comparing the three knowledge-seeking agents (KSA): Kullback-Leibler (Definition 15), Square (Definition 16), and Shannon (Definition 17). We compare their exploration performance, and discuss how this performance varies with model class. We also present an environment that is adversarial to the Square and Shannon KSA."
    }, {
      "heading" : "4.1.1 Hooked on noise",
      "text" : "As was discussed in section 2.3.2, the entropy-seeking agents Shannon-KSA and SquareKSA will generally not perform well in stochastic environments. We can illustrate this starkly by adversarially constructing a gridworld with a noise source adjacent to the agent’s starting position. The noise source is a tile that emits uniformly random percepts over a\n§4.1 Knowledge-seeking agents 57\nsufficiently large alphabet such that the probability of any given percept ξ (e) is lower (and hence more attractive) than anything else the agent expects to experience by exploring.\nIn this way, we can ‘trap’ the Square and Shannon agents, causing them to stop exploring and watch the noise source incessantly; see Figure 4.2. In contrast, the KullbackLeibler KSA is uninterested in the noise source, since watching the noise source will not induce a change in the entropy of its posterior w (·). This experiment corresponds to the ‘Hooked on noise’ demo."
    }, {
      "heading" : "4.1.2 Stochastic gridworld",
      "text" : "Now, we compare the exploration performance ft of Kullback-Leibler, Shannon, and Square on a stochastic gridworld, using both the dispenser-parametrized mixture Mloc defined in section 3.4.1 and the factorized Dirichlet model MDirichlet defined in section 3.4.2. We plot the results, averaged over 50 runs, in Figure 4.3 and Figure 4.4.\nAll three KSAs perform better – that is, they explore considerably more of the environment – using MDirichlet than with Mloc. In particular, they have both higher mean and significantly lower variance in ft. In particular, we are interested in the mean µt and variance σt at the end of the simulation, t = 200. We report 2 and interpret the results for the three agents:\n• KL-KSA achieves f200 = 98.8±0.93 usingMDirichlet, and f200 = 77.2±20.6 using Mloc. Using Mloc, KL-KSA starts random walking after it finds the dispenser, since (as discussed in section 3.4.1) the posterior w (ν|æ<t) collapses to the identity I [ν = µ],\n2Note that we report the results in the format f = µ ± σ, unlike the more common f = µ ± 2σ (i.e., 95% confidence) interval."
    }, {
      "heading" : "58 Experiments",
      "text" : "with entropy zero. No action will reduce the entropy of w (·) further, and so every subsequent action is of zero value. In other words, once KL-KSA learns everything there is to know (i.e. the location of the dispenser), every action is equally unrewarding, and, since we break ties in Equation (2.11) at random, the agent executes a random walk. Thus, if KL-KSA finds the dispenser before having explored the whole environment, then it will take a long time to random walk into areas of the environment that it hasn’t already seen. This explains the observation that, using Mloc , KL-KSA tends not to explore the whole environment, and hence achieves a relatively low ft-score in mean.\nRecall that, due to the Monte Carlo tree search and random tie-breaking, the agent’s policy is stochastic, and so the order in which it explores the environment will differ from experimental run to run. Moreover, the dispensers are also stochastic (recall that θ = 0.75). For the reasons discussed above, the time at which the agent discovers the dispenser is highly consequential to how much exploration it does; there may be runs in which KL-KSA explores the whole Gridworld before finally finding the dispenser, and runs in which it happens to get lucky and stumble onto the dispenser straight away, and random-walks thereafter. Given the three sources of stochasticity, both in the agent’s policy and in the percepts, this introduces a lot of variability into the agent’s performance, and explains the high variance we see in ft in Figure 4.3.\nIn contrast, recall from section 3.4.2 that MDirichlet doesn’t have the ‘posterior collapse’ property ofMloc, since the agent’s beliefs about each tile are independent. This means that even if KL-KSA-Dirichlet happens to find the dispenser early on, it will still be motivated to explore, since its model will still have a lot of uncertainty about tiles that it hasn’t yet visited; see Figure 4.5 for a visualization. This is borne out by the remarkable performance we see in Figure 4.4; after only 100 cycles, KLKSA-Dirichlet explores over 90% of the environment on average, and explores nearly 99% on average after 200 cycles.\n• Square KSA achieves f200 = 86.9 ± 7.8 using MDirichlet, and f200 = 66.2 ± 27.4 using Mloc; Shannon KSA achieves f200 = 72.7 ± 10.0 using MDirichlet, and f200 = 65.9± 29.6 using Mloc.\nUsing Mloc, the performance of the Shannon KSA is essentially indistinguishable from that of the Square KSA; both agents explore roughly 66% of the environment over 200 interaction cycles. This is to be expected; once the agents discover the dispenser, their posterior collapses to the dispenser tile, making the dispenser the only source of entropy in the Bayes mixture ξ, since the rest of the environment is now both deterministic and known. Given that the Square and Shannon agents are both entropy-seeking (recall Equation (2.15) and Equation (2.16)), they will remain on the dispenser tile indefinitely (and cease exploring), as the dispenser is the only source of noise in an otherwise bland environment.\nThe fact that both Square/Shannon KSA will remain on the dispenser tile instead of random walking as KL-KSA does, also helps to explain the difference in means (µ200 ≈ 66 for Square/Shannon, while µ200 ≈ 77 for KL). In other words, while all the KSA stop exploring purposefully once the dispenser is found, KL-KSA ekes out slightly better exploration performance due (at least in part) to its subsequent random walk.\n§4.1 Knowledge-seeking agents 59\nBoth the Square and Shannon KSA explore more, and with lower variance, using MDirichlet than withMloc. This difference is for similar reasons to those described for the KL-KSA above, and we do not dwell on them. What is interesting is that the Dirichlet model differentiates the performance of the Square and Shannon KSA, which until now have performed almost identically: µ200 ≈ 87 for Square KSA, while µ200 ≈ 73 for Shannon KSA. This result is counter-intuitive, and raises a red flag that we mentioned in section 3.2.1, namely, that Shannon KSA will have difficulty planning correctly in Monte Carlo tree search due to its unbounded utility function. To see why we may be more prone to this with the Dirichlet model than with the mixture model, recall from Equation (3.5) that, for some tile s that happens to be empty, if the agent visits s a total of v times, then its posterior belief that s is empty will be\nPr (s = Empty) = v + 1\nv + 2 ,\nFrom Equation (3.1) and Equation (3.3), and using the mean-sampling approximation, we see that\nρ (eD|s) ≤ 1\nv + 2 .\nIf β is an underestimate, then as the agent spends more time v on any given Empty tile, the probability ρ of sampling a percept eD characteristic of dispensers goes like v−1, but Shannon KSA’s utility blows up quickly , at a rate of − log v−1, yielding positive net expected utility. Hence Shannon KSA will be prone to chasing vanishing probabilities, and will perform suboptimally. Conversely, if β is an overestimate, then for sufficiently high probability events, the agent’s normalized value estimator 1 m(β−α) V̂ will be vanishingly small, and the agent will compute a suboptimal policy by having an effectively enormous UCT parameter C. Because Square KSA’s utility function is bounded, it doesn’t have this problem, and so it outperforms the Shannon KSA.\nFinally, we remark that the KL-KSA handily outperforms Square and Shannon on both model classes; the difference under the MDirichlet model in particular is stark. By now, this shouldn’t surprise us: the Kullback-Leibler KSA is far better adapted for stochastic environments than the entropy seeking agents Shannon-KSA and Square-KSA. Our experiments seem to confirm that seeking to maximize expected information gain is both a principled, and empirically successful exploration strategy.\nFrom Figure 4.3 we see that, using the mixture model class, the Square and Shannon exploration performance flattens out after around 150 cycles. This is because they find the dispenser and get hooked on noise. But, in this Gridworld environment, it happens that the only source of noise is also the only source of reward. This prompts us to ask: could Shannon and/or Square KSA ‘unintentionally’ outperform AIξ in terms of accumulated reward, by virtue of being better at exploration, and by the quirk of the environment meaning that the optimal entropy-seeking policy (given a collapsed posterior) is actually also the optimal reward-seeking policy?\nWe run this experiment, and plot the results in Figure 4.6; we find that indeed, both entropy-seeking agents outperform AIξ in terms of average reward. We emphasize that apart from their utility functions, these agents are configured the same; they have the same prior w (uniform), discount function (geometric, γ = 0.99), planning horizon (m = 6), and Monte Carlo samples budget (κ = 600). This appears to be empirical evidence of the"
    }, {
      "heading" : "60 Experiments",
      "text" : "Bayes-optimal agent AIξ not exploring optimally. This result is slightly perplexing. We have no strong theoretical grounds on which to expect AIξ to underperform so drastically in this scenario, given a uniform prior; we expect AIξ’s performance (w.r.t. reward) to be an upper bound on the performance of any other Bayesian agent given the same model class and prior. We have two (weakly held) hypotheses for what could be going on here:\n1. Somehow, finding and exploiting sources of entropy is easier and more sample-\nefficient for the Monte Carlo planner to do than it is for it to find and exploit sources of (stochastic) rewards. We find this implausible, as we re-ran the experiment, this time giving far more resources (κ = 2 × 103) to AIXI’s planner than to KSA’s, with a similar result.\n2. There is a bug in our MCTS implementation that is somehow being expressed only\nfor reward-based agents and not for utility-based agents. This also seems rather implausible, as our code is fully modular, and the difference between one agent and the other is one line of code, which defines their respective utility functions.\nIt seems that Figure 4.6 will remain an enigma, for now; we have no better hypotheses that could explain this behavior. Reluctantly, we leave this as an open problem for further experiments."
    }, {
      "heading" : "4.2 AIµ and AIξ",
      "text" : "So much for the knowledge-seeking agents. We now experiment with properties of the Bayes agent AIξ.\nWe begin by comparing the performance of the informed agent AIµ with the Bayes-\noptimal agent AIξ, using the dispenser-parametrized model class; see Figure 4.7.\n§4.2 AIµ and AIξ 61\nAs expected, AIµ outperforms AIξ by a large margin; naturally, having perfect prior knowledge of the true environment wins. Though this result is as expected, there are some observations that we might pause to consider here:\n1. AIξ’s performance has very high variance over the 50 trials. This shouldn’t surprise\nus given the design of the gridworld; see Figure 4.1. The dispenser is tucked away in a corner, and the gridworld, while small, is sufficiently maze-like that it’s easy to go ‘down the rabbit-hole’ searching in far-off places for rewards. Combine this with the fact that the dispenser is stochastic, and so even walking onto the dispenser tile is often insufficient to confirm its location; one needs to spend numerous cycles on each tile. Thus, given a uniform prior, some agents will get lucky and find the dispenser early and accumulate a lot of reward, some will find it late in the simulation, while others may wander around and not find it in the allotted time.\n2. AIµ’s performance has low, but non-zero variance. This can be almost fully ac-\ncounted for by stochasticity in the dispenser. However, this also relates to the third observation:\n3. AIµ performs worse in mean than the theoretical optimal mean3 – that is, r̄AIµt ≤ r̄∗t ∀ t; the solid blue line is below the dashed black line. This is due to the particularities of planning with the history-based Monte Carlo tree search algorithm, ρUCT.\nBecause the planning module makes no assumptions about the environment, and because our environment is partially observable, the agent will waste a lot of time considering plans that are cyclic in the state space. That is, it will sample from plans\n3Note that, due to stochasticity in the dispensers, we expect AIµ to outperform the optimal mean around half of the time."
    }, {
      "heading" : "62 Experiments",
      "text" : "such as Left,Right,Left,Right, . . . ; even though we know that Left,Right corresponds to the identity, the Monte Carlo planner doesn’t know this! Hence, even though we run AIµ, the planner is inefficient, and, being Monte Carlo-based, introduces stochasticity and noise into the agent’s policy. Couple this with stochasticity in the dispensers, and there will be times in which AIµ will take sub-optimal actions due to effectively not having enough samples to work with in its planning. We explore the issues of planning with MCTS in Section 4.6."
    }, {
      "heading" : "4.2.1 Model classes",
      "text" : "We compare the average reward performance of AIξ using Mloc and MDirichlet; see Figure 4.8. Note that, similar to the KSA case discussed previously, the variance in performance is lower for MC-AIXI-Dirichlet than it is for MC-AIXI. AIξ performs considerably worse using the Dirichlet model than with the mixture model, since the Dirichlet model is less constrained (in other words, less informed), which makes the environment harder to learn.\nNotice the bump around cycles 20-50 in the average reward for MC-AIXI-Dirichlet: this means that the agent sometimes discovers the dispenser, but is incentivized to move away from it and keep exploring, since its model still assigns significant probability to there being dispensers elsewhere. This is borne out by Figure 4.9, which shows that, on average, MC-AIXI-Dirichlet explores significantly more of the Gridworld than MC-AIXI with the naive model class."
    }, {
      "heading" : "4.2.2 Dependence on priors",
      "text" : "We construct a model class and prior such that AIξ believes that the squares adjacent to it are traps with high (but less than 1) probability; this is the so-called dogmatic prior of\n§4.3 Thompson Sampling 63\nLeike and Hutter (2015). The agent never moves to falsify this belief, since falling into the trap incurs a penalty of −5 per time step for eternity, compared to merely −1 per time step for waiting in the corner. The agent therefore sits in the corner for the duration of the simulation, and collects no positive rewards. This makes for a very boring demo (and reward plot), so we omit reproducing a visualization of this result. Thus, unlike the Bayesian learner in the passive case, AIξ never overcomes the bias in its prior. In this way, an adversarial prior can make the agent perform (almost) as badly as is possible, even though the true environment is benign, and has no traps at all."
    }, {
      "heading" : "4.3 Thompson Sampling",
      "text" : "Recall from Algorithm 2.3 that Thompson sampling (TS) re-samples an environment ρ from the posterior w every effective horizon Hγ (ε) before re-sampling ρ ′ from its posterior. Recall also that we use the Monte Carlo tree search horizon m as a surrogate for the effective horizon Hγ (ε). We run Thompson sampling with the standard dispenserparametrized model class; since we don’t represent the Dirichlet model class as a mixture, it is much more natural to use the naive mixture. For the purposes of planning, TS only needs to compute the value V ∗ρ for some ρ ∈ M, as opposed to V ∗ξ , which mixes over all of M. For this reason, planning with TS is cheaper to compute by a factor of |M|. This means that we can get away with more MCTS samples and a longer horizon.\nIn practice, in our experiments on gridworlds, TS performs quite poorly in comparison\nto AIξ; see Figure 4.10. This is caused by two issues:\n1. The parametrization of the model class means that TS effectively ‘pretends’ that the\ndispenser is at some grid location (i, j) for a whole horizon m (of the order of 10-15"
    }, {
      "heading" : "64 Experiments",
      "text" : "cycles). It computes the corresponding optimal policy, which is to seek out (i, j) and sit there until it is time to re-sample from the posterior. For all but very low values of θ or m, this is an inefficient strategy for discovering the location of the dispenser. For example, with θ = 0.75, it takes only four cycles of sitting on any given tile to convince yourself that it is not a dispenser with greater than 99% probability.\n2. The performance of TS is strongly curtailed by limitations of the MCTS planner. If\nthe agent samples an environment ρ which places the dispenser outside its planning horizon – that is, more than m steps away – then the agent will not be sufficiently far-sighted to see this, and so will do nothing useful. Even if ρ is within the planning horizon, MCTS is not guaranteed to find it, especially if it is deep in the search tree, or MCTS isn’t given enough samples to work with; see Section 4.6 for more discussion on the limitations of ρUCT.\nNote that the pragmatic considerations in Item 1 and Item 2 are opposed to each other. On the one hand (Item 1), we want to reduce m so as to reduce the agent’s tendency to waste time overcommitting to irrelevant or suboptimal policies, and spend more time learning the environment. On the other hand (Item 2), we want to increase the horizon m so that the agent can plan sufficiently far ahead to compute the ρ-optimal policy in all instances. These two desires are fundamentally opposed, and we are not aware of a way to effectively compromise them. It seems that we have inadvertently constructed our Gridworld so as to perfectly frustrate Thompson sampling!\n§4.4 MDL Agent 65"
    }, {
      "heading" : "4.3.1 Random exploration",
      "text" : "For comparison, we contrast Thompson sampling’s performance with -greedy tabular Qlearning with optimistic initialization.4 We use α = 0.9, = 0.05, and optimistically initialize Q (s, a) = 100 ∀s, a. Note that this being a POMDP, Q-learning will experience perceptual aliasing ; that is, it will erroneously aggregate different situations into the same ‘state’ in its Q-value table. We present this merely so as to contrast Thompson sampling’s comparatively weak performance with the performance of a policy that explores purely at random (i.e., with probability , take a random action). As we can see from Figure 4.11, Q-learning rarely discovers the dispenser; on average, r̄ Q-Learning t is still negative even after t = 200 cycles. This demonstrates that random, model-free exploration is not effective in this environment."
    }, {
      "heading" : "4.4 MDL Agent",
      "text" : "Recall from Algorithm 2.2 that the MDL agent uses the ρ-optimal policy until ρ is falsified (i.e. wρ = 0), where ρ is the simplest environment in its model class. Clearly, the MDL agent fails in stochastic environments, since falsification in this sense is a condition that cannot be met in noisy environments. We use the standard dispenser Gridworld and mixture model class, and run two experiments: one with a stochastic environment (0 < θ < 1), and one with a deterministic environment (θ = 1).\nSince each model in the mixture differs only in the position of the dispenser, they have\n4We omitted any treatment of tabular methods in Chapter 2, in the service of clarity and conciseness. We must assume at this point that the reader has some familiarity with the basic algorithms of reinforcement learning covered in Sutton and Barto (1998)."
    }, {
      "heading" : "66 Experiments",
      "text" : "(approximately) equal complexity. For this reason, we simply order them lexicographically; models with a lower index in the enumeration of the model class Mloc are chosen first. In other words, we use the Kolmogorov complexity of the index of ν in this enumeration as a surrogate for K (ν)."
    }, {
      "heading" : "4.4.1 Stochastic environments",
      "text" : "In Figure 4.4.1, we see that the agent chooses to follow the ρ-optimal policy, which believes that the goal is at Tile (0, 0). Recall that the only thing to differentiate the dispenser tile from empty tiles is the reward signal. Since the dispensers are Bernoulli (θ) processes, with θ known (in this model class), the agent’s posterior on Tile (0, 0) being a dispenser goes like\nw0 = (1− θ)t ,\nwhich, though it approaches zero exponentially quickly, is never outright falsified, and\nso the MDL agent stays at (0, 0) for the length of the simulation.5"
    }, {
      "heading" : "4.4.2 Deterministic environments",
      "text" : "The above result (failure in a stochastic environment) seems like a strong indictment of the MDL agent. But, if we take the environment from Figure 4.1 and make it deterministic by setting θ = 1, we find that the MDL agent significantly outperforms the Bayes agent AIξ with a uniform prior; see Figure 4.1. This is because the MDL agent is biased towards\n5If the simulation is run longer enough, eventually we will lose numerical precision and encounter underflow and round to zero, allowing the agent to move on.\n§4.5 Wireheading 67\nenvironments with low indices; using the Mloc model class, this corresponds to environments in which the dispenser is close to the agent’s starting position. In comparison, AIξ’s uniform prior assigns significant probability mass to the dispenser being deep in the maze. This motivates it to explore deeper in the maze, often neglecting to thoroughly explore the area near the start of the maze; see Figure 4.14."
    }, {
      "heading" : "4.5 Wireheading",
      "text" : "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents. In short, a sufficiently intelligent reinforcement learner will be motivated to subvert its designer’s intentions and take direct control of its reward signal and/or sensors, so as to maximize its reward signal directly, rather than indirectly by conforming to the intentions of its designer. This is known in the literature as wireheading, and is an open and significant problem in AI safety research Everitt et al. (2016); Everitt and Hutter (2016). We construct a simple environment in which the agent has an opportunity to wirehead: it is a normal Gridworld similar to those above, except that there is a tile which, if visited by the agent, will allow it to modify its own sensors so that all percepts have their reward signal replaced with the maximum number feasible; in JavaScript, this is Number.MAX SAFE INTEGER, which is approximately 1016. This clearly dominates the reward that the agent could get otherwise by following the ‘rules’ and using the reward signal that was initially specified. As far as a reinforcement learner is concerned, wireheading is – almost by definition – the most rational thing to do if one wishes to maximize rewards; the demo shown in Figure 4.15 is designed to illustrate this."
    }, {
      "heading" : "68 Experiments",
      "text" : ""
    }, {
      "heading" : "4.6 Planning with MCTS",
      "text" : "In Section 3.7, we discussed the time complexity of planning with ρUCT and mixture models, and concluded that the major computational bottleneck in our agent-environment simulations is the MCTS planner. It should come as no surprise, then, that the limiting factor in our agent’s performance is the capacity of the planner. In these experiments that follow, we investigate how the agent’s performance depends on the ρUCT parameters.\nAs previously discussed, the ρUCT planning algorithm makes no assumptions about the environment. This makes planning very inefficient, especially for long horizons in stochastic environments. We experiment with the three planning parameters we have available: κ, the number of Monte Carlo samples; m, the planning horizon, and C, the UCT exploration parameter from Equation (2.20). In all cases we use AIµ, the informed agent. When varying one parameter, we hold the others constant; in particular, the default values are κ = 600, m = 6, and C = 1.\nWe show AIµ’s dependence on κ in Figure 4.16. As we increase the number of samples κ available to ρUCT, we see AIµ’s performance converges to optimal. In general, the number of samples required for good performance depends on the model class and the environment. In particular, AIξ requires more samples than AIµ to perform well, because the mixture model ξ introduces added stochasticity, since we sample percepts from it by ancestral sampling ; that is, we first sample an environment ρ from w (·), then sample a percept e from ρ (et|æ<tat). This, the number of samples κ required for acceptable performance with AIµ should be regarded as a loose lower bound on the minimal acceptable number of samples required for AIξ. We see from Figure 4.16 that κ = 400 seems to be a realistic baseline.\n§4.6 Planning with MCTS 69\nWe find empirically that the agent’s performance is not very sensitive to the size of the horizon m. This is unsurprising; to plan accurately with a large horizon, we need an exponentially large number of samples, since the number of leaf nodes grows exponentially in m, so increasing the horizon in isolation does little to alter performance. On many Gridworld maze layouts, one can often get away with quite short horizons, even as short as m = 2, if planning for AIξ with a uniform prior. The reason this works is because the agent can often simply ‘follow its nose’ and exploit the probability mass its model assigns to its immediately adjacent tiles, as long as there aren’t too many ‘dead-ends’ for the agent to follow its nose into and waste time in.\nFinally we experiment with the UCT parameter C, and use the chain environment from Figure 3.5. Recall that the chain environment rewards far-sightedness; being greedy and near-sighted results in drastically suboptimal rewards. The optimal policy is for the"
    }, {
      "heading" : "70 Experiments",
      "text" : "agent to delay gratification for N cycles at a time; in our experiments, we use N = 6, and set rb = 10 3, ri = 4, and r0 = 0; see section 3.3.2 for details of the setup.\nNote that experimenting with the agent’s horizon is not particularly interesting here; AIµ finds the optimal policy for m ≥ 6 and chooses a suboptimal policy otherwise. Varying the UCT parameter generates more interesting results. In Figure 4.17 we can see that for very low values of C (0.01), the agent is too myopic to generate plans that collect the distant reward, while for very high values of C (1, 5, and 10), the agent does find the distant reward, but not reliably enough to achieve optimal average reward. In the midrange of values, the agent’s performance is optimal and stable across an order of magnitude of variation (0.05, 0.1, 0.5).\nRecall that the UCT parameter controls the shape of the expectimax trees that the planner generates: high values of UCT will lead to shorter, bushy trees, and low values will lead to longer, deeper trees (Veness et al., 2011). This appears to be borne out by our results. For very low values of C, the planner doesn’t explore alternative plans sufficiently, and easily gets stuck in the local maximum of the instant-gratification policy π→; searching more-or-less naively over the space of plans of length m ≥ 6, the planner is exponentially unlikely to find the optimal policy π99K. In contrast, for very high values of C the planner will consider many moderate-sized plans, and will occasionally get lucky and find the optimal policy, but will often miss it; these outcomes are represented by the blue, green, and red curves in Figure 4.17. Finally, for values of C in the ‘sweet spot’ that balances exploration with exploitation in the planner’s simulated action selection, the optimal policy is virtually guaranteed: this situation is represented by the orange,\n§4.6 Planning with MCTS 71\npink, and black curves."
    }, {
      "heading" : "72 Experiments",
      "text" : "Chapter 5\nConclusion\nThe AI does not hate you, nor does it love you, but you are made out of\natoms which it can use for something else.\nThe next few decades seem to offer much promise for the field of artificial intelligence and machine learning. Of course, it remains to be seen whether or not superintelligent general AI will come about in this time frame, if at all. Regardless of the time scales involved, though, it seems clear that questions relating to formal theories of intelligence and rationality will only grow in importance over time. Hutter’s AIXI model and its variants represent some of the first steps along the path towards an understanding of general intelligence. Our ultimate hope is that the software developed in this thesis will grow and serve as a useful research tool, an educational reference, and as a playground for ideas as the field of general reinforcement learning matures. At a minimum, we expect it to be of value to students and researchers trying to learn the fundamentals of GRL. We now provide a short summary of what we have achieved, and provide some reflections and ideas on future directions for AIXIjs.\nSummary\nIn this thesis, we have presented:\n• A review of general reinforcement learning, bringing together the various agents due to Hutter, Orseau, Lattimore, Leike, and others, under a single consistent and\naccessible notation and conceptual set-up.\n• The design and open-source implementation of a framework for running and testing these agents, including environments, environment models, and the agents them-\nselves,\n• A suite of illuminating experiments in which we realized and compared different approaches to rational behavior, and\n• An educational and interactive demo, complete with visualizations and explanations, to assist newcomers to the field.\nFuture directions\nIn the course of developing AIXIjs, we have made numerous insights into GRL, and raised several new questions:\n73"
    }, {
      "heading" : "74 Conclusion",
      "text" : "• What is a principled way to normalize the first term of Equation (2.20) for the Shannon KSA agent, whose utility function is unbounded from above? Is it possible\nto change the normalization 1m(β−α) adaptively?\n• What are some general principles for constructing efficient models for certain classes of environments, in the context of applied Bayesian general reinforcement learning?\nConstructing bespoke models such as the MDirichlet model is time-consuming and doesn’t generalize to new environments. On the other hand, very generic approaches like context-tree weighting learn too slowly to be useful. Is there a middle ground?\n• Is there a way to represent the Dirichlet model MDirichlet as a mixture, in the form of Equation (2.10)? This would make it more convenient to run, for example,\nThompson sampling.\n• Why do the entropy-seeking agents seemingly outperform AIξ at its own game, as in Figure 4.2? This is a confronting result. Is there a bug in the implementation, or\njust something we don’t understand?\n• Can we make our JavaScript implementations more efficient, and scale up the demos to more impressive environments? How far can we scale these agents in the browser?\n• Planning with ρUCT is often like a black box. Is it possible to construct a good visualization of the state of a Monte Carlo search tree, to illuminate what it is doing?\nIn addition, there are some low-level ‘jobs’ that can be done to improve and extend AIXIjs in the near term:\n• Construct working visualizations for the bandit, FSMDP, and Iterated prisoner’s dilemma environments (not presented here).\n• Implement the regularized version of the MDL agent (Leike, 2016a).\n• Figure out how to implement optimistic AIXI.\n• Implement planning-as-inference algorithms such as Compress and Control (Veness et al., 2015).\n• Finish implementing the CTW model class.\n• Extend the implementation to include TD-learning agents and DQN.\nWorking on an open-source project, implementing state-of-the-art models of rationality has been both rewarding and thought-provoking. We’re excited to continue to contribute to the AIXIjs project over the coming months, and to see where new ideas in reinforcement learning will take us."
    }, {
      "heading" : "76 Bibliography",
      "text" : "Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement learning. In Advances in Neural Information Processing Systems, pages 89–96, 2009.\nAndrew G. Barto and Thomas G. Dietterich. Reinforcement Learning and Its Relationship to Supervised Learning, pages 45–63. John Wiley & Sons, Inc., 2004. ISBN 9780470544785. doi: 10.1002/9780470544785.ch2. URL http://dx.doi.org/10.1002/ 9780470544785.ch2.\nMarc G. Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Rémi Munos. Unifying count-based exploration and intrinsic motivation. CoRR, abs/1606.01868, 2016. URL http://arxiv.org/abs/1606.01868.\nDimitri P Bertsekas and John Tsitsiklis. Dynamic Programming and Optimal Control. Athena Scientific, 1995.\nChristopher M Bishop. Pattern Recognition and Machine Learning. Springer, 2006.\nMichael Bostock. Chord diagram example, 2016. URL http://bl.ocks.org/mbostock/ 1046712.\nNick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym, 2016.\nC. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. IEEE Transactions on Computational Intelligence and AI in Games, 4(1): 1–43, March 2012. ISSN 1943-068X. doi: 10.1109/TCIAIG.2012.2186810.\nLeon Chen. Keras-js, 2016. https://transcranial.github.io/keras-js/.\nTom Everitt and Marcus Hutter. Avoiding wireheading with value reinforcement learning. In Artificial General Intelligence, 2016.\nTom Everitt, Daniel Filan, Mayank Daswani, and Marcus Hutter. Self-modification of policy and utility function in rational agents. In Artificial General Intelligence, 2016.\nGoogle. What we learned in Seoul with AlphaGo. https://googleblog.blogspot.com. au/2016/03/what-we-learned-in-seoul-with-alphago.html, March 2016. Accessed: 2016-03-28.\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer, 2nd edition, 2009.\nBill Hibbard. Model-based utility functions. Journal of Artificial General Intelligence, 3 (1):1–24, 2012.\nStephen J Hoch and George Loewenstein. Time-inconsistent preferences and consumer self-control. Journal of Consumer Research, 17(4):492–507, 1991. URL http: //EconPapers.repec.org/RePEc:oup:jconrs:v:17:y:1991:i:4:p:492-507.\nBibliography 77\nJohn Holdren, Ed Felten, Terah Lyons, and Michael Garris. Preparing for the future of artificial intelligence, 2016.\nMarcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity. Technical report, IDSIA, 2000. http://arxiv.org/abs/cs.AI/0004001.\nMarcus Hutter. Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures. In Computational Learning Theory, pages 364–379. Springer, 2002.\nMarcus Hutter. A gentle introduction to the universal algorithmic agent AIXI. Technical report, IDSIA, 2003. ftp://ftp.idsia.ch/pub/techrep/IDSIA-01-03.ps.gz.\nMarcus Hutter. Universal Artificial Intelligence. Springer, 2005.\nMarcus Hutter. Open problems in universal induction & intelligence. Algorithms, 3(2): 879–906, 2009.\nEdwin T Jaynes. Probability Theory: The Logic of Science. Cambridge University Press, 2003.\nYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\nAndrej Karpathy. Reinforcejs, 2015. http://cs.stanford.edu/people/karpathy/ reinforcejs/.\nLevente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In Proceedings of the 17th European Conference on Machine Learning, ECML’06, pages 282–293, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN 3-540-45375-X, 978-3-540-45375-8. doi: 10. 1007/11871842 29. URL http://dx.doi.org/10.1007/11871842_29.\nRay Kurzweil. The Singularity is Near: When Humans Transcend Biology. Viking Books, 2005.\nGuillaume Lample and Devandra Singh Chaplot. Playing FPS games with deep reinforcement learning. arXiv preprint, 2016.\nTor Lattimore. Theory of General Reinforcement Learning. PhD thesis, Australian National University, 2013.\nTor Lattimore and Marcus Hutter. Asymptotically optimal agents. In Algorithmic Learning Theory, pages 368–382. Springer, 2011.\nTor Lattimore and Marcus Hutter. General time consistent discounting. Theoretical Computer Science, 519:140–154, 2014a.\nTor Lattimore and Marcus Hutter. Bayesian reinforcement learning with exploration. In ALT, pages 170–184. Springer, 2014b.\nTor Lattimore, Marcus Hutter, and Peter Sunehag. The sample-complexity of general reinforcement learning. CoRR, abs/1308.4828, 2013. URL http://arxiv.org/abs/ 1308.4828."
    }, {
      "heading" : "78 Bibliography",
      "text" : "Y. LeCun and Y. Bengio. Convolutional networks for images, speech, and time-series. In M. A. Arbib, editor, The Handbook of Brain Theory and Neural Networks. MIT Press, 1995.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553): 436–444, 2015.\nShane Legg. Machine Super Intelligence. PhD thesis, University of Lugano, 2008.\nJan Leike. Nonparametric General Reinforcement Learning. PhD thesis, Australian National University, 2016a.\nJan Leike. Balancing exploration and exploitation in model-based reinforcement learning. 2016b. Under preparation.\nJan Leike and Marcus Hutter. Bad universal priors and notions of optimality. In Conference on Learning Theory, pages 1244–1259, 2015.\nJan Leike, Tor Lattimore, Laurent Orseau, and Marcus Hutter. Thompson sampling is asymptotically optimal in general environments. In Uncertainty in Artificial Intelligence, 2016.\nJ. J. Leonard and H. F. Durrant-Whyte. Simultaneous map building and localization for an autonomous mobile robot. In Intelligent Robots and Systems ’91. ’Intelligence for Mechanical Systems, Proceedings IROS ’91. IEEE/RSJ International Workshop on, pages 1442–1447 vol.3, Nov 1991. doi: 10.1109/IROS.1991.174711.\nMing Li and Paul M. B. Vitányi. An Introduction to Kolmogorov Complexity and Its Applications. Texts in Computer Science. Springer, 3rd edition, 2008.\nMaxwell W. Libbrecht and William Stafford Noble. Machine learning applications in genetics and genomics. Nature Reviews Genetics, 16(6):321–32, 5 2015.\nDavid J. C. MacKay. Information Theory, Inference & Learning Algorithms. Cambridge University Press, New York, NY, USA, 2002. ISBN 0521642981.\nJarryd Martin, Tom Everitt, and Marcus Hutter. Death and suicide in universal artificial intelligence. In Artificial General Intelligence, 2016.\nJohn McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. A proposal for the Dartmouth summer research project on artificial intelligence. 1955.\nFrederic P. Miller, Agnes F. Vandome, and John McBrewster. AI Winter. Alpha Press, 2009. ISBN 6130079133, 9786130079130.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. Technical report, Google DeepMind, 2013. http://arxiv.org/abs/1312.5602.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.\nBibliography 79\nJames Moor. The Dartmouth College artificial intelligence conference: The next fifty years. AI Magazine, 27(4):87, 2006.\nHans Moravec. Mind Children: The Future of Robot and Human Intelligence. Harvard University Press, Cambridge, MA, USA, 1988. ISBN 0-674-57616-0.\nOskar Morgenstern and John von Neumann. Theory of Games and Economic Behavior. Princeton University Press, 1944.\nKevin P Murphy. Machine Learning: A Probabilistic Perspective. MIT Press, 2012.\nVincent C Müller and Nick Bostrom. Future progress in artificial intelligence: A survey of expert opinion. Fundamental Issues of Artificial Intelligence, pages 553–571, 2016.\nStephen M Omohundro. The basic AI drives. In Artificial General Intelligence, pages 483–492, 2008.\nLaurent Orseau. Optimality issues of universal greedy agents with static priors. In Algorithmic Learning Theory, pages 345–359. Springer, 2010.\nLaurent Orseau. Universal knowledge-seeking agents. In Algorithmic Learning Theory, pages 353–367. Springer, 2011.\nLaurent Orseau. Asymptotic non-learnability of universal agents with computable horizon functions. Theoretical Computer Science, 473:149–156, 2013.\nLaurent Orseau. Universal knowledge-seeking agents. Theoretical Computer Science, 519:127–139, 2014.\nLaurent Orseau, Tor Lattimore, and Marcus Hutter. Universal knowledge-seeking agents for stochastic environments. In Algorithmic Learning Theory, pages 158–172. Springer, 2013.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.\nClifton Phua, Vincent C. S. Lee, Kate Smith-Miles, and Ross W. Gayler. A comprehensive survey of data mining-based fraud detection research. CoRR, abs/1009.6119, 2010. URL http://arxiv.org/abs/1009.6119.\nStuart J Russell and Peter Norvig. Artificial Intelligence. A Modern Approach. Prentice Hall, 3rd edition, 2010.\nHasim Sak, Andrew W. Senior, Kanishka Rao, and Françoise Beaufays. Fast and accurate recurrent neural network acoustic models for speech recognition. CoRR, abs/1507.06947, 2015. URL http://arxiv.org/abs/1507.06947.\nJ. Schmidhuber. Curious model-building control systems. In Proc. Int. J. Conf. Neural Networks, pages 1458–1463. IEEE Press, 1991.\nJürgen Schmidhuber. Deep learning in neural networks: An overview. Neural Networks, 61:85–117, 2015."
    }, {
      "heading" : "80 Bibliography",
      "text" : "David Silver and Joel Veness. Monte-Carlo planning in large POMDPs. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 2164–2172. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/ 4031-monte-carlo-planning-in-large-pomdps.pdf.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.\nDaniel Smilkov and Shan Carter. Tensorflow Playground, 2016. http://http:// playground.tensorflow.org/.\nMalcolm Strens. A Bayesian framework for reinforcement learning. In International Conference on Machine Learning, pages 943–950, 2000.\nPeter Sunehag and Marcus Hutter. Optimistic AIXI. In Artificial General Intelligence, pages 312–321. Springer, 2012.\nPeter Sunehag and Marcus Hutter. Rationality, optimism and guarantees in general reinforcement learning. Journal of Machine Learning Research, 16:1345–1390, 2015.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1998.\nRichard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems, volume 12, pages 1057–1063. MIT Press, 1999.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. URL http://arxiv.org/abs/1512.00567.\nWilliam R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, pages 285–294, 1933.\nS. Thrun. The role of exploration in learning control. In D.A. White and D.A. Sofge, editors, Handbook for Intelligent Control: Neural, Fuzzy and Adaptive Approaches. Van Nostrand Reinhold, Florence, Kentucky 41022, 1992.\nSebastian Thrun, Dieter Fox, Wolfram Burgard, and Frank Dellaert. Monte carlo localization for mobile robots. In In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA, 1999.\nA. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. WaveNet: A Generative Model for Raw Audio. ArXiv e-prints, September 2016.\nJoel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Silver. A MonteCarlo AIXI approximation. Journal of Artificial Intelligence Research, 40(1):95–142, 2011.\nBibliography 81\nJoel Veness, Marc G Bellemare, Marcus Hutter, Alvin Chua, and Guillaume Desjardins. Compress and control. In AAAI, 2015.\nVernor Vinge. The coming technological singularity. Vision 21: Interdisciplinary Science and Engineering in the Era of Cyberspace, 1:11–22, 1993. http://www.rohan.sdsu.edu/ faculty/vinge/misc/singularity.html.\nChristopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279– 292, 1992.\nMartin Wattenberg, Fernanda Viégas, and Ian Johnson. How to use t-sne effectively. http://distill.pub/2016/misread-tsne/, 2016.\nEliezer Yudkowsky. Rationality: From AI to Zombies. Amazon, 2015."
    } ],
    "references" : [ {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Bibliography Peter Auer", "Thomas Jaksch", "Ronald Ortner" ],
      "venue" : "In Advances in Neural Information Processing Systems, pages 89–96,",
      "citeRegEx" : "Auer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2009
    }, {
      "title" : "Supervised Learning, pages 45–63",
      "author" : [ "G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Rémi Munos" ],
      "venue" : null,
      "citeRegEx" : "Bellemare et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2004
    }, {
      "title" : "Dynamic Programming and Optimal Control",
      "author" : [ "Dimitri P Bertsekas", "John Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "Bertsekas and Tsitsiklis.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bertsekas and Tsitsiklis.",
      "year" : 2016
    }, {
      "title" : "Superintelligence: Paths, Dangers, Strategies",
      "author" : [ "Nick Bostrom" ],
      "venue" : "OpenAI Gym,",
      "citeRegEx" : "1046712",
      "shortCiteRegEx" : "1046712",
      "year" : 2014
    }, {
      "title" : "A survey of monte carlo tree search methods",
      "author" : [ "C.B. Browne", "E. Powley", "D. Whitehouse", "S.M. Lucas", "P.I. Cowling", "P. Rohlfshagen", "S. Tavener", "D. Perez", "S. Samothrakis", "S. Colton" ],
      "venue" : "IEEE Transactions on Computational Intelligence and AI in Games,",
      "citeRegEx" : "Browne et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Browne et al\\.",
      "year" : 2012
    }, {
      "title" : "Avoiding wireheading with value reinforcement learning",
      "author" : [ "Tom Everitt", "Marcus Hutter" ],
      "venue" : "Artificial General Intelligence,",
      "citeRegEx" : "Everitt and Hutter.,? \\Q2016\\E",
      "shortCiteRegEx" : "Everitt and Hutter.",
      "year" : 2016
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : "Springer, 2nd edition,",
      "citeRegEx" : "Hastie et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2016
    }, {
      "title" : "Time-inconsistent preferences and consumer self-control",
      "author" : [ "Stephen J Hoch", "George Loewenstein" ],
      "venue" : "Journal of Consumer Research,",
      "citeRegEx" : "Hoch and Loewenstein.,? \\Q1991\\E",
      "shortCiteRegEx" : "Hoch and Loewenstein.",
      "year" : 1991
    }, {
      "title" : "Preparing for the future of artificial intelligence, 2016. Marcus Hutter. A theory of universal artificial intelligence based on algorithmic complexity",
      "author" : [ "John Holdren", "Ed Felten", "Terah Lyons", "Michael Garris" ],
      "venue" : "Technical report, IDSIA,",
      "citeRegEx" : "Holdren et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Holdren et al\\.",
      "year" : 2000
    }, {
      "title" : "Self-optimizing and Pareto-optimal policies in general environments based on Bayes-mixtures",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Technical report, IDSIA,",
      "citeRegEx" : "Hutter.,? \\Q2002\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2002
    }, {
      "title" : "Universal Artificial Intelligence",
      "author" : [ "Marcus Hutter" ],
      "venue" : "Edwin T Jaynes. Probability Theory: The Logic of Science",
      "citeRegEx" : "Hutter.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hutter.",
      "year" : 2005
    }, {
      "title" : "Caffe: Convolutional architecture for fast feature embedding",
      "author" : [ "Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell" ],
      "venue" : "arXiv preprint arXiv:1408.5093,",
      "citeRegEx" : "Jia et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jia et al\\.",
      "year" : 2014
    }, {
      "title" : "Bandit based monte-carlo planning",
      "author" : [ "Levente Kocsis", "Csaba Szepesvári" ],
      "venue" : "In Proceedings of the 17th European Conference on Machine Learning,",
      "citeRegEx" : "Kocsis and Szepesvári.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis and Szepesvári.",
      "year" : 2006
    }, {
      "title" : "Playing FPS games with deep reinforcement learning",
      "author" : [ "Guillaume Lample", "Devandra Singh Chaplot" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "2005",
      "shortCiteRegEx" : "2005",
      "year" : 2016
    }, {
      "title" : "Asymptotically optimal agents",
      "author" : [ "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Lattimore and Hutter.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lattimore and Hutter.",
      "year" : 2011
    }, {
      "title" : "The sample-complexity of general reinforcement learning",
      "author" : [ "Springer", "2014b. Tor Lattimore", "Marcus Hutter", "Peter Sunehag" ],
      "venue" : "CoRR, abs/1308.4828,",
      "citeRegEx" : "Springer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Springer et al\\.",
      "year" : 2013
    }, {
      "title" : "Convolutional networks for images, speech, and time-series",
      "author" : [ "Y. LeCun", "Y. Bengio" ],
      "venue" : "The Handbook of Brain Theory and Neural Networks. MIT Press,",
      "citeRegEx" : "LeCun and Bengio.,? \\Q1995\\E",
      "shortCiteRegEx" : "LeCun and Bengio.",
      "year" : 1995
    }, {
      "title" : "Bad universal priors and notions of optimality",
      "author" : [ "Jan Leike", "Marcus Hutter" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Leike and Hutter.,? \\Q2015\\E",
      "shortCiteRegEx" : "Leike and Hutter.",
      "year" : 2015
    }, {
      "title" : "Simultaneous map building and localization for an autonomous mobile robot. In Intelligent Robots and Systems ’91",
      "author" : [ "J.J. Leonard", "H.F. Durrant-Whyte" ],
      "venue" : "’Intelligence for Mechanical Systems, Proceedings IROS ’91. IEEE/RSJ International Workshop on,",
      "citeRegEx" : "2016",
      "shortCiteRegEx" : "2016",
      "year" : 1991
    }, {
      "title" : "An Introduction to Kolmogorov Complexity and Its Applications",
      "author" : [ "Ming Li", "Paul M.B. Vitányi" ],
      "venue" : "Texts in Computer Science. Springer,",
      "citeRegEx" : "Li and Vitányi.,? \\Q2008\\E",
      "shortCiteRegEx" : "Li and Vitányi.",
      "year" : 2008
    }, {
      "title" : "Death and suicide in universal artificial intelligence",
      "author" : [ "Jarryd Martin", "Tom Everitt", "Marcus Hutter" ],
      "venue" : "In Artificial General Intelligence,",
      "citeRegEx" : "Martin et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Martin et al\\.",
      "year" : 2002
    }, {
      "title" : "Playing Atari with deep reinforcement learning",
      "author" : [ "Frederic P. Miller", "Agnes F. Vandome", "John McBrewster. AI Winter" ],
      "venue" : "Alpha Press,",
      "citeRegEx" : "Miller et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Miller et al\\.",
      "year" : 2009
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "The Dartmouth College artificial intelligence conference: The next fifty years",
      "author" : [ "James Moor" ],
      "venue" : "AI Magazine,",
      "citeRegEx" : "Moor.,? \\Q2006\\E",
      "shortCiteRegEx" : "Moor.",
      "year" : 2006
    }, {
      "title" : "Theory of Games and Economic Behavior",
      "author" : [ "Oskar Morgenstern", "John von Neumann" ],
      "venue" : "Fundamental Issues of Artificial Intelligence,",
      "citeRegEx" : "Morgenstern and Neumann.,? \\Q1944\\E",
      "shortCiteRegEx" : "Morgenstern and Neumann.",
      "year" : 1944
    }, {
      "title" : "The basic AI drives",
      "author" : [ "Stephen M Omohundro" ],
      "venue" : "In Artificial General Intelligence, pages 483–492,",
      "citeRegEx" : "Omohundro.,? \\Q2008\\E",
      "shortCiteRegEx" : "Omohundro.",
      "year" : 2008
    }, {
      "title" : "Asymptotic non-learnability of universal agents with computable horizon functions",
      "author" : [ "Laurent Orseau" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "Orseau.,? \\Q2011\\E",
      "shortCiteRegEx" : "Orseau.",
      "year" : 2011
    }, {
      "title" : "Universal knowledge-seeking agents for stochastic environments",
      "author" : [ "Laurent Orseau", "Tor Lattimore", "Marcus Hutter" ],
      "venue" : "In Algorithmic Learning Theory,",
      "citeRegEx" : "Orseau et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Orseau et al\\.",
      "year" : 2013
    }, {
      "title" : "Scikit-learn: Machine learning in Python",
      "author" : [ "M. Brucher", "M. Perrot", "E. Duchesnay" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brucher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Brucher et al\\.",
      "year" : 2011
    }, {
      "title" : "Artificial Intelligence. A Modern Approach",
      "author" : [ "Stuart J Russell", "Peter Norvig" ],
      "venue" : "CoRR, abs/1507.06947,",
      "citeRegEx" : "Russell and Norvig.,? \\Q2010\\E",
      "shortCiteRegEx" : "Russell and Norvig.",
      "year" : 2010
    }, {
      "title" : "Curious model-building control systems",
      "author" : [ "J. Schmidhuber" ],
      "venue" : "In Proc. Int. J. Conf. Neural Networks,",
      "citeRegEx" : "Schmidhuber.,? \\Q1991\\E",
      "shortCiteRegEx" : "Schmidhuber.",
      "year" : 1991
    }, {
      "title" : "Monte-Carlo planning in large POMDPs",
      "author" : [ "Bibliography David Silver", "Joel Veness" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Silver and Veness.,? \\Q2010\\E",
      "shortCiteRegEx" : "Silver and Veness.",
      "year" : 2010
    }, {
      "title" : "Optimistic AIXI",
      "author" : [ "2000. Peter Sunehag", "Marcus Hutter" ],
      "venue" : "Conference on Machine Learning,",
      "citeRegEx" : "Sunehag and Hutter.,? \\Q2012\\E",
      "shortCiteRegEx" : "Sunehag and Hutter.",
      "year" : 2012
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Richard S. Sutton", "Andrew G. Barto" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "Rethinking the inception architecture for computer vision",
      "author" : [ "Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna" ],
      "venue" : "CoRR, abs/1512.00567,",
      "citeRegEx" : "Szegedy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2015
    }, {
      "title" : "The role of exploration in learning control",
      "author" : [ "S. Thrun" ],
      "venue" : "Kentucky 41022,",
      "citeRegEx" : "Thrun.,? \\Q1992\\E",
      "shortCiteRegEx" : "Thrun.",
      "year" : 1992
    }, {
      "title" : "WaveNet: A Generative Model for Raw Audio",
      "author" : [ "A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu" ],
      "venue" : "Robotics and Automation (ICRA,",
      "citeRegEx" : "Oord et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Oord et al\\.",
      "year" : 2016
    }, {
      "title" : "Compress and control",
      "author" : [ "Joel Veness", "Marc G Bellemare", "Marcus Hutter", "Alvin Chua", "Guillaume Desjardins" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Veness et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Veness et al\\.",
      "year" : 2015
    }, {
      "title" : "Rationality: From AI to Zombies",
      "author" : [ "Eliezer Yudkowsky" ],
      "venue" : null,
      "citeRegEx" : "Yudkowsky.,? \\Q2015\\E",
      "shortCiteRegEx" : "Yudkowsky.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 33,
      "context" : "Reinforcement learning (RL; Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1995) is a general and powerful framework with which to study and implement artificial intelligence (AI; Russell and Norvig, 2010).",
      "startOffset" : 23,
      "endOffset" : 83
    }, {
      "referenceID" : 29,
      "context" : "Reinforcement learning (RL; Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1995) is a general and powerful framework with which to study and implement artificial intelligence (AI; Russell and Norvig, 2010).",
      "startOffset" : 178,
      "endOffset" : 208
    }, {
      "referenceID" : 22,
      "context" : "Recent advances in deep learning (Schmidhuber, 2015) have enabled RL algorithms to achieve impressive performance in restricted domains such as playing Atari video games (Mnih et al., 2015) and, recently, the board game Go (Silver et al.",
      "startOffset" : 170,
      "endOffset" : 189
    }, {
      "referenceID" : 17,
      "context" : "Recently, AIXI has been shown to be flawed in important ways; it doesn’t explore enough to be asymptotically optimal (Orseau, 2010), and it can perform poorly with certain priors (Leike and Hutter, 2015).",
      "startOffset" : 179,
      "endOffset" : 203
    }, {
      "referenceID" : 26,
      "context" : "Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al.",
      "startOffset" : 122,
      "endOffset" : 136
    }, {
      "referenceID" : 27,
      "context" : "Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al., 2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike, 2016a), Thompson sampling (Leike et al.",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 10,
      "context" : "It is in this way that AIXI incorporates both the famous principles of Epicurus and Ockham (Hutter, 2005).",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 14,
      "context" : "2 MDL Agent (Lattimore and Hutter, 2011) .",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 21,
      "context" : "While the field has notoriously over-promised and under-delivered in the past (Moravec, 1988; Miller et al., 2009), there now seems to be a growing body of evidence in favor of optimism.",
      "startOffset" : 78,
      "endOffset" : 114
    }, {
      "referenceID" : 29,
      "context" : "Algorithms and ideas that have been developed over the past thirty years or so are being applied with significant success in numerous domains; natural language processing, image recognition, medical diagonosis, robotics, and many more (Russell and Norvig, 2010).",
      "startOffset" : 235,
      "endOffset" : 261
    }, {
      "referenceID" : 38,
      "context" : "1) This, and all subsequent chapter quotes, are taken from Rationality: From AI to Zombies (Yudkowsky, 2015).",
      "startOffset" : 91,
      "endOffset" : 108
    }, {
      "referenceID" : 11,
      "context" : ", 2016), Caffe (Jia et al., 2014), and TensorFlow (Abadi et al.",
      "startOffset" : 15,
      "endOffset" : 33
    }, {
      "referenceID" : 34,
      "context" : "High profile breakthroughs of statistical machine learning include image recognition (Szegedy et al., 2015), voice recognition (Sak et al.",
      "startOffset" : 85,
      "endOffset" : 107
    }, {
      "referenceID" : 6,
      "context" : "where L is constructed in such a way as to penalize prediction error Bishop (2006); Hastie et al. (2009); Murphy (2012).",
      "startOffset" : 84,
      "endOffset" : 105
    }, {
      "referenceID" : 6,
      "context" : "where L is constructed in such a way as to penalize prediction error Bishop (2006); Hastie et al. (2009); Murphy (2012). High profile breakthroughs of statistical machine learning include image recognition (Szegedy et al.",
      "startOffset" : 84,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "The field of hard, or general AI has infamously had a history of overpromising and under-delivering, virtually since its birth (Moor, 2006; Miller et al., 2009).",
      "startOffset" : 127,
      "endOffset" : 160
    }, {
      "referenceID" : 21,
      "context" : "The field of hard, or general AI has infamously had a history of overpromising and under-delivering, virtually since its birth (Moor, 2006; Miller et al., 2009).",
      "startOffset" : 127,
      "endOffset" : 160
    }, {
      "referenceID" : 33,
      "context" : "The framework of choice for most researchers working in pursuit of AGI is called reinforcement learning (RL; Sutton and Barto, 1998).",
      "startOffset" : 104,
      "endOffset" : 132
    }, {
      "referenceID" : 22,
      "context" : "Some recent successes using systems based on this technique include achieving humanlevel performance at numerous Atari-2600 video games (Mnih et al., 2015), super-human performance at the board game Go (Silver et al.",
      "startOffset" : 136,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : ", 2016; Google, 2016), and super-human performance at the first-person shooter Doom (Lample and Chaplot, 2016). This has inspired a whole sub-field called deep reinforcement learning, which is moving quickly and generating many publications and software implementations. While this is all very impressive, these are primarily engineering successes, rather than scientific ones. The fundamental ideas and algorithms used in DQN date from the early nineties; Q-learning is due to Watkins and Dayan (1992), and convolutional neural networks and deep learning are usually attributed to LeCun and Bengio (1995).",
      "startOffset" : 2,
      "endOffset" : 503
    }, {
      "referenceID" : 16,
      "context" : "The fundamental ideas and algorithms used in DQN date from the early nineties; Q-learning is due to Watkins and Dayan (1992), and convolutional neural networks and deep learning are usually attributed to LeCun and Bengio (1995). Arguably, the scientific breakthroughs necessary for AGI are yet to be made, and are still some way",
      "startOffset" : 204,
      "endOffset" : 228
    }, {
      "referenceID" : 33,
      "context" : "AIXI is formulated as a Bayesian reinforcement learner, and makes few assumptions about the nature of its environment; notably, when studying AIXI we lift the ubiquitous Markov assumption on which algorithms like Q-learning depend for convergence (Sutton and Barto, 1998).",
      "startOffset" : 247,
      "endOffset" : 271
    }, {
      "referenceID" : 17,
      "context" : "Recently, there have been a number of key negative results proven about AIXI; namely that it isn’t asymptotically optimal (Orseau, 2010, 2013) – a concept we will formally introduce in Chapter 2 – and it can be made to perform poorly with certain priors (Leike and Hutter, 2015).",
      "startOffset" : 254,
      "endOffset" : 278
    }, {
      "referenceID" : 26,
      "context" : "These results have motivated, in part, the development of alternative GRL agents: entropy-seeking agents (Orseau, 2011), optimistic AIXI Sunehag and Hutter (2012), knowledge-seeking agents (Orseau et al.",
      "startOffset" : 105,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : "These results have motivated, in part, the development of alternative GRL agents: entropy-seeking agents (Orseau, 2011), optimistic AIXI Sunehag and Hutter (2012), knowledge-seeking agents (Orseau et al., 2013), minimum description length agents (Lattimore, 2013), Bayes with exploration (Lattimore, 2013; Lattimore and Hutter, 2014b), and Thompson sampling (Leike et al.",
      "startOffset" : 189,
      "endOffset" : 210
    }, {
      "referenceID" : 9,
      "context" : "One proposed answer to the first of these questions is the famous AIXI model, which is a parameter-free (up to a choice of prior) and general model of unbounded rationality in unknown environments (Hutter, 2000, 2002, 2005). AIXI is formulated as a Bayesian reinforcement learner, and makes few assumptions about the nature of its environment; notably, when studying AIXI we lift the ubiquitous Markov assumption on which algorithms like Q-learning depend for convergence (Sutton and Barto, 1998). Because of this important distinction, we refer to AIXI as a general reinforcement learning4 (GRL) agent (Lattimore et al., 2013). Recently, there have been a number of key negative results proven about AIXI; namely that it isn’t asymptotically optimal (Orseau, 2010, 2013) – a concept we will formally introduce in Chapter 2 – and it can be made to perform poorly with certain priors (Leike and Hutter, 2015). These results have motivated, in part, the development of alternative GRL agents: entropy-seeking agents (Orseau, 2011), optimistic AIXI Sunehag and Hutter (2012), knowledge-seeking agents (Orseau et al.",
      "startOffset" : 198,
      "endOffset" : 1072
    }, {
      "referenceID" : 9,
      "context" : "Representative examples include Keras-js, a demo of very large convolutional neural networks (Chen, 2016); TensorFlow Playground, a highly interactive demo designed to give intuition for how neural networks classify data (Smilkov Elsewhere in the literature – most prominently by Hutter (2005) and Orseau (2011) – the term universal AI is used.",
      "startOffset" : 280,
      "endOffset" : 294
    }, {
      "referenceID" : 9,
      "context" : "Representative examples include Keras-js, a demo of very large convolutional neural networks (Chen, 2016); TensorFlow Playground, a highly interactive demo designed to give intuition for how neural networks classify data (Smilkov Elsewhere in the literature – most prominently by Hutter (2005) and Orseau (2011) – the term universal AI is used.",
      "startOffset" : 280,
      "endOffset" : 312
    }, {
      "referenceID" : 15,
      "context" : "For a more complete and rigorous treatment of GRL, we refer the reader to the excellent PhD theses of Leike (2016a) and Lattimore (2013), and of course to the seminal book, Universal Artificial Intelligence by Hutter (2005).",
      "startOffset" : 109,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "For a more complete and rigorous treatment of GRL, we refer the reader to the excellent PhD theses of Leike (2016a) and Lattimore (2013), and of course to the seminal book, Universal Artificial Intelligence by Hutter (2005). The Chapter is laid out as follows: In Section 2.",
      "startOffset" : 210,
      "endOffset" : 224
    }, {
      "referenceID" : 29,
      "context" : "For some general background, we refer the reader to MacKay (2002) for probability and information theory, Bishop (2006) for machine learning and statistics, Russell and Norvig (2010) for artificial intelligence, and Sutton and Barto (1998) for reinforcement learning.",
      "startOffset" : 157,
      "endOffset" : 183
    }, {
      "referenceID" : 29,
      "context" : "For some general background, we refer the reader to MacKay (2002) for probability and information theory, Bishop (2006) for machine learning and statistics, Russell and Norvig (2010) for artificial intelligence, and Sutton and Barto (1998) for reinforcement learning.",
      "startOffset" : 157,
      "endOffset" : 240
    }, {
      "referenceID" : 33,
      "context" : "In contrast to machine learning, in the reinforcement learning setting, the training data that the system receives is now dependent on its actions; we thus introduce agency to the learning problem (Sutton and Barto, 1998).",
      "startOffset" : 197,
      "endOffset" : 221
    }, {
      "referenceID" : 9,
      "context" : "This set-up has been honed by (for example) Lattimore and Hutter (2011) and Leike et al.",
      "startOffset" : 58,
      "endOffset" : 72
    }, {
      "referenceID" : 9,
      "context" : "This set-up has been honed by (for example) Lattimore and Hutter (2011) and Leike et al. (2016).",
      "startOffset" : 58,
      "endOffset" : 96
    }, {
      "referenceID" : 9,
      "context" : "Definition 7 (ε-Effective horizon; Lattimore and Hutter (2014a)).",
      "startOffset" : 49,
      "endOffset" : 64
    }, {
      "referenceID" : 14,
      "context" : "Definition 10 (Asymptotic optimality; Lattimore and Hutter, 2011).",
      "startOffset" : 14,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : "The quest for good notions of optimality is currently an open problem in the theory of GRL (Leike and Hutter, 2015; Leike, 2016a).",
      "startOffset" : 91,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "For the purpose of studying the general reinforcement learning problem, we consider primarily Bayesian agents, as they are the most general and principled way to think about the problem of induction (Hutter, 2005).",
      "startOffset" : 199,
      "endOffset" : 213
    }, {
      "referenceID" : 19,
      "context" : "where U is a universal Turing machine (Li and Vitányi, 2008).",
      "startOffset" : 38,
      "endOffset" : 60
    }, {
      "referenceID" : 37,
      "context" : "One can derive computable approximations of Solomonoff induction, most notably by using a generalization of the Context-Tree Weighting algorithm, which is a mixture over Markov models up to some finite order n, weighted by their complexity; this is used in the well-known MC-AIXI-CTW implementation due to Veness et al. (2011).",
      "startOffset" : 306,
      "endOffset" : 327
    }, {
      "referenceID" : 17,
      "context" : "Theorem 3 (Dogmatic prior; Leike and Hutter, 2015).",
      "startOffset" : 10,
      "endOffset" : 50
    }, {
      "referenceID" : 35,
      "context" : "A principled solution to exploration by intrinsic motivation is one of the central problems in reinforcement learning (Thrun, 1992).",
      "startOffset" : 118,
      "endOffset" : 131
    }, {
      "referenceID" : 26,
      "context" : "• They remove the dependence on arbitrary reward signals or utility functions; up to a choice of model class and prior, ‘knowledge’ is an objective quantity (Orseau, 2011).",
      "startOffset" : 157,
      "endOffset" : 171
    }, {
      "referenceID" : 26,
      "context" : "Definition 14 (Utility Agent; Orseau, 2011).",
      "startOffset" : 14,
      "endOffset" : 43
    }, {
      "referenceID" : 26,
      "context" : "Definition 16 (Square-KSA; Orseau, 2011).",
      "startOffset" : 14,
      "endOffset" : 40
    }, {
      "referenceID" : 26,
      "context" : "Definition 17 (Shannon-KSA; Orseau, 2011).",
      "startOffset" : 14,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "The Shannon-KSA, with its expected utility being measured in bits, is closely related to Schmidhuber’s ‘curiosity learning’, which gets utility from making compression progress (Schmidhuber, 1991).",
      "startOffset" : 177,
      "endOffset" : 196
    }, {
      "referenceID" : 14,
      "context" : "2 MDL Agent (Lattimore and Hutter, 2011) Inputs: Model class M; prior w : M→ (0, 1]; a total ordering over M.",
      "startOffset" : 12,
      "endOffset" : 40
    }, {
      "referenceID" : 33,
      "context" : "verges to the value of the optimal policy (Sutton and Barto, 1998):",
      "startOffset" : 42,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "Its use dates back several decades, but was popularized and formalized in the last decade or so in the context of planning for computer Go (Browne et al., 2012).",
      "startOffset" : 139,
      "endOffset" : 160
    }, {
      "referenceID" : 29,
      "context" : "Analogously to minimax (Russell and Norvig, 2010), we construct a game tree, with Max (the agent) playing one turn, and Environment (some distribution over percepts) playing the other turn.",
      "startOffset" : 23,
      "endOffset" : 49
    }, {
      "referenceID" : 3,
      "context" : "Its use dates back several decades, but was popularized and formalized in the last decade or so in the context of planning for computer Go (Browne et al., 2012). Analogously to minimax (Russell and Norvig, 2010), we construct a game tree, with Max (the agent) playing one turn, and Environment (some distribution over percepts) playing the other turn. The branching factor of Max nodes is of course |A|, while the branching factor of Environment nodes is upper bounded by |E|. In contrast to minimax, which is used for deterministic games, we must collect sufficient samples from Environment nodes to get a good estimator V̂ of the expected value for this node. Needless to say, we wish to avoid expanding the tree out by naively visiting every history æt:m. Analogously to α-β pruning in the context of minimax, UCT is a MCTS algorithm due to Kocsis and Szepesvári (2006) that avoids expanding the whole tree, by only investigating ‘promising’-looking histories.",
      "startOffset" : 140,
      "endOffset" : 873
    }, {
      "referenceID" : 0,
      "context" : "The log T term in the numerator ensures that, asymptotically, we continue to visit every state-action pair infinitely often; this is necessary to establish regret bounds (Auer et al., 2009).",
      "startOffset" : 170,
      "endOffset" : 189
    }, {
      "referenceID" : 31,
      "context" : "(2011) present this generalization, ρUCT, in their famous MC-AIXI-CTW implementation paper, based on earlier work in Monte Carlo planning on partially-observable MDPs (Silver and Veness, 2010).",
      "startOffset" : 167,
      "endOffset" : 192
    }, {
      "referenceID" : 36,
      "context" : "Veness et al. (2011) present this generalization, ρUCT, in their famous MC-AIXI-CTW implementation paper, based on earlier work in Monte Carlo planning on partially-observable MDPs (Silver and Veness, 2010).",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 37,
      "context" : "4, we present a (slightly expanded, for clarity) version of the ρUCT algorithm due to Veness et al. (2011).",
      "startOffset" : 86,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "This simple environment models a classic situation from economics and decision theory in which humans have been known to be time-inconsistent – that is, informally, an agent acts impulsively on desires that don’t agree with its long-term preferences (Hoch and Loewenstein, 1991).",
      "startOffset" : 250,
      "endOffset" : 278
    }, {
      "referenceID" : 37,
      "context" : "For this reason, we eschew very general and flexible models such as the famous context-tree weighting data compressor used by Veness et al. (2011), since they will take too long to learn the environments for a practical demo.",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 37,
      "context" : "In this section, we discuss some subtle differences between our implementation and the referencee implementation by Veness et al. (2011), and we make some remarks about planning by",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 9,
      "context" : "Leike and Hutter (2015). The agent never moves to falsify this belief, since falling into the trap incurs a penalty of −5 per time step for eternity, compared to merely −1 per time step for waiting in the corner.",
      "startOffset" : 10,
      "endOffset" : 24
    }, {
      "referenceID" : 33,
      "context" : "We must assume at this point that the reader has some familiarity with the basic algorithms of reinforcement learning covered in Sutton and Barto (1998).",
      "startOffset" : 129,
      "endOffset" : 153
    }, {
      "referenceID" : 25,
      "context" : "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents.",
      "startOffset" : 85,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents.",
      "startOffset" : 85,
      "endOffset" : 143
    }, {
      "referenceID" : 5,
      "context" : "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents. In short, a sufficiently intelligent reinforcement learner will be motivated to subvert its designer’s intentions and take direct control of its reward signal and/or sensors, so as to maximize its reward signal directly, rather than indirectly by conforming to the intentions of its designer. This is known in the literature as wireheading, and is an open and significant problem in AI safety research Everitt et al. (2016); Everitt and Hutter (2016).",
      "startOffset" : 118,
      "endOffset" : 626
    }, {
      "referenceID" : 5,
      "context" : "In the context of designing artificial general intelligence, the wireheading problem (Omohundro, 2008; Hibbard, 2012; Everitt and Hutter, 2016) is a significant issue for reinforcement learning agents. In short, a sufficiently intelligent reinforcement learner will be motivated to subvert its designer’s intentions and take direct control of its reward signal and/or sensors, so as to maximize its reward signal directly, rather than indirectly by conforming to the intentions of its designer. This is known in the literature as wireheading, and is an open and significant problem in AI safety research Everitt et al. (2016); Everitt and Hutter (2016). We construct a simple environment in which the agent has an opportunity to wirehead: it is a normal Gridworld similar to those above, except that there is a tile which, if visited by the agent, will allow it to modify its own sensors so that all percepts have their reward signal replaced with the maximum number feasible; in JavaScript, this is Number.",
      "startOffset" : 118,
      "endOffset" : 653
    }, {
      "referenceID" : 10,
      "context" : "It is in this way that AIXI incorporates both the famous principles of Epicurus and Ockham (Hutter, 2005).",
      "startOffset" : 91,
      "endOffset" : 105
    }, {
      "referenceID" : 37,
      "context" : "• Implement planning-as-inference algorithms such as Compress and Control (Veness et al., 2015).",
      "startOffset" : 74,
      "endOffset" : 95
    } ],
    "year" : 2017,
    "abstractText" : "Reinforcement learning (RL; Sutton and Barto, 1998; Bertsekas and Tsitsiklis, 1995) is a general and powerful framework with which to study and implement artificial intelligence (AI; Russell and Norvig, 2010). Recent advances in deep learning (Schmidhuber, 2015) have enabled RL algorithms to achieve impressive performance in restricted domains such as playing Atari video games (Mnih et al., 2015) and, recently, the board game Go (Silver et al., 2016). However, we are still far from constructing a generally intelligent agent. Many of the obstacles and open questions are conceptual: What does it mean to be intelligent? How does one explore and learn optimally in general, unknown environments? What, in fact, does it mean to be optimal in the general sense? The universal Bayesian agent AIXI (Hutter, 2000, 2003, 2005) is a model of a maximally intelligent agent, and plays a central role in the sub-field of general reinforcement learning (GRL). Recently, AIXI has been shown to be flawed in important ways; it doesn’t explore enough to be asymptotically optimal (Orseau, 2010), and it can perform poorly with certain priors (Leike and Hutter, 2015). Several variants of AIXI have been proposed to attempt to address these shortfalls: among them are entropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al., 2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike, 2016a), Thompson sampling (Leike et al., 2016), and optimism (Sunehag and Hutter, 2015). We present AIXIjs, a JavaScript implementation of these GRL agents. This implementation is accompanied by a framework for running experiments against various environments, similar to OpenAI Gym (Brockman et al., 2016), and a suite of interactive demos that explore different properties of the agents, similar to REINFORCEjs (Karpathy, 2015). We use AIXIjs to present numerous experiments illustrating fundamental properties of, and differences between, these agents. As far we are aware, these are the first experiments comparing the behavior of GRL agents in non-trivial settings. Our aim is for this software and accompanying documentation to serve several purposes: 1. to help introduce newcomers to the field of general reinforcement learning, 2. to provide researchers with the means to demonstrate new theoretical results relating to universal AI at conferences and workshops, 3. to serve as a platform with which to run empirical studies on AIXI variants in small environments, and 4. to serve as an open-source reference implementation of these agents.",
    "creator" : "LaTeX with hyperref package"
  }
}