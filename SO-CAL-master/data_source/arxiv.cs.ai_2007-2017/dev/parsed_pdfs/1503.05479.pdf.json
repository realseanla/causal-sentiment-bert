{
  "name" : "1503.05479.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm",
    "authors" : [ "Qinqing Zheng", "Ryota Tomioka" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ n+ √ HK−1) bound, in which the parameter H controls\nthe blend from the non-convex estimator to mode-wise nuclear norm minimization. Furthermore, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising performance even with H = O(1)."
    }, {
      "heading" : "1 Introduction",
      "text" : "Tensor is a natural way to express higher order interactions for a variety of data and tensor decomposition has been successfully applied to wide areas ranging from chemometrics, signal processing, to neuroimaging; see [15, 18] for a survey. Moreover, recently it has become an active area in the context of learning latent variable models [3].\nMany problems related to tensors, such as, finding the rank, or a best rank-one approaximation of a tensor is known to be NP hard [11, 8]. Nevertheless we can address statistical problems, such as, how well we can recover a low-rank tensor from its randomly corrupted version (tensor denoising) or from partial observations (tensor completion). Since we can convert a tensor into a matrix by an operation known as unfolding, recent work [25, 19, 20, 13] has shown that we do get nontrivial guarantees by using some norms or singular value decompositions. More specifically, Richard & Montanari [20] has shown that when a rank-one Kth order tensor of size n × · · · × n is corrupted by standard Gaussian noise, a nontrivial bound can be shown with high probability if the signal to noise ratio β/σ % ndK/2e/2 by a method called the recursive unfolding1. Note\n1We say an % bn if there is a constant C > 0 such that an ≥ C · bn.\nar X\niv :1\n50 3.\n05 47\n9v 2\n[ cs\n.L G\n] 2\n7 O\nthat β/σ % √ n is sufficient for matrices (K = 2) and also for tensors if we use the best rankone approximation (which is known to be NP hard) as an estimator. On the other hand, Jain & Oh [13] analyzed the tensor completion problem and proposed an algorithm that requires O(n3/2 · polylog(n)) samples forK = 3; while information theoretically we need at least Ω(n) samples and the intractable maximum likelihood estimator would requireO(n·polylog(n)) samples. Therefore, in both settings, there is a wide gap between the ideal estimator and current polynomial time algorithms. A subtle question that we will address in this paper is whether we need to unfold the tensor so that the resulting matrix become as square as possible, which was the reasoning underlying both [19, 20].\nAs a parallel development, non-convex estimators based on alternating minimization or nonlinear optimization [1, 21] have been widely applied and have performed very well when appropriately set up. Therefore it would be of fundamental importance to connect the wisdom of nonconvex estimators with the more theoretically motivated estimators that recently emerged.\nIn this paper, we explore such a connection by defining a new norm based on Kronecker products of factors that can be obtained by simple mode-wise singular value decomposition (SVD) of unfoldings (see notation section below), also known as the higher-order singular value decomposition (HOSVD) [6, 7]. We first study the non-asymptotic behavior of the leading singular vector from the ordinary (rectangular) unfolding X(k) and show a nontrivial bound for signal to noise ratio β/σ % nK/4. Thus the result also applies to odd order tensors confirming a conjecture in [20]. Furthermore, this motivates us to use the solution of mode-wise truncated SVDs to construct a new norm. We propose the subspace norm, which predicts an unknown low-rank tensor as a mixture of K low-rank tensors, in which each term takes the form\nfoldk(M (k)(P̂ (1) ⊗ · · · ⊗ P̂ (k−1) ⊗ P̂ (k+1) ⊗ · · · ⊗ P̂ (K) )>),\nwhere foldk is the inverse of unfolding (·)(k), ⊗ denotes the Kronecker product, and P̂ (k) ∈ Rn×H is a orthonormal matrix estimated from the mode-k unfolding of the observed tensor, for k = 1, . . . , K; H is a user-defined parameter, and M (k) ∈ Rn×HK−1 . Our theory tells us that with sufficiently high signal-to-noise ratio the estimated P̂ (k) spans the true factors.\nWe highlight our contributions below: 1. We prove that the required signal-to-noise ratio for recovering a Kth order rank one tensor from the ordinary unfolding is O(nK/4). Our analysis shows a curious two phase behavior: with high probability, when nK/4 - β/σ - nK/2, the error shows a fast decay as 1/β4; for β/σ % nK/2, the error decays slowly as 1/β2. We confirm this in a numerical simulation. 2. The proposed subspace norm is an interpolation between the intractable estimators that directly control the rank (e.g., HOSVD) and the tractable norm-based estimators. It becomes equivalent to the latent trace norm [23] when H = n at the cost of increased signal-to-noise ratio threshold (see Table 1). 3. The proposed estimator is more efficient than previously proposed norm based estimators, because the size of the SVD required in the algorithm is reduced from n× nK−1 to n×HK−1. 4. We also empirically demonstrate that the proposed subspace norm performs nearly optimally for constant order H .\nNotation Let X ∈ Rn1×n2×···×nK be a Kth order tensor. We will often use n1 = · · · = nK = n to simplify the notation but all the results in this paper generalizes to general dimensions. The inner product between a pair of tensors is defined as the inner products of them as vectors; i.e., 〈X ,W〉 = 〈vec(X ), vec(W)〉. For u ∈ Rn1 ,v ∈ Rn2 ,w ∈ Rn3 , u ◦ v ◦ w denotes the n1 × n2 × n3 rank-one tensor whose i, j, k entry is uivjwk. The rank of X is the minimum number of rank-one tensors required to write X as a linear combination of them. A mode-k fiber of tensor X is an nk dimensional vector that is obtained by fixing all but the kth index of X . The mode-k unfolding X(k) of tensor X is an nk × ∏ k′ 6=k nk′ matrix constructed by concatenating all the mode-k fibers along columns. We denote the spectral and Frobenius norms for matrices by ‖ · ‖ and ‖ · ‖F , respectively."
    }, {
      "heading" : "2 The power of ordinary unfolding",
      "text" : ""
    }, {
      "heading" : "2.1 A perturbation bound for the left singular vector",
      "text" : "We first establish a bound on recovering the left singular vector of a rank-one n×m matrix (with m > n) perturbed by random Gaussian noise.\nConsider the following model known as the information plus noise model [4]:\nX̃ = βuv> + σE, (1)\nwhere u and v are unit vectors, β is the signal strength, σ is the noise standard deviation, and the noise matrix E is assumed to be random with entries sampled i.i.d. from the standard normal distribution. Our goal is to lower-bound the correlation between u and the top left singular vector û of X̃ for signal-to-noise ratio β/σ % (mn)1/4 with high probability.\nA direct application of the classic Wedin perturbation theorem [28] to the rectangular matrix X̃ does not provide us the desired result. This is because it requires the signal to noise ratio β/σ ≥ 2‖E‖. Since the spectral norm of E scales as Op( √ n + √ m) [27], this would mean that we require β/σ % m1/2; i.e., the threshold is dominated by the number of columns m, if m ≥ n.\nAlternatively, we can view û as the leading eigenvector of X̃X̃ >\n, a square matrix. Our key insight is that we can decompose X̃X̃ > as follows:\nX̃X̃ > = (β2uu> +mσ2I) + (σ2EE> −mσ2I) + βσ(uv>E> +Evu>).\nNote that u is the leading eigenvector of the first term because adding an identity matrix does not change the eigenvectors. Moreover, we notice that there are two noise terms: the first term is a centered Wishart matrix and it is independent of the signal β; the second term is Gaussian distributed and depends on the signal β.\nThis implies a two-phase behavior corresponding to either the Wishart or the Gaussian noise term being dominant, depending on the value of β. Interestingly, we get a different speed of convergence for each of these phases as we show in the next theorem (the proof is given in Appendix D.1).\nTheorem 1. There exists a constant C such that with probability at least 1− 4e−n, if m/n ≥ C,\n|〈û,u〉| ≥  1− Cnm (β/σ)4 , if √ m > β σ ≥ (Cnm) 1 4 ,\n1− Cn (β/σ)2 , if β σ ≥ √ m,\notherwise, |〈û,u〉| ≥ 1− Cn (β/σ)2\nif β/σ ≥ √ Cn.\nIn other words, if X̃ has sufficiently many more columns than rows, as the signal to noise ratio β/σ increases, û first converges to u as 1/β4, and then as 1/β2. Figure 1(a) illustrates these results. We randomly generate a rank-one 100 × 10000 matrix perturbed by Gaussian noise, and measure the distance between û and u. The phase transition happens at β/σ = (nm)1/4, and there are two regimes of different convergence rates as Theorem 1 predicts."
    }, {
      "heading" : "2.2 Tensor Unfolding",
      "text" : "Now let’s apply the above result to the tensor version of information plus noise model studied by [20]. We consider a rank one n×· · ·×n tensor (signal) contaminated by Gaussian noise as follows:\nY = X ∗ + σE = βu(1) ◦ · · · ◦ u(K) + σE , (2) where factors u(k) ∈ Rn, k = 1, . . . , K, are unit vectors, which are not necessarily identical, and the entries of E ∈ Rn×···×n are i.i.d samples from the normal distributionN (0, 1). Note that this is slightly more general (and easier to analyze) than the symmetric setting studied by [20].\nSeveral estimators for recovering X ∗ from its noisy version Y have been proposed (see Table 1). Both the overlapped nuclear norm and latent nuclear norm discussed in [23] achives the relative performance guarantee\n|||X̂ − X ∗|||F/β ≤ Op ( σ √ nK−1/β ) , (3)\nwhere X̂ is the estimator. This bound implies that if we want to obtain relative error smaller than ε, we need the signal to noise ratio β/σ to scale as β/σ % √ nK−1/ε.\nMu et al. [19] proposed the square norm, defined as the nuclear norm of the matrix obtained by grouping the first bK/2c indices along the rows and the last dK/2e indices along the columns. This norm improves the right hand side of inequality (3) to Op(σ √ ndK/2e/β), which translates to\nrequiring β/σ % √ ndK/2e/ε for obtaining relative error ε. The intuition here is the more square the unfolding is the better the bound becomes. However, there is no improvement for K = 3. Richard and Montanari [20] studied the (symmetric version of) model (2) and proved that a recursive unfolding algorithm achieves the factor recovery error dist(û(k),u(k)) = ε with β/σ %√ ndK/2e/ε with high probability, where dist(u,u′) := min(‖u − u′‖, ‖u + u′‖). They also showed that the randomly initialized tensor power method [7, 16, 3] can achieve the same error ε with slightly worse threshold β/σ % max( √ n/ε2, nK/2) √ K logK also with high probability.\nThe reasoning underlying both [19] and [20] is that square unfolding is better. However, if we take the (ordinary) mode-k unfolding\nY (k) = βu (k) ( u(k−1) ⊗ · · · ⊗ u(1) ⊗ u(K) ⊗ · · · ⊗ u(k+1) )> + σE(k), (4)\nwe can see (4) as an instance of information plus noise model (1) where m/n = nK−2. Thus the ordinary unfolding satisfies the condition of Theorem 1 for n or K large enough.\nCorollary 1. Consider a K(≥ 3)th order rank one tensor contaminated by Gaussian noise as in (2). There exists a constant C such that if nK−2 ≥ C, with probability at least 1−4Ke−n, we have\ndist2(û(k),u(k)) ≤  2CnK (β/σ)4 , if n K−1 2 > β/σ ≥ C 1 4n K 4 , 2Cn\n(β/σ)2 , if β/σ ≥ n\nK−1 2 ,\nfor k = 1, . . . , K,\nwhere û(k) is the leading left singular vector of the rectangular unfolding Y (k).\nThis proves that as conjectured by [20], the threshold β/σ % nK/4 applies not only to the even order case but also to the odd order case. Note that Hopkins et al. [10] have shown a similar result without the sharp rate of convergence. The above corollary easily extends to more general n1 × · · · × nK tensor by replacing the conditions by √∏ ` 6=k n` > β/σ ≥ (C ∏K k=1 nk) 1/4 and\nβ/σ ≥ √∏\n6̀=k n`. The result also holds when X ∗ has rank higher than 1; see Appendix E. We demonstrate this result in Figure 1(b). The models behind the experiment are slightly more general ones in which [n1, n2, n3] = [20, 40, 60] or [40, 80, 120] and the signal X ∗ is rank two with β1 = 20 and β2 = 10. The plot shows the inner products 〈u(1)1 , û (1) 1 〉 and 〈u (1) 2 , û (1) 2 〉 as a measure of the quality of estimating the two mode-1 factors. The horizontal axis is the normalized noise standard deviation σ( ∏K k=1 nk)\n1/4. We can clearly see that the inner product decays symmetrically around β1 and β2 as predicted by Corollary 1 for both tensors."
    }, {
      "heading" : "3 Subspace norm for tensors",
      "text" : "Suppose the true tensor X ∗ ∈ Rn×···×n admits a minimum Tucker decomposition [26] of rank (R, . . . , R):\nX ∗ = ∑R i1=1 · · · ∑R iK=1 βi1i2...iKu (1) i1 ◦ · · · ◦ u(K)iK . (5)\nIf the core tensor C = (βi1...iK ) ∈ RR×···×R is superdiagonal, the above decomposition reduces to the canonical polyadic (CP) decomposition [9, 15]. The mode-k unfolding of the true tensor X ∗ can be written as follows:\nX∗(k) = U (k)C(k)\n( U (1) ⊗ · · · ⊗U (k−1) ⊗U (k+1) ⊗ · · · ⊗U (K) )> , (6)\nwhereC(k) is the mode-k unfolding of the core tensor C;U (k) is a n×RmatrixU (k) = [u(k)1 , . . . ,u (k) R ] for k = 1, . . . , K. Note that U (k) is not necessarily orthogonal. LetX∗(k) = P (k)Λ(k)Q(k) > be the SVD ofX∗(k). We will observe that\nQ(k) ∈ Span ( P (1) ⊗ · · · ⊗ P (k−1) ⊗ P (k+1) ⊗ · · · ⊗ P (K) ) (7)\nbecause of (6) and U (k) ∈ Span(P (k)). Corollary 1 shows that the left singular vectors P (k) can be recovered under mild conditions; thus the span of the right singular vectors can also be recovered. Inspired by this, we define a norm that models a tensor X as a mixture of tensors Z(1), . . . ,Z(K). We require that the modek unfolding of Z(k), i.e. Z(k)(k), has a low rank factorization Z (k) (k) = M (k)S(k) > , where M (k) ∈ Rn×HK−1 is a variable, and S(k) ∈ RnK−1×HK−1 is a fixed arbitrary orthonormal basis of some subspace, which we choose later to have the Kronecker structure in (7).\nIn the following, we define the subspace norm, suggest an approach to construct the right factor S(k), and prove the denoising bound in the end."
    }, {
      "heading" : "3.1 The subspace norm",
      "text" : "Consider a Kth order tensor of size n× · · ·n.\nDefinition 1. Let S(1), . . . ,S(K) be matrices such that S(k) ∈ RnK−1×HK−1 with H ≤ n. The subspace norm for a Kth order tensor X associated with {S(k)}Kk=1 is defined as\n|||X |||s :=\n{ inf{M (k)}Kk=1 ∑K k=1 ‖M\n(k)‖∗, if X ∈ Span({S(k)}Kk=1), +∞, otherwise,\nwhere ‖·‖∗ is the nuclear norm, and Span({S(k)}Kk=1) := { X ∈ Rn×···×n : ∃M (1), . . . ,M (K),X =∑K\nk=1 foldk(M (k)S(k)\n> ) } .\nIn the next lemma (proven in Appendix D.2), we show the dual norm of the subspace norm has a simple appealing form. As we see in Theorem 2, it avoids the O( √ nK−1) scaling (see the first column of Table 1) by restricting the influence of the noise term in the subspace defined by S(1), . . . ,S(K).\nLemma 1. The dual norm of |||·|||s is a semi-norm\n|||X |||s∗ = max k=1,...,K ‖X(k)S(k)‖,\nwhere ‖ · ‖ is the spectral norm."
    }, {
      "heading" : "3.2 Choosing the subspace",
      "text" : "A natural question that arises is how to choose the matrices S(1), . . . ,S(k).\nLemma 2. Let the X∗(k) = P (k)Λ(k)Q(k) be the SVD of X∗(k), where P (k) is n × R and Q(k) is nK−1 ×R. Assume that R ≤ n and U (k) has full column rank. It holds that for all k,\ni) U (k) ∈ Span(P (k)), ii) Q(k) ∈ Span ( P (1) ⊗ · · · ⊗ P (k−1) ⊗ P (k+1) ⊗ · · · ⊗ P (K) ) .\nProof. We prove the lemma in Appendix D.4.\nCorollary 1 shows that when the signal to noise ratio is high enough, we can recover P (k) with high probability. Hence we suggest the following three-step approach for tensor denoising:\n(i) For each k, unfold the observation tensor in mode k and compute the top H left singular\nvectors. Concatenate these vectors to obtain a n×H matrix P̂ (k) .\n(ii) Construct S(k) as S(k) = P̂ (1) ⊗ · · · ⊗ P̂ (k−1) ⊗ P̂ (k+1) ⊗ · · · ⊗ P̂ (K) .\n(iii) Solve the subspace norm regularized minimization problem\nmin X\n1 2 |||Y − X |||2F + λ|||X |||s, (8)\nwhere the subspace norm is associated with the above defined {S(k)}Kk=1.\nSee Appendix B for details."
    }, {
      "heading" : "3.3 Analysis",
      "text" : "Let Y ∈ Rn×···×n be a tensor corrupted by Gaussian noise with standard deviation σ as follows:\nY = X ∗ + σE . (9)\nWe define a slightly modified estimator X̂ as follows:\nX̂ = arg min X ,{M (k)}Kk=1\n{1 2 |||Y − X |||2F + λ|||X |||s : X = K∑ k=1 foldk ( M (k)S(k) >) , {M (k)}Kk=1 ∈M(ρ) } (10)\nwhere M(ρ) is a restriction of the set of matrices M (k) ∈ Rn×HK−1 , k = 1, . . . , K defined as follows:\nM(ρ) := { {M (k)}Kk=1 : ‖foldk(M (k))(`)‖ ≤ ρ K ( √ n+ √ HK−1),∀k 6= ` } .\nThis restriction makes sure thatM (k), k = 1, . . . , K, are incoherent, i.e., eachM (k) has a spectral norm that is as low as a random matrix when unfolded at a different mode `. Similar assumptions were used in low-rank plus sparse matrix decomposition [2, 12] and for the denoising bound for the latent nuclear norm [23].\nThen we have the following statement (we prove this in Appendix D.3).\nTheorem 2. Let Xp be any tensor that can be expressed as\nXp = K∑ k=1 foldk ( M (k)p S (k)> ) ,\nwhich satisfies the above incoherence condition {M (k)p }Kk=1 ∈M(ρ) and let rk be the rank ofM (k)p for k = 1, . . . , K. In addition, we assume that each S(k) is constructed as S(k) = P̂ (k−1) ⊗ · · · ⊗ P̂ (k+1) with (P̂ (k) )>P̂ (k)\n= IH . Then there are universal constants c0 and c1 such that any solution X̂ of the minimization problem (10) with λ = |||Xp−X ∗|||s∗+c0σ (√ n+ √ HK−1 + √ 2 log(K/δ) ) satisfies the following bound\n|||X̂ − X ∗|||F ≤ |||Xp −X ∗|||F + c1λ √∑K k=1 rk,\nwith probability at least 1− δ.\nNote that the right-hand side of the bound consists of two terms. The first term is the approximation error. This term will be zero if X ∗ lies in Span({S(k)}Kk=1). This is the case, if we choose S(k) = InK−1 as in the latent nuclear norm, or if the condition of Corollary 1 is satisfied for the smallest βR when we use the Kronecker product construction we proposed. Note that the regularization constant λ should also scale with the dual subspace norm of the residual Xp −X ∗.\nThe second term is the estimation error with respect to Xp. If we take Xp to be the orthogonal projection of X ∗ to the Span({S(k)}Kk=1), we can ignore the contribution of the residual to λ, because (Xp − X ∗)(k)S(k) = 0. Then the estimation error scales mildly with the dimensions n, HK−1 and with the sum of the ranks. Note that if we take S(k) = InK−1 , we have HK−1 = nK−1, and we recover the guarantee (3) ."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this section, we conduct tensor denoising experiments on synthetic and real datasets, to numerically confirm our analysis in previous sections."
    }, {
      "heading" : "4.1 Synthetic data",
      "text" : "We randomly generated the true rank two tensor X ∗ of size 20 × 30 × 40 with singular values β1 = 20 and β2 = 10. The true factors are generated as random matrices with orthonormal columns. The observation tensor Y is then generated by adding Gaussian noise with standard deviation σ to X ∗.\nOur approach is compared to the CP decomposition, the overlapped approach, and the latent approach. The CP decomposition is computed by the tensorlab [22] with 20 random initializations. We assume CP knows the true rank is 2. For the subspace norm, we use Algorithm 2 described in Section 3. We also select the top 2 singular vectors when constructing Û (k)\n’s. We computed the solutions for 20 values of regularization parameter λ logarithmically spaced between 1 and 100. For the overlapped and the latent norm, we use ADMM described in [25]; we also computed 20 solutions with the same λ’s used for the subspace norm.\nWe measure the performance in the relative error defined as |||X̂ − X ∗|||F/|||X ∗|||F . We report the minimum error obtained by choosing the optimal regularization parameter or the optimal initialization. Although the regularization parameter could be selected by leaving out some entries and measuring the error on these entries, we will not go into tensor completion here for the sake of simplicity.\nFigure 2 (a) and (b) show the result of this experiment. The left panel shows the relative error for 3 representative values of λ for the subspace norm. The black dash-dotted line shows the minimum error across all the λ’s. The magenta dashed line shows the error corresponding to the theoretically motivated choice λ = σ(maxk( √ nk + √ HK−1) + √ 2 log(K)) for each σ. The two vertical lines are thresholds of σ from Corollary 1 corresponding to β1 and β2, namely, β1/( ∏ k nk) 1/4 and β2/( ∏ k nk) 1/4. It confirms that there is a rather sharp increase in the error around the theoretically predicted places (see also Figure 1(b)). We can also see that the optimal λ\nshould grow linearly with σ. For large σ (small SNR), the best relative error is 1 since the optimal choice of the regularization parameter λ leads to predicting with X̂ = 0.\nFigure 2 (b) compares the performance of the subspace norm to other approaches. For each method the smallest error corresponding to the optimal choice of the regularization parameter λ is shown. In addition, to place the numbers in context, we plot the line corresponding to\nRelative error =\n√ R ∑\nk nk log(K)\n|||X ∗|||F · σ, (11)\nwhich we call “optimistic”. This can be motivated from considering the (non-tractable) maximum likelihood estimator for CP decomposition (see Appendix A).\nClearly, the error of CP, the subspace norm, and “optimistic” grows at the same rate, much slower than overlap and latent. The error of CP increases beyond 1, as no regularization is imposed (see Appendix C for more experiments). We can see that both CP and the subspace norm are behaving near optimally in this setting, although such behavior is guaranteed for the subspace norm whereas it is hard to give any such guarantee for the CP decomposition based on nonlinear optimization."
    }, {
      "heading" : "4.2 Amino acids data",
      "text" : "The amino acid dataset [5] is a semi-realistic dataset commonly used as a benchmark for low rank tensor modeling. It consists of five laboratory-made samples, each one contains different amounts of tyrosine, tryptophan and phenylalanine. The spectrum of their excitation wavelength (250-300 nm) and emission (250-450 nm) are measured by fluorescence, which gives a 5× 201× 61 tensor. As the true factors are known to be these three acids, this data perfectly suits the CP model. The true rank is fed into CP and the proposed approach asH = 3. We computed the solutions of CP for 20 different random initializations, and the solutions of other approaches with 20 different values of λ. For the subspace and the overlapped approach, λ’s are logarithmically spaced between 103\nand 105. For the latent approach, λ’s are logarithmically spaced between 104 and 106. Again, we include the optimistic scaling (11) to put the numbers in context.\nFigure 2(c) shows the smallest relative error achieved by all methods we compare. Similar to the synthetic data, both CP and the subspace norm behaves near ideally, though the relative error of CP can be larger than 1 due to the lack of regularization. Interestingly the theoretically suggested scaling of the regularization parameter λ is almost optimal."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We have settled a conjecture posed by [20] and showed that indeed O(nK/4) signal-to-noise ratio is sufficient also for odd order tensors. Moreover, our analysis shows an interesting two-phase behavior of the error. This finding lead us to the development of the proposed subspace norm. The proposed norm is defined with respect to a set of orthonormal matrices P̂ (1) , . . . , P̂ (K) , which are estimated by mode-wise singular value decompositions. We have analyzed the denoising performance of the proposed norm, and shown that the error can be bounded by the sum of two terms, which can be interpreted as an approximation error term coming from the first (non-convex) step, and an estimation error term coming from the second (convex) step."
    }, {
      "heading" : "A Maximum likelihood estimator",
      "text" : "Let Y ∈ Rn1×···×nK be a noisy observed tensor generated as follows:\nY = X ∗ + σE = R∑ r=1 βru (1) r ◦ · · · ◦ u(K)r + σE ,\nwhere E is a noisy tensor whose entries are i.i.d. normal N (0, 1). Let X̂MLE be the (intractable) estimator defined as\nX̂MLE = arg min X\n( |||Y − X |||2F : rank(X ) ≤ R ) .\nWe have the following performance guarantee for X̂MLE:\nTheorem 3. Let R ≤ mink nk/2. Then there is a constant c such that\n|||X̂MLE −X ∗|||F ≤ cσ √√√√RK K∑ k=1 nk log(2K/K0) + log(2/δ),\nwith probability at least 1− δ, where K0 = log(3/2).\nNote that the factor RK in the square root is rather conservative. In the best case, this factor reduces to linear in R and this is what we present in Section 4 as “optimistic” ignoring constants and δ; see Eq. (11).\nProof of Theorem 3. Since X̂MLE is a minimizer and X ∗ is also feasible, we have\n|||Y − X̂MLE|||2F ≤ |||Y − X ∗|||2F ,\nwhich implies\n|||X ∗ − X̂MLE|||2F ≤ σ〈E , X̂MLE −X ∗〉\n≤ σ|||E|||op|||X̂MLE −X ∗|||nuc,\nwhere\n|||X |||op := sup u(1),...,u(K) { ∑ i1,i2,...,iK Xi1,i2,...,iKu (1) i1 u (2) i2 · · ·u(K)iK :\n‖u(1)‖ = ‖u(2)‖ = · · · = ‖u(K)‖ = 1 }\nis the tensor spectral norm and the nuclear norm\n|||X |||nuc := inf u(1),...,u(K) {∑ r ‖u(1)r ‖ · ‖u(2)r ‖ · · · ‖u(K)r ‖ :\nX = R∑ r=1 u(1)r ◦ · · · ◦ u(K)r }\nis the dual of the spectral norm. Since both X̂MLE and X ∗ are rank at most R, the difference X̂MLE − X ∗ is rank at most 2R. Moreover, any rank-R CP decomposition with R ≤ mink nkcan be reduced to an orthogonal CP decomposition with rank at most RK via the Tucker decomposition [14]. Thus, denoting this orthogonal decomposition by X̂MLE−X ∗ = ∑RK r=1 ũ (1) r ◦· · ·◦ũ(K)r and using βr := ‖ũ(1)r ‖ · · · ‖ũ(K)r ‖, we have\n|||X̂MLE −X ∗|||nuc ≤ RK∑ r=1 βr ≤ √ RK √∑RK r=1 β2r\n= √ RK |||X̂MLE −X ∗|||F ,\nwhere the last equality follows because the decomposition is orthogonal. Finally applying the tail bound for the spectral norm ‖E‖op of random Gaussian tensor E [24], we obtain what we wanted."
    }, {
      "heading" : "B Details of optimization",
      "text" : "For solving problem (8), we follow the alternating direction method of multipliers described in [25]. We scale the objective function in (8) by 1/λ, and consider the dual problem\nmin D,{W (k)}Kk=1\nλ 2 |||D|||2F − 〈D,Y〉\ns.t. max k ‖W (k)‖ ≤ 1,\nW (k) = D(k)S (k), k = 1, . . . , K,\n(12)\nwhere D ∈ Rn1×n2×···×nK is the dual tensor that corresponds to the residual in the primal problem (8), andW (k)’s are auxiliary variables introduced to make the problem equality constrained.\nAlgorithm 1: Tensor denoising via the subspace norm Input: noisy tensor Y , subspace dimension H , regularization constant λ for k = 1 to K do P̂ (k) ←− top H left singular vectors of Y (k)\nend for for k = 1 to K do S(k) ←− P̂ (1) ⊗ · · · ⊗ P̂ (k−1) ⊗ P̂ (k+1) ⊗ · · · ⊗ P̂ (K) end for Output: X̂ = arg minX 12 |||Y − X ||| 2 F + λ|||X |||s.\nThe augmented Lagrangian function of problem (12) could be written as follows:\nLη(D, {W (k)}Kk=1, {M (k)}Kk=1)\n= λ\n2 |||D|||2 − 〈D,Y〉+ K∑ k=1 ( 〈M (k),D(k)S(k) −W (k)〉\n+ η\n2 ‖D(k)S(k) −W (k)‖2F + 1‖·‖≤1(W (k))\n) ,\nwhere M (k)’s are the multipliers, η is the augmenting parameter, and 1‖·‖≤1 is the indicator function of the unit spectral norm ball.\nWe follow the derivation in [25] and conclude that the updates of D, M (k) and W (k) can be computed in closed forms. We further combine the updates ofW (k) and other steps so that it needs not to be explicitly computed. The sum of the products ofM (k) and S(k) > finally converges to the solution of the primal problem (8), see Algorithm 2. The update for the Lagrangian multipliers M (k) (k = 1, . . . , K) is written as singular value soft-thresholding operator defined as\nproxtrη (Z) = P max(Σ− η, 0)Q>,\nwhere Z = PΣQ> is the SVD of Z. A notable property of the subspace norm is the computational efficiency. The update of M (k) requires singular value decomposition, which usually dominates the costs of computation. For problem (12), the size ofM (k) is only nk ×HK−1. Comparing with previous approaches, e.g. the latent approach whose multipliers are nk × ∏ k′ 6=k nk′ matrices, the size of our variables is much smaller, so the per-iteration cost is reduced."
    }, {
      "heading" : "C Additional experiments",
      "text" : "We report the experimental results when the input rank of CP and the subspace approach is are over-specified, on the same synthetic dataset as Section 4. We consider the case where the input rank is 8.\nAlgorithm 2: ADMM for subspace norm minimization\nInput: Y , λ, S(1), . . . ,S(K), η, initializations D0, {M (1)0 , . . . ,M (K) 0 } t = 0 repeat\nDt+1 = 1λ+ηK ( Y +KηDt − ∑ k foldk ( (2M (k) t −M (k) t−1)S (k)>)) for k = 1 to K do M\n(k) t+1 = prox tr η\n( M\n(k) t + ηD(k),t+1S\n(k) )\nend for t← t+ 1\nuntil convergence Output: X̂ = ∑K k=1M (k) t S (k)>.\nWe impose the `2 regularizations on the factors of CP. We test 20 values that are logarithmically spaced between 0.01 and 10 are the regularization parameter. For each value, we compute 20 solutions with random initializations and select the one with lowest objective value.\nFor the subspace approach, we computed solutions for 20 values of the regularization parameter that are logarithmically spaced between 1 and 1000.\nAs before, we report the minimum relative error obtained by the same method. The results are shown in Figure 3. We include the case the rank is specified incorrectly for comparison. Clearly, even if the rank is much larger than the truth, the subspace approach and CP are robust with proper regularization."
    }, {
      "heading" : "D Proofs",
      "text" : "D.1 Proof of Theorem 1 We consider the second moment of X̃:\nX̃X̃ > = β2uu> + σ2EE> + βσ(uv>E> +Evu>)\n= B︷ ︸︸ ︷ β2uu> +mσ2I +\nG︷ ︸︸ ︷ σ2EE> −mσ2I + βσ(uv>E> +Evu>) .\nThe eigenvalue decomposition ofB can be written as\nB = [u U 2]\n[ β2 +mσ2\nmσ2I\n] [ u>\nU>2\n] .\nWe first show a deterministic lower bound for |〈û,u〉| assuming β2 ≥ 2‖G‖, where û is the leading eigenvector of X̃X̃ > . Then we bound the spectral norm ‖G‖ of the noise term (Lemma 3) and derive the sufficient condition for β. Let û be the leading eigenvector of X̃X̃ > with eigenvalue λ̂, r = Bû − λ̂û = −Gû. We have U>2 r = (mσ 2 − λ̂)U>2 û. Hence, for all β2 > 2‖G‖, it holds that\n| sin(û,u)| = ‖U>2 û‖2 = ‖U>2 r‖2 λ̂−mσ2 ≤ ‖G‖ β2 − ‖G‖ ≤ 2‖G‖ β2 ,\nwhere we used ‖U>2 r‖2 = ‖U>2Gû‖2 ≤ ‖G‖, and λ̂ ≥ u>X̃X̃ > u> ≥ β2 + mσ2 − ‖G‖. Therefore,\n|〈û,u〉| = | cos(û,u)| ≥ √ 1− 4‖G‖ 2\nβ4 ≥ 1− 4‖G‖ 2 β4 ,\nif β2 ≥ 2‖G‖. It follows from Lemma 3 (shown below) that\n‖G‖ ≤\n{ 2C̄σ2 √ mn, if β/σ < √ m,\n2C̄βσ √ n, otherwise,\nwhere C̄ is a universal constant with probability at least 1− 4e−n.\nNow consider the first case (β/σ < √ m) and assume β2 ≥ 4C̄σ2 √ mn ≥ 2‖G‖. Note that this\ncase only arises when √ m ≥ 4C̄ √ n. Denoting C = 16C̄2, we obtain the first case in the theorem. Next, consider the second case (β/σ ≥ √ m). If √ m ≥ 4C̄ √ n as above, we have β/σ ≥ 4C̄ √ n, which implies β2 ≥ 2‖G‖ and we obtain the second case in the theorem. On the other hand, if√ m < 4C̄ √ n, we require β/σ ≥ 4C̄ √ n to obtain the last case in the theorem.\nLemma 3. Let G be constructed as in Theorem 1. If m ≥ n, there exists an universal constant C̄ such that ‖G‖ ≤ C̄σ2 (√ mn+ √ n(β/σ)2 ) ,\nwith probability at least 1− 4e−n.\nProof. The proof is an ε-net argument. Let λ = 2σ2 (√ 4mn+ 4n+ √ 8n(β/σ)2 ) .\nThe goal is to control |x>Gx| for all the vectors x on the unit Euclidean sphere Sn−1. In order to do this, we first bound the probability of the tail event |x>Gx| > λ, for any fixed x ∈ Sn−1. Then we bound the probability that |x>Gx| > λ for all the vectors in a ε-net Nε. Finally, we establish the connection between supx∈Nε |x>Gx| and ‖G‖.\nTo bound P(|x>Gx| > λ) for a fix x ∈ Sn−1, we expand x>Gx as\nx>Gx = σ2(‖z‖2 −m) + 2βσ(u>x)γ,\nwhere z = E>x and γ = v>z. Since z ∼ N (0, I), we can see that ‖z‖2 is χ2 distributed with m degrees of freedom and γ ∼ N (0, 1).\nFirst we bound the deviation of the χ2 term. By the corollary of Lemma 1 in [17], we have\nP( ∣∣‖z‖2 −m∣∣ > λ1) ≤ 2e−4n, (13)\nwhere λ1 = 2( √\n4mn+ 4n). Next we bound the deviation of the Gaussian term. Using the Gaussian tail inequality, we have\nP (|γ| > λ2) ≤ 2e−4n, (14)\nwhere λ2 = √\n8n. Combining inequalities (22) and (14), we have\nP(|x>Gx| > λ) ≤ P ( σ2 ∣∣‖z‖2 −m∣∣+ |2βσ(u>x)γ| > σ2λ1 + 2βσλ2)\n≤ P (∣∣‖z‖2 −m∣∣ > λ1 ∨ |γ| > λ2)\n≤ P (∣∣‖z‖2 −m∣∣ > λ1)+ P (|γ| > λ2) ≤ 4e−4n,\nwhere the second to last line follows from the union bound. Furthermore, using Lemma 5.2 and 5.4 of [27], for any ε ∈ [0, 1), it holds that\n|Nε| ≤ (1 + 2/ε)n,\nand ‖G‖ ≤ (1− 2ε)−1 sup\nx∈Nε |x>Gx|.\nTaking the union bound over all the vectors in N1/4, we obtain\nP ( sup\nx∈N1/4 |x>Gx| > λ\n) ≤ |N1/4|4e−4n < 4e−n.\nFinally, the statement is obtained by noticing that n ≤ m. We prove a more general version of the theorem that allows the signal part to be rank R in\nAppendix E.\nD.2 Proof of Lemma 1 Proof. By definition,\n|||Y|||s∗ = sup {M (k)}Kk=1 〈Y , K∑ k=1 foldk(M (k)S(k) > )〉\ns.t. K∑ k=1 ‖M (k)‖∗ ≤ 1\n= sup {M (k)}Kk=1 K∑ k=1 〈Y (k)S(k),M (k)〉\ns.t. K∑ k=1 ‖M (k)‖∗ ≤ 1\n= max k ‖Y (k)S(k)‖,\nwhere we used the Hölder inequality in the last line.\nD.3 Proof of Theorem 2 First we decompose the error as\n|||X ∗ − X̂ |||F ≤ |||X ∗ −Xp|||F + |||Xp − X̂ |||F .\nThe first term is an approximation error that depends on the choice of the subspace S(k). The second term corresponds to an estimation error and we analyze the second term below.\nSince X̂ is the minimizer of (10) and Xp is feasible,\n1 2 |||Y − X̂ |||2F + λ K∑ k=1 ‖M̂ (k) ‖∗ ≤ 1 2 |||Y − Xp|||2F + λ K∑ k=1 ‖M (k)p ‖∗,\nfrom which we have\n1 2 |||Xp − X̂ |||2F ≤ |||Y − Xp|||s∗|||Xp − X̂ |||s + λ K∑ k=1 ( ‖M (k)p ‖∗ − ‖M̂ (k) ‖∗ ) . (15)\nNext we define ∆k := M̂ (k) −M (k)p ∈ Rnk×H K−1 and define its orthogonal decomposition ∆k = ∆ ′ k + ∆ ′′ k as\n∆′′k := (Ink − P Up)∆k(IHK−1 − P Vp),\nwhere P Up and P Vp are projection matrices to the column and row spaces of M (k) p , respectively, and ∆′k := ∆k −∆′′k. The above definition allows us to decompose ‖M̂ (k) ‖∗ as follows:\n‖M̂ (k) ‖∗ = ‖M (k)p + ∆′′k + ∆′k‖∗ ≥ ‖M (k)p ‖∗ + ‖∆′′k‖∗ − ‖∆′k‖∗. (16)\nMoreover,\n|||Xp − X̂ |||s ≤ K∑ k=1 ‖∆k‖∗ ≤ K∑ k=1 (‖∆′k‖∗ + ‖∆′′k‖∗) (17)\nCombining inequalities (15)–(17), we have\n1 2 |||Xp − X̂ |||2F ≤ (|||Y − Xp|||s∗ + λ) K∑ k=1 ‖∆′k‖∗ + (|||Y − Xp|||s∗ − λ) K∑ k=1 ‖∆′′k‖∗. (18)\nSince\n|||Y − Xp|||s∗ ≤ σ|||E|||s∗ + |||X ∗ −Xp|||s∗ ,\nif λ ≥ σ|||E|||s∗ + |||X ∗ − Xp|||s∗ , the second term in the right-hand side of inequality (18) can be\nignored and we have\n1 2 |||Xp − X̂ |||2F ≤ 2λ K∑ k=1 ‖∆′k‖∗\n≤ 2λ K∑ k=1 √ 2rk‖∆′k‖F\n≤ 2λ K∑ k=1 √ 2rk‖∆k‖F\n≤ 2 √ 2λ √√√√ K∑ k=1 rk √√√√ K∑ k=1 ‖∆k‖2F , (19)\nwhere in the second line we used a simple observation that rank(∆′k) ≤ 2rk. Next, we relate the norm |||Xp − X̂ |||F to the sum ∑K k=1 ‖∆k‖2F in the right-hand side of inequality (19). First suppose that ∑K k=1 ‖∆k‖2F ≤ |||Xp − X̂ ||| 2 F . Then from inequality (19), we have\n|||Xp − X̂ |||F ≤ 4 √ 2λ √√√√ K∑ k=1 rk\nby dividing both sides by |||Xp − X̂ |||F . On the other hand, if |||Xp − X̂ |||2F ≤ ∑K k=1 ‖∆k‖2F , we use the following lemma\nLemma 4. Suppose {M (k)p }Kk=1, {M̂ (k) }Kk=1 ∈ M(ρ), and S(k) is constructed as a Kronecker product ofK−1 ortho-normal matrices P̂ (`) asS(k) = P̂ (k−1) ⊗· · ·⊗P̂ (k+1) , where (P̂ (`) )>P̂ (`) =\nIH for ` = 1, . . . , K. Then forXp = ∑K k=1 foldk ( M (k)p S (k)> ) and X̂ = ∑K k=1 foldk ( M̂ (k) S(k) >) , the following inequality holds:\n1\n2 K∑ k=1 ‖∆k‖2F ≤ 1 2 |||Xp − X̂ |||2F + ρmax k ( √ nk + √ HK−1) K∑ k=1 ‖∆k‖∗. (20)\nProof. The proof is presented in Section D.5.\nCombining inequalities (18) and (20), we have\n1\n2 K∑ k=1 ‖∆k‖2F ≤ ( |||Y − Xp|||s∗ + ρmax k ( √ nk + √ HK−1) + λ ) K∑ k=1 ‖∆′k‖∗\n+ ( |||Y − Xp|||s∗ + ρmax k ( √ nk + √ HK−1)− λ ) K∑ k=1 ‖∆′′k‖∗.\nThus if we take λ ≥ σ|||E|||s∗ + |||X ∗ − Xp|||s∗ + ρmaxk( √ nk + √ HK−1), the second term in the right-hand side can be ignored and following the derivation leading to inequality (19) and dividing\nboth sides by √∑K\nk=1 ‖∆k‖2F , we have\n|||Xp − X̂ |||F ≤ √√√√ K∑ k=1 ‖∆k‖2F ≤ 4 √ 2λ √√√√ K∑ k=1 rk,\nwhere the first inequality follows from the assumption. The final step of the proof is to bound the norm |||E|||s∗ with sufficiently high probability. By Lemma 1,\n|||E|||s∗ = max k ‖E(k)S(k)‖.\nTherefore, taking the union bound, we have\nP (\nmax k ‖E(k)S(k)‖ ≥ t\n) ≤ K∑ k=1 P ( ‖E(k)S(k)‖ ≥ t ) . (21)\nNow since each E(k)S(k) ∈ Rnk×H K−1 is a random matrix with i.i.d. standard Gaussian entries, P ( ‖E(k)S(k)‖ ≥ √ nk + √ HK−1 + t ) ≤ exp(−t2/(2σ2)).\nTherefore, choosing t = maxk( √ nk + √ HK−1) + √ 2 log(K/δ) in inequality (21), we have\nmax k ‖E(k)S(k)‖ ≤ max k ( √ nk + √ HK−1) +\n√ 2 log(K/δ),\nwith probability at least 1− δ. Plugging this into the condition for the regularization parameter λ, we obtain what we wanted.\nD.4 Proof of Lemma 2 Proof. i) Let ⊗k′∈[K]\\kU (k ′) denote U (1) ⊗ · · · ⊗U (k−1) ⊗U (k+1) ⊗ · · · ⊗U (K). We have\nX∗(k) = U (k)C(k) ( ⊗k′∈[K]\\kU (k ′) )>\n= U (k)C(k) ( ⊗k′∈[K]\\k(U (k ′))> )\n= P (k)Λ(k)(Q(k))>.\nBecause of the minimality of the Tucker decomposition (5), X∗(k), C(k) and U (k) are all of rank R, for all k ∈ [K]. Therefore, both C(k) and ⊗k′∈[K]\\k(U (k ′))> have full row rank. Hence, C(k) has a Moore-Penrose pseudo inverse C † (k) such that C(k)C † (k) = I , and so does ⊗k′∈[K]\\k(U (k ′))>. As a result, we have\nU (k) = P (k)Λ(k)(Q(k))> ( ⊗k′∈[K]\\k(U (k ′))> )† C†(k).\nii) Similarly, we have Q(k)Λ(k)(P (k))> = ( ⊗k′∈[K]\\kU (k ′) ) C>(k)(U (k))>.\nBy the definition of SVD, Λ is invertible and (P (k))>P (k) = I . Hence, Q(k) = ( ⊗k′∈[K]\\kU (k ′) ) C>(k)(U (k))>P (k)(Λ(k))−1.\nThis meansQ(k) ∈ span ( ⊗k′∈[K]\\kU (k ′) ) and we then concludeQ(k) ∈ span ( ⊗k′∈[K]\\kP (k ′) )\nusing (i).\nD.5 Proof of Lemma 4 Expanding Xp and X̂ , we have\n|||Xp − X̂ |||2F = ||| ∑K k=1 foldk ( ∆kS (k)>)|||2F =\nK∑ k=1 ‖∆k‖2F + ∑ k 6=` 〈foldk(∆kS(k) > ), fold`(∆`S (`)>)〉\n= K∑ k=1 ‖∆k‖2F + ∑ k 6=` 〈foldk(∆k)×k′ 6=k P̂ (k′) , fold`(∆`)×`′ 6=` P̂ (`′) 〉\n= K∑ k=1 ‖∆k‖2F + ∑ k 6=` 〈foldk(∆k)×` P̂ (`) , fold`(∆`)×k P̂ (k) 〉\n= K∑ k=1 ‖∆k‖2F − ∑ k 6=` 〈∆k(IH ⊗ · · · ⊗ P̂ (`) ⊗ · · · ⊗ IH)>, P̂ (k) (fold`(∆`))(k)〉\n≥ K∑ k=1 ‖∆k‖2F − ∑ k 6=` ‖∆k‖∗ · ‖(fold`(∆`))(k)‖\n≥ K∑ k=1 ‖∆k‖2F − 2ρmax k ( √ nk + √ HK−1) K∑ k=1 ‖∆k‖∗,\nfrom which the lemma holds. Here we regarded foldk(∆kS(k)) as a Tucker decomposition with the core tensor foldk(∆k) and factor matrices P̂ (k′)\nfor k′ 6= k. Most of the factors except for k and ` cancel out when calculating the inner product between two such tensors in the third line, because (P̂ (k′) )>P̂ (k′)\n= IH . After unfolding the inner product at the kth mode in the fifth line, we notice that a multiplication by an ortho-normal matrix does not affect the nuclear norm or the spectral norm. In the last line we used {∆k}Kk=1 ∈ M(2ρ), which follows from the assumption that both {M (k)p }Kk=1, {M̂ (k) }Kk=1 ∈M(ρ)."
    }, {
      "heading" : "E Generalization of Theorem 1 to the higher rank case",
      "text" : "Theorem 4. Suppose that X = ∑R\nr=1 βrurv > r , where u1, . . . ,uR ∈ Rn and v1, . . . ,vR ∈ Rm\nare unit orthogonal vectors respectively. Let X̃ = X + σE be the noisy observation ofX . There exists an universal constant C such that with probability at least 1− 3e−n, if m/n ≥ C(β1/βR)4, then\n| cos(Û ,U )| ≥  1− Cmn (βR/σ)4 , if β1√ m < σ ≤ βR (Cmn) 1 4 , 1− Cn(β1/βR) 2\n(βR/σ)2 , if σ ≤ β1√ m ,\notherwise, | cos(Û ,U)| ≥ 1− Cn(β1/βR) 2\n(βR/σ)2 if σ ≤ β2R/(Cn) 1 2β1.\nSuppose thatX = ∑R\nr=1 β 2 rurv > r and X̃ = X + σE. We consider the second moment of X̃:\nX̃X̃ > = R∑ r=1 β2ruru > r + σ ( R∑ r=1 βr ( urv > r E > +Evru > r )) + σ2EE>\n= B︷ ︸︸ ︷ R∑ r=1 β2ruru > r +mσ 2I +\nG︷ ︸︸ ︷ σ2EE> −mσ2I + σ ( R∑ i=1 βr ( urv > r E > +Evru > r )) .\nThe eigenvalue decomposition ofB can be written as\nB = [U U 2]\n[ Σ +mσ2I\nmσ2I\n] [ U>\nU>2\n] ,\nwhere U ∈ Rn×R and Σ = diag(β21 , . . . , β2R). Similarly, the eigenvalue decomposition of X̃X̃ > can be written as\nX̃X̃ >\n= [Û Û 2]\n[ Σ̂\nΣ̂′\n][ Û >\nÛ > 2\n] ,\nwhere Σ̂ = diag(λ̂1, . . . , λ̂R) and Σ̂′ = diag(λ̂R+1, . . . , λ̂n) s.t. λ̂1 ≥ · · · ≥ λ̂n are the eigenvalues of X̃X̃ T .\nWe first show a deterministic lower bound for | sin(Û ,U)| assuming β2R ≥ 2‖G‖. Then we bound the spectral norm ‖G‖ of the noise term (Lemma 3) and derive the sufficient condition for β2R.\nThe maximum singular value of mσ2I is mσ2. The minimum singular value of Σ̂ is |λ̂R|. By Wely’s theorem, ‖G‖ ≥ |λ̂R − β2R −mσ2|, which means\nλ̂R ≥ mσ2 + β2R − ‖G‖.\nLetR = GÛ . Since β2R ≥ 2‖G‖, we can apply the Wedin theorem and obtain\n| sin(Û ,U)| = ‖U>2 Û‖ ≤ ‖R‖ β2R − ‖G‖ = ‖GÛ‖ β2R − ‖G‖ ≤ ‖G‖ β2R − ‖G‖ ≤ 2‖G‖ β2R ,\nwhere we used the property that the spectral norm is sub-multiplicative and ‖Û‖ = 1 in the second to last step.\nTherefore,\n| cos(Û ,U)| ≥ √ 1− 4‖G‖ 2\nβ4R ≥ 1− 4‖G‖ 2 β4R ,\nif β2R ≥ 2‖G‖. It follows from Lemma 5 (shown below) that with probability at least 1− 3e−n\n‖G‖ ≤\n{ 2C̄σ2 √ mn, if β1/σ < √ m,\n2C̄σ √ nβ1, otherwise,\nwhere C̄ is an universal constant. Let C = 16C̄2. Now consider the first situation where m/n > C(β1/βR)4. If σ > β1√m , we have ‖G‖ ≤ σ2 2 (Cmn) 1 2 . Meanwhile, if σ ≤ βR\n(Cmn) 1 4\n, then we have β2R ≥ σ2(Cmn) 1 2 ≥ 2‖G‖.\nCombining these two conditions we obtain the first case in the theorem. When σ ≤ β1√ m , we can see that ‖G‖ ≤ σ 2 (Cn) 1 2β1. Moreover, since m/n > C(β1/βR)4, it is implied that σ ≤ β2R/(Cn) 1 2β1 and thus β2R ≥ 2‖G‖. This gives us the second case. On the other hand, if m/n ≤ C(β1/βR)4, we require σ ≤ β2R/(Cn) 1 2β1 to obtain the last case in the theorem.\nLemma 5. LetG be constructed as in the proof of Theorem 4. If m ≥ n, there exists an universal constant C̄ such that\n‖G‖ ≤ C̄σ2 (√ mn+ √ nβ1/σ ) ,\nwith probability at least 1− 3e−n.\nProof. The proof is an ε-net argument. Let λ = 2σ2 (√ 4mn+ 4n+ √ R + 8n+ 4 √ Rn · β1/σ ) .\nThe goal is to control |x>Gx| for all the vectors x on the unit Euclidean sphere Sn−1. In order to do this, we first bound the probability of the tail event |x>Gx| > λ, for any fixed x ∈ Sn−1. Then\nwe bound the probability that |x>Gx| > λ for all the vectors in a ε-net Nε. Finally, we establish the connection between supx∈Nε |x>Gx| and ‖G‖.\nTo bound P(|x>Gx| > λ) for a fix x ∈ Sn−1, we expand x>Gx as\nx>Gx = σ2(‖z‖2 −m) + 2σ R∑ r=1 βrγr(u > r x),\nwhere z = E>x and γr = v>r z. It is easy to see that γr ∼ N (0, 1), z ∼ N (0, I) and ‖z‖2 is χ2 distributed with m degrees of freedom.\nLet γ = [γ1, . . . , γR] and ω = [u>1 x, . . . ,u > Rx]. We have\n|x>Gx| ≤σ2 ∣∣‖z‖2 −m∣∣+ 2σ∣∣∣∣ R∑\nr=1\nβrγr(u > r x) ∣∣∣∣ ≤σ2\n∣∣‖z‖2 −m∣∣+ 2σ R∑ r=1 max r∈[R] |βr| · |γr| · |u>r x|\n≤σ2 ∣∣‖z‖2 −m∣∣+ 2σβ1 · ‖γ‖ · ‖ω‖\n≤σ2 ∣∣‖z‖2 −m∣∣+ 2σβ1 · ‖γ‖,\nwhere we used the Cauchy-Schwarz inequality in the second to last line and the fact ‖ω‖ ≤ 1 in the last line. Note that γ1, . . . , γR are i.i.d standard Gaussian distributed so that ‖γ‖2 is χ2 distributed with R degrees.\nFirst we bound the deviation of the χ2m term. By the corollary of Lemma 1 in [17], we have P( ∣∣‖z‖2 −m∣∣ > λ1) ≤ 2e−4n, (22)\nwhere λ1 = 2( √\n4mn+ 4n). Next we bound the χ2R term. Similarly, we have\nP(‖γ‖2 −R > λ2) ≤ e−4n, (23)\nwhere λ2 = 2( √\n4Rn+ 4n). Combining inequalities (22) and (23), we have\nP(|x>Gx| > λ) ≤ P ( σ2 ∣∣‖z‖2 −m∣∣+ 2σβ1‖γ‖ > σ2λ1 + 2σβ1√R + λ2)\n≤ P (∣∣‖z‖2 −m∣∣ > λ1 ∨ ‖γ‖ >√R + λ2)\n≤ P (∣∣‖z‖2 −m∣∣ > λ1)+ P(‖γ‖ >√R + λ2)\n≤ 3e−4n,\nwhere the second to last line follows from the union bound.\nFurthermore, using Lemma 5.2 and 5.4 of [27], for any ε ∈ [0, 1), it holds that\n|Nε| ≤ (1 + 2/ε)n,\nand ‖G‖ ≤ (1− 2ε)−1 sup\nx∈Nε |x>Gx|.\nTaking the union bound over all the vectors in N1/4, we obtain\nP(‖G‖ ≤ 2λ) ≤ P ( sup\nx∈N1/4 |x>Gx| > λ\n) ≤ |N1/4|3e−4n < 3e−n.\nFinally, the statement is obtained by noticing that n ≤ m and R ≤ n."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "<lb>We consider the problem of recovering a low-rank tensor from its noisy observation. Previ-<lb>ous work has shown a recovery guarantee with signal to noise ratioO(ndK/2e/2) for recovering<lb>a Kth order rank one tensor of size n× · · · × n by recursive unfolding. In this paper, we first<lb>improve this bound to O(nK/4) by a much simpler approach, but with a more careful analysis.<lb>Then we propose a new norm called the subspace norm, which is based on the Kronecker prod-<lb>ucts of factors obtained by the proposed simple estimator. The imposed Kronecker structure<lb>allows us to show a nearly ideal O(<lb>√<lb>n+<lb>√<lb>HK−1) bound, in which the parameter H controls<lb>the blend from the non-convex estimator to mode-wise nuclear norm minimization. Further-<lb>more, we empirically demonstrate that the subspace norm achieves the nearly ideal denoising<lb>performance even with H = O(1).",
    "creator" : "LaTeX with hyperref package"
  }
}