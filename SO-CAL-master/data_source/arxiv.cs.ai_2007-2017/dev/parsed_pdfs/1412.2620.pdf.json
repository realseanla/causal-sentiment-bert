{
  "name" : "1412.2620.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Cells in Multidimensional Recurrent Neural Networks",
    "authors" : [ "Gundram Leifert", "Tobias Strauß", "Tobias Grüning", "Welf Wustlich", "Roger Labahn", "GUNDRAM LEIFERT" ],
    "emails" : [ "GUNDRAM.LEIFERT@UNI-ROSTOCK.DE", "TOBIAS.STRAUSS@UNI-ROSTOCK.DE", "TOBIAS.GRUENING@UNI-ROSTOCK.DE", "WELF.WUSTLICH@UNI-ROSTOCK.DE", "ROGER.LABAHN@UNI-ROSTOCK.DE" ],
    "sections" : [ {
      "heading" : "Cells in Multidimensional Recurrent Neural Networks",
      "text" : "Gundram Leifert GUNDRAM.LEIFERT@UNI-ROSTOCK.DE Tobias Strauß TOBIAS.STRAUSS@UNI-ROSTOCK.DE Tobias Grüning TOBIAS.GRUENING@UNI-ROSTOCK.DE Welf Wustlich WELF.WUSTLICH@UNI-ROSTOCK.DE Roger Labahn ROGER.LABAHN@UNI-ROSTOCK.DE"
    }, {
      "heading" : "University of Rostock Institute of Mathematics",
      "text" : ""
    }, {
      "heading" : "18051 Rostock, Germany",
      "text" : "Editor: Yoshua Bengio\nKeywords: LSTM, MDRNN, CTC, handwriting recognition, neural network Abstract\nThe transcription of handwritten text on images is one task in machine learning and one solution to solve it is using multi-dimensional recurrent neural networks (MDRNN) with connectionist temporal classification (CTC). The RNNs can contain special units, the long short-term memory (LSTM) cells. They are able to learn long term dependencies but they get unstable when the dimension is chosen greater than one. We defined some useful and necessary properties for the one-dimensional LSTM cell and extend them in the multi-dimensional case. Thereby we introduce several new cells with better stability. We present a method to design cells using the theory of linear shift invariant systems. The new cells are compared to the LSTM cell on the IFN/ENIT and Rimes database, where we can improve the recognition rate compared to the LSTM cell. So each application where the LSTM cells in MDRNNs are used could be improved by substituting them by the new developed cells.\nar X\niv :1\n41 2.\n26 20\nv2 [\ncs .A\nI] 1\n6 Fe\nb 20\n16"
    }, {
      "heading" : "1. Introduction",
      "text" : "Since the last decade, artificial neural networks (NN) became state-of-the-art in many fields of machine learning, for example they can be applied to pattern recognition. Typical NN are feedforward NN (FFNN) or recurrent NN (RNN), whereas the latter contain recurrent connections. When nearby inputs depend on each other, providing these inputs as additional information to the NN can improve its recognition result. FFNNs obtain these dependencies by making this nearby inputs accessible. If RNNs are used, the recurrent connections can be used to learn if the surrounding input is relevant, but these connections result in a vanishing dependency over time. In S. Hochreiter (1997) the authors develop the long short-term memory (LSTM) which is able to have a long term dependency. This LSTM is extended in A. Graves and Schmidhuber (2007) to the multi-dimensional (MD) case and is used in a hierarchical multi-dimensional RNN (MDRNN) which performed best in three competitions at the International Conference on Document Analysis and Recognition (ICDAR) in 2009 without any feature extraction and knowledge of the recognized language model. In this paper we analyse these MD LSTM regarding the ability to provide long term dependencies in MDRNNs and show that it can easily have an unwanted growing dependency for higher dimensions. We define a more general description of an LSTM—a cell—and change the LSTM architecture which leads to new MD cell types, which also can provide long term dependencies. In two experiments we show that substituting the LSTM in MDRNNs by these cells works well. Due to this we assume that substituting the LSTM cell by the best performing cell, the LeakyLP cell, will improve the performance of an MDRNN also in other scenarios. Furthermore the new cell types could also be used for the one-dimensional (1D) case, so using them in a bidirectional RNN with LSTMs (BLSTM) could lead to better recognition rates. In Section 2 we introduce the reader to the development of the LSTM cells (S. Hochreiter, 1997) and its extension (F. A. Gers and Cummins, 1999). Based on that in Section 3 we define two properties that probably lead to the good performance of the 1D LSTM cells. Both together guarantee that the cell can have a long term dependency. A third property ensures that gradient cannot explode over time. In Section 4 we show that the MD version of the LSTM is still able to provide long term dependency whereas the gradient can explode easily for dimension greater than 1. In Section 5 we change the architecture of the MD LSTM cell and reduce it to the 1D LSTM cell so that the cell fulfills the two properties for any dimension. Nevertheless the internal cell state can linearly grow over time. This problem is solved in Section 6 using a trainable convex combination of the input and the previous internal cell states. The new cell type can provide long term dependencies and does not suffer from exploding gradients. Motivated by the last sections we introduce a more general way to define MD cells in Section 7. Using the theory of linear shift-invariant systems and their frequency analysis we are able to get a new interpretation of the cells and we create 5 new cell types. To test the performance of the cells in Section 8 we take two data sets from the ICDAR 2009 competitions, where the MDRNNs with LSTM cell won. On these data sets we compare the recognition results of the MDRNNs when we substitute the LSTM cells by the new developed cells. On both data sets, the IFN/ENIT data set and the RIMES data set we can improve the recognition rate using the new developed cells."
    }, {
      "heading" : "2. Previous Work",
      "text" : "In this section we briefly want to introduce a recurrent neural network (RNN) and the development of the LSTM cell. In previous literature there are various notation to describe the update equations of RNNs an LSTMs. To unify the notations we will refer to their notation using “,” (F. A. Gers and Cummins, 1999; S. Hochreiter, 1997; Graves and Schmidhuber, 2008). Therefore we concentrate on a simple hierarchical RNN with one input layer with the set of neurons I , one recurrent hidden layer with the set of neurons H and one output layer with the set of neurons O. For each time step t ∈ N the layers are updated asynchronously in the order I,H,O. In one specific layer all neurons can be updated synchronously. In the hidden layer for one neuron c ∈ H at time t ∈ N we calculate the neuron’s input activation netc by(\nac(t) , ) netc(t) = ∑ i∈I wc,iyi(t) + ∑ h∈H wc,hyh(t− 1). (1)\nwith weights w[target neuron],[source neuron]. A bias in (1) can be added by extending the set I := I ∪ {bias} with ybias(t) = 1∀t ∈ N and hence we will not write the bias in the equations, but we use them in our RNNs in Section 8. The neuron’s output activation is calculated by(\nyc(t), bc(t) , ) yc(t) = fc (netc(t))\nwith a differentiable sigmoid activation function fc. To make (1) suitable for t ≤ 0 we define ∀h ∈ H,∀t ∈ Z \\ N : yh(t) = 0. This simple neuron with a linear function of activations as input and one activation function we call unit (compare to Figure 1). In (1) the activation of the unit is dependent on the current activations of the layer below and the previous activations of the units from the same layer. When there are no recurrent connections (∀c, h ∈ H : wc,h = 0), the layer is called feed-forward layer, otherwise recurrent layer."
    }, {
      "heading" : "2.1 The Long Short-Term Memory",
      "text" : "A standard LSTM cell c has one input with an input activation ycin(t) a set of gates, one internal state sc and one output(-activation) yc (, yc). The gates are also units and their task is to learn whether a signal should pass the gate or not. They almost always have the logistic activation function\nflog(x) := 1 1+exp(−x) (, f1(x)). The input of the standard LSTM cell is calculated from a unit with an odd activation function with a slope of 1 at x = 0. We use fc(x) = tanh (x) in this paper, another solution could be fc(x) = 2 tanh ( x 2 ) (see S. Hochreiter, 1997). The standard LSTM has two gates: The input gate (IG or ι) and the output gate (OG or ω). These both gates are calculated like a unit, so that\nnetι(t) = ∑ i∈I wι,iyi(t) + ∑ h∈H\nwι,hyh(t− 1)( yinc(t), bι(t) , ) yι(t) = flog (netι(t))\nand\nnetω(t) = ∑ i∈I wω,iyi(t) + ∑ h∈H\nwω,hyh(t− 1)( youtc(t), bω(t) , ) yω(t) = flog (netω(t)) .\nThe input of an LSTM is defined like in (1) by( netc(t) , ) netcin(t) = ∑ i∈I wc,iyi(t) + ∑ h∈H\nwc,hyh(t− 1),( g (netc(t)) , f2 (netc(t)) , ) ycin(t) = fc (netcin(t)) .\nThe internal state sc(t) is calculated by\nsc(t) = ycin(t) · yι(t) + sc(t− 1), (2)\nthe output activation yc(t) of the LSTM is calculated from( yc(t), bc(t) , ) yc(t) = hc (sc(t)) · yω(t) (3)\nwith hc (x) := tanh(x) (, f3(x)). The LSTM can be interpreted as a kind of memory module where the internal state stores the information. For a given input ycin(t) ∈ (−1, 1) the IG “decides” if the new input is relevant for the internal state. If so, the input is added to the internal state. The information of the input is now saved in the activation of the internal state. The OG determines whether or not the internal activation should be displayed to the rest of the network. So the information, stored in the LSTM is just “readable” when the OG is active. To sum up, an open IG can be seen as a “write”-operation into the memory and an open OG as a “read”-operation of the memory.\nAnother way to understand the LSTM is to take a look at the gradient propagated through it. To analyse the LSTM properly, we have to ignore gradients comming from recurrent weights. We define the truncated gradient similar to S. Hochreiter (1997) and F. A. Gers and Cummins (1999).\nDefinition 1 (truncated gradient) Let γ ∈ {cin, ι, ω} be any input or gate unit and yc(t− 1) any previous output activation. The truncated gradient differs from the exact gradient only by setting recurrent weighted gradient propagation ∂netγ(t)∂yc(t−1) to zero. We write\n∂netγ(t)\n∂yc(t− 1)\n( = wγ,c ) = tr 0.\nNow, let E be an arbitrary error which is used to train the RNN and ∂E(t)∂yc(t) the resulting derivative at the output of the LSTM. The OG can eliminate the gradient coming from the output, because\n∂yc(t) ∂sc(t) = h′c (sc(t))︸ ︷︷ ︸\n∈(0,1] · yω(t)︸ ︷︷ ︸ ∈(0,1) ,\nso the OG decides when the gradient should go into the internal state. Especially for |sc(t)| 1 we get\n∂yc(t) ∂sc(t) ≈ yω(t).\nThe key idea of the LSTMs is that an error that occurs at the internal state neither explode nor vanish over time. Therefore, we take a look at the partial derivative ∂sc(t)∂sc(t−1) , which is also known as error carousel (for more details see S. Hochreiter, 1997). Using the truncated gradient of Definition 1 for this derivative, we get\n∂sc(t)\n∂sc(t− 1) =ycin(t) ·\n∂yι(t)\n∂sc(t− 1) + yι(t) ·\n∂ycin(t)\n∂sc(t− 1) + 1\n=ycin(t) · ∂yι(t) ∂yc(t− 1) ∂yc(t− 1) ∂sc(t− 1) + yι(t) · ∂ycin(t) ∂yc(t− 1) ∂yc(t− 1) ∂sc(t− 1) + 1\n=ycin(t) · ∂yι(t)\n∂netι(t)\n∂netι(t)\n∂yc(t− 1)︸ ︷︷ ︸ = tr 0\n∂yc(t− 1) ∂sc(t− 1)\n+ yι(t) · ∂ycin(t)\n∂netcin(t)\n∂netcin(t) ∂yc(t− 1)︸ ︷︷ ︸ = tr 0 ∂yc(t− 1) ∂sc(t− 1) + 1\n⇒ ∂sc(t) ∂sc(t− 1) = tr 1. (4)\nSo, once having a gradient at the internal state we can use the chain rule and get ∀τ ∈ N : ∂sc(t)∂sc(t−τ) =tr 1. This is called constant error carousel. Like the OG can eliminate the gradient coming from the LSTM output, the IG can do the same with the gradient coming from the internal state, that means it decides when the gradient should be injected to the source activations. This can be seen by taking a look at the partial derivative\n∂sc(t)\n∂netcin(t) =\n∂sc(t)\n∂ycin(t)\n∂ycin(t)\n∂netcin(t) = yι(t)f\n′ c (netcin(t)) .\nIf there is a small input |netcin(t)| 1, we get f ′c (netcin(t)) ≈ 1 and can estimate\n∂sc(t)\n∂netcin(t) ≈ yι(t).\nAll in all, this LSTM is able to store information and learn long-term dependencies, but it has one drawback which will be discussed in 2.2."
    }, {
      "heading" : "2.2 Learning to Forget",
      "text" : "For long time series the internal state is unbounded (compare with F. A. Gers and Cummins, 1999, 2.1). Assuming a positive or negative input and a non zero activation of the IG, the absolute activation of the internal state grows over time. Using the weight-space symmetries in a network with at least one hidden layer (Bishop, 2006, 5.1.1) we assume without loss of generality ycin(t) ≥ 0, so sc(t) t→∞−−−→∞. Hence, the activation function hc saturates and (3) can be simplified to\nyc(t) = hc (sc(t))︸ ︷︷ ︸ →1 yω(t) ≈ yω(t).\nThus, for great activations of sc(t) the whole LSTM works like a unit with a logistic activation function. A similar problem can be observed for the gradient. The gradient coming from the output is multiplied by the activation of the OG and the derivative of hc. For great values of sc(t) we get h′c (sc(t))→ 0 and we can estimate the partial derivative\n∂yc(t)\n∂sc(t) = h′c ((sc(t)) · yω(t) ≈ 0,\nwhich can be interpreted that the OG is not able to propagate back the gradient into the LSTM. Some solutions to solve the linear growing state problem are introduced in F. A. Gers and Cummins (1999). They tried to stabilize the LSTM with a “state decay” by multiplying the internal state in each time step with a value ∈ (0, 1), which did not improve the performance. Another solution was to add an additional gate, the forget gate (FG or φ). The last state sc(t − 1) is multiplied by the activation of the FG before it is added to the current state sc(t). So we can substitute (2) by\nsc(t) = ycin(t) · yι(t) + sc(t− 1) · yφ(t),\nso that the truncated gradient in (4) is changed to\n∂sc(t)\n∂sc(t− 1) = ycin(t) ·\n∂yι(t)\n∂sc(t− 1) + yι(t) ·\n∂ycin(t)\n∂sc(t− 1) + yφ(t)\n= tr yφ(t)\nand for longer time series we get ∀τ ∈ N\n∂sc(t)\n∂sc(t− τ) = tr τ−1∏ t′=0 yφ(t− t′).\nNow, the Extended LSTM is able to learn to forget its previous state. However, an Extended LSTM is still able to work like an standard LSTM without FG by having an activation yφ(t) ≈ 1. In this paper we denote the Extended LSTM as LSTM Another point of view was introduce in Bengio et al. (1994): To learn long-term dependencies a system must have an architecture to that an input can be saved over long time and does not suffer from the “vanishing gradient” problem. On the other hand the system should avoid an “exploding gradient”, which means that a small disturbance has a growing influence over time. In this paper we do not want to solve the problem of vanishing and exploding gradient for a whole system, we want to solve this problem only for one single cell. But we think that it is an necessary condition to provide long time dependencies of a system.\nyI(t) yH(t− 1)\nyI(t) yH(t− 1)"
    }, {
      "heading" : "3. Cells and Their Properties",
      "text" : "In this section we want to introduce a general cell and figure out properties for these cells which probably lead to the good performance observed by LSTM cells.\nDefinition 2 (Cell, cf. Fig. 2) A cell, c, of order k consists of\n• one designated input unit, cin, with sigmoid activation function fc (typically fc = tanh unless specified otherwise);\n• a set Γ (not containing cin) of units called gates γ1, γ2, . . . with sigmoid activation functions fγi , i = 1, . . . (typically logistic fγi = flog unless specified otherwise);\n• an arbitrary function, gint, and a cell activation function, gout, mapping into [−1, 1].\nEach unit of Γ∪ {cin} receives the same set of input activations. The cell update in time step t ∈ N is performed in three subsequent phases:\n1. Following the classical update scheme of neurons (see Section 2), all units in Γ ∪ {cin} calculate synchronously their activations, which will be denoted by yΓ(t) := ( yγ(t) ) γ∈Γ and\nycin(t). Furthermore, we call ycin(t) the input activation of the cell.\nyI(t) yH(t− 1)\nyI(t) yH(t− 1)\nyI(t) yH(t− 1)\n2. Then, the cell computes it’s so-called internal state\nsc(t) := gint (yΓ(t), ycin(t), sc(t− 1), . . . , sc(t− k)) .\n3. Finally, the cell computes it’s so-called output activation\nyc(t) := gout (yΓ(t), ycin(t), sc(t), sc(t− 1), . . . , sc(t− k)) .\nIn this paper we concentrate on first order cells (k = 1). Now, we use Definition 2 to re-introduce the (Extended) LSTM cell.\nRemark 3 (LSTM cell) An LSTM cell is a cell of order 1 where hc = tanh and\n• Γ = {ι, φ, ω}\n• sc(t) := gint (yΓ(t), ycin(t), sc(t− 1)) := ycin(t)yι(t) + sc(t− 1)yφ(t)\n• yc(t) := gout (yΓ(t), sc(t)) := hc (sc(t)) yω(t)\nProperties of cells. Developing the 1D LSTM cells, the main idea is to save exactly one piece of information over a long time series and to propagate the gradient back over this long time, so that the system can learn precise storage of this piece of information. In instance a given input ycin (which represent the information) at time tin should be stored into the cell state sc until the information is required at time tout. To be able to prove the following properties, we will assume the truncated gradient defined in Definition 1. Nevertheless we will use the full gradient in our Experiments, because it turned out that it works much better. The next two properties of a cell ensure the ability to work as such a memory. The first property should ensure that an input ycin at time tin can be memorized (the cell input is open) in the internal activation sc until tout (the cell memorizes) and has a negligibly influence on the internal activation for t > tout (the cell forgets). In addition, the cell is able to prevent influence of other inputs at time steps t 6= tin (the cell input is closed). Definition 4 (Not vanishing gradient (NVG)) A cell c allows an NVG :⇔ For arbitrary tin, tout ∈ N, tin ≤ tout, ∀δ > 0 there exist gate activations yΓ(t) such that for any t1, t2 ∈ N\n∂sc (t2)\n∂ycin (t1) ∈ tr\n{ [1− δ, 1] for t1 = tin and tin ≤ t2 ≤ tout\n[0, δ] otherwise (5)\nholds.\nThe next definition guaranties that at any time t ∈ N the gate activations can (the cell output is open) or not (the cell output is closed) distribute the piece of information saved in sc to the network. This is an important property because the piece of information can be memorized in the cell without presenting it to the network. Note that the decision is just dependent on gate activations at time t and there are no constraints to previous gate activations. In Definition 2 we require yc(t) ∈ [−1, 1] whereas sc(t) ∈ R. So we cannot have arbitrarily small intervals of the derivative as in (5), but we can ensure two distinct intervals for open and closed cell output. When we take Definition 4 and 5 together, a cell is able to save an input over long term series, can decide at each time step whether or not it is presented to the network and can forget the saved input.\nDefinition 5 (Controllable output dependency (COD)) A cell c of order k allows an COD :⇔ There exist δ1, δ2 ∈ (0, 1), δ2 < δ1 so that for any time t ∈ N there exists a gate vector yΓ(t) leading to open output dependency\n∂yc (t) ∂sc (t) ∈ [δ1, 1] (6)\nand there exists another gate vector yΓ(t) leading to a closed output dependency\n∂yc (t) ∂sc (t) ∈ [0, δ2] . (7)\nThe third property is a kind of stability criterion. An unwanted case is that a small change (caused by any noisy signal) at time step tin has a growing influence at later time steps. This is equivalent to an exploding gradient over time. Controlling the gradient of the whole system and avoiding him not to explode is a hard problem. But we can at least avoid the exploding gradient in one cell. This should be prohibited for any gate activations.\nDefinition 6 (Not exploding gradient (NEG)) A cell c has an NEG :⇔ For any time steps tin, t ∈ N, tin < t and any gate activations yΓ(t) the truncated gradient in bounded by\n∂sc (t)\n∂sc (tin) ∈ tr [0, 1] .\nWe think that a cell fulfilling these three properties can work as stable memory. To be able to prove these properties for the LSTM cell we have to considerate the gate activations. In general, the activation function of the gates does not have to be the logistic activation function flog, whereas for this paper we set ∀γ ∈ Γ : fγ := flog. So the activation of gates can never be exactly 0 or 1, because of a finite input activation netγ(t) to the gate activation function. But a gate can have an activation yγ(t) ∈ [1− ε, 1) if it is opened or yγ(t) ∈ (0, ε] if it is closed, because for a realistic large input activation netγ(t) ≥ 7 (low input activation netγ(t) < −7) we get an activation within the interval yγ(t) ∈ [1 − ε, 1) (yγ(t) ∈ (0, ε]) with ε < 11000 . Handling with these activation intervals we can prove the definitions for the LSTM cell. Now we can prove whether or not the LSTM cell has these properties.\nTheorem 7 (Properties of the LSTM cell) The 1D LSTM cell allows NVG and has an NEG, but does not allow COD.\nProof see A.1 in appendix."
    }, {
      "heading" : "4. Expanding to More Dimensions",
      "text" : "In A. Graves and Schmidhuber (2007) the 1D LSTM cell is extended to an arbitrary number of dimensions; this is solved by using one FG for each dimension. In many publications using the MD LSTM cell in MDRNNs outperform state-of-the-art recognition systems (for example see Graves and Schmidhuber, 2008). But by expanding the cell to the MD case, the absolute value of the internal state |sc| can grow faster than linear over time. When\n∣∣spc ∣∣ → ∞ and there are peephole connections (for peephole connection details see F. A. Gers and Schmidhuber, 2002), the cells have an output activation of ypc ∈ {−1, 0, 1}: The internal state multiplied by the peephole weight overlays the other activationweight-products and this leads to an activation of the OG ypω ∈ {0, 1} and a squashed internal state hc ( spc ) ∈ {−1, 1}. So the output of the cell is ypωhc ( spc )\n= ypc ∈ {−1, 0, 1}. But also without peephole connections the internal state can grow, which leads to hc ( spc ) ∈ {−1, 1} and the cell works like a conventional unit with a logistic activation function yc(t) ≈ ±yω(t). Our goal is to transfer the Definitions 4, 5 and 6 defined in Section 3 into the MD case and we will see that the MD LSTM cell has an exploding gradient. In the next sections we will provide alternative cell types, that fulfill two or all of these definitions. In the 1D case it is clear, that there is just one way to come from date t1 to date t2, when t1 < t2, by incrementing t1 as long as t2 is reached. For the MD case the number of paths depends on the number of dimensions and the distance between these two dates. An MD path is defined as follows.\nDefinition 8 (MD path) Let p,q ∈ ND be two dates. A p-q-path π of length k ≥ 0 is a sequence\nπ := {p = p0, p1, . . . , pk = q}\nwith ∀i ∈ {1, . . . , k}∃!d ∈ {1, . . . , D} : (pi) − d = pi−1. Further, let πi := pi.\nWe can define the distance vector\n−→pq := q − p =  q1 − p1... qD − pD  =  −→pq1 ... −→pqD  between the dates p and q . When −→pq has at least one negative component, there exists no p-q-path. Otherwise there exist exactly\n#{−→pq} :=  ∑Di=1−→pq i −→pq1, . . . ,−→pqD  = (∑D i=1 −→pq i ) !∏D i=1 −→pq i!\np-q-paths (compare with the multinomial coefficient). We write p < q when #{−→pq} ≥ 1 and p ≤ q when p = q ∨ p < q . Now we can extend the definitions of the 1D case to the MD case, whereas we concentrate on the MD cells of order 1.\nDefinition 9 (MD cell) An MD cell, c, of order 1 and dimension D consists of the same parts as a 1D cell of order 1. The cell update in date p ∈ ND is performed in three subsequent phases:\n1. Following the classical update scheme of neurons (see Section 2), all units in Γ ∪ {cin} synchronously calculate their activations, which will be denoted by ypΓ = ( ypγ ) γ∈Γ. Furthermore,\nwe call ypcin the input activation of the cell.\n2. Then, the cell computes it’s so-called internal state\nspc := gint ( ypΓ, y p cin , s p−1 c , . . . , s p−D c ) .\n3. Finally, the cell computes it’s so-called output activation\nypc := gout ( ypΓ, y p cin , s p c , s p−1 c , . . . , s p−D c ) .\nUsing this, we can reintroduce the LSTM cell as well as Definition 4, 5 and 6 for the MD case:\nDefinition 10 (MD LSTM cell) An MD LSTM cell is a cell of dimension D and order 1 where hc = tanh and\n• Γ = {ι, (φ, 1) , . . . , (φ,D) , ω} • spc = gint ( ypΓ, y p cin , s p−1 c , . . . , s p−D c ) = ypι y p cin + D∑ d=1 s p−d c y p φ,d\n• ypc = gout ( ypΓ, s p c ) = hc ( spc ) ypω\nDefinition 11 (MD Not vanishing gradient (NVG)) An MD cell c allows an NVG :⇔ For arbitrary pin, pout ∈ ND, pin ≤ pout,∀δ > 0 there exist ∀p ∈ ND gate activations y p Γ such that for any p1, p2 ∈ ND\n∂s p2 c ∂y p1 cin ∈ tr\n{ [1− δ, 1] for p1 = pin and pin ≤ p2 ≤ pout\n[0, δ] otherwise (8)\nholds.\nDefinition 12 (MD Controllable output dependency (COD)) An MD cell c allows an COD :⇔ There exist δ1, δ2 ∈ (0, 1), δ2 < δ1 so that for any time t ∈ N there exists a gate vector ypΓ leading to open output dependency\n∂ypc ∂spc ∈ [δ1, 1] (9)\nand there exists another gate vector ypΓ leading to a closed output dependency\n∂ypc ∂spc ∈ [0, δ2] . (10)\nDefinition 13 (MD Not exploding gradient (NEG)) An MD cell c has an NEG :⇔ For any time steps pin, p ∈ ND, pin < p and any gate activations y p Γ the truncated gradient in bounded by\n∂spc\n∂s pin c ∈ tr [0, 1]\nWe can now consider these definitions for the MD LSTM cell.\nTheorem 14 (NVG of MD LSTM cells) An MD LSTM cell allows an NVG.\nProof see A.2 in appendix\nFor arbitrary activations of FGs the partial derivative ∂s p c\n∂s pin c\ncan grow over time:\nTheorem 15 (NEG of MD LSTM cells) An MD LSTM cell can have an exploding gradient, when D ≥ 2.\nProof see A.3 in appendix.\nThe MD LSTM cell does not allow the COD, because the 1D case is a special case of the MD case. Our idea for the next section is to change the MD LSTM layout, so that it has an NEG."
    }, {
      "heading" : "5. Reducing the MD LSTM Cell to One Dimension",
      "text" : "In the last section, we showed that the MD LSTM cell can have an exploding gradient. We tried different ways to solve this problem. For example we divided the activation of the FG by the number of dimensions. Then the gradient cannot explode over time, but the gradient vanishes along some paths rapidly. Another approach was to give the cells the opportunity to learn to stabilize itself, when the internal state starts diverging. Therefore we add an additional peephole connection between the\nsquare value of the previous internal states ( s p−d c )2 and the FGs so that the cell is able to learn that\nit has to close the FG for large internal states. This also does not make a significant difference. Also forcing the cell to learn to stabilize itself by adding an error\nLossstate = ε ‖spc‖p\nwith p = {1, 2, 3, 4} and different learning rates ε does not work. So we tried to change the layout of the MD LSTM cell."
    }, {
      "heading" : "5.1 MD LSTM Stable Cell",
      "text" : "In Section 3 we realized that 1D LSTM cells work good and the gradient does not explode, but in the MD case it does. Our idea is to combine the previous states s p−d c at date p to one previous state sp − c and take the 1D form of the LSTM cell. For this reason we call this cell LSTM Stable cell. Therefore, a function\nsp − c = f ( s p−1 c , . . . , s p−D c ) is needed, so that the following two benefits of the 1D LSTM cell remain:\n1. The MD LSTM Stable cell has an NEG\n2. The MD LSTM Stable cell allows NVG.\nThe convex combination\nsp − c = f ( s p−1 c , . . . , s p−D c ) = D∑ d=1 λpds p−d c , ∀d = 1, . . . , D : λpd ≥ 0, D∑ d=1 λpd = 1 (11)\nof all states satisfies these both points (see Theorems 17 and 18). To calculate these D coefficients we want to use the activation of D gates and we call them lambda gates (LG or λ).\nDefinition 16 (MD LSTM Stable cell) An MD LSTM Stable cell is a cell of dimension D and order 1 where hc = tanh and\n• Γ = {ι, (λ, 1) , . . . , (λ,D) , φ, ω}\n• sp − c = gconv ( ypΓ, s p−1 c , . . . , s p−D c ) = D∑ d=1 s p−d c\nypλ,d D∑ d′=1 yp λ,d′\n• spc = gint ( ypΓ, y p cin , s p− c ) = ypι y p cin + s p− c y p φ\n• ypc = gout ( ypΓ, s p c ) = ypωhc ( spc )\nUsing these equations we can test the cell for its properties. The MD LSTM Stable cell does not have the COD, because the 1D LSTM cell also does not have this property. For the other propertiese we get:\nTheorem 17 (LTD of MD LSTM Stable cells) An MD LSTM Stable cell allows NVG.\nProof See A.4 in appendix.\nTheorem 18 (NEG of MD LSTM Stable cells) An MD LSTM Stable cell has an NEG.\nProof See A.5 in appendix.\nReducing the number of gates by one. When D ≥ 2 an MD LSTM Stable cell has one more gate than a classical MD LSTM (for D = 1 the both cells are equivalent). But it is possible to reduce the number of LGs by one. One solution is to choose one dimension d′ ∈ {1, . . . , D} which does not get an LG. Its activation is calculated by\nypλ,d′ = ∏\nd∈{1,...,D}\\{d′}\n( 1− ypλ,d ) .\nIn the special case of D = 2 we can choose d′ = 2 and we get ∑2\nd′=1 yλ,d′ = yλ,1 + (1− yλ,1) = 1 and the update equation of the internal state can be simplified to\nspc = gint ( ypι , y p λ,1, y p φ, s p−1 c , s p−2 c ) = ypι y p cin + y p λ,1s p−1 c + ( 1− ypλ,1 ) s p−2 c ."
    }, {
      "heading" : "6. Bounding the Internal State",
      "text" : "In the last sections we discussed the growing of the EC over time and we found a solution to have a NGEC for higher dimensions. Nevertheless it is possible that the internal state grows linearly over time. When we take a look at Definition 10, we see that the partial derivative for p = pout depends on h′c ( spc ) . So having the inequality\n∂ypc ∂spc ≤ h′c (spc) with h′c (spc) |spc |→∞−−−−−→ 0\nthe cell allows NVG defined in Definition 11, but actually we have ∂y pout c\n∂y pin cin |spoutc |→∞−−−−−−−→ 0 for arbitrary gate activations. Again, ideas like state decay, additional peephole connections or additional loss functions like mentioned in Section 4 either do not work or destroy the NVG of the LSTM and LSTM Stable cell. So, our solution is to change the architecture of the MD LSTM Stable cell, so\nthat it fulfills has an NEG and allows NVG and COD. The key idea is to bound the internal state, so that for all inputs ∣∣ypcin∣∣ ≤ 1, p ∈ ND the internal state is bounded by ∣∣spc ∣∣ ≤ 1. Note that this is comparable with the well-known Bounded-Input-Bounded-Output-Stability (BIBOStability). To create an MD cell that has an NEG, allows NVG and has a bounded internal state, we take the MD LSTM Stable cell proposed in the last section and change its layout. Therefore we calculate the activation of the IG as function of the FG, so that we achieve\n∣∣spc ∣∣ ≤ 1 by choosing ypι := 1 − ypφ. So the activation of the FG controls how much leaks from the previous states. The activation of the FG can also be interpreted as switch, if the internal activation, the new activation or a convex combination of these both activations should be stored in the cell. So the sc can be seen as time-dependent exponential moving average of ycin .\nDefinition 19 (MD Leaky cell) An MD Leaky cell is a cell of dimension D and order 1 where hc = tanh and\n• Γ = {(λ, 1) , . . . , (λ,D) , φ, ω}\n• sp − c = gconv ( ypΓ, s p−1 c , . . . , s p−D c ) = D∑ d=1 s p−d c\nypλ,d D∑ d′=1 yp λ,d′\n• spc = gint ( ypΓ, y p cin , s p− c ) = ( 1− ypφ ) ypcin + s p− c y p φ\n• ypc = gout ( ypΓ, s p c ) = ypωhc ( spc )\nNow we can prove that the resulting cell has all benefits.\nTheorem 20 The MD Leaky cell has an NEG and allows NVG and COD.\nProof See A.6 in appendix.\nThe MD Leaky cell can have one gate less than the MD LSTM cell and the MD LSTM Stable cell and because of this, the update path requires less computations."
    }, {
      "heading" : "7. General Derivation of Leaky Cells",
      "text" : "So far we proposed cells for the MD case, which are able to provide long term memory. But especially in MDRNNs with more than one MD layer it is hard to measure if and how much long term dependencies are used and even if it is useful. Another way to interpret the cell is to consider them as kind of MD feature extractor like “feature maps” in Convolutional Neural Networks (Bengio and LeCun, 1995). Then the aim is to construct an MD cell which is able to generate useful features. Having a hierarchical Neural Network like in Bengio and LeCun (1995) and Graves and Schmidhuber (2008) over the hierarchies the number of features increases with a simultaneously decreasing feature resolution. Features in a layer with low resolution can be seen as low frequency features in comparison to features in a layer with high resolution. So it would be useful to construct a cell as feature extractor which produces a low frequency output in comparison to its input. In appendix B we take a closer look at the theory of linear shift invariant (LSI)-systems and their frequency analysis and analyse a first order LSI-system regarding its free selectable parameters using the Fand Z-transform. There, we derive the MD LeakyLP cell (see Definition 21) and 5 additional first order MD cells, which we do not test in Section 8.\nDefinition 21 (MD LeakyLP cell) An MD LeakyLP cell is a cell of dimensionD and order 1 where hc = tanh and\n• Γ = {(λ, 1) , . . . , (λ,D) , φ, ω0, ω1}\n• sp − c = gconv ( ypΓ, s p−1 c , . . . , s p−D c ) = D∑ d=1 s p−d c\nypλ,d D∑ d′=1 yp λ,d′\n• spc = gint ( ypΓ, y p cin , s p− c ) = ( 1− ypφ ) ypcin + s p− c y p φ • ypc = gout ( ypΓ, s p c , s p− c ) = hc ( spcy p ω0 + s p− c y p ω1\n) Setting the second OG (ypω1) to zero, the LeakyLP cell corresponds to the Leaky cell, hence it fulfills all three properties, but has one more gate, which is as much gates as the LSTM cell."
    }, {
      "heading" : "8. Experiments",
      "text" : "RNNs with 1D LSTM cells are well studied. In some experiments the activations of the gates and the internal state are observed and one can see that the cell can really learn, when to “forget” information and when the internal state should be accessible for the network (see F. A. Gers and Schmidhuber, 2002). However, we did not find experiments like these for the MD case and we do not want to transfer these experiments into the MD case. Instead we compare the different cell types with each other in two scenarios where the MD RNNs with LSTM cells perform very well. In both benchmarks the task is to transcribe a handwritten text on an image, so we have a 2D RNN. In this case we compare the cells on the IFN/ENIT (Pechwitz et al., 2002) and the Rimes database (Augustin et al., 2006). Both tasks are solved with the MD RNN layout described in Graves and Schmidhuber (2008) and shown in Figure 4. All networks are trained with Backpropagation through time (BPTT) . To compare the different cell types in RNNs with each other we take 10 RNNs with different weight initializations of each cell type and calculate the minimum, the maximum and the median of the best label error rate (LER) on a validation set of these 10 RNNs. In all tables we present these three LERs to compare the cell types. We think it is more important to have stable cells in the lower MD layers because of two reasons: First, when we have just a few cells in a layer, the saturation of one cell has a greater effect on the performance of the network. Second, in lower layers there are longer time series so having an unstable cell in such a layer, it has time to saturate. So our first experiment compares the recognition results when we substitute the LSTM cells in the lowest layer (which is “2D layer 1” in Figure 4) by the newly developed cells. In the second experiment we compare the LSTM cell and the LeakyLP cell also in the higher MD layers (“2D layer 2 and 3 in Figure 4), to evaluate if the LeakyLP cell work better also in long time series. In Bengio (2012, 3.1.1) it is mentioned, that an important hyper parameter for a training is the learning rate, so another experiment is to train all networks with stochastic gradient decent with different learning rates δ ∈ { 1 · 10−3, 5 · 10−4, 2 · 10−4, 1 · 10−4 } and compare the best LER according a fixed learning rate."
    }, {
      "heading" : "8.1 The IFN/ENIT Database",
      "text" : "This database contains handwritten names of towns and villages of Tunisia. The set is divided into 7 (a-f,s) sets, where 5 (a-e) are available for training and validation (for details see Pechwitz et al., 2002). With all information we got from A. Graves, we were able to get comparable results to Graves and Schmidhuber (2008). Therefor we divide the sets a-e into 30000 training samples and 2493 validation samples. All network are trained 100 epochs with a fixed learning rate δ = 1 ·10−4. The LER is calculated on the validation set."
    }, {
      "heading" : "8.1.1 DIFFERENT CELLS IN THE LOWEST MD LAYER",
      "text" : "In our first experiment we substitute the LSTM cell in the lowest MD layer. We take some of the cells described in this paper. In Table 1 the results are shown. The first row is the same RNN layout used in Graves and Schmidhuber (2008). We can see, that the LeakyLP cell performs the best. Nevertheless the worst RNN with LeakyLP cells in the lowest MD layer performs worth than the best RNN with LSTM cells. So we cannot say, that LeakyLP is always better. But it can be observed that the variance of the RNN performance is very high with LSTM cells in the lowest MD layer. Our interpretation is that LSTM cells have a comparable performance like the LeakyLP cells in the lowest layer, when they do not saturate. Note, that the Leaky cell has one gate less, so they are faster and have less trainable weights."
    }, {
      "heading" : "8.1.2 DIFFERENT CELLS IN OTHER MD LAYERS",
      "text" : "Now we want to compare the best new developed cell—the LeakyLP cell—with the LSTM cell in the other MD layers. So we also substitute the LSTM cell in the upper MD layers. We enumerate the 2D layers like shown in Figure 4. In Table 2 we can see that substituting the LSTM cells only in the lowest or in the both lowest layer perform slightly better. The best results can be achieved when we use LeakyLP cells in 2D layer 1 and LSTM cells in 2D layer 3. Using LSTM in the middle layer seems to work slightly better than using the LeakyLP cells instead. This fits to our intuition mentioned before that the LSTM cells perform better when they do not have a too long time series and when there are enough cells in one layer which do not saturate."
    }, {
      "heading" : "8.1.3 PERFORMANCE OF CELLS REGARDING LEARNING-RATE",
      "text" : "When we take a look at the update equations and the proofs of the NEG it can be assumed, that the gradient going through the cells is lower for LeakyLP cells in contrast to LSTM cells. So we think the learning rate have to be larger for LeakyLP cells. In Table 3 we compare the networks with\neither only LSTM or LeakyLP cells. There we can see that the learning rate have to be much higher for the LeakyLP cells. In addition, the RNNs with LeakyLP cells are more robust to the choice of the learning rate."
    }, {
      "heading" : "8.2 The Rimes Database",
      "text" : "One task of the Rimes database is the handwritten word recognition (for more details see E. Grosicki and Geoffrois, 2008; Grosicki and El-Abed, 2011). It contains 59292 images of french single words. It is divided into distinct subsets; a training set of 44196 samples, a validation set of 7542 samples and a test set of 7464 samples. We train the MD RNNs by using the training set for training and calculate the LER over the validation set, so the network is trained on 44196 training samples each epoch. The network used in this section differs only in the subsampling rate between two layers from the network used in Graves and Schmidhuber (2008). When there is a subsampling between layers, the factors are 3 × 2 instead of 4 × 3 or 4 × 2. The rest of the experiment is the same like described in Section 8.1."
    }, {
      "heading" : "8.2.1 DIFFERENT CELLS IN THE LOWEST MD LAYER",
      "text" : "In Table 4 we can see that substituting the LSTM in the lowest layer by one of the three cells improves the performance of the network, even the Leaky cell with one gate less."
    }, {
      "heading" : "8.2.2 DIFFERENT CELLS IN OTHER MD LAYERS",
      "text" : "We want to see the effect of the substitution of the LSTM cell by the LeakyLP cell in the upper MD layers. In Table 5 we can see that using LeakyLP cells in both lowest layers perform very well. So we also take this setup to try different learning rates.\nPerformance of Cells Regarding Learning-Rate. Using different learning rates we can see that the RNN with LeakyLP cells in the both lowest layers and the LSTM cells in the top layer can significantly improve the performance . Even the maximal LER of this RNN works better than the best network with LSTM cells in each layer."
    }, {
      "heading" : "9. Conclusion",
      "text" : "In this paper we took a look at the one-dimensional LSTM cell and discussed the benefits of this cell. We found two properties, that probably make these cells so powerful in the one dimensional case. Expanding these properties to the multi dimensional case, we saw that the LSTM does not fulfill one of these properties any more. We solved this problem by changing the architecture of the cell. In addition we presented a more general idea how to create one dimensional or multi dimensional cells. We compare some newly developed cells with the LSTM cell on two data sets and we can improve the performance using the new cell types. Due to this we think that substituting the multidimensional LSTM cells by the multi-dimensional LeakyLP cell could improve the performance of many system working with a multi-dimensional space."
    }, {
      "heading" : "Appendix A. Proofs",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of 7",
      "text" : "Proof Let c be a 1D LSTM cell. To get the derivative ∂sc(t2)∂sc(t1) according the truncated gradient between two time steps t1, t2 ∈ N we have to take a look at gint.\n∂sc (t2) ∂sc (t1) = ∂gint (yΓ(t2), ycin(t2), sc(t2 − 1)) ∂sc (t1) (12)\n= ∂ (ycin(t2)yι(t2))\n∂sc (t1)︸ ︷︷ ︸ = tr 0\n+ ∂sc(t2 − 1) ∂sc (t1) yφ(t2) + sc(t2 − 1) ∂yφ(t2)\n∂sc (t1)︸ ︷︷ ︸ = tr 0\n= tr ∂sc(t2 − 1) ∂sc (t1) yφ(t2)\n= tr t2∏ t=t1+1 yφ(t) (13)\nIn addition, ∀t ∈ N we have ∂sc (t)\n∂ycin (t) = yι(t) and\n∂yc (t)\n∂sc (t) = h′c (sc(t)) yω(t). (14)\nWe will prove the properties successively. NEG: For the LSTM cell the FG fφ = flog ensures yφ(t) ∈ (0, 1), so using these bounds in (13) with\n∂sc (t)\n∂sc (tin) = tr t∏ t′=tin+1 yφ(t ′) ∈ (0, 1)\nthe LSTM cell has an NEG. NVG: Therefore, we choose\nyι (t) ∈ {\n[1− ε, 1) if t = tin (0, ε] otherwise ,\nyφ (t) ∈ {\n[1− ε, 1) if tin < t ≤ tout (0, ε] otherwise ,\nwith a later chosen ε > 0. Let t1, t2 ∈ N, t1 ≤ t2 be two arbitrary dates, where we want to calculate the gradient ∂sc(t2)∂ycin (t1)\n. First, we want to show that the LSTM cell allows NVG for t1 = tin and tin ≤ t2 ≤ tout: We have yι(t1) ∈ [1− ε, 1) and ∀t = tin + 1, . . . , tout : yφ(t) ∈ [1− ε, 1). Then, we can estimate the derivative from (12) and (14) by\n∂sc (t2) ∂ycin (t1) = ∂sc (t2) ∂sc (t1) ∂sc (t1) ∂ycin (t1) = tr yι (t1) t2∏ t=t1+1 yφ (t)\n∈ tr\n[ (1− ε)\nt2∏ t=t1+1 (1− ε) , 1\n) ⊆ [ (1− ε)tout−tin+1 , 1 ) .\nTo fulfill the equation for NVG we choose ε depending on δ such that\n1− δ ≤ (1− ε)tout−tin+1\n⇔ ε ≤ 1− (1− δ) 1 tout−tin+1\nholds. Second, we have to show, that the derivative is in [0, δ], when t1 = tin and tin ≤ t2 ≤ tout is not fulfilled. In the case of t1 6= tin when ε ≤ δ we can use the NEG which leads to\n∂sc (t2) ∂ycin (t1) = ∂sc (t2)\n∂sc (t1)︸ ︷︷ ︸ ∈ tr [0,1]\n∂sc (t1)\n∂ycin (t1)︸ ︷︷ ︸ ∈(0,ε]\n⊆ [0, ε] ⊆ [0, δ].\nWhen t1 = tin we have two cases: t2 < tin or t2 > tout. For the case t2 < tin the derivative is zero (⊂ [0, δ]), because the cell is causal. For t2 > tout we can split the derivative at tout and get\n∂sc (t2)\n∂ycin (t1) = tr yι(t1) tout∏ t=t1+1\nyφ(t)︸ ︷︷ ︸ ∈(0,1)\nt2∏ t=tout+1 yφ(t)︸ ︷︷ ︸ ∈(0,ε]\n∈ tr\n( 0, εt2−tout ] ⊂ [0, ε] ⊆ [0, δ].\nFor ε ≤ min { δ, 1− (1− δ) 1 tout−tin+1 } the LSTM cell allows NVG.\nCOD: To prove that the LSTM cell has no COD, we show that there are gate activations such that in Definition 5 we get δ2 > δ1. Therefore, we assume that all gate activations are arbitrary (yγ(t) ∈ (0, 1)), closed (yγ(t) ∈ (0, ε]) or opened (yγ(t) ∈ [1 − ε, 1)) with a later chosen ε > 0. We take a look at the right side of (14). For sc(t) = 0 we get h′c (sc(t)) = 1. In Definition 5 we have to satisfy ∃yΓ(t) : ∂yc(t)∂sc(t) ∈ [0, δ2] an choose the OG yω(t) ∈ (0, ε] with\nε ≤ δ2. (15)\nBut then for t′ = 1, . . . , t− 1 we can choose the IG and FG open with the same ε so that\nyφ(t ′), yι(t ′) ∈ [1− ε, 1) .\nWhen for all time steps t′ = 1, . . . , t there is a positive input ycin(t ′) ∈ [c, 1), c ∈ (0, 1) ⊂ R and an internal state sc(t′ − 1) < c (1−ε)ε , the internal state is growing over time, because\nsc(t ′) = ycin(t ′)yι(t ′) + sc(t ′ − 1)yφ(t′) ≥ c(1− ε) + sc(t′ − 1)(1− ε) ≥ sc(t′ − 1) + c(1− ε)− sc(t′ − 1)ε\n> sc(t ′ − 1) + c(1− ε)− c(1− ε)\nε ε\n> sc(t ′ − 1).\nFor large sc(t) ≥ c (1−ε)ε 1 we can estimate\ntanh(sc(t))︸ ︷︷ ︸ ≈exp(−2sc(t))\n≤ exp (−sc(t)) ≤ exp ( −c(1− ε)\nε\n) .\nThis yields in (14) to the bound ∣∣∣∣∂yc (t)∂sc (t) ∣∣∣∣ = ∣∣h′c (sc(t)) yω(t)∣∣ (16) ≤ exp ( −c(1− ε)\nε\n) (17)\nso in Definition 5 we get\nδ1 ≤ exp ( −c(1− ε)\nε\n) . (18)\nBut when we combine (15), (18) and the restriction in Definition 5, we have ε ≤ δ2 < δ1 ≤ exp ( −c(1− ε)\nε\n) ,\nbut there exist ε, c, such that the inequality is not fulfilled, which is a contradiction. Summarized, the 1D LSTM cell allows an NVG and has an NEG, but does not allow COD."
    }, {
      "heading" : "A.2 Proof of 14",
      "text" : "Proof Let c be an MD LSTM cell of dimension D, p,p1, p2, pin, pout ∈ ND, pin ≤ pout arbitrary dates and hc = tanh the sigmoid function. Besides ε > 0 is a later chosen value. In the first step we want to show that there are activations of the forget gates, so that\n∂spc\n∂s pin c ∈ tr\n{ [ (1− ε)‖p−pin‖1 , 1 ] for pin ≤ p ≤ pout\n[0, Dε] otherwise (19)\nis fulfilled. The prove is done using induction over k = ‖p − pin‖1 with p ≥ pin. The base k = 0 is clear. Let be k ≥ 1. We define\nPp := { d ∈ {1, . . . , D} | p−d ≥ pin } the set of dimensions d, in which are pin-p − d -paths. Note, that this set cannot be empty, because\np > pin for k ≥ 1. When we have a dimension d ∈ Pp then ∥∥p−d − pin∥∥1 = k − 1 and we assume\n∂s p−d c ∂s pin c ∈ tr\n[ (1− ε)‖p − d −pin‖1 , 1 ] = [ (1− ε)k−1 , 1 ] . (20)\nThen we choose the activations of the FG to be\nypφ,d ∈\n{ [ 1−ε |Pp | , 1 |Pp | ) for d ∈ Pp and pin < p ≤ pout\n[0, ε] otherwise . (21)\nThen we can estimate the derivative for pin ≤ p ≤ pout using (20) and (21) to\n∂spc\n∂s pin c\n= tr ∑ d∈Pp ∂s p−d c ∂s pin c ypφ,d ∈ ∑ d∈Pp ∂s p−d c ∂s pin c 1− ε |Pp| , ∑ d∈Pp ∂s p−d c ∂s pin c 1 |Pp|  ⇒ ∂s p c\n∂s pin c ∈ tr\n[ |Pp| (1− ε)k−1\n1− ε |Pp| , |Pp| 1 |Pp| ) ⇔ ∂s p c\n∂s pin c ∈ tr\n[ (1− ε)‖p−pin‖1 , 1 ) , (22)\nso (19) is fulfilled for pin ≤ p ≤ pout. If we have p < pin in (19), the derivative is 0, because we have a causal system. For p > pout in (19), we choose ε ≤ 1D ≤ 1 |Pp | in (21) to ensure ∀p ∈ N D: ∣∣∣ ∂spc ∂s pin c ∣∣∣ ≤ 1 (see (22)) and we get\n∂spc\n∂s pin c\n= tr ∑ d=1,...,D ∂s p−d c ∂s pin c ypφ,d ∈ 0, Dε max d=1,...,D ∂s p−d c ∂s pin c  ⊆ (0, Dε] , (23) and (19) is fulfilled. In the second step let p1 ≤ p2 be the date, for which we want to calculate the truncated gradient ∂s p2 c ∂y p1 cin . We choose the IG activation as\nypι ∈ { [1− ε, 1) if p = pin (0, ε] otherwise\n(24)\nand we get ∂s p c ∂ypcin = ypι . Using (22), (23) and (24), we can estimate the partial derivative by\n∂s p2 c ∂y p1 cin = ∂s p2 c ∂s p1 c ∂s p1 c ∂y p1 cin\n⇒ ∂s p2 c\n∂s p1 c ∈ tr\n{ [ (1− ε)(1− ε)‖p2−pin‖1 , 1 ] for p1 = pin and pin ≤ p2 ≤ pout\n[0, Dε] otherwise .\nand setting\nε := min\n{ δ\nD , 1− (1− δ)\n1 ‖pin−pout‖1+1 }\nthe conditions of Definition 11 are fulfilled."
    }, {
      "heading" : "A.3 Proof of 15",
      "text" : "Proof Let c be an MD cell of dimension D with the internal state sc and pin, pk ∈ ND, pin ≤ pk two dates. Let pk be a date k steps further in each dimension than a fixed date pin. So the distance\nbetween them is ‖pin − pk‖1 = Dk. Let Π be the set of all pin-pk-paths, then there exist |Π| = #{−−−→pinpk} paths (see Definition 8). We assume\nypφ,d ∈ [ε, 1− ε]\nwith ε ∈ (0, 0.5) and we can estimate the partial derivative, using the truncated gradient, with\n∂s pk c ∂s pin c = tr ∑ π∈Π k∏ i=1 yπiφ,d\n∈ tr\n[ εk#{−−−→pinpk}, (1− ε) k #{−−−→pinpk} ] .\nFor D = 1 we get |Π| = 1 and the cell has a NGEC. When D ≥ 2 we can count the number of paths using the Stirling’s approximation and we can estimate the number of paths with\n#{−−−→pinpk} =\n( D∑ i=1 (−−−→pinpk)i)! D∏ i=1 (−−−→pinpk)i! = (Dk)! (k!)D k 1−−−→ √ 2πDk ( Dk e )Dk(√ 2πk ( k e )k)D = √ DDDk √ 2πk D−1 .\nWhen we combine it with the FG activations we can estimate the derivative for great k with the Stirling’s approximation and get\n∂s pk c ∂s pin c ∈ tr\n[ εDk#{−−−→pinpk}, (1− ε) Dk #{−−−→pinpk} ]\n(25)\nk 1⇒ ∈ tr\n[ √ D\n√ 2πk\nD−1 (Dε) Dk ,\n√ D\n√ 2πk\nD−1 (D (1− ε)) Dk\n] .\nThe upper bound of this interval can grow for great k, if [D (1− ε)] > 1 and this is the case for D ≥ 2. So the MD LSTM cell can have an exploding gradient for D ≥ 2. When the weights to the FGs are initialized with small values, we have ypφ,d ≈ 0.5. Then we have an exploding gradient when D ≥ 3, when the training is starting. In the worst case we have ypφ,d ≈ 1 and the derivative in (25) goes for great k to\n∂s pk c ∂s pin c ≈\n√ D\n√ 2π\nD−1k 1−D 2 (D)Dk ."
    }, {
      "heading" : "A.4 Proof of 17",
      "text" : "Proof Let c be an MD LSTM Stable cell of dimension D ≥ 2 (for D = 1 the proof is equivalent to the 1D case of the LSTM cell), p,p1, p2, pin, pout ∈ ND, pin ≤ pout arbitrary dates and hc = tanh the sigmoid function. Besides ε > 0 is a later chosen value. In the first step we want to show that there are activations of the forget gates, so that\n∂spc\n∂s pin c ∈ tr\n{ [ (1− (D − 1)ε)2‖p−pin‖1 , 1 ] for pin ≤ p ≤ pout\n[0, ε] otherwise (26)\nis fulfilled. The prove is done using induction over k = ‖p−pin‖1. The base k = 0 is clear. Let be k ≥ 1. We define\nPp := { d ∈ {1, . . . , D} | p−d ≥ pin } the set of dimensions d, in which are pin-p − d -paths. Note, that this set cannot be empty, because\np > pin for k ≥ 1. When we have a dimension d ∈ Pp then ∥∥p−d − pin∥∥1 = k − 1 and we assume\n∂s p−d c ∂s pin c ∈ tr\n[ (1− (D − 1)ε)2‖p − d −pin‖1 , 1 ] = [ (1− (D − 1)ε)2(k−1) , 1 ] . (27)\nWhen we choose the activations of the LGs to be ypλ,d ∈ { [1− ε, 1) for d ∈ Pp and pin < p ≤ pout (0, ε] otherwise , we can estimate ∑ d∈Pp y p λ,d∑D\nd′=1 y p λ,d′ ∈ (1− (D − 1)ε, 1], because\n1 ≥ ∑ d∈Pp y p λ,d∑D\nd′=1 y p λ,d′\n=\n∑ d∈Pp y\np λ,d∑\nd∈Pp y p λ,d + ∑ d∈{1,...,D}\\Pp y p λ,d′\n(28)\n≥ |Pp| (1− ε) |Pp| (1− ε) + (D − |Pp|)︸ ︷︷ ︸\n≤D−1\nε\n≥ |Pp| (1− (D − 1)ε) |Pp| (1− (D − 1)ε) + (D − 1) ε ≥ (1− (D − 1)ε) |Pp| |Pp| − ε(D − 1) (|Pp| − 1)\n≥ (1− (D − 1)ε).\nSetting the FG to\nypφ ∈ { [1− ε, 1) for pin < p ≤ pout (0, ε] otherwise\n(29)\nwe can estimate the derivative for pin ≤ p ≤ pout using (27),(28) and (29) to\n∂spc\n∂s pin c\n= tr ypφ  ∑ d∈Pp ∂s p−d c ∂s pin c ypλ,d∑D d′=1 y p λ,d′ + ∑ d∈{1,...,D}\\Pp ∂s p−d c ∂s pin c ypλ,d∑D d′=1 y\np λ,d′︸ ︷︷ ︸\n=0  ∈ tr ( (1− ε) (1− (D − 1)ε)2(k−1) (1− (D − 1)ε), 1\n) ⇒ ∂s p c\n∂s pin c ∈ tr\n( (1− (D − 1)ε)2k , 1 ) (30)\nso (26) is fulfilled for pin ≤ p ≤ pout. If we have p < pin in (26), the derivative is 0, because we have a causal system. For p > pout the FG is closed (see (29)), and using the upper bounds of (27) and (28) we get\n∂spc\n∂s pin c\n= tr ypφ  D∑ d=1 ∂s p−d c ∂s pin c ypλ,d∑D d′=1 y p λ,d′  (31) ∈ tr (0, ε]\nand (26) is fulfilled. In the second step let p1 ≤ p2 be the date, for which we want to calculate the truncated gradient ∂s p2 c ∂y p1 cin . We choose the IG activation as\nypι ∈ { [1− ε, 1) if p = pin (0, ε] otherwise\n(32)\nand we get ∂s p c ∂ypcin = ypι . Using (30), (31) and (32), we can estimate the partial derivative by\n∂s p2 c ∂y p1 cin = ∂s p2 c ∂s p1 c ∂s p1 c ∂y p1 cin\n⇒ ∂s p2 c\n∂s p1 c ∈ tr\n{ [ (1− ε)(1− (D − 1)ε)2‖p2−pin‖1 , 1 ] for p1 = pin and pin ≤ p2 ≤ pout\n[0, ε] otherwise .\nand setting\nε := min { δ, ( 1− (1− δ) 1 2‖pin−pout‖1+1 ) 1\nD − 1 } the conditions of Definition 11 are fulfilled."
    }, {
      "heading" : "A.5 Proof of 18",
      "text" : "Proof Let c be a MD LSTM Stable cell of dimension D with the internal state sc and pin, p ∈ ND, pin ≤ p two arbitrary dates and ‖pin − p‖1 = k. Let all gate activations be arbitrary in [0, 1]. We show that\n∂spc\n∂s pin c ∈ tr [0, 1] (33)\nis fulfilled ∀k ∈ N using induction over k. For the base case k = 0 we get ∂s p c\n∂s pin c\n= ∂s pin c\n∂s pin c\n= 1. Let (33) be fulfilled for k − 1. That means if p−d ≥ pin we have ∥∥p−d − pin∥∥1 = k − 1 and this leads to ∂s p− d c\n∂s pin c ∈ tr [0, 1]. If p−d pin then there is no pin-p − d -path and we have\n∂s p− d c ∂s pin c = 0 for this\ndimension. Then we can calculate the derivative\n0 ∂spc\n∂s pin c\n= tr ypφ D∑ d=1 ∂s p−d c ∂s pin c\nypλ,d D∑ d′=1 ypλ,d′ ∈\n[ 0,max d∈Pp { ∂s p−d c ∂s pin c\nypλ,d D∑ d′=1\nypλ,d′︸ ︷︷ ︸ ≤1\n}]\n∈ tr [0, 1] ,\nwhich gives us the desired interval."
    }, {
      "heading" : "A.6 Proof of 20",
      "text" : ""
    }, {
      "heading" : "Proof",
      "text" : "NEG: The cell has an NEG, because all gates have the same bounds as the MD Stable cell. NVG: To prove the NVG, we use the proof of Theorem 17. The difference between the MD Stable cell and the MD Leaky cell is that the activations of the FG and IG are dependent on each other for the Leaky cell. Let pin, p ∈ ND, pin ≤ p be two arbitrary dates like in Theorem 17. The IG has just the a restriction that for p = pin it has to hold y p ι ∈ [1− ε, 1) . Here, the FG can have an arbitrary activation, so we chose ypφ = 1− y p ι . For all p > pin the FG have to be in the ranges, shown in (29), while the IG has no restriction and we choose ypι = 1− ypφ, so the MD Leaky cell has the NVG. COD: The proof that the MD Leaky cell allows COD can be done by estimating the bounds of spc . From the update equations of the cell we get∣∣∣sp−c ∣∣∣ ≤ max\ni=1,...,D ∣∣∣∣sp−dc ∣∣∣∣ . Now we can estimate the internal state using the ranges ypcin ∈ [−1, 1], recursion over p\n|spc | = ∣∣∣(1− ypφ) ypcin + ypφsp−c ∣∣∣ ≤ max{∣∣ypcin∣∣ , ∣∣∣∣sp−1c ∣∣∣∣ , . . . , ∣∣∣∣sp−Dc ∣∣∣∣} ≤ maxq<p {∣∣yqcin∣∣} ≤ 1\nand get spc ∈ [−1, 1]. To fulfill the derivatives in Definition 12, for δ1 we choose ypω ∈ [1− ε, 1) and get\nδ1 ≤ min spc\n{ h′c (s p c) } (1− ε) = h′c (1) (1− ε). (34)\nFor δ2 we choose y p ω ∈ (0, ε] and get\nδ2 ≥ max spc\n{ h′c (s p c) } ε = ε. (35)\nTo fulfill the derivatives in Definition 12 we use (34), (35) and h′c (1) > 1 3 and with\nε ≤ δ2 < δ1 ≤ h′c (1) (1− ε)\n⇒ ε ≤ 1 4 <\nh′c (1)\nh′c (1) + 1\nthe COD is proven."
    }, {
      "heading" : "Appendix B. Theory to Create First Order MD Cells",
      "text" : "If one wants to take a closer look at the theory of linear shift invariant (LSI)-systems and their frequency analysis and analyse a first order LSI-system regarding its free selectable parameters using the F- and Z-transform, it is highly recommended to be familar with these theories (for a good overview and more details see Poularikas, 2000; Schlichthärle, 2000). Adding the knowledge of reducing the MD case to the 1D case (see Section 5) we create new cell types for the MD case."
    }, {
      "heading" : "B.1 Analysing a First Order LSI-System",
      "text" : "The update equations of a first order LSI-system with one input u, one internal state x and one output y can be written as\nx[n] = h1(u[n], x[n− 1]) = α0u[n] + α1x[n− 1], (36) y[n] = h2(x[n], x[n− 1]) = b0x[n] + b1x[n− 1] (37)\nwith the free selectable coefficients α0, α1, b0, b1 ∈ R. Let U(z) = Z {u[n]} be the Z-transformed signal of u[n] and X(z), Y (z) respectively. Then we can write the so called transfer functions\nH1(z) := X(z)\nU(z) = α0 1− α1z−1 ,\nH2(z) := Y (z)\nX(z) = b0 + b1z\n−1,\nH(z) := Y (z)\nU(z) = H2(z)H1(z).\nTo analyse (36) and (37) according their frequency response we use the relationship between the F-transform and the Z-transform:\nRemark 22 Let u[n] = ejωn be a harmonic input sequence with the imaginary number j2 = −1 and H(z) = Y (z)U(z) be a transfer functions of an LSI-system. When the poles of H(z) are inside the circle |z| = 1, we can change from Z- to F-transform using the substitution\nH(ω) = H(z) ∣∣∣∣ z=ejω\nwith the harmonic sequence y[n] = H(ω)u[n] = H(ω)ejωn with the same frequency ω but with a different amplitude and a different phase dependent on the frequency ω.\nWe only want to analyse the amplitude of this harmonic sequence |y[n]| = ∣∣H(z)ejωn∣∣ = |H(z)| = |H2(z)| |H1(z)|\nand do that by analysing both transfer functions H1(z) and H2(z) separately. The amplitude of H1(ω) = H1(z)|z=ejω is calculated by\n|H1(ω)| = |α0|√\n(1− α1 cos(ω))2 + α21 sin2(ω) .\nyI(t) yH(t− 1)\nyI(t) yH(t− 1)\nyI(t) yH(t− 1)\nLike mentioned before, in many tasks, the information signal has a low frequency. To have the largest amplitude at ω = 0 we have to choose α1 ≥ 0. As mentioned in Remark 22 the poles of H1(z) =\nα0 1−α1z−1 = zα0 z−α1 have to be in the circle |z| = 1, so we have the additional constraint\n|α1| < 1. This leads to the bounds α1 ∈ [0, 1). But for α1 → 1 we have H1 (0) → ∞, so we have to choose α0 dependent on α1. We set a maximum gain of max\nω |H1(ω)| = |H1(0)| = 1, so we get\nthe constraint\n|α0| ≤ 1− α1. (38)\nIn the same way we analyse H2(z): |H2(ω)| = ∣∣b0 + b1e−jω∣∣ = √(b0 + b1 cos(ω))2 + b21 sin2(ω)\nTo get the maximal gain at low frequency the parameters b0 and b1 must have the same sign."
    }, {
      "heading" : "B.2 Creating a First Order Cell",
      "text" : "With these constraints for the parameters we now can define a new cell type. The parameters α0, α1, b0, b1 should be activations of gates like in LSTM cells. We have to find the right activation functions to fulfill the inequalities above. Using the weight-space symmetries in a network with at least one hidden layer (Bishop, 2006, 5.1.1), without loss of generality we set α0, α1, b0, b1 ≥ 0. To fulfill the bounds for H1, we set α1 as activation of a gate with activation function flog. So we\nhave α1 ∈ (0, 1). This is comparable with the FG in the previous sections. To select the α0 we choose α0 := 1 − α1 (see (38)). So the value of α0 is comparable with the activation of the IG. For H2 we set both values b0, b1 as activations of a gate with activation function flog which leads to max ω |H2(ω)| = max {b0 + b1} = 2, so the amplitude response is bounded by 2.\nWith these bounds we can define a cell with a cell input ypcin = u[n], a previous internal state sp − c = x[n−1], an internal state spc = x[n] and a cell output ypc = y[n]. We substitute the coefficients by time dependent gate activations\nα0 := 1− ypφ = y p ι IG α1 := y p φ FG b0 := y p ω0 OG b1 := y p ω1 OG of the previous internal state\nwhich leads to the transfer functions\nH ypφ 1 (z) = α0 1− α1z−1\n= 1− ypφ\n1− ypφz−1 ,\nH ypω0 ;y p ω1\n2 (z) = b0 + b1z −1 = ypω0 + y p ω1z −1,\nH(z) = Hy p φ;y p ω0 ;ypω1 (z) = Y (z)\nU(z) = α0\nb0 + b1z −1 1− α1z−1 = 1− ypφ 1− ypφz−1\n( ypω0 + y p ω1z −1) . (39)\nand the update equations\nx[n] =α0u[n] + α1x[n− 1] ⇔ spc = (1− y p φ)y p cin + y p φs p− c ,\ny[n] =b0x[n] + b1x[n− 1] ⇔ ypc = ypω0s p c + y p ω1s p− c . (40)\nThe output of the cell is already bounded in [−2, 2], but to fulfill Definition 9 we change (40) to\nypc = hc ( ypω0s p c + y p ω1s p− c ) (41)\nwith hc = tanh to ensure y p c ∈ [−1, 1]. This additional non-linearity is not necessary but leads to a better performance. This new cell type called MD Leaky lowpass (LeakyLP) cell is defined in Definition 21. A block diagram of a 1D LeakyLP cell is shown in Figure 5 and different frequency responses in Figure 6."
    }, {
      "heading" : "B.3 General First Order MD Cells",
      "text" : "With the theory of this section we can easily create new cell types. In general, a cell has a number of gates γ1, γ2, . . . ∈ Γc. For D = 1 a previous state sp − c is given directly. Otherwise the previous state is calculated as trainable convex combination of D previous states, like described in Section 5. In Table 7 cell layouts are depicted whereby type A is the cell developed in Section 7 (compare to (39)). For the other types we briefly want to describe the main ideas."
    }, {
      "heading" : "B.3.1 THE MD BUTTERWORTH LOWPASS FILTER",
      "text" : "The cell of type B is a special case of the LeakyLP cell. When we set ypω0 = y p ω1 = 0.5 there is a direct relation between the cutoff frequency of a discrete Butterworth lowpass filter and the activation of ypφ: Let fcutoff be the frequency, where amplitude response is reduced to 1√ 2 of the\nmaximal gain. We can calculate fcutoff by\nfcutoff = 1\nπ arctan ( 1− ypφ 1 + ypφ ) (42)\n⇔ ypφ = 1 + tan(πfcutoff)\n1− tan(πfcutoff)\nwith the bounds fcutoff ∈ (0, 0.5) and ypφ ∈ (−1, 1) (for more details see Schlichthärle, 2000, 2.2;6.4.2). For ypφ ∈ (0, 1) we get fcutoff ∈ (0, 0.25). In Figure 7 (left) we can see, that even for a negative value of ypφ and a highpass characteristic of H1(z) the impulse response H(z) has a lowpass characteristic."
    }, {
      "heading" : "B.3.2 ADDING AN ADDITIONAL STATE GATE",
      "text" : "In B.2 we fulfilled (38) for the MD LeakyLP cell by setting α0 := 1 − α1, so α0 is directly connected with α1. Another solution would be to add an additional value γ ∈ (0, 1) and choose α0 := γ (1− α1). So we can extend the MD LeakyLP cell by adding an additional gate γ4 for the previous state (see type C). Unfortunately this does not lead to a better performance and one more gate has to be calculated."
    }, {
      "heading" : "B.3.3 ANOTHER SOLUTION FOR THE OUTPUT",
      "text" : "The cell of type D is another solution to choose b0 and b1 in Section B.2. For the LeakyLP cell we calculate the output as described in (41). Now we set b0 = γ p 2γ p 3 and b1 = ( 1− γp2 ) γp3 , and get\nypc = γ p 3 ( γp2s p c + ( 1− γp2 ) sp − c ) .\nThis cell actually works as well as the MD LeakyLP cell and has the same number of gates. In this case we do not need a squashing function hc, because we already have y p c ∈ [−1, 1]."
    }, {
      "heading" : "B.3.4 AN MD CELL AS MD PID-CONTROLLER",
      "text" : "Type E has a completely different interpretation: In controlling engineering a PID-controller gets an error as input. In our case the gate activations have to decide, if the proportional (P), the integral (I) or the derivative (D) term of the error is important for the output. When γp1 ≈ 0 we have y p cin ≈ s p c so the internal state is proportional to the input. Then γp2 gates the proportional part (P) of the input. The second gate γp3 gates the difference between the last and the current input, which can be seen as a discrete derivative (D). If γp1 ≈ 1 the internal state is an exponential moving average of y p cin which is an integral term. So γp2 gates a mainly integral part of the input (I), whereas γ p 3 gates a mainly proportional part of the input (P). Dependent on γp1 type E can be a PD-controller, a PI-controller or a mix of these both. In Figure 7(right) can be seen the frequency response of this cell for different gate activations."
    } ],
    "references" : [ {
      "title" : "Multi-dimensional recurrent neural networks",
      "author" : [ "S. Fernandez A. Graves", "J. Schmidhuber" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Graves and Schmidhuber.,? \\Q2007\\E",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2007
    }, {
      "title" : "RIMES evaluation campaign for handwritten mail processing",
      "author" : [ "E. Augustin", "J.-M Brodin", "M. Carré", "E. Geoffrois", "E. Grosicki", "F. Prêteux" ],
      "venue" : "In Proc. of the Workshop on Frontiers in Handwriting Recognition,",
      "citeRegEx" : "Augustin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Augustin et al\\.",
      "year" : 2006
    }, {
      "title" : "Practical recommendations for gradient-based training of deep architectures",
      "author" : [ "Y. Bengio" ],
      "venue" : "CoRR, abs/1206.5533,",
      "citeRegEx" : "Bengio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2012
    }, {
      "title" : "Convolutional networks for images, speech, and time-series",
      "author" : [ "Y. Bengio", "Y. LeCun" ],
      "venue" : null,
      "citeRegEx" : "Bengio and LeCun.,? \\Q1995\\E",
      "shortCiteRegEx" : "Bengio and LeCun.",
      "year" : 1995
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Y. Bengio", "P. Simard", "P. Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Pattern Recognition and Mashine Learning",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop.,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop.",
      "year" : 2006
    }, {
      "title" : "RIMES evaluation campaign for handwritten mail processing",
      "author" : [ "J.-M. Brodin E. Grosicki", "M. Carré", "E. Geoffrois" ],
      "venue" : "In Proc. of the Int. Conf. on Frontiers in Handwriting Recognition,",
      "citeRegEx" : "Grosicki et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Grosicki et al\\.",
      "year" : 2008
    }, {
      "title" : "Learning to forget: Continual prediction with lstm",
      "author" : [ "J. Schmidhuber F.A. Gers", "F. Cummins" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Gers and Cummins.,? \\Q1999\\E",
      "shortCiteRegEx" : "Gers and Cummins.",
      "year" : 1999
    }, {
      "title" : "Learning precise timing with lstm recurrent networks",
      "author" : [ "N. Schraudolph F.A. Gers", "J. Schmidhuber" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gers and Schmidhuber.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gers and Schmidhuber.",
      "year" : 2002
    }, {
      "title" : "Offline handwriting recognition with multidimensional recurrent neural networks",
      "author" : [ "A. Graves", "J. Schmidhuber" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Graves and Schmidhuber.,? \\Q2008\\E",
      "shortCiteRegEx" : "Graves and Schmidhuber.",
      "year" : 2008
    }, {
      "title" : "ICDAR 2011: French handwriting recognition competition",
      "author" : [ "E. Grosicki", "H. El-Abed" ],
      "venue" : "In Proc. of the Int. Conf. on Document Analysis and Recognition,",
      "citeRegEx" : "Grosicki and El.Abed.,? \\Q2011\\E",
      "shortCiteRegEx" : "Grosicki and El.Abed.",
      "year" : 2011
    }, {
      "title" : "Ifn/enit-database of handwritten arabic words",
      "author" : [ "M. Pechwitz", "S. Maddouri", "V. Märgner", "N. Ellouze", "H. Amiri" ],
      "venue" : "In Proc. of CIFED,",
      "citeRegEx" : "Pechwitz et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Pechwitz et al\\.",
      "year" : 2002
    }, {
      "title" : "The Transforms and Applications Handbook",
      "author" : [ "A.D. Poularikas" ],
      "venue" : "CRC Press, 2. edition edition,",
      "citeRegEx" : "Poularikas.,? \\Q2000\\E",
      "shortCiteRegEx" : "Poularikas.",
      "year" : 2000
    }, {
      "title" : "Long short-term memory",
      "author" : [ "J. Schmidhuber S. Hochreiter" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter.",
      "year" : 1997
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Hochreiter (1997) the authors develop the long short-term memory (LSTM) which is able to have a long term dependency.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "Graves and Schmidhuber (2007) to the multi-dimensional (MD) case and is used in a hierarchical multi-dimensional RNN (MDRNN) which performed best in three competitions at the International Conference on Document Analysis and Recognition (ICDAR) in 2009 without any feature extraction and knowledge of the recognized language model.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 9,
      "context" : "To unify the notations we will refer to their notation using “,” (F. A. Gers and Cummins, 1999; S. Hochreiter, 1997; Graves and Schmidhuber, 2008).",
      "startOffset" : 65,
      "endOffset" : 146
    }, {
      "referenceID" : 12,
      "context" : "Hochreiter (1997) and F.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 7,
      "context" : "Gers and Cummins (1999). Definition 1 (truncated gradient) Let γ ∈ {cin, ι, ω} be any input or gate unit and yc(t− 1) any previous output activation.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "Gers and Cummins (1999). They tried to stabilize the LSTM with a “state decay” by multiplying the internal state in each time step with a value ∈ (0, 1), which did not improve the performance.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "In this paper we denote the Extended LSTM as LSTM Another point of view was introduce in Bengio et al. (1994): To learn long-term dependencies a system must have an architecture to that an input can be saved over long time and does not suffer from the “vanishing gradient” problem.",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "Graves and Schmidhuber (2007) the 1D LSTM cell is extended to an arbitrary number of dimensions; this is solved by using one FG for each dimension.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 3,
      "context" : "Another way to interpret the cell is to consider them as kind of MD feature extractor like “feature maps” in Convolutional Neural Networks (Bengio and LeCun, 1995).",
      "startOffset" : 139,
      "endOffset" : 163
    }, {
      "referenceID" : 1,
      "context" : "Another way to interpret the cell is to consider them as kind of MD feature extractor like “feature maps” in Convolutional Neural Networks (Bengio and LeCun, 1995). Then the aim is to construct an MD cell which is able to generate useful features. Having a hierarchical Neural Network like in Bengio and LeCun (1995) and Graves and Schmidhuber (2008) over the hierarchies the number of features increases with a simultaneously decreasing feature resolution.",
      "startOffset" : 140,
      "endOffset" : 317
    }, {
      "referenceID" : 0,
      "context" : "Having a hierarchical Neural Network like in Bengio and LeCun (1995) and Graves and Schmidhuber (2008) over the hierarchies the number of features increases with a simultaneously decreasing feature resolution.",
      "startOffset" : 73,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "In this case we compare the cells on the IFN/ENIT (Pechwitz et al., 2002) and the Rimes database (Augustin et al.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : ", 2002) and the Rimes database (Augustin et al., 2006).",
      "startOffset" : 31,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Both tasks are solved with the MD RNN layout described in Graves and Schmidhuber (2008) and shown in Figure 4.",
      "startOffset" : 58,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "Graves, we were able to get comparable results to Graves and Schmidhuber (2008). Therefor we divide the sets a-e into 30000 training samples and 2493 validation samples.",
      "startOffset" : 50,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "The first row is the same RNN layout used in Graves and Schmidhuber (2008). We can see, that the LeakyLP cell performs the best.",
      "startOffset" : 45,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "One task of the Rimes database is the handwritten word recognition (for more details see E. Grosicki and Geoffrois, 2008; Grosicki and El-Abed, 2011).",
      "startOffset" : 67,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "The network used in this section differs only in the subsampling rate between two layers from the network used in Graves and Schmidhuber (2008). When there is a subsampling between layers, the factors are 3 × 2 instead of 4 × 3 or 4 × 2.",
      "startOffset" : 114,
      "endOffset" : 144
    } ],
    "year" : 2016,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}