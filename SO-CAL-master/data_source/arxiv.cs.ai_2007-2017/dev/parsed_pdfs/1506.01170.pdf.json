{
  "name" : "1506.01170.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent Systems",
    "authors" : [ "Stefano V. Albrecht" ],
    "emails" : [ "s.v.albrecht@sms.ed.ac.uk", "s.ramamoorthy@ed.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We are concerned with the ad hoc coordination problem, in which the goal is to design an autonomous agent, called the ad hoc agent, which is able to achieve optimal flexibility and efficiency in a multiagent system that admits no prior coordination between the ad hoc agent and the other agents. Flexibility describes the ad hoc agent’s ability to solve its task with a variety of other agents in the system. Efficiency is the relation between the ad hoc agent’s payoffs and time needed to solve the task. No prior coordination means that the ad hoc agent\ndoes not know ahead of time who the other agents are and how they behave. In particular, there are no prior agreements on information sharing, communication and action protocols, standards, etc.\nThis problem is motivated by the fact that there is a growing number of agents, both robotic and virtual, which are employed in an increasing number of areas. Given that a primary goal in agents research is to increase the autonomy and thus lifetime of agents, it can be expected that agents based on different technologies may have to interact in nontrivial ways, without knowing a priori who the other agents are. This motivates both the notion of flexibility, since the other agents could be based on any kind of technology, and efficiency, since there may be no time for long learning periods, especially if interactions are sparse. Human-machine interaction problems (e.g. robots used in rescue scenarios or software agents used in trading markets) can be viewed as a special case of ad hoc coordination, since humans have extremely variable behaviour (flexibility) and expect agents to be able to interact quickly (efficiency), while there may be no prior description of the human’s behaviour (no prior coordination).\nThere have been several attempts to address ad hoc coordination in multiagent systems, e.g. [Bowling and McCracken, 2005,Dias et al., 2006,Stone et al., 2010a]. While all of these works are relevant to ad hoc coordination, the assumptions made by the solutions therein imply that they only address certain aspects of the larger problem. For example, in [Bowling and McCracken, 2005, Dias et al., 2006] it is assumed that all agents follow pre-specified plans which include roles and synchronised action sequences for each role, and in [Stone and Kraus, 2010, Stone et al., 2010b, Barrett et al., 2011,Agmon and Stone, 2012] it is assumed that the other agents’ behaviours are fixed and known, and that all agents have common payoffs. We also note that the problem descriptions in these works are of a procedural nature, associated with the specific tasks considered therein. Therefore, there is a need for a formal model\nar X\niv :1\n50 6.\n01 17\n0v 1\n[ cs\n.G T\n] 3\nJ un\n2 01\nof the ad hoc coordination problem, general enough to accommodate a wide spectrum of problems.\nA related problem is known in game theory as the incomplete information game. Therein, each player has some private information relevant to its decision making of which the other players are not aware, which is what relates the incomplete information game to the ad hoc coordination problem. [Harsanyi, 1967] introduced Bayesian games in which the private information of a player is abstractly represented by its type, admitting a solution in the form of the Bayesian Nash equilibrium. Since then, there have been several works on learning in Bayesian games, e.g. [Jordan, 1991,Kalai and Lehrer, 1993,Dekel et al., 2004]. While the notion of private information is useful to describe the ad hoc coordination problem, the learning processes and solutions studied therein are not directly applicable, since the focus has traditionally been on equilibrium considerations but not on efficiency. On the other hand, much work in multiagent systems has focused on efficiency, whilst often making central assumptions about the other agent’s behaviours [Albrecht and Ramamoorthy, 2012]. Therefore, it is natural to ask if these fields can be combined to address ad hoc coordination in a useful way.\nInspired by this question, we model the problem using a game-theoretic construct called the stochastic Bayesian game, in which a player’s behaviour is determined by its type. Based on this model, we give formal definitions of flexibility and efficiency, and we define ad hoc coordination as the problem of optimising flexibility and efficiency, subject to the constraint that the ad hoc agent is unaware of the players’ type spaces, and hence the rules by which their types are assigned. Our model allows for both the definition of Bayesian Nash equilibrium and, since it satisfies the Markov property, the definition of Bellman optimal control [Bellman, 1957], a key result in intelligent agents. We combine these two concepts to obtain a solution which we call HarsanyiBellman Ad Hoc Coordination (HBA). HBA does not rely on a central assumption about the other agents’ behaviours. Instead, it allows for the specification of multiple such assumptions which are provided to HBA as a set of user-defined types, each corresponding to a different hypothesis of how an agent might behave. Based on the agents’ observed actions, HBA computes probability distributions over the user-defined types, called posteriors, and utilises them in a planning procedure to find optimal actions.\nHBA has a number of useful features with respect to ad hoc coordination. The fact that the user-defined types may encapsulate any kind of behaviour means that HBA can potentially deal with a variety of different agents, including agents which maintain beliefs about the behaviour of the HBA agent, or any other type of recursive\nreasoning. We show this in a human-machine experiment conducted at a public science exhibition, in which HBA was able to manipulate the beliefs of humans in repeated Prisoner’s Dilemma such that both ended up cooperating, thus maximising its efficiency. HBA also supports the possibility that agents may switch between different behaviours. We address this by introducing temporally reweighted posteriors which allow HBA to quickly recognise changed types. In our human-machine experiment, this allowed HBA to achieve a significantly higher winning rate in Rock-Paper-Scissors than the human participants and an alternative algorithm.\nA central feature of HBA is that it can use the types to plan in the entire state space of the problem (including unseen states) provided that the posteriors and user-defined types are reasonably accurate. To accommodate the case in which none of the user-defined types accurately describe an agent’s behaviour, HBA is able to include methods for opponent modelling. We propose an opponent modelling method, called conceptual type, which can be viewed as a kind of type that specifies the conceptualisation underlying a behaviour, rather than specifying the behaviour directly. The conceptualisation is combined with the observed actions of an agent to generalise its actions to unseen states and improve accuracy in rarely visited states. We demonstrate these features in a multiagent logistics domain called level-based foraging, in which HBA is able to achieve significantly higher flexibility and efficiency than three alternative algorithms (JAL [Claus and Boutilier, 1998], CJAL [Banerjee and Sen, 2007], WoLF-PHC [Bowling and Veloso, 2002]), using just a few user-defined types."
    }, {
      "heading" : "2 Defining Ad Hoc Coordination",
      "text" : ""
    }, {
      "heading" : "2.1 Stochastic Bayesian Games",
      "text" : "As discussed earlier, ad hoc coordination can be defined based on the notion of private information in Bayesian games. However, in their original form [Harsanyi, 1967], Bayesian games are not descriptive enough to allow us to model the kinds of problems we are interested in, as they do neither include states nor time. Therefore, we combine Bayesian games with the concept of stochastic games [Shapley, 1953] to obtain a more descriptive model which we call stochastic Bayesian game:1\nDefinition 1. A stochastic Bayesian game (SBG) consists of:\n• discrete state space S with initial state s0 ∈ S and terminal states S̄ ⊂ S\n1A related model are I-POMDP, in which agents face incomplete information with respect to the state of the world and the behaviour of other agents [Gmytrasiewicz and Doshi, 2005]. However, I-POMDP are extremely complex and their solution methods are infeasible in most problems.\n• players N = {1, ..., n} and for each i ∈ N : – set of actions Ai (where A = A1 × ...×An) – type space Θi (where Θ = Θ1 × ...×Θn) – payoff function ui : S ×A×Θi → R – strategy πi : H×Ai ×Θi → [0, 1]\n• state transition function T : S ×A× S → [0, 1]\n• type distribution ∆ : N0 ×Θ→ [0, 1]\nH contains all histories Ht = 〈s0, a0, s1, a1, ..., st〉 with t ≥ 0, (sτ , aτ ) ∈ S ×A for 0 ≤ τ < t, and st ∈ S.\nWe also define several classes of type distributions:\nDefinition 2. A type distribution ∆ is called static if ∀t, t̂ ∀θ ∈ Θ : ∆(t, θ) = ∆(t̂, θ), else it is called dynamic. Definition 3. A type distribution ∆ is called pure if ∀t ∃θ ∈ Θ : ∆(t, θ) = 1, else it is called mixed.\nA SBG starts at time t = 0 in state s0. In state st, the types θt1, ..., θ t n are sampled from Θ with probability ∆(t, (θt1, ..., θ t n)), and each player i ∈ N is only informed about its own type θti . Based on the history H t, each player i chooses an action ati ∈ Ai with probability πi(H t, ati, θ t i). Given the joint action a t = (at1, ..., a t n), the game transitions into a successor state st+1 ∈ S with probability T (st, at, st+1) and every player i receives an individual payoff given by ui(s\nt, at, θti). This process is repeated until the game reaches a terminal state st ∈ S̄, after which the game stops.\nOur definition of types follows the original definition of [Harsanyi, 1967], which means that a type determines a player’s payoffs and strategies. However, since we define strategies with respect to a history of states and actions (rather than just the current state), a type may in fact specify strategies which change over time (such as players who learn or use recursive reasoning), and we thus also refer to it as behaviour. Therefore, our interpretation of types is that of a “programme” which governs the behaviour of a player.\nEach player may correspond to a specific role in the game. For instance, if we model a soccer team, player 1 may correspond to the goal keeper. Therefore, in the following sections, we implicitly assume that the ad hoc agent, denoted α, controls the player of interest, denoted i, by which we mean that α chooses the strategy πi. Furthermore, i has a fixed type which is known to α, and we denote its payoffs by ui(s t, at, α)."
    }, {
      "heading" : "2.2 Flexibility & Efficiency",
      "text" : "Two important aspects of ad hoc coordination are flexibility and efficiency. We now define each of them formally within the SBG model. The definitions rely on the notion of paths and probabilities of paths:\nDefinition 4. A path ρ in SBG Γ is a sequence 〈s0ρ, θ0ρ, a0ρ, s1ρ, θ1ρ, a1ρ, ..., s tρ ρ 〉 where sτρ ∈ S, θτρ ∈ Θ, aτρ ∈ A, and s0ρ = s0. A path ρ is terminating if s tρ ρ ∈ S̄, otherwise it is non-terminating. Given a type distribution ∆ for Γ, the probability of path ρ is defined as Pr(ρ|Γ,∆) = tρ−1∏ τ=0 ∆(τ, θτρ)T (s τ ρ , a τ ρ , s τ+1 ρ ) ∏ k∈N πk(H τ ρ , (a τ ρ)k, (θ τ ρ)k)\nwhere Hτρ is the history extracted from ρ until time τ .\nFor Pr(ρ|Γ,∆) to be well-defined (i.e. there is a set X with ∀ρ ∈X : Pr(ρ|Γ,∆)≥ 0 and ∑ ρ∈X Pr(ρ|Γ,∆) = 1), it is important to note the following two implications in the definition of SBGs. Firstly, no path ρ can be prefixed by a terminating path, i.e., there is no sτρ ∈ ρ such that τ < tρ and s τ ρ ∈ S̄. This is important since otherwise Pr(ρ|Γ,∆) might assign positive probability to a path which is prefixed by a terminating path and, thus, could never occur. Secondly, the only paths that can occur are either terminating (and hence finite) or non-terminating and infinite (i.e. t → ∞). Thus, if Φ is the set of all terminating paths and Ψ the set of all infinite nonterminating paths, then ∑ ρ∈Φ∪Ψ Pr(ρ|Γ,∆) = 1.\nBased on the notion of paths, we define the flexibility and efficiency of ad hoc agent α as follows:\nDefinition 5. Let Φ be the set of all terminating paths in SBG Γ. Given a set of type distributions D for Γ, the flexibility F (α|Γ,D) and efficiency E(α|Γ,D) of α in Γ with respect to D are defined as\nF (α|Γ,D) = 1 |D| ∑ ∆∈D ∑ ρ∈Φ Pr(ρ|Γ,∆)\nE(α|Γ,D) = 1 |D| ∑ ∆∈D ∑ ρ∈Φ Pr(ρ|Γ,∆)\n(∑tρ−1 τ=0 ui(s τ ρ , a τ ρ , α) )r1 (tρ)r2\nwhere Pr(ρ|Γ,∆) = Pr(ρ|Γ,∆)∑ ρ′∈Φ Pr(ρ\n′|Γ,∆) , and r1, r2 ≥ 1 specify the relative importance between payoff and time.\nF (α|Γ,D) and E(α|Γ,D) can be interpreted as, respectively, the average probability that α solves a task in Γ and the average payoff per time step α received in solved tasks, where D specifies all constellations of types that can occur. There may be problems in which flexibility is not a relevant metric because termination is guaranteed for some reason. In such cases, the primary metric is efficiency."
    }, {
      "heading" : "2.3 The Ad Hoc Coordination Problem",
      "text" : "We are now in a position to formally define the ad hoc coordination problem. The core aspect is that there is no prior coordination between the ad hoc agent and the\nAlgorithm 1 Evaluation procedure\nInput: SBG Γ, set of type distributions D, ad hoc agent α, player i (to be controlled by α) Output: flexibility F (α|Γ,D), efficiency E(α|Γ,D) F ← 0 E ← 0 Repeat K times:\nRandomly draw type distribution ∆ ∈ D Generate path ρ in Γ with ∆ (α controls i) If ρ terminates do\nF ← F + 1 E ← E + (∑tρ−1 τ=0 ui(s τ ρ , a τ ρ , α) )r1 ∗ (tρ)−r2\nF (α|Γ,D)← F/K E(α|Γ,D)← E/K\nother agents in the system. We express this formally by requiring that the ad hoc agent does not know the type spaces Θj of the other players and, therefore, the type distribution ∆ of the game.\nDefinition 6. Let Γ be a SBG with type spaces Θj , and let D be a set of type distributions for Γ. The ad hoc coordination problem is to optimise the flexibility F (α|Γ,D) and efficiency E(α|Γ,D) of ad hoc agent α, subject to the constraint that α does not know Θj (and, therefore, the type distributions ∆).\nComputing F (α|Γ,D) and E(α|Γ,D) exactly is infeasible for all but the simplest games. We propose to approximate these by using the procedure given in Algorithm 1. The procedure generates K samples Fk ∼ F (α|Γ,D) and Ek∼E(α|Γ,D), based on which it approximates F (α|Γ,D) = 1K ∑ k Fk and E(α|Γ,D) = 1 K ∑ k Ek. Since all Fk and Ek, respectively, come from the same distribution, by the law of large numbers this will converge to the true values of F (α|Γ,D) and E(α|Γ,D) for K → ∞. The procedure needs some means to determine if a path is non-terminating. This could be done, for instance, by checking if the path reached a state space which contains no terminal states and cannot be left anymore, or by setting a maximum path length."
    }, {
      "heading" : "3 Harsanyi-Bellman Ad Hoc Coordination",
      "text" : "The problem of incomplete information is solved in Bayesian games by assuming that the type spaces Θj and type distribution ∆ are common knowledge. This admits a solution in the form of the Bayesian Nash equilibrium [Harsanyi, 1968], here defined for SBGs:\nDefinition 7. Let Ht be the history at time t and define Θ−i = ×j 6=i Θj . A Bayesian Nash equilibrium (BNE) in state st is a strategy profile (π1, ..., πn) in\nwhich, for all i ∈ N and θi ∈ Θi, πi maximises∑ θ̂−i∈Θ−i ∆(t, θ̂−i|θi) ∑ a∈A ui(s t, a, θi)π(H t, a, (θi, θ̂−i)) (1) where\n∆(t, θ−i|θi) = ∆(t, (θi, θ−i))∑\nθ̂−i∈Θ−i ∆(t, (θi, θ̂−i)) π(Ht, a, θ) = ∏ k∈N πk(H t, ak, θk).\nIn ad hoc coordination problems, the ad hoc agent does not know the type spaces Θj and, hence, the type distribution ∆ of the game. Therefore, it cannot compute ∆(t, θ−i|θi). However, using the history Ht, it can compute a posterior Pr(θ−i|Ht) = ∏ j 6=i Pr(θj |Ht) with Pr(θj |Ht) being the probability that player j has type θj based on history H t\nPr(θj |Ht) = L(Ht|θj)P (θj)∑\nθ̂j∈Θj L(H t|θ̂j)P (θ̂j)\n(2)\nwhere L(Ht|θj) = ∏t−1 τ=0 πj(H\nτ , aτj , θj) is the probability of history Ht if the type of player j is θj , and P (θj) is the agent’s prior belief that player j has type θj .\n[Kalai and Lehrer, 1993] studied single-state SBGs (with static pure type distributions) with players who choose actions to maximise their expected long-term payoff. They have shown that, if player i maintains a posterior according to (2), and if the type distribution ∆ is absolutely continuous with respect to the posterior (i.e., ∆(t, (θi, θ−i)) > 0 ⇒ Pr(θ−i|Ht) > 0), then player i’s predictions of future play will eventually be correct, regardless of player i’s own strategy (Theorem 1 in [Kalai and Lehrer, 1993]). It follows that, if all players maintain such posteriors (where ∆ is absolutely continuous with each posterior), and if all players choose their strategies according to a modified version of (1) which replaces the immediate payoff with the expected long-term payoff, then play will converge to a Nash equilibrium (NE) of the game (Theorem 2 in [Kalai and Lehrer, 1993]). A similar result was shown by [Jordan, 1991] for myopic players (i.e. maximising immediate payoffs).\nWhile these are encouraging theoretical results, there are several potential objections concerning the use of NE: Firstly, if there are multiple NE, then the players may converge to a sub-optimal equilibrium. Secondly, a NE is incomplete in that it does not specify strategies for off-equilibrium paths. Finally, [Dekel et al., 2004] have shown that if the posteriors of the players are not identical, then they might converge to a solution which is not a NE. However, our main concern with NE is that it makes strong behavioural assumptions about the\nplayers’ behaviours (such as perfect rationality) which may be difficult to justify in ad hoc coordination. For instance, there is no guarantee that all players maintain posteriors according to (2). The same arguments hold for solution concepts in extensive form games, such as the perfect Bayesian equilibrium and sequential equilibrium [Fudenberg and Tirole, 1991].\nRather than attempting to converge to NE, it is appealing to use (1) as a best-response rule, since it maximises the expected payoff with respect to what types the ad hoc agent believes the other players to have and their strategies for all types. Based on Theorem 1 in [Kalai and Lehrer, 1993], we know that the agent’s beliefs, and hence its expected payoffs, will be correct after some time. However, in its current form, (1) only considers immediate payoffs whereas optimal behaviour may require an agent to take payoffs of future states into account. Therefore, we propose to combine (1) with the Bellman optimality equation [Bellman, 1957] to obtain a best-response rule which we call Harsanyi-Bellman Ad Hoc Coordination. Since ad hoc coordination requires that the agent does not know the type spaces Θj , we assume instead that the ad hoc agent is provided with user-defined type spaces Θ∗j , and we sometimes refer to Θj as the true type spaces.\nDefinition 8. Let Γ be an ad hoc coordination problem where ad hoc agent α controls player i and has access to user-defined type spaces Θ∗−i = ×j 6=i Θ∗j . Harsanyi-Bellman Ad Hoc Coordination (HBA) is defined as ati ∼ arg maxai E ai st (H\nt), where Eais (Ĥ) =∑ θ∗−i ∈Θ∗−i Pr(θ∗−i|Ht) ∑ a−i ∈A−i Qai,−is (Ĥ) ∏ j 6=i πj(Ĥ, aj , θ ∗ j )\nis the expected long-term payoff for player i of taking action ai in state s after history Ĥ (ai,−i , (ai, a−i)), and Qas(Ĥ) =∑ s′∈S T (s, a, s′) [ ui(s, a, α) + γmax ai Eais′ ( 〈Ĥ, a, s′〉 )] (3) is the expected long-term payoff for player i when joint action a is executed in state s after history Ĥ, with 0 ≤ γ ≤ 1 being the discount factor.\nHBA is a modification of (1) which replaces ∆(t, θ−i|θi) by the posterior Pr(θ−i|Ht) (2), and in which the immediate payoff ui is replaced by an altered version (3) of the Bellman optimality equation. The actual history Ht is used to compute the posterior, and the projected histories Ĥ are used to generate all future trajectories.\nEach user-defined type θ∗j ∈ Θ∗j is a hypothesis about the behaviour of player j. While this gives HBA great flexibility (as Θ∗j may include a variety of behaviours), it is important to note that the accuracy of (3), and\nhence efficiency of HBA, depends on how closely the user-defined types capture the players’ true types. In this respect, we state two useful properties of HBA:\nProposition 1. Let Γ be a SBG with static pure type distribution ∆. If all players i ∈ N are controlled by an HBA agent αi with user-defined type spaces Θ ∗,i j , and if ∀j 6= i : Θj ⊆ Θ∗,ij , then play will converge to NE.\nThis follows from Theorems 1 and 2 in [Kalai and Lehrer, 1993] together with the fact that Θj ⊆ Θ∗,ij for all i and j (with i 6= j), which means that the type distribution ∆ is always absolutely continuous with respect to the players’ posteriors. Note that, while this proposition does not directly relate to ad hoc coordination, its does guarantee the minimum requirements of convergence and optimality in self-play, as formulated in [Bowling and Veloso, 2002].\nFor the next proposition, we define the class of deterministic learners, denoted ΘD, which consists of all types θj where, for all times t and histories H\nt, there exists a unique sequence (χaj )aj∈Aj such that πj(〈Ht, (a, s)〉, aj , θj) + χaj = πj(Ht, aj , θj), for all (a, s) ∈ A×S. In other words, a deterministic learner always learns the same from a given history. By definition, this includes all fixed (i.e. non-changing) behaviours.\nProposition 2. Let Γ be a SBG with static pure type distribution ∆, where α controls i. If ∀j 6= i : Θj ⊆ ΘD ∧Θj ⊆ Θ∗j , then α will be optimally efficient.\nThis follows from the fact that there is some point after which HBA knows the players’ types (Theorem 1 in [Kalai and Lehrer, 1993]) and, since all types are deterministic learners, the expected payoffs (3) are correct. Since HBA chooses actions with maximum expected payoffs, according to the Bellman principle [Bellman, 1957], it follows that it achieves optimal efficiency. Note that HBA is itself a deterministic learner, hence HBA achieves optimal efficiency in self-play.\nBoth propositions assume that (3) can be implemented directly, which is often infeasible. In Sections 4 and 5, we show how HBA can be implemented as a reinforcement learning procedure and an exact planning procedure."
    }, {
      "heading" : "3.1 Temporally Reweighted Posteriors",
      "text" : "A potential problem with the posterior defined in (2) is that it assigns zero probability to a type θj if πj(H\nt, atj , θj) is zero for any t. This can be problematic for the following reasons: If the game uses a dynamic or mixed type distribution, and if Pr(θj |Ht) = 0 for a type θj that is not currently the true type of player j, then Pr(θj |Hτ ) = 0 for all times τ > t, even if player j’s type changes to θj . Furthermore, if we have a user-defined type θ∗j which approximates the true type θj of player j in a subset S∗ ⊂ S (i.e. πj(Ht, aj , θ∗j ) ≈ πj(Ht, aj , θj)\nfor st ∈ S∗), but not outside S∗, then (2) might assign zero probability to θ∗j once player j leaves S\n∗. However, θ∗j may be the best approximation we have for S\n∗, so it would be useful if (2) was able to quickly reassign positive probability to θ∗j once player j returns to S\n∗. To address these problems, we introduce temporally reweighted posteriors:\nDefinition 9. A temporally reweighted posterior (TRposterior) is defined as in (2) by redefining\nL(Ht|θj) = t−1∑ τ=0 f(t− τ)πj(Hτ , aτj , θj) (4)\nwhere f(ξ) ≥ 0 and f(ξ) ≥ f(ξ + 1), for all ξ ∈ N+.\nThe function f is called the time weight and can assume various forms. An example of a simple but useful time weight, called the general time weight, is given by f(ξ) = max[0, a−b(ξ−1)c] where a, b, c ∈ R+0 . This time weight can be used to produce various behaviours, depending on the parameters a, b, c. In particular, it can be used to give greater importance to more recent events, which means that HBA is able to quickly reassign probabilities. However, the crucial aspect of (4) is that it defines a sum rather than a product, which means that the problems described above do not occur."
    }, {
      "heading" : "3.2 Conceptual Types",
      "text" : "If the user-defined type space Θ∗j for player j does not include the true type space Θj (i.e. Θj 6⊂ Θ∗j ), then j might assume a type which is unknown to HBA, causing its expected payoffs to be inaccurate. In such cases, it would be useful if HBA was able to learn new types from experience. This opens up the possibility of using methods for opponent modelling (e.g. case-based reasoning [Wendler and Bach, 2004] or recursive modelling [Gmytrasiewicz and Durfee, 2000]) which can be included in Θ∗j . In this work, we use a combination of case-based reasoning and fictitious play [Brown, 1951], called conceptual types. Conceptual types are based on the observation that behaviour may not be specified on a state-by-state basis but rather on abstractions of state spaces. (An example are the “information sets” in extensive form games.) That is, there may be some world conceptualisation inherent in a behaviour. While the types in Θ∗j are used to hypothesise behaviours directly, a conceptual type can be used to hypothesise a world conceptualisation underlying a player’s behaviour. Combined with the player’s observed actions, this can be used to generalise actions to unseen states and increase accuracy in rarely visited states.\nDefinition 10. A conceptual type (c-type) θcj for player j is a tuple (dj , r, f), where dj : S × S → R+0 is a symmetric distance function for pairs of states, r ∈\nR+ is a radius, and f is a time weight (as defined in Section 3.1), with\nπj(H t, aj , θ c j) = { |Aj |−1 if @τ <t : g(st, sτ )>0 else η ∑ aτ∈Ht: aτj=aj f(t− τ) g(st, sτ )\nwhere g(s1, s2) = max [ 0 , 1− dj(s1, s2) r−1 ] and η is a\nnormalisation constant s.t. ∑ aj πj(H t, aj , θ c j) = 1.\nThe function g is the hypothesised world conceptualisation of player j, where dj and r specify how similar two states are from the perspective of player j (examples given in Section 4). The time weight f can be used to give greater importance to recent events, which allow c-types to adapt quickly to changing behaviours. Note that we can include multiple c-types in Θ∗j , each corresponding to a different world conceptualisation, and the posterior filters out those types which do not fit."
    }, {
      "heading" : "4 Simulated Experiments",
      "text" : ""
    }, {
      "heading" : "4.1 Experimental Setup",
      "text" : "We evaluated different configurations of HBA in a multiagent logistics domain called level-based foraging (see Figure 1). A level-based foraging problem consists of a rectangular grid with n players and m foods. Each field in the grid is either empty or occupied by one player or one food. All players and foods have a level (∈ N+) where no food has a level greater than the sum of any 4 players’ levels. A player can choose among 5 actions: N , E, S, W , and load. The first 4 actions move the player into the corresponding direction if the field is empty and inside the grid. A group of 1 to 4 players can load a food if they are placed on fields next to the food and if the sum of their levels is at least as high as the food’s level. A player which successfully loads a food obtains a payoff equal to the level of the loaded food. At all other times, it receives a negative payoff of -0.01. To avoid conflicts and keep this solvable, the foods are placed such that the Euclidean distance between each of them is greater than 1, and no food is placed at any border of the grid. The players’ goal is to\ncollect all foods in minimal time, while also trying to maximise their own payoffs. Since the players have different abilities (i.e. levels) and are spatially distributed, this requires strong coordination of their behaviours.\nWe specify 6 classes of types. The first 4 classes contain types with fixed behaviours (i.e. they do not change over time). They each have a parameter σ which specifies the radius of their sight: H1 always goes to the closest visible food. H2 goes to the one visible food which is closest to the centre of all visible players. H3 always goes to the closest visible food with compatible level (i.e. it can load it) and H4 goes to the one visible food which is closest to all visible players such that the sum of their and H4’s level is sufficient to load the food. H1-4 try to load the food once they are next to it. If they do not see a food, they go into a random direction. The last two classes specify types with learning behaviours: Class 5 contains all instances of JAL and class 6 all instances of CJAL, as specified in the next paragraph.\nWe evaluated various configurations of HBA and three alternative algorithms: JAL [Claus and Boutilier, 1998] learns the action frequencies of each player in each state (i.e. opponent modelling) and uses them to compute expected action payoffs; CJAL [Banerjee and Sen, 2007] is similar to JAL but learns the frequencies conditioned on its own actions; WoLF-PHC [Bowling and Veloso, 2002] is a hill-climbing method in the space of mixed strategies. All three algorithms behave differently in ad hoc coordination [Albrecht and Ramamoorthy, 2012].\nA single framework (Algorithm 2) was used to implement each ad hoc agent. We assume that the ad hoc agent is able to observe the states of the game, each player’s actions, and its own payoffs. For simplicity, we also assume that the agent knows the levels of all players and foods. The framework uses a table Q to learn the expected long-term payoffs of joint actions, similar to Q-learning [Watkins and Dayan, 1992]. To accelerate learning, it uses an eligibility trace e (see [Sutton and Barto, 1998]) to connect current payoffs with past actions. We assume that the agent has access to a simulator Simulate(s, a) which, based on the transition (T ) and payoff (ui) functions of the game, returns a successor state s′ and payoff u after taking joint action a in state s. This simulator is used in a sampling-based planning procedure [Kearns et al., 1999] Expand(d, s, ê) which, starting in state s, generates a future trajectory of length d and updates Q using the eligibility trace ê. The function ExpPay(Q, s, ai) computes the expected payoff for taking action ai in state s based on Q, and the function OppActions(s) samples actions for all other players j 6= i in state s. HBA implements Expand using (1) and its posterior, and OppActions using its posterior and user-defined types. C/JAL implement these functions using their learned action frequencies.\nAlgorithm 2 Reinforcement learning framework\nSet Q(s, a)← 0 and e(s, a)← 0 for all (s, a) ∈ S ×A Repeat until st ∈ S̄:\nObserve: current state st With probability 1− 1: ati = ChooseAction(st), else sample ati ∼ Ai Observe: joint action at, own payoff uti, next state s t+1\nUpdateQ(st, at, uti, s t+1, e) Repeat x times: Expand(d, st+1,Copy(e))\nExpand(d, s, ê):\nRepeat d times or until s ∈ S̄: With probability 1− 2: ai = ChooseAction(s),\nelse sample ai ∼ Ai a−i ← OppActions(s) (ui, s\n′)← Simulate(s, (ai, a−i)) UpdateQ(s, (ai, a−i), ui, s ′, ê) s← s′\nUpdateQ(s, a, u, s′, ê):\nδ = β(u+ γmaxâiExpPay(Q, s ′, âi)−Q(s, a)) ê(s, a)← 1 For all (ŝ, â) ∈ S ×A s.t. ê(ŝ, â) ≥ emin do: Q(ŝ, â)← Q(ŝ, â) + δ ê(ŝ, â) ê(ŝ, â)← λ ê(ŝ, â)\nChooseAction(s):\nReturn ai∼ arg maxâiExpPay(Q, s, âi)\nFor WoLF-PHC, the framework defines Q and e on S × Ai (rather than S × A) and ExpPay(Q, s, ai) is simply defined as Q(s, ai). Since WoLF-PHC does not model its opponents, we implement OppActions the same way as in JAL. The function ChooseAction(s) is redefined to ai ∼ π(s), where π is the mixed strategy maintained in WoLF-PHC (cf. Tables 5 and 6 in [Bowling and Veloso, 2002]).\nAll algorithms used identical parameters: β = .2, γ = .9, λ = .9, emin = .01, 1 = 0, 2 = .2, x = 3, d = 20. For WoLF-PHC, we used learning rates δw(t) = (1000 + t\n10 ) −1 and δl(t) = 2 δw(t). For HBA, we used uniform prior beliefs (P (θ∗j ) = |Θ∗j |−1) and a = 10, b = .01, c = 3 for the general time weight. To obtain estimates of flexibility and efficiency, we used Algorithm 1 with i = 1, r1 = r2 = 1,K = 1000, where we assumed a path to be non-terminating if it reached t = 1000. The initial states were generated with random positions and levels for all players and foods, with the maximum level being equal to the number of players. All agents were tested on the same sequence of games and random numbers."
    }, {
      "heading" : "4.2 Results",
      "text" : "We tested the effectiveness of TR-posteriors by simulating the two situations described in Section 3.1. All\ntests were run on a 8×8 grid with 2 players and 5 foods. In Figure 2a, we used Θ2 = Θ ∗ 2 = {H1–H4 |σ =∞} and a dynamic pure type distribution which changed the type of player 2 after every 10 to 20 time steps. In Figure 2b, we used Θ2 = {H1–H4 |σ = 3, 5, 7}, Θ∗2 = {H1–H4 |σ =∞} (i.e the types in Θ∗2 were accurate only for subsets S∗ ⊂ S) and a static pure type distribution. In both cases, the efficiency of HBA was significantly higher when using a TR-posterior with general time weight (Gtw) compared to both the normal posterior defined in (2) (Unl) and a normal posterior which was limited to the 9 most recent events (Lim), which is the same time frame used in Gtw. In Figure 2a, Gtw even achieved the same efficiency as a version of HBA which always knew the correct type of the other player (Cor). All HBA agents achieved a perfect flexibility of 1.\nWe tested HBA with 4 conceptual types θcj = (d c j , r, f) where f(ξ) = [ξ < 10]1 and r = 1. In the following, we write s.pj (s.fk) to refer to the position of player j (food fk) in state s, and fk ∈ s to say that food fk is available in state s. The distance functions dcj are\nd1j (s1,s2)=[s1 6= s2]1∞\nd2j (s1,s2)=[s1.pj = s2.pj ∧ ∀k : fk ∈ s1 ⇔ fk ∈ s2]1∞ d3j (s1,s2)=φ(s1.pj ,s2.pj)+ ∑ k:fk∈s1∨fk∈s2ψ(s1.fk, µ) −32\nd4j (s1,s2)=d 3 j (s1, s2)+ ∑ v φ(s1.pv, s2.pv)ω −1.5 v\nwhere φ(x1, x2)=log(1+ψ(x1,x2)) 1 2 , µ=s1.pj+ 1 2 (s2.pj− s1.pj), ωv=min[ψ(s1.pv, µ), ψ(s2.pv, µ)], and ψ(x1, x2) denotes the Euclidean distance between x1 and x2. All tests were run on a 8×8 grid with 2 players and 5 foods, using Θ2 = {H1–H4,JAL,CJAL |σ =∞} (C/JAL used same parameters as HBA), Θ∗2 = {θc2} (each c = 1, ..., 4 tested separately), and a static pure type distribution. The results in Figure 2c show that HBA achieved good efficiency (compared to Cor) using θ2j , while the other c-types were less efficient. All HBA agents achieved statistically equivalent flexibilities of 0.86± 0.01.\nFinally, we tested HBA, JAL, CJAL, and WoLF-PHC\non a 10 × 10 grid with 3 players and 8 foods, using Θ2,3 = {H1–H4,JAL,CJAL |σ = 5, 7, 9} and Θ∗2,3 = {H1–H4 |σ =∞}. To add more realism, players 2 and 3 were “defective” with probability 0.2, where a defective player changed its type randomly every 10 to 30 time steps. While the potential of HBA is demonstrated by Cor, it would also be useful to know the optimal solution to the problem. However, with a complex problem such as this one, we were unable to compute optimal solutions. Instead, we had 6 humans play the game in a graphical user interface (each one played the full 1000 runs, distributed over 7 days at their own convenience), where no human was familiar with the technical details of this work. We do not necessarily claim that humans produce optimal solutions, but we expect them to perform consistently well in this setting. To cope with the increased problem size, we set the planning power of the algorithms to x = 10 and d = 30 (cf. Algorithm 2).\nThe results (Figure 2d) show that HBA clearly outperformed all alternative algorithms, with Unl and Gtw being over 100% and 200% more efficient, respectively. This is despite the fact that the user-defined types Θ∗2,3 did not include any true types of the players. We also tested HBA with the c-types θ1j and θ 3 j (added separately to Θ∗2,3) but found that the efficiency of HBA did not improve significantly. This is since C/JAL learned similar behaviours to H1 and H3, which were already covered in Θ∗2,3. We found that HBA’s posteriors often assigned high probabilities to H1/3 when the true type of the player was in fact C/JAL. Since H1/3 ignore other players, this means that C/JAL did not effectively coordinate their behaviours with other players. We found similar results for WoLF-PHC. As was expected, the humans achieved high efficiency (Figure 2d shows the best human) and outperformed even Cor. One reason for this is the fact that the humans had much greater planning power than HBA. Lastly, HBA achieved higher flexibilities (.83± .01) than JAL (.734), CJAL (.749), and WoLF-PHC (.744), while the humans all achieved perfect flexibility (1.0)."
    }, {
      "heading" : "5 Human-Machine Experiment",
      "text" : ""
    }, {
      "heading" : "5.1 Experimental Setup",
      "text" : "We conducted a large-scale human-machine experiment at the Royal Society Summer Science Exhibition 2012. Therein, the human participants played repeated Prisoner’s Dilemma (PD) and Rock-Paper-Scissors (RPS) against HBA and alternative algorithms, where each game was played for 20 rounds. We collected data from 427 participants, of which 186 played PD and 241 played RPS. The lowest and highest recorded ages were 9 and 72, respectively, with an average age of about 17.\nA large public exhibition such as this one is an excellent testbed environment for ad hoc agents, since the visitors vary widely in factors such as age, intelligence, and behaviour. However, in order to make statistically relevant comparisons, we required data from many participants. Therefore, the games needed to be simple enough so participants would understand them quickly, yet they also needed to be interesting in terms of coordination strategies. PD and RPS are two widely studied problems in game theory which we believe cover these properties. In PD, the symmetric payoffs are u1(C,C)=3, u1(D,D)=1, u1(C,D)=0, u1(D,C)=5. The problem here is that the only NE, and hence stable outcome, is at (D,D), while (C,C) is the only outcome that has both the highest welfare (sum of payoffs) and fairness (product of payoffs) but is unstable since the players could deviate to obtain higher immediate payoffs. In RPS, the payoffs are +1/0/-1 for won/even/lost games. The only NE is for all players to play randomly. However, even if humans attempt to play randomly, they often fall back to patterns [Wagenaar, 1972] against which the other player can coordinate its actions.\nOur hypothesis for the experiment was that the human would switch between several simple behaviours, as opposed to having one complex behaviour. Therefore, we modelled the problem as a SBG with a dynamic mixed type distribution (unknown to us) which governed the type of the human, and we provided HBA with a small set of types (given in Tables 1 and 2) which we believed the human could have. HBA did not use any conceptual types.\nThe alternative algorithms were CJAL for PD, which was shown to outperform both JAL and WoLF-PHC in PD [Banerjee and Sen, 2007], and JAL for RPS, which is guaranteed to converge to NE in self-play in zero-sum games [Brown, 1951]. We implemented all algorithms using a single framework (Algorithm 3), where we set l∗ = 10 for PD, l∗ = 1 for RPS, and t∗ = 20. The function OppStrat(sτ , aτ ) returns the probability that players j 6= i choose actions aτj in state sτ . HBA implements this by averaging over all user-defined types in Θ∗j using its current posterior, and C/JAL do this\nAlgorithm 3 Exact planning framework\nRepeat:\nObserve current state st For all ai ∈ Ai do:\nΩ(ai) = { 〈st, at, ..., st+l, at+l〉 | ati = ai } where l = min[l∗, t∗ − t]− 1 E(ai) = ∑\nω∈Ω(ai)\n[ t+l∏ τ=t OppStrat(sτ , aτ ) t+l∑ τ=t ui(s τ , aτ ) ]\nSample action ati ∼ arg maxai E(ai)\nusing their learned actions frequencies. While PD and RPS have no states, we found that the performance of C/JAL could be further improved by introducing “artificial” states, which we simply defined as st = at−1 (in the first round, C/JAL assumed the opponent to play randomly). HBA used uniform prior beliefs and the general time weight with a = 10, b = 0.05, c = 3.\nThe procedure of the experiment was as follows: First, we randomly sampled a participant from the set of visitors which were currently at our exhibit. The participant was then brought to a dedicated table with a chair and a laptop on it. The laptop ran a programme, with an intuitive graphical user interface, which prompted the participant to choose between PD and RPS. The rules of the games were explained both textually in the programme and in person by one of our staff members to make sure the participant understood the rules. The game was then played in two matches, each lasting 20 rounds. One of the matches was against HBA and the other match against C/JAL, but this was hidden from the participant and the order was chosen randomly. The programme displayed the current match, round, and scores of all players, and also allowed to display the rules at any time. At the end of each round, the participant was shown the actions and scores of both players, and at the end of each match, the participant was given a summary of the scores."
    }, {
      "heading" : "5.2 Results",
      "text" : "In the following, all significance statements are based on paired t-tests with 5% significance level. Figures 3a and 3b show the results for PD and RPS, respectively. In both games, the average total payoffs of HBA and C/JAL were statistically equivalent. Since the time was fixed to 20 rounds, it means that they achieved equal efficiency. This is, in fact, a positive result considering that C/JAL are strong candidates in PD/RPS. In addition, as we discuss in the following, HBA behaved very differently from C/JAL, with beneficial side effects.\nIn PD, the most desirable long-term outcome is (C,C)\nsince it is both welfare and fairness optimal, and since it is a non-myopic equilibrium [Brams, 1993], meaning that no player has a long-term incentive to deviate. With this in mind, we point out that in over 28% of the games, HBA and the human played (C,C) in at least 50% of the final 10 rounds of the game, while CJAL did not achieve this in any game. Thus, HBA achieved a significantly higher total welfare than CJAL (Figure 3a). This is despite the fact that neither of them was optimised for social welfare. The reason for this is that HBA was planning more accurately than CJAL. When computing the expected payoffs E(ai), CJAL uses its learned action frequencies to obtain probabilities for each trajectory in Ω(ai). However, these probabilities can only be accurate for states that have been visited frequently enough. Moreover, if a player changes its behaviour, CJAL requires new evidence from all states to accurately reflect the change. On the other hand, HBA uses its posterior and types to compute probabilities of trajectories. Therefore, once HBA has an accurate posterior, it can use the types to accurately plan in the entire state space of the game, including unseen states. This also allows HBA to plan the effects of its actions on the other player, which means that HBA may take actions to manipulate the player’s decisions. Finally, if a player changes its behaviour, HBA only needs to update its posterior, which requires much less information than the update in CJAL.\nIn RPS, the crucial questions is whether a player is winning or not. Interestingly, the winning rate of HBA (53.71%) was significantly higher than the winning rate of JAL (43.98%), as shown in Figure 3b. While in PD the good performance of HBA was due to its planning capabilities, in RPS this was not as relevant since the planning horizon was limited to trajectories of length 1. Rather, HBA’s good performance was due to the fact that it recognised changed behaviours faster than JAL. Indeed, in a game such as RPS, it can be expected that the human players change frequently between different strategies. This is confirmed by the statistics shown in Figure 3c, which show the average number of types used by the human players and the average duration. The statistics are based on HBA’s posteriors, where the\nnumber of types for player i in a play corresponds to the number q in 〈t0, t1, ..., tq〉, with t0 = 0 and tq = 20, for which arg maxθi Pr(θi|Hτ ) ⊆ arg maxθi Pr(θi|Hτ+1) for all ty−1 ≤ τ < ty and y ∈ {1, ..., q}, and where the average duration is 1q ∑ y ty − ty−1. According to these statistics, the human players had 4.45 types with a duration of 4.96 rounds in PD, and 8.25 types with a duration of 2.46 rounds in RPS. Clearly, with a duration of only 2.46 rounds, planning was not as important as recognising changed types. By using TR-posteriors, HBA was able to do this effectively."
    }, {
      "heading" : "6 Summary & Open Questions",
      "text" : "This work is concerned with the ad hoc coordination problem, in which the goal is to design an autonomous agent (the ad hoc agent) which can achieve optimal flexibility and efficiency in a multiagent system in which the behaviour of the other agents is not a priori known. We make three important contributions to the ad hoc coordination problem:\n1. We propose a game-theoretic model, SBG, which captures the notion of private information in the form of types. Based in this model, we give formally concise definitions of flexibility, efficiency, and the ad hoc coordination problem. We also provide a procedure which can be used to estimate the ad hoc agent’s flexibility and efficiency.\n2. From this model, we derive a principled solution, HBA, which utilises a set of user-defined types in a planning procedure to find optimal actions in the sense of Bayesian Nash equilibrium and Bellman optimal control. We also propose two possible extensions which enable HBA to recognise changed types and learn new types.\n3. We show how HBA can be implemented as a reinforcement learning and exact planning procedure, and we provide extensive empirical evaluations in a complex multiagent logistics domain and a large-scale human-machine experiment. Our results show that HBA is both more flexible and efficient than alternative methods.\nThe work presented in this paper provides a rich ground for future research, including the following open questions:\n• A crucial design parameter of HBA are the userdefined type spaces Θ∗j provided to it. In this regard, an important direction for future research would be to analyse how closely Θ∗j must approximate Θj for HBA to be able to achieve optimal flexibility and efficiency.\n• Another design parameter of HBA is the posterior Pr(·|Ht), and in this work we discussed two different formulations (the product posterior and TR-posteriors). It would be interesting to explore alternative posterior formulations and to analyse the conditions under which they are guaranteed to converge to the type distribution of the game.\n• The prior belief P can be considered a metaparameter of HBA (it is a parameter of the posterior, which in turn is a parameter of HBA), and in our experiments we assumed that the prior beliefs were uniform. An interesting question in this regard is whether HBA could automatically derive prior beliefs from the user-defined type spaces so as to further maximise its efficiency.\n• HBA currently assumes that an expert can provide manually specified types for the problem at hand. However, this can be a cumbersome task in complex domains. Future work could investigate how HBA might generate useful types from the problem description so that the burden of having to manually specify types can be alleviated, or perhaps eliminated altogether.\n• Finally, as we employ HBA in increasingly complex problem domains, it becomes apparent that the type specifications, likewise, become increasingly complex. One way to reduce this type complexity might be to use a hierarchical type specification, in which types are structured into smaller sub-types."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was partially supported by grants from the UK Engineering and Physical Sciences Research Council (EP/H012338/1), the European Commission (TOMSY Grant 270436, FP7-ICT-2009.2.1 Call 6) and a Royal Academy of Engineering Ingenious grant."
    } ],
    "references" : [ {
      "title" : "Leading ad hoc agents in joint action settings with multiple teammates",
      "author" : [ "Agmon", "Stone", "N. 2012] Agmon", "P. Stone" ],
      "venue" : null,
      "citeRegEx" : "Agmon et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agmon et al\\.",
      "year" : 2012
    }, {
      "title" : "Comparative evaluation of MAL algorithms in a diverse set of ad hoc team problems",
      "author" : [ "Albrecht", "Ramamoorthy", "S. 2012] Albrecht", "S. Ramamoorthy" ],
      "venue" : "In 11th International Conference on Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "Albrecht et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Albrecht et al\\.",
      "year" : 2012
    }, {
      "title" : "Reaching pareto-optimality in prisoner’s dilemma using conditional joint action learning",
      "author" : [ "Banerjee", "Sen", "D. 2007] Banerjee", "S. Sen" ],
      "venue" : "Autonomous Agents and Multiagent Systems,",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2007
    }, {
      "title" : "Empirical evaluation of ad hoc teamwork in the pursuit domain",
      "author" : [ "Barrett et al", "S. 2011] Barrett", "P. Stone", "S. Kraus" ],
      "venue" : "In 10th International Conference on Autonomous Agents and Multiagent Systems",
      "citeRegEx" : "al. et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2011
    }, {
      "title" : "Coordination and adaptation in impromptu teams",
      "author" : [ "Bowling", "McCracken", "M. 2005] Bowling", "P. McCracken" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "Bowling et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Bowling et al\\.",
      "year" : 2005
    }, {
      "title" : "Multiagent learning using a variable learning rate",
      "author" : [ "Bowling", "Veloso", "M. 2002] Bowling", "M. Veloso" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Bowling et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bowling et al\\.",
      "year" : 2002
    }, {
      "title" : "The dynamics of reinforcement learning in cooperative multiagent systems",
      "author" : [ "Claus", "Boutilier", "C. 1998] Claus", "C. Boutilier" ],
      "venue" : "In Proceedings of the National Conference on Artificial Intelligence,",
      "citeRegEx" : "Claus et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Claus et al\\.",
      "year" : 1998
    }, {
      "title" : "Learning to play",
      "author" : [ "Dekel et al", "E. 2004] Dekel", "D. Fudenberg", "D. Levine" ],
      "venue" : "Bayesian games. Games and Economic Behavior,",
      "citeRegEx" : "al. et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2004
    }, {
      "title" : "Dynamically formed human-robot teams performing coordinated tasks",
      "author" : [ "Dias et al", "M. 2006] Dias", "T. Harris", "B. Browning", "E. Jones", "B. Argall", "M. Veloso", "A. Stentz", "A. Rudnicky" ],
      "venue" : "In AAAI Spring Symposium “To Boldly Go Where No Human-Robot",
      "citeRegEx" : "al. et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2006
    }, {
      "title" : "Perfect Bayesian equilibrium and sequential equilibrium",
      "author" : [ "Fudenberg", "Tirole", "D. 1991] Fudenberg", "J. Tirole" ],
      "venue" : "Journal of Economic Theory,",
      "citeRegEx" : "Fudenberg et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Fudenberg et al\\.",
      "year" : 1991
    }, {
      "title" : "A framework for sequential planning in multiagent settings",
      "author" : [ "Gmytrasiewicz", "Doshi", "P. 2005] Gmytrasiewicz", "P. Doshi" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Gmytrasiewicz et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gmytrasiewicz et al\\.",
      "year" : 2005
    }, {
      "title" : "Rational coordination in multi-agent environments",
      "author" : [ "Gmytrasiewicz", "Durfee", "P. 2000] Gmytrasiewicz", "E. Durfee" ],
      "venue" : "Autonomous Agents and Multi-Agent Systems,",
      "citeRegEx" : "Gmytrasiewicz et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Gmytrasiewicz et al\\.",
      "year" : 2000
    }, {
      "title" : "Rational learning leads to Nash equilibrium",
      "author" : [ "Kalai", "Lehrer", "E. 1993] Kalai", "E. Lehrer" ],
      "venue" : null,
      "citeRegEx" : "Kalai et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Kalai et al\\.",
      "year" : 1993
    }, {
      "title" : "A sparse sampling algorithm for nearoptimal planning in large Markov decision processes",
      "author" : [ "Kearns et al", "M. 1999] Kearns", "Y. Mansour", "A. Ng" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "al. et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 1999
    }, {
      "title" : "Ad hoc autonomous agent teams: Collaboration without pre-coordination",
      "author" : [ "Stone et al", "P. 2010a] Stone", "G. Kaminka", "S. Kraus", "J. Rosenschein" ],
      "venue" : "In 24th AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "Leading a best-response teammate in an ad hoc team. In Agent-Mediated Electronic Commerce: Designing Trading Strategies and Mechanisms for Electronic Markets, pages",
      "author" : [ "Stone et al", "P. 2010b] Stone", "G. Kaminka", "J. Rosenschein" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2010
    }, {
      "title" : "To teach or not to teach? Decision making",
      "author" : [ "Stone", "Kraus", "P. 2010] Stone", "S. Kraus" ],
      "venue" : null,
      "citeRegEx" : "Stone et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Stone et al\\.",
      "year" : 2010
    }, {
      "title" : "Reinforcement learning: An introduction",
      "author" : [ "Sutton", "Barto", "R. 1998] Sutton", "A. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Recognizing and predicting agent behavior with case based reasoning",
      "author" : [ "Wendler", "Bach", "J. 2004] Wendler", "J. Bach" ],
      "venue" : "RoboCup 2003: Robot Soccer World Cup VII,",
      "citeRegEx" : "Wendler et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Wendler et al\\.",
      "year" : 2004
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner’s Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.",
    "creator" : "LaTeX with hyperref package"
  }
}