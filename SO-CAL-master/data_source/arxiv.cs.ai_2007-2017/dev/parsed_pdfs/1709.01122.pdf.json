{
  "name" : "1709.01122.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exact Inference for Relational Graphical Models with Interpreted Functions: Lifted Probabilistic Inference Modulo Theories",
    "authors" : [ "Rodrigo de Salvo Braz", "Ciaran O’Reilly" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 9.\n01 12\n2v 1\n[ cs\n.A I]\n4 S\nep 2\nProbabilistic Inference Modulo Theories (PIMT) is a recent framework that expands exact inference on graphical models to use richer languages that include arithmetic, equalities, and inequalities on both integers and real numbers. In this paper, we expand PIMT to a lifted version that also processes random functions and relations. This enhancement is achieved by adapting Inversion, a method from Lifted First-Order Probabilistic Inference literature, to also be modulo theories. This results in the first algorithm for exact probabilistic inference that efficiently and simultaneously exploits random relations and functions, arithmetic, equalities and inequalities."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Graphical models such as Bayesian networks and Markov networks (Pearl, 1988) are a well-principled way of representing probabilistic models. They represent the dependences between random variables in a factorized form that can be exploited by probabilistic inference algorithms (such as Variable Elimination (Zhang and Poole, 1994) and Belief Propagation (Pearl, 1988)) for greater efficiency.\nHowever, traditional graphical models representations are unable to represent other types of useful structures:\n• Relational structure occurs when families of random variables share the same dependences. Suppose a\nmodel involves random variables sunny, happy(Ann), happy(Bob), . . . and the analogous dependences P (happy(Ann)|sunny), P (happy(Bob)|sunny) and so on. Intuitively, this structure can be exploited for greater efficiency since the same inference can often be performed only once for an entire family of random variables. Traditional graphical model representations cannot explicitly indicate this structure and thus algorithms cannot directly exploit it.\n• Algebraic structure occurs when dependences between random variables (such as conditional proba-\nbilities) can be compactly described by an algebraic expression. For example, it may be that P (x ∈ {1, . . . , 1000}|y ∈ {1, . . . , 1000}) is equal to ifx = y then0.8 else0.2/999. Intuitively, this can be exploited for greater efficiency because large groups of values can be treated in the same way (in this case, all pairs of values (x, y) for which x 6= y are equally probable). Traditional graphical model representations, however, represent such functions as a lookup table (a large 1000 × 1000 one for this example)1, thus not explicitly representing the algebraic structure, which prevents algorithms from directly exploiting it.\nThese types of structure are commonly found in real-world applications and fields such as probabilistic programming, so research has been conducted on exploiting both:\n• Lifted probabilistic inference (Poole, 2003; de Salvo Braz, 2007; Milch et al., 2008;\nVan den Broeck et al., 2011; Kersting, 2012) exploits relational structure that is explicitly represented with richer languages generally known as Probabilistic Relational Models. These representations resemble universal quantification in first-order logic such as ∀x ∈ People P (happy(x)|sunny) for People a discrete set {Ann,Bob, . . . }. In logic terminology, this employs uninterpreted functions, since the random relations are Boolean functions without a fixed interpretation (for example, the value of happy(Ann) is not fixed in advance).\n• Probabilistic Inference Modulo Theories (PIMT) (de Salvo Braz et al., 2016) exploits algebraic struc-\nture by explicitly representing and manipulating function definitions of random variable dependences in the form of algebraic expressions. In logic terminology, this employs interpreted functions, because functions like\n1But note exceptions in Boutilier et al. (1996); Sanner and Abbasnejad (2012).\nequality (=) and arithmetic functions (+,× etc), among others, have fixed interpretations: 3 = 3 is always TRUE, and 1 + 3 is always 4.\nIn this paper, we present the first lifted probabilistic algorithm on languages with interpreted functions. This is done by unifying these two lines of research by incorporating uninterpreted functions and an important lifted probabilistic inference operation, Inversion, into PIMT. (Another major lifted operation, Counting (de Salvo Braz, 2007; Milch et al., 2008), is left for future work.) We call this fusion of lifted inference and PIMT Lifted Probabilistic Inference Modulo Theories (LPIMT).\nMoreover, we achieve this unification modularly, by using PIMT’s ability to be extended by solvers for specific theories, and by encapsulating lifted inference into it as an uninterpreted functions theory solver. This means the algorithm can apply lifted inference even to unanticipated theories, because solvers for different theories are orthogonally applied by the general, theory-agnostic level of PIMT.\nCasting lifted inference in the PIMT framework also makes it simpler and more powerful than previous lifted inference methods. Besides the above mentioned advantages, it also uses standard mathematical notation (e.g., not making a hard distinction between “logical” and “random” variables); does not separate constraints from potential function definitions; and accepts random function applications even as arguments to other functions (nested applications)."
    }, {
      "heading" : "2 BACKGROUND",
      "text" : ""
    }, {
      "heading" : "2.1 Graphical Models and Variable Elimination",
      "text" : "Graphical models are a standard framework for reasoning with uncertainty. The most common types are Bayesian networks and Markov networks. In both cases, a joint probability distribution for each assignment tuple x toN random variables is defined as a normalized product of nonnegative real functions {φi}i∈1..M , where 1..M is short for {1, . . . ,M}, each of them applied to a subtuple xi of x:2\nP (x) = 1\nZ\n∏\ni\nφi(xi),\nwhere Z is a normalization constant equal to ∑\nx\n∏\ni φi(xi). Functions φi are called factors and map each assignment on their arguments to a potential, a non-negative real number that represents how likely the assignment xi is. This representation is called factorized due to its breaking the joint probability into this product. In Bayesian networks,M = N and factors are conditional probabilities P (xi|Pai), for each random variable xi in x, where Pai are its parents in a directed acyclic graph.\n2For simplicity, we use the same symbols for both random variables and their values, but the meaning should be clear.\nThe marginal probability (MAR) problem consists of computing P (q) = ∑\nr P (x), where q is a subtuple of x containing queried variables, and r are all the remaining variables in x. It can be shown that P (q) = 1 Zq ∑ r ∏ i φi(xi) for Zq a normalization constant over q. Therefore, the problem can be easily reduced to computing a summation over products of factors, which the rest of the paper focuses on.\nThe belief update (BEL) problem consists of computing P (q|e), the posterior probability on variables q given assignment e to evidence variables (an observation). BEL can be easily reduced to two instances of MAR, since Bayes’ theorem establishes that P (q|e) = P (q, e)/P (e), where P (q, e) is the joint value of q and e.\nComputing ∑\nr\n∏\ni φi(xi) is crucial for inference with graphical models, but solving it naively has exponential cost in the size of r. Variable Elimination (VE) is an algorithm that takes advantage of the factorized representation to more efficiently compute this sum. For example, consider the following summation given a factorization on the variables (h)appy, (w)eekday, (t)emperature, (m)onth with range sizes of 2 (true or false), 7 (weekdays), 3 (hot,mild, cold), and 12 (months) respectively:\n∑\nh,w,t,m\nP (h|w, t)P (w)P (t|m)P (m).\nA naive computation iterates over 504 assignments to (h,w, t,m). However, the joint probability factorization enables us to manipulate the expression and compute it with iterations over fewer assignments:\n∑\nh,w,t,m\nP (h|w, t)P (w)P (t|m)P (m)\n= ∑\nh,w,t\nP (h|w, t)P (w) ∑\nm\nP (t|m)P (m).\nNow, we sum m out (or eliminate it), obtaining a new factor φ, defined on t alone, that replaces the last summation. This requires going over each value t and computing φ(t) = ∑\nm P (t|m)P (m) (thus iterating over the values of m). Therefore, computing φ takes iterating over 36 assignments to t,m and we are left with the following new summation of a product of functions:\n∑\nh,w,t\nP (h|w, t)P (w)φ(t)\n= ∑\nw\nP (w) ∑\nh,t\nP (h|w, t)φ(t)\n= ∑\nw\nP (w)φ′(w) (after 42 assignments to w, h, t)\n= φ′′ (after 7 assignments to w).\nVariable Elimination therefore decreases the number of required iterations to 85, a six-fold reduction, by exploiting\nthe fact that not all variables share factors with all variables (for example,m only shares a factor with t).\nVE applies not only to summations of products, but to any commutative associative semiring (Bistarelli et al., 1997): maximization of products, disjunctions of conjunctions, conjunctions of disjunctions, and so on. We use ⊕ and ⊗ for the additive and multiplicative operators of the semiring. We call quantifiers the intensional versions of operators: ∀ for∧, ∃ for∨, ∑ and ∫\nfor+ (for discrete and continuous indices respectively), ∏\nfor ×, Max for max, and so on. Quantifiers corresponding to ⊕ and ⊗ are denoted ⊕ and ⊗ , respectively. We use ⊙ for denoting any type of quantifier. VE works by selecting the next variable v in r to be eliminated and computing a new factor\nφ(u) = ⊕\nv\n⊗\ni∈f(v)\nφi(. . . , v, . . . ),\nwhere f(v) is the set of indices of factors of v and u are the variables sharing factors with v. It then includes φ in the product of factors and proceeds until all variables in r are eliminated."
    }, {
      "heading" : "2.2 Probabilistic Inference Modulo Theories",
      "text" : "Typically, graphical model inference algorithms assume that factors can only be accessed as opaque lookup tables, that is, by providing an assignment to its random variables. This requires Variable Elimination to iterate over all assignments to all random variables in a summation, as seen in the example for computing φ(t) with Variable Elimination.\nHowever, often the definitions of factors are available in the form of symbolic algebraic expressions that use operators from specific theories. For example, the conditional probability of temperature given month, and the prior probability of month in our original example could be represented by3\nP (t|m) = if m ≤ 3\nthen if t = cold then 0.8 else 0.1\nelse if t = cold then 0.4 else 0.3\nP (m) = 1/12.\nWhile this form does not preclude regular VE to access it as a lookup table (by computing its result given an assignment to t andm), the symbolic expression is itself available as a data structure. This makes the structure of the factor evident; for example, specific values of m do not matter, but only whetherm ≤ 3.\nProbabilistic Inference Modulo Theories (PIMT), exploits this available symbolic information for much faster inference. It does so by using Symbolic Evaluation Modulo Theories (SEMT), which is a method for simplifying\n3Note that P (t|m) sums up to 1 for each m because hot and mild both have the else case probability mass.\n(and, when possible, completely evaluating) symbolic algebraic expressions, including eliminating quantifiers such as ∑\n, ∏\n, ∃, and ∀. SEMT applies to general expressions, and not only probabilistic reasoning-related ones. PIMT is simply the use of SEMT applied to expressions that happen to be (unnormalized) marginal probabilities, so we mostly refer to SEMT from now on, keeping in mind that it includes PIMT as a special case.\nSEMT and PIMT are similar to SatisfiabilityModulo Theories (SMT) solvers (Barrett et al., 2009; de Moura et al., 2007), which also take modular theory-specific solvers to solve multi-theory problems. However, they generalize SMT in two important ways: they are quantifierparametric, that is, they eliminate multiple types of quantifiers and deal with expressions of any type, including Boolean, numeric and categorical, as opposed to SMT only solving existential quantification on Boolean-valued formulas; and they are symbolic, that is, can process free variables and return results expressed in terms of them, as opposed to assuming all variables to be existentially quantified as in SMT.\nSEMT is a variant of SGDPLL(T ) (de Salvo Braz et al., 2016) slightly generalized to deal with any expression, and not just sequences of nested quantifiers, and to make use of contexts, defined below. We give an overview of SEMT here but refer to the SGDPLL(T ) reference for details.\nSEMT receives a pair (E,C) as input. E is the expression being evaluated, and C the context. Let x be the free variables in E. Context C is a formula on x that indicates that only the assignments on x satisfyingC need be considered. For example, the expression ifx 6= 1 then2 else4, under context x = 2 ∨ x = 3, can be safely evaluated to 4 because x 6= 1 is never true under that context.\nSEMT traverses the expression top-down and evaluates each sub-expression according to a set of rewriting rules (presented below) until no rule applies to any subexpressions. It also keeps track of the context holding for sub-expression, depending on the expressions above it. For example, in ifx = 1 then ∑\ni∈1..10:i6=3 γ elseϕ, γ is evaluated under context x = 1 ∧ i 6= 3.\nLet E[α/β] be the substitution of all occurrences of expressionα in expressionE by β. The SEMT rewriting rules are the following (an example is provided afterwards):\n• simplification: simplifiers for each theory are applied when possible: examples are 1+2 → 3, x = x → true, if true then1 else2 → 1, 0×x → 0 and so on. These always decrease the size of the expression.\n• literal determination: if the expression is a literalL under a context C and ∀V : C ⇒ L, where V are the free variables in C and L, rewrite the expression to TRUE; if ∀V : C ⇒ ¬L, rewrite it to FALSE; the tautology may be decided by SEMT itself or an SMT solver;\n• factoring out: if φ1 does not involve index i, ⊙\ni∈D:C(φ1 ⊙ φ2) → φ1 ⊙ ⊙ i∈D:C φ2.\n• if-splitting: if the expression is φ and contains a literal L undefined by the context C, rewrite it to ifL thenφ[L/TRUE] elseφ[L/FALSE].\n• quantifier-splitting: rewrite expressions of the form ⊙\ni∈D:C φ, where i is a variable of type D, F is a conjunction of literals in the theory for type D, and φ is expression containing a literal L containing i, to a new expression containing two quantified expressions, each with one less literal in its body: (\n⊙ i:F∧L φ[L/TRUE] ) ⊙ ( ⊙ i:F∧¬L φ[L/FALSE] ) .\n• theory-specific quantifier elimination: if φ does not contain any literals, solve ⊙\ni∈D:F φ with a provided, modular theory-specific solver for the type of i.\nSEMT always returns a quantifier-free expression since the theory-specific quantifier elimination is eventually invoked for each quantifier. For an expression representing a marginal over a product of factors, it reproduces Variable Elimination by summing out one variable at a time, with the advantage of exploiting factors represented symbolically.4\nConsider the computation of φ(t) = ∑\nm P (t|m)P (m) below. Regular VE requires iterating over 36 assignments to t,m, but SEMT only needs to consider the 4 truth assignments for literals t = cold andm ≤ 3:\n∑\nm\n(if m ≤ 3 then if t = cold then 0.8 else 0.1\nelse if t = cold then 0.4 else 0.3)/12\n→ (by if-splitting on t = cold and simplification)\nif t = cold\nthen\n∑\nm\n(if m ≤ 3 then 0.8 else 0.4)/12\nelse\n∑\nm\n(if m ≤ 3 then 0.1 else 0.3)/12\n→ (by quantifier-splitting onm ≤ 3 in first summation)\nif t = cold\nthen\n∑\nm:m≤3\n(if TRUE then 0.8 else 0.4)/12\n+ ∑\nm:¬(m≤3)\n(if FALSE then 0.8 else 0.4)/12\nelse\n∑\nm\n(if m ≤ 3 then 0.1 else 0.3)/12\n4The variable order used by VE is encoded in the order of quantifiers, and efficient ordering can be introduced by a rule that re-orders sequences of quantifiers.\n→ (by simplification and quantifier-splitting onm ≤ 3)\nif t = cold then 0.8/12× 3 + 0.4/12× 9\nelse 0.1/12× 3 + 0.3/12× 9\n→ if t = cold then 0.5 else 0.25,\nwhich represents the resulting new factor φ(t) (which happens to be P (t)) to be used in the next steps of VE. Note that the expressions above are not just a mathematical argument, but the actual data structures manipulated by SEMT.\nSEMT (and consequently PIMT) is modulo theories because symbolic evaluation is theory-agnostic, working for any theory T given an encapsulated theory solver that provides quantifier elimination solvers and simplification rules for T . Theory solvers must be provided only for the simpler expressions of the type ⊙\ni∈D:F φ where φ is literal-free and F is a conjunction of literals in the theory. de Salvo Braz et al. (2016) details a solver for summation over polynomials, with literals in difference arithmetic on bounded integers. A similar solver for integrals over polynomials and linear real arithmetic literals has also been defined since. An example of SEMT onmultiple theorieswith only 4 cases is:\n∑\ni∈1..7\n∫\nx∈[0;10]\nif i ≥ 3 thenifx < 5 thenx2 else0 else i\n→ ∑\ni∈1..7\nif i ≥ 3 then\n∫\nx∈[0;10]\nifx < 5 thenx2 else0\nelse\n∫\nx∈[0;10]\ni\n→ ∑\ni∈1..7\nif i ≥ 3 then\n∫\nx∈[0;10]:x<5\nx2 +\n∫\nx∈[0;10]:x≥5\n0\nelse 10i\n→ ∑\ni∈1..7\nif i ≥ 3 then 125/3 else 10i\n→ (\n∑\ni∈1..7:i≥3\n125/3 ) + (\n∑\ni∈1..7:i<3\n10i )\n→ 625/3 + 30 → 715/3."
    }, {
      "heading" : "2.3 Relational Graphical Models",
      "text" : "Relational graphical models (RGM) specify probability distributions over families of random variables that can be seen as relations. There are many RGM languages in the literature; they vary significantly in syntax and superficial features, but are typically equivalent and convertible to one another. Markov Logic Networks (MLN) (Richardson and Domingos, 2004) is a well-known one. It consists of a set of weight formulas, an example of which is\n2.5 : Smoker(Bob)\n1.4 : Smoker(X) ∧ Friends(X,Y ) ⇒ Smoker(Y )\nfor logical variablesX and Y ranging over a finite setD = {Ann,Bob, Charlie, . . .}.\nThe random variables in this MLN are the groundings, or instantiations, of the relations in it for every assignment to the logical variables: Sm(Ann), Sm(Bob), . . . , Fr(Ann,Ann), Fr(Ann,Bob), . . . (we abbreviate Smoker and Friends from now on). A formula with weight w defines a factor for each of its instantiations (one for each assignments to its logical variables). The potential assigned by such factors is ew if the formula is true, and 1 otherwise. Therefore, some of the factors of this MLN are: φ1(Sm(Bob)), φ2(Sm(Ann), F r(Ann,Bob), Sm(Bob)), φ2(Sm(Ann), F r(Ann,Charlie), Sm(Charlie)), and φ2(Sm(Bob), F r(Bob,Ann), Sm(Ann)), where φ1 and φ2 are potential functions applied to all factors instantiated from the first and second formulas respectively:\nφ1(s) = if s then e 2.5 else1 φ2(s1, f, s2) = if s1 ∧ f ⇒ s2 then e 1.4 else 1.\nBecause the number of instantiations can be huge, performing inference on RGMs by simply instantiating them as regular graphical models is often infeasible. The next section describes Inversion, one of the operations used in lifted probabilistic inference, which exponentially improves efficiency in RGMs in many cases."
    }, {
      "heading" : "3 INVERSION AND INVERSION MODULO THEORIES",
      "text" : "This section presents this paper’s main contributions. We present a new formulation for RGMs and Inversion that is more algebraically standard, and then use this formulation to generalize Inversion to Inversion Modulo Theories."
    }, {
      "heading" : "3.1 RGMS with Function-Valued Random Variables",
      "text" : "While the RGM literature considers Sm(Ann), Sm(Bob), . . . as individual (Boolean) random variables, it is conceptually simpler, and more mathematically standard, to talk about Sm as a single random variable that happens to be function-valued. From this point of view, this MLN has only two random variables: Sm and Fr, and defines the following joint probability distribution:\nP (Sm,Fr) = 1\nZ × φ1(Sm(Bob))\n× ∏\nX∈D\n∏\nY ∈D\nφ2(Sm(X), F r(X,Y ), Sm(Y ))\nwhere Z is defined as usual, as well as marginalization:\nP (Sm) = ∑\nFr∈D×D→Boolean\nP (Sm,Fr).\nNote that, given the form of P (Sm,Fr), marginal probabilities in RGMs are sums of products of factors, including intensional products using ∏ . This fact is heavily exploited by Inversion.\nTo compute the marginal of a specific function application, say, Sm(Ann), we need to marginalize over all remaining random variables, that is, Fr and all Sm(x) for x ∈ D \\ {Ann}. This requires splitting the variable Sm into two distinct function-valued random variables: Sm′ : {Ann} → Boolean and Sm′′ : D \\ {Ann} → Boolean, and replace each application Sm(θ) of the original Sm on argument expression θ by if θ = Ann thenSm′(Ann) elseSm′′(θ). Then we can compute\nP (Sm′) = ∑\nSm′′:D\\{Ann}→Boolean\n∑\nFr\nP (Sm′, Sm′′, F r).\nThe above shows that solving RGMs is simply equivalent to allowing function-valued random variables in the model. However, summations over functions are expensive due to their high number of possible values (2|D| 2\nfor Fr, for instance). The next section describes a method that exponentially decreases this cost in some cases."
    }, {
      "heading" : "3.2 Inversion on Function-Valued Variables",
      "text" : "Lifted probabilistic inference algorithms seek to exploit the structure of random functions for greater efficiency. It includes a few operations, but in this paper we consider only one: Inversion (also called Lifted Decomposition).\nInversion uses the fact that summations indexed by functions of products of factors may under certain conditions be transformed into exponentially cheaper summations over “slices” of the original function. Its name comes from the fact that a summation of products becomes a cheaper product of a summation ( ∑∏ → ∏∑ ), thus “inverting” the quantifiers. Consider an example in which the body φ of the sum-product depends on a single application of a function f ranging over the set of functions 1..10 → 1..5:\n∑\nf∈(1..10→1..5)\n∏\nx∈1..10\nφ(f(x)) = ∏\nx∈1..10\n∑\nf∈({x}→1..5)\nφ(f(x)).\nNote that, while f ranges over (1..10 → 1..5) on the lefthand side, and thus over 510 possible values, the domain of f on the right-hand side is a singleton set, because x is bound by the now outer ∏\nx. This reduces the number\nof possible values of f to only 5, making iterating over it exponentially cheaper.\nThe equality holds because x “slices” f into independent\nportions:\n∑\nf∈(1..10→1..5)\n∏\nx∈1..10\nφ(f(x))\n= ∑\nf1∈{1}→1..5\n. . . ∑\nf10∈{10}→1..5\nφ(f1(1))× ...× φ(f10(10))\n= ∑\nf1∈{1}→1..5\nφ(f1(1)) . . . ∑\nf10∈{10}→1..5\nφ(f10(10)) (*)\n= (\n∑\nf1∈{1}→1..5\nφ(f1(1)) ) . . . (\n∑\nf10∈{10}→1..5\nφ(f10(10)) )\n= ( ∑\nf∈{1}→1..5\nφ(f(1)) ) . . . (\n∑\nf∈{10}→1..5\nφ(f(10)) )\n= ∏\nx∈1..10\n∑\nf∈({x}→1..5)\nφ(f(x)).\nOnce transformed in this way, we proceed as follows:\n∏\nx∈1..10\nφ(1) + φ(2) + φ(3) + φ(4) + φ(5)\n= ∏\nx∈1..10\nφ′ = (φ′)10 = φ′′\nby using the fact that constant φ′ does not depend on x.\nThe above transformation is valid because ∏\nx φ(f(x)) contains 10 factors, each of them only involving the application of f to a single value of x. This means they can be factored out of the summations indexed by other applications of the function, resulting in smaller and equivalent summations that are computed only once and then exponentiated. Similar transformations may be applied even if there are products over more than one variable:\n∑\nf∈A1×A2→B\n∏\nx\n∏\ny\nφ(f(x, y)) = ∏\nx\n∏\ny\n∑\nf∈{(x,y)}→B\nφ(f(x, y))\nby an analogous argument.\nHowever, the transformation is not always valid:\n∑\nf∈A1×A2→B\n∏\nx\n∏\ny\nφ(f(x, y), f(y, x))\n6= ∏\nx\n∏\ny\n∑\nf∈{(x,y)}→B\nφ(f(x, y), f(y, x)).\nTo see this, consider a pair (a, b) ∈ A1 × A2. The summation ∑\nf∈{(a,b)}→B φ(f(a, b), f(b, a)) de-\npends on f(a, b) and f(b, a), which both occur in ∑\nf∈{(b,a)}→B φ(f(b, a), f(a, b)), and this shared dependence prevents factoring as in step (*) above.\nThis does not mean that having more than one application of f in φ admits no Inversion, but it may restrict the inversion to just some of the products:\n∑\nf∈((A1×A2×A3)→B)\n∏\nx\n∏\ny\nφ(f(x, y, x), f(x, 3, x)) (1)\n= ∏\nx\n∑\nf∈(({x}×A2×{x})→B)\n∏\ny\nφ(f(x, y, x), f(x, 3, x)),\nwhich does not decrease the function’s domain size to 1 but still exponentially decreases the evaluation cost.\nAll these cases are covered by the following theorem:\nTheorem 3.1 (Inversion). Let E be an expression in which all applications of a function f have their first k arguments (without loss of generality because they can be permutated) equal to xi1 , . . . , xik for i a k-tuple of indices in {1, . . . ,m}. If operator⊕ distributes over operator ⊗,\n⊕\nf∈(A1×···×An)→B\n⊗\nx1\n· · · ⊗\nxm\nE\n= ⊗\nx1\n· · · ⊗\nxm\n⊕\nf∈({(xi1 ,...,xik )}×Ak+1×···×An)→B\nE,\nwhich is exponentially cheaper to evaluate.\nNote thatE may contain quantifiers itself; this was the case in Equation (1). The theorem’s proofmirrors the operations shown in the examples above.\nCrucially, Theorem 3.1 uses a syntactic check that relies on the language allowing only simple terms (variables and constants) as arguments to functions. It therefore does not apply to the important extensions in which more complex terms, using theory-specific operators such as arithmetic or inequalities, or even operators from unanticipated theories, are used as function arguments. We will see how this can be done with a semantic check in the next section."
    }, {
      "heading" : "3.3 Inversion Modulo Theories",
      "text" : "We now present this paper’s main contribution: Inversion Modulo Theories, which is a version of the lifted probabilistic inference operation Inversion in the presence of factors defined through symbolic algebraic expressions involving multiple theories. Like regular Inversion, Inversion Modulo Theories does not cover every possible problem, in which case one must fall back to currently used methods like grounding or sampling. Future work includes generalizing other lifted inference methods like Counting (de Salvo Braz, 2007; Milch et al., 2008) to also be modulo theories, thus decreasing the need for the fallback methods.\nPrevious work on Inversion assumed RGMs expressed in a simple language in which the only allowed arguments in function applications are variable or constant symbols. This enables the Inversion condition to consist of a syntactic check. However, once we allow function arguments\nto be arbitrary terms from any of the available theories, these syntactic tests no longer apply, since the arguments semantics depends on the particular theory and the syntactic check does not take semantics into account. The new Inversion test must apply to arguments from any theory and, in fact, to arguments in even new, as of yet unanticipated theories. This requires this new test to be theory-agnostic, and defined in such a way that theory solvers may be transparently employed.\nWe start by defining ocf [E], an expression representing the portion of the domain of a given function f involved in a given expression E.\nDefinition 3.1. The set of argument tuples for f : A → B occurring in an expression E is denoted ocf [E] and inductively defined as follows:\n• if E does not contain f , ocf [E] is ∅;\n• if E is f(t) for t a tuple, ocf [E] is {t};\n• if E is f , ocf [E] is A;\n• if E is ifC thenE1 elseE2 andC does not contain f , then ocf [E] is ifC then ocf [E1] else ocf [E2]; otherwise, ocf [E] is ocf [C] ∪ ocf [E1] ∪ ocf [E2];\n• if E is g(t1, . . . , tk) for g a function symbol distinct from f , or E is {t1, . . . , tk}, ocf [E] is ocf [t1] ∪ · · · ∪ ocf [tk];\n• if E is Qx∈T :CE′ for Q an arbitrary quantifier,\n– if C does not contain f , then ocf [E] is ocf [T ] ∪ ⋃ x∈T :C ocf [E ′]; – otherwise, ocf [E] is ocf [T ] ∪ ⋃ x∈T (ocf [C] ∪ ocf [E ′]).\nFor example,\nocf [ f(x, y) + ∑\nz∈1..10:z!=3\nf(x, z) ∏\nw∈1..10\nf(w, z) ]\n= {(x, y)} ∪ ⋃\nz∈1..10:z!=3\n( {(x, z)} ∪ ⋃\nw∈1..10\n{(w, z)} ) ,\nin which x and y are free variables.\nAs a further preliminary step, we add simplification rules for tuples, and for testing whether sets are empty, to be used by SEMT. This will be useful later in manipulating ocf [E] expressions.\nTheorem 3.2 (Tuple and Empty Set Simplifiers). The following tuple and empty set simplifiers\n1. (r1, . . . , rn) = (s1, . . . , sn) → r1 = s1 ∧ · · · ∧ rm = sm (or its negation for 6=).\n2. t ∈ {t1, . . . , tn} → (t = t1) ∨ t ∈ {t2, . . . , tn}.\n3. t ∈ ⋃\ni∈D:C φ → ∃i ∈ D : (C ∧ t ∈ φ).\n4. ( ⋃ i∈D:C φ ) = ∅ → ∀i ∈ D : (¬C ∨ φ = ∅).\n5. S ∩ ∅ = ∅ → TRUE.\n6. S ∪ ∅ = ∅ → S = ∅.\n7. S ∩ {t1, . . . , tn} = ∅ → (t1 /∈ S) ∧ (S ∩ {t2, . . . , tn} = ∅).\n8. ( ⋃ i∈D:C φ ) ∩ ( ⋃ i′∈D′:C′ φ ′ )\n= ∅ → ∀i ∈ D : C ⇒ ∀i′ ∈ D′ : C′ ⇒ (φ ∩ φ′ = ∅).\n9. (S1 ∪ S2) ∩ S3 = ∅ → (S1 ∩ S3) ∪ (S2 ∩ S3) = ∅.\n10. S1 ∪ S2 = ∅ → S1 = ∅ ∧ S2 = ∅.\n11. {t1, . . . , tn} = ∅ → FALSE if n > 0, TRUE otherwise,\nwhen included in SEMT, rewrite ocf [E] = ∅ expressions to equivalent formulas free of tuple and set expressions.\nThe proof is presented in Supplementary Materials. It is based on gradual distribution of ∩ over ∪, and conversion of intersections to comparisons between set elements:\n⋃\nx∈1..5\n(\n{(x, y)} ∪ ⋃\nz∈3..5\n{(z, 3)} ) ∩ ⋃\nw∈1..10\n{(1, w)} = ∅\n→ (rule 8)\n∀x ∈ 1..5 : ∀w ∈ 1..10 : (\n{(x, y)} ∪ ⋃\nz∈3..5\n{(z, 3)} ) ∩ {(1, w)} = ∅\n→ (rule 9 distributes ∩ over ∪)\n∀x ∈ 1..5 : ∀w ∈ 1..10 :\n{(x, y)} ∩ {(1, w)} ∪ ( ⋃\nz∈3..5\n{(z, 3)} ) ∩ {(1, w)} = ∅\n→ (rule 10)\n∀x ∈ 1..5 : ∀w ∈ 1..10 : {(x, y)} ∩ {(1, w)} = ∅ ∧ ( ⋃\nz∈3..5\n{(z, 3)} ) ∩ {(1, w)} = ∅\n→ (rules 7, 2, and 8)\n∀x ∈ 1..5 : ∀w ∈ 1..10 :\n(x, y) 6= (1, w) ∧ ∀z ∈ 3..5 : {(z, 3)} ∩ {(1, w)} = ∅\n→ (rule 1 on first conjunct, rules 7 and 2 on second one)\n∀x ∈ 1..5 : ∀w ∈ 1..10 :\n(x 6= 1 ∨ y 6= w) ∧ ∀z ∈ 3..5 : (z, 3) 6= (1, w)\n→ (rule 1 for breaking tuples)\n∀x ∈ 1..5 : ∀w ∈ 1..10 :\n(x 6= 1 ∨ y 6= w) ∧ ∀z ∈ 3..5 : z 6= 1 ∨ 3 6= w\n→ (integer-specific solver for ⊙ = ∀ for z, w and x)\n∀x ∈ 1..5 : ∀w ∈ 1..10 : (x 6= 1 ∨ y 6= w) ∧ 3 6= w\n→ ∀x ∈ 1..5 : FALSE → FALSE.\nDeciding whether a set expression is equivalent to the empty set is crucial for Inversion Modulo Theories, which can now be formally stated:\nTheorem 3.3 (Inversion Modulo Theories). Let E be an expression and Ti, Ci be type and constraint, respectively, in a theory for which we have a satisfiability solver. Then,\n⊕\nf∈A→B\n⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\nE,\nwhere A = ocf [ ⊗\nx1∈T1:C1 · · ·\n⊗\nxk∈Tk:Ck E] (this is re-\nlaxed in Section 3.4), is equivalent to , and therefore can be rewritten as, ⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\n⊕\nf ∈ ocf [E]→B\nE,\nif ⊗ distributes over ⊕ and\n∀x′1 ∈ T1 . . . ∀x ′ k ∈ Tk ∀x ′′ 1 ∈ T1 . . . ∀x ′′ k ∈ Tk \n\nC1[x1/x ′ 1] ∧ · · · ∧ Ck[xk/x ′ k]\n∧ C1[x1/x′′1 ] ∧ · · · ∧ Ck[xk/x ′′ k] ∧ (x′1, . . . , x ′ k) 6= (x ′′ 1 , . . . , x ′′ k)\n\n\n⇒\n\n\nocf [E][x1/x ′ 1, . . . , xk/x ′ k] ∩ ocf [E][x1/x ′′ 1 , . . . , xk/x ′′ k ]\n\n = ∅ (2)\n(that is, the set of argument tuples in applications of f in E for different value assignments to x1, . . . , xk are always disjoint — we refer to this as Condition (2) from now on). Moreover, the rewritten expression is exponentially (O(2 ∏ i |{xi∈Ti:Ci}|)) cheaper to evaluate than the original.\nThe theorem (proven in Supplementary Materials) covers significantly more general cases than the original examples may suggest: f may have any arity; its arguments need not be only variables or constants, but even other (interpreted or uninterpreted) function applications; E may contain quantifiers itself; and quantifiers may involve constraints (Ci). The following example involves all these characteristics:\nExample 3.4. Let w and g be free variables. ∑\nf∈1..10×(1..10\\{8})×{w+3}→1..5 ∏\nx∈(1+g(w))..(10+g(w))\n∏\ny∈1..10:y 6=8 ∑\nz∈1..10\nf(x− g(w), y, w + 3)z\n→ (Inversion on x, y; see Eqn. (3) for Condition (2)) ∏\nx∈(1+g(w))..(10+g(w))\n∏\ny∈1..10:y 6=8 ∑\nf∈{(x−g(w),y,w+3)}→1..5\n∑\nz∈1..10\nf(x− g(w), y, w + 3)z\n→ (f has a singleton domain since x, y, g and w are fixed\nin the first summation, so it behaves like a variable v) ∏\nx∈(1+g(w))..(10+g(w))\n∏\ny∈1..10:y 6=8\n∑\nv∈1..5\n∑\nz∈1..10\nvz.\nThis transformation decreases the time complexity of the first summation from the time to iterate over 510×9 values of f to 5 values of v only. But, in fact, from now on SEMT completes the calculation with no iteration at all by using symbolic integer-specific solvers for ∑ and ∏ :\n∏\nx∈(1+g(w))..(10+g(w))\n∏\ny∈1..10:y 6=8\n∑\nv∈1..5\n∑\nz∈1..10\nvz\n→ ∏\nx∈(1+g(w))..(10+g(w))\n∏\ny∈1..10:y 6=8\n∑\nv∈1..5\n55v\n→ ∏\nx∈(1+g(w))..(10+g(w))\n∏\ny∈1..10:y 6=8\n55×15→ 82510×9 → 82590.\nIn this example, Condition (2) is\n∀x′ ∈ (1 + g(w))..(10 + g(w)) ∀y′ ∈ 1..10 ∀x′′ ∈ (1 + g(w))..(10 + g(w)) ∀y′′ ∈ 1..10\n(x′, y′) 6= (x′′, y′′) ∧ y′ 6= 8 ∧ y′′ 6= 8 ⇒ ⋃\nz∈1..10\n(x′ − g(w), y′, w + 3)\n∩ ⋃\nz∈1..10\n(x′′ − g(w), y′′, w + 3) = ∅ (3)\nwhich can be solved by SEMT with ⊕\ninstantiated as ∀ over difference arithmetic, after including tuple and empty set simplifiers from Definition 3.2).\nExample 3.5. Consider monitoring crop growth from satellite images to alert against famine. The growth (g) of crops can be determined from their color (c) in the images, and depends on whether the region was in drought (d) 3 months previously. This can be modeled as:\n∀m ∈ Months, f ∈ Fields :\nP (c(f,m)|g(f,m)) = if g(f,m) > 2.3 then if c(f,m) . . .\nP (g(f,m+ 3)|d(m)) = if d(m) then if g(f,m+ 3) . . .\nThe query P (d|c) requires solving the following marginalization over growth:\n∑\ng\n∏\nm\n∏\nf\nP (c(f,m+3)|g(f,m+3))P (g(f,m+3)|d(m))\nInversion Modulo Theories applies (because each (f,m) involves a single instance of g(f,m+ 3)) and we obtain\n∏\nm\n∏\nf\n∑\ng(f,m+3)\nP (c(f,m+3)|g(f,m+3))P (g(f,m+3)|d(m))\nwhich makes the summation on growth exponentially cheaper to compute and produces\n∏\nm\n∏\nf\nφ(c(f,m + 3), d(m)),\nfor φ the summation result. This can then be evaluated directly, given the evidence on color for each m and f ,\nproducing a factorized representation of the marginal on drought(m), for each month. If the number of fields is 2000, and growths domain size is 5, this cuts the cost of exactly eliminating growth from 52000 iterations to only 5."
    }, {
      "heading" : "3.4 Dealing with Arbitrary Domains for f",
      "text" : "Let OCf be ocf [ ⊗\nx1∈T1:C1 · · ·\n⊗\nxk∈Tk:Ck E]. Theorem\n3.3 requires that A = OCf , that is, that the domain of f coincide with its portion being used inside ⊕\nf∈A→B .\nWhen A 6= OCf , we use function splitting (Section 3.1):\n⊕\nf∈A→B\nΨ = ⊕\nf ′∈(A\\OCf )→B\n⊕\nf ′′∈(OCf\\A)→B\n⊕\nf∈(A∩OCf)→B\nΨ′\nfor Ψ′ obtained from Ψ after replacing each f(α) by\nif α ∈ A\nthen ifα ∈ OCf then f(α) else f ′(α) else f ′′(α).\nThis technique splits the original f into three different functions f , f ′, f ′′, and replaces each of its applications by the one corresponding to its arguments. After this, the domain of f coincides with OCf , satisfying the corresponding requirement in Theorem 3.3.\n3.5 Dealing with Multiple Separate ⊗ Quantifiers\nTheorem 3.3 requires a single nested sequence of ⊗ quantifiers inside ⊕ . However, summations on products of separate ⊗ quantifiers can be rewritten to the required form:\n⊕\nf∈A→B\n(\n(\n⊗\nx∈Tx:Cx\nEx\n)(\n⊗\ny∈Ty :Cy\nEy\n)\n)\n→ ⊕\nf∈A→B\n⊗\nx∈Tx:Cx\n⊗\ny∈Ty :Cy\nE 1 |{y∈Ty :Cy}| x E 1 |{x∈Tx:Cx}| y ,\nwhere the exponents compensate for the extra multiplications of Ex and Ey by moving them inside ⊗ y and ⊗ x respectively (this is sometimes called scaling). Set cardinalities can be computed as a special case of summation.\nIn the special case in which x and y range over the same values (that is, {x ∈ Tx : Cx} = {y ∈ Ty : Cy}, which can be evaluated by SEMT), the two ⊗ quantifiers can be merged into a single one (or aligned), and the original expression is rewritten instead to the cheaper ⊕\nf∈A→B\n⊗\nx∈Tx:Cx ExEy [y/x]. These transformations\ncan be easily generalized to cases with more than two ⊗ quantifications, and nested ⊗ expressions."
    }, {
      "heading" : "3.6 A Proof-of-concept Experiment",
      "text" : "Since SEMT and Inversion Modulo Theories are elaborate symbolic algorithms, two immediate questions are whether they can be effectively implemented and how they compare\nwith simpler alternatives such as sampling. As a proofof-concept test, we used our implementation of SEMT for computing Example 3.4 using two alternatives: one, in which the sum-product is simplified by Inversion Modulo Theories, and another in which the sum and product are computed by sampling. As expected, Inversion Modulo Theories vastly outperforms sampling in this example, computing the exact answer (82590) in less than 300 ms, whereas sampling requires 10 minutes, and 10,000 samples per quantifier, to be within an order of magnitude of the exact answer, and 17 minutes to be within 10% error."
    }, {
      "heading" : "4 RELATED WORK AND CONCLUSION",
      "text" : "As mentioned in the introduction, LPIMT generalizes work in the lifted inference, probabilistic inference modulo theories, and satisfiability modulo theories literatures. It is also related to Probabilistic Programs (PP) (Goodman et al., 2012; Milch et al., 2005), a class of high-level representations for probabilistic models that uses interpreted and uninterpreted functions. The current prevalent inference method in PPs is sampling, which is approximate and whose convergence rate depends on the size of the grounded model, that is, on the size of the domain. LPIMT can be applied to some fragments of PPs and is an exact inference alternative that avoids iterating over the domain. The closest approach to LPIMT in the PP area is Hakaru (Carette and Shan, 2016; Narayanan et al., 2016), which employs symbolic methods for simplification and integration of PPs, but does not include lifted inference on random functions. Model Counting Modulo Theories (Phan, 2015) leverages SMT solvers to compute model counts, but does not cover weighted model counting (and thus probabilistic reasoning), and does not exploit factorization. Weighted Model Counting and Integration (Michels et al., 2015; Belle et al., 2015; Michels et al., 2016) and Symbolic Variable Elimination (Sanner and Abbasnejad, 2012) are similar in spirit to PIMT, but not to LPIMT; they apply to Boolean and linear real arithmetic random variables, but not yet to random functions. (Belle, 2017) focuses on weighted model counting (WMC) with function symbols on infinite domains, reducing it to standard WMC, but does not currently use lifting techniques. Group Inversion (Taghipour et al., 2012) expands Inversion to cover some extra cases, but not interpreted functions. Extending it to do so, along with Counting, is the most immediate possibility for future work.\nTo conclude, we have defined Inversion Modulo Theories, an expansion of the lifted inference operation Inversion for the case with interpreted functions, and added it to Probabilistic Inference Modulo Theories framework, thus defining the first algorithm to perform exact lifted inference in the presence of interpreted functions.\nACKNOWLEDGMENTS We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Probabilistic Programming for Advanced Machine Learning Program under Air Force Research Laboratory (AFRL) prime contract no. FA8750-14C-0005."
    }, {
      "heading" : "A SUPPLEMENTARY MATERIALS",
      "text" : "A.1 Theorems 3.2 and 3.3 and their proofs\nTheorem 3.2 (Tuple and Empty Set Simplifiers). The following tuple and empty set simplifiers\n1. (r1, . . . , rn) = (s1, . . . , sn) → r1 = s1 ∧ · · · ∧ rm = sm (or its negation for 6=).\n2. t ∈ {t1, . . . , tn} → (t = t1) ∨ t ∈ {t2, . . . , tn}.\n3. t ∈ ⋃\ni∈D:C φ → ∃i ∈ D : (C ∧ t ∈ φ).\n4. ( ⋃ i∈D:C φ ) = ∅ → ∀i ∈ D : (¬C ∨ φ = ∅).\n5. S ∩ ∅ = ∅ → TRUE.\n6. S ∪ ∅ = ∅ → S = ∅.\n7. S ∩ {t1, . . . , tn} = ∅ → (t1 /∈ S) ∧ (S ∩ {t2, . . . , tn} = ∅).\n8. ( ⋃ i∈D:C φ ) ∩ ( ⋃ i′∈D′:C′ φ ′ )\n= ∅ → ∀i ∈ D : C ⇒ ∀i′ ∈ D′ : C′ ⇒ (φ ∩ φ′ = ∅).\n9. (S1 ∪ S2) ∩ S3 = ∅ → (S1 ∩ S3) ∪ (S2 ∩ S3) = ∅.\n10. S1 ∪ S2 = ∅ → S1 = ∅ ∧ S2 = ∅.\n11. {t1, . . . , tn} = ∅ → FALSE if n > 0, TRUE otherwise,\nwhen included in SEMT, rewrite ocf [E] = ∅ expressions to equivalent formulas free of tuple and set expressions.\nProof. Intuitively, this theorem is analogous to an algorithm that converts propositional formulas into an equivalent disjunctive normal form (DNF), that is, a disjunction of conjunctive clauses (that is, conjunctions of literals). Once a DNF is reached, contradictory conjunctive clauses are eliminated, and therefore the formula is satisfiable if and only if there is at least one conjunctive clause with at least one literal. In this analogy, conjunctions and disjunctions correspond to intersection and union, and sets correspond to conjunctive clauses. Empty sets are eliminated, and if at the end we have a union of sets, and the empty ones have been eliminated, this means that the resulting set is not empty.\nFormally, the theorem is proven by induction on the distance vector, a tuple that measures how far an expression is from being solved. Before we define the distance vector, we need to inductive define, for any expression E, the intersection-union nesting N∩∪(E):\nN∩∪(E) =\n\n \n \n∑\ni N∩∪(Ei), if E = E1 ∩ · · · ∩En\n1 + maxiN∩∪(Ei), if E = E1 ∪ · · · ∪ En 0, if E is any other expression.\nIntuitively, N∩∪ measures how far we are from a “flat” union of intersections.\nThe distance vector of an expression E is a vector of nonnegative integers that is lexicographically ordered, with the most significant component listed first:\n1. N∩∪(E);\n2. number of intensional unions ( ⋃ );\n3. number of existential and universal quantifications;\n4. number of ∩ applications;\n5. sum of lengths of extensionally defined sets ({. . . });\n6. number of ∈ applications;\n7. number of comparisons to ∅;\n8. number of tuples.\nIn the base case, the distance vector is a tuple of zeros, and therefore the expression is a formula without any tuple or set operators, satisfying the theorem.\nOtherwise, the distance vector contains at least one nonzero component. If we can show that there is always at least one applicable simplifier, and that every simplifier application results in an expression with a smaller distance vector with respect to the lexicographical order, then the theorem will have been proven by induction.\nThere is always an applicable simplifier to expressions with non-zero distance vector, because in that case there is at least one tuple operator or a comparison between a set expression and ∅:\n• if there is a set expression, it must be one of ∩, ∪ applications or ⋃ , and there is at least one simplifier\nfor each of these;\n• If there is a tuple anywhere, it is either inside a tuple comparison, or inside a set; if it is in a comparison,\nsimplifier 1 applies; if it is in a set, one of the set simplifiers applies.\nOnce it is established that there is always an applicable simplifier, the next step is whether the distance vector is always decreased according to its lexicographical order. Simplifiers 1, 2, 4, 5, 6, 10, and 11 strictly decrease one or more of the distance vector components without increasing any other, so for expressions for which any of them apply, the theorem is proven by induction on the distance vector.\nThe remaining simplifiers decrease a distance vector component while increasing others, but the ones increased are always less significant in the lexicographical order than the one decreased:\n• Simplifier 3 decreases the number of intensional unions at the cost of the less significant number of ex-\nistential quantifications;\n• Simplifier 7 decreases the sum of lengths of extensionally defined sets at the cost of the less significant num-\nber of ∈ applications;\n• Simplifier 8 decreases the number of intensional unions at the cost of the less significant number of uni-\nversal quantifications;\n• Simplifier 9 duplicates S3 and therefore doubles all distance vector components in S3, with the exception the most significant one, N∩∪, which is decreased:\nN∩∪((S1 ∪ S2) ∩ S3 = ∅)\n= max(N∩∪((S1 ∪ S2) ∩ S3), 0)\n= N∩∪((S1 ∪ S2) ∩ S3)\n= N∩∪(S1 ∪ S2) +N∩∪(S3)\n= 1 +max(N∩∪(S1), N∩∪(S2)) +N∩∪(S3)\n= 1 +max(N∩∪(S1) +N∩∪(S3),\nN∩∪(S2) +N∩∪(S3))\n= 1 +max(N∩∪(S1 ∩ S3),\nN∩∪(S2 ∩ S3))\n= 1 +N∩∪((S1 ∩ S3) ∪ (S2 ∩ S3))\n> N∩∪((S1 ∩ S3) ∪ (S2 ∩ S3))\n= N∩∪((S1 ∩ S3) ∪ (S2 ∩ S3) = ∅).\nTo summarize, we have shown that, for every expression in the language of interest, there is always an applicable simplifier, and that all simplifiers decrease the distance vector until it reaches the base case all-zero distance vector, which is free of tuple and set operators.\nTheorem 3.3 (Inversion Modulo Theories). Let E be an expression and Ti, Ci be type and constraint, respectively, in a theory for which we have a satisfiability solver. Then,\n⊕\nf∈A→B\n⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\nE,\nwhere A = ocf [ ⊗\nx1∈T1:C1 · · ·\n⊗\nxk∈Tk:Ck E] (this is re-\nlaxed in Section 3.4), is equivalent to , and therefore can be rewritten as,\n⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\n⊕\nf ∈ ocf [E]→B\nE,\nif ⊗ distributes over ⊕ and\n∀x′1 ∈ T1 . . . ∀x ′ k ∈ Tk ∀x ′′ 1 ∈ T1 . . . ∀x ′′ k ∈ Tk \n\nC1[x1/x ′ 1] ∧ · · · ∧ Ck[xk/x ′ k]\n∧ C1[x1/x′′1 ] ∧ · · · ∧ Ck[xk/x ′′ k] ∧ (x′1, . . . , x ′ k) 6= (x ′′ 1 , . . . , x ′′ k)\n\n\n⇒\n\n\nocf [E][x1/x ′ 1, . . . , xk/x ′ k] ∩ ocf [E][x1/x ′′ 1 , . . . , xk/x ′′ k ]\n\n = ∅ (2)\n(that is, the set of argument tuples in applications of f in E for different value assignments to x1, . . . , xk are always disjoint — we refer to this as Condition (2) from now on). Moreover, the rewritten expression is exponentially (O(2 ∏ i |{xi∈Ti:Ci}|)) cheaper to evaluate than the original.\nProof. Let m be the number of possible assignments to x1, . . . , xm. We prove the theorem by induction on m. If m is 0,\n⊕\nf∈A→B\n⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\nE\n= ⊕\nf∈∅→B\n1\n= ⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\n⊕\nf ∈ ∅→B\n1\n= ⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\n⊕\nf ∈ ∅→B\nE\n= ⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\n⊕\nf ∈ ocf [E]→B\nE,\nbecause the empty products allow the substitution of 1 by E and ∅ by ocf [E] without change.\nIf m > 0, let x̄ be the first possible assignment to x1, . . . , xk satisfying C1 ∧ · · · ∧ Ck. Then\n⊕\nf∈A→B\n⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\nE\n= (separating x̄ from other assignments) ⊕\nf1∈ocf [E][x1,...,xk/x̄]→B ⊕\nf∈(A\\ocf [E][x1,...,xk/x̄])→B\nE1 ⊗ ⊗\n(x1,...,xk)∈T1×···×Tk:C1∧···∧Ck∧(x1,...,xk) 6=x̄\nE,\nwhere E1 = E[f/f1]\n= (E1 has no occurrences of f ) (\n⊕\nf1∈ocf [E][x1,...,xk/x̄]→B\nE1\n)\n⊗\n⊕\nf∈(A\\ocf [E][x1,...,xk/x̄])→B ⊗\n(x1,...,xk)∈T1×···×Tk:C1∧···∧Ck∧(x1,...,xk) 6=x̄\nE\n= (by induction onm) (\n⊕\nf1∈ocf [E][x1,...,xk/x̄]→B\nE1\n)\n⊗\n⊗\n(x1,...,xk)∈T1×···×Tk:C1∧···∧Ck∧(x1,...,xk) 6=x̄\n⊕\nf∈ocf [E]→B\nE\n= (renaming f1 to f and using the fact that E1 = E[f/f1]) (\n⊕\nf∈ocf [E][x1,...,xk/x̄]→B\nE )\n⊗\n⊗\n(x1,...,xk)∈T1×···×Tk:C1∧···∧Ck∧(x1,...,xk) 6=x̄\n⊕\nf∈ocf [E]→B\nE\n= (introducing intensional products on x1, . . . , xk bound to x̄) ⊗\n(x1,...,xk)∈T1×···×Tk:C1∧···∧Ck∧(x1,...,xk)=x̄ (\n⊕\nf∈ocf [E][x1,...,xk/x̄]→B\nE )\n⊗\n⊗\n(x1,...,xk)∈T1×···×Tk:C1∧···∧Ck∧(x1,...,xk) 6=x̄\n⊕\nf∈ocf [E]→B\nE\n= (merging ⊗\nby disjuncting their constraints) ⊗\n(x1,...,xk)∈T1×···×Tk:C1∧···∧Ck∧ ( (x1,...,xk)=x̄∨(x1,...,xk) 6=x̄ )\n⊕\nf∈ocf [E]→B\nE\n= (eliminating tautology on x̄ and separating ⊗\nper index) ⊗\nx1∈T1:C1\n· · · ⊗\nxk∈Tk:Ck\n⊕\nf ∈ ocf [E]→B\nE\nThe final expression isO(2 ∏ i |{xi∈Ti:Ci}|) cheaper to evaluate because the final f has all xi bound to a single value, mapping each of their ∏\ni |{xi ∈ Ti : Ci}| assignments to a single one and thus dividing the total size of the domain over which one must iterate."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Probabilistic Inference Modulo Theories (PIMT) is a recent framework that expands exact inference on graphical models to use richer languages that include arithmetic, equalities, and inequalities on both integers and real numbers. In this paper, we expand PIMT to a lifted version that also processes random functions and relations. This enhancement is achieved by adapting Inversion, a method from Lifted First-Order Probabilistic Inference literature, to also be modulo theories. This results in the first algorithm for exact probabilistic inference that efficiently and simultaneously exploits random relations and functions, arithmetic, equalities and inequalities.",
    "creator" : "LaTeX with hyperref package"
  }
}