{
  "name" : "1501.05290.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Bernardo Gonçalves" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "National Laboratory for Scientific Computing\nGraduate Program in Computational Modeling\nManaging large-scale scientific hypotheses as uncertain\nand probabilistic data\nBy\nBernardo Gonçalves\nPETRÓPOLIS, RJ - BRASIL\nFEBRUARY - 2015\nar X\niv :1\n50 1.\n05 29\n0v 2\n[ cs\n.D B\n] 1\n2 Fe\nb 20\n15\nMANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES AS\nUNCERTAIN AND PROBABILISTIC DATA\nBernardo Gonçalves\nTHESIS SUBMITTED TO THE EXAMINING COMMITTEE IN PARTIAL FULFILLMENT OF THE REQUIREMENTS FOR THE DEGREE OF DOCTOR OF SCIENCES IN COMPUTATIONAL MODELING.\nApproved by:\nProf. Fabio Porto, D.Sc.\n(Chair)\nProf. Pedro L. Dias, Ph.D.\nProf. Marco A. Casanova, Ph.D.\nProf. Ana Carolina Salgado, D.Sc.\nPETRÓPOLIS, RJ - BRASIL FEBRUARY - 2015\n© 2015, Bernardo Nunes Gonçalves.\nAll rights reserved.\nGonçalves, Bernardo\nG635m Managing large-scale scientific hypotheses as uncertain and probabilis-\ntic data / Bernardo Gonçalves. Petropólis, RJ. : National Laboratory for Scientific Computing, 2015.\nxvii, 128p. : il.; 29 cm\nOrientador: Fabio Porto\nThesis (D.Sc.) – National Laboratory for Scientific Computing, 2015. 1. Hypothesis management. 2. Predictive analytics. 3. Uncertain and\nprobabilistic data. 4. Causal reasoning. 5. Probabilistic database design. I. Porto, Fabio. II. LNCC/MCT. III. T́ıtulo.\nCDD – 629.8\nDedicatory\nTo my parents Tania and Francisco,\nand to my special love, Marcelle,\nfor being an island of certainty in an\nuncertain world."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This thesis work has been supported by LNCC (Graduate Program in Com-\nputational Modeling), CNPq (grant 141838/2011-6), FAPERJ (grant ‘Nota 10’ E-26/100.286/2013) and IBM (Ph.D. Fellowship 2013/2014).\nI would like to express my gratitude to my advisor Fabio Porto for the gift\nof the challenging topic of this thesis, so special to me. I am grateful to him for being an inspiring advisor, and for nicely influencing me towards database research. I also thank my thesis committee for their attention and time devoted in the assessment of my work. I am indebted to Ana Maria Moura for her generous advice and support throughout my Ph.D. research, and to Frederico C. Silva and Adolfo Simões for their support to my research project at DEXL/LNCC.\nI would like to thank very much all researchers at LNCC who give lectures in\nthe graduate program, specially Prof. José Karam Filho for teaching me generously about the roots of mathematical modeling. They have contributed significantly to my education as a cross-disciplinary thinker and the shaping of my scientific and mathematical skills. I thank my Ph.D. colleagues and friends at LNCC, specially Eduardo Lima, Ramon Costa, Klaus Wehmuth, Raquel Lopes, Karine Guimarães and Diego Paredes for their companionship and joy shared in the pursuit of their Ph.D. theses. I would also like to gratefully recall my earlier professors at UFES, José Gonçalves, Giancarlo Guizzardi, Rosane Caruso and Berilhes Garcia for shaping the most essential building blocks in my education.\nFinally, I would like to thank my family for their support: my father Fran-\ncisco, my example of simplicity and goodness; my mother, Tania, for her tenacity and true love; Leo and Sche, my dear brother and sister, true union for life; my grandmothers Zeca, Orizontina and Rosita for their love and prayers. I thank also Marcel and Maris for the greatest gift, their daughter and my near future wife, Marcelle – the best partner one could ever hope for.\n“Originally, there was just experimental science, and then there was theoretical science, with Kepler’s Laws, Newton’s Laws of Motion, Maxwell’s equations, and so on. Then, for many problems, the theoretical models grew too complicated to solve analytically, and people had to start simulating. These simulations have carried us through much of the last half of the last century. At this point, these simulations are generating a whole lot of data, along with a huge increase in data from the experimental sciences.”\n— Jim Gray, 2007\nAbstract of Thesis presented to LNCC/MCT in partial fulfillment of the requirements for the degree of Doctor of Sciences (D.Sc.)\nMANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES AS\nUNCERTAIN AND PROBABILISTIC DATA\nBernardo Gonçalves\nFebruary - 2015\nAdvisor: Fabio Porto, D.Sc.\nIn view of the paradigm shift that makes science ever more data-driven, in this thesis we propose a synthesis method for encoding and managing large-scale deterministic scientific hypotheses as uncertain and probabilistic data.\nIn the form of mathematical equations, hypotheses symmetrically relate as-\npects of the studied phenomena. For computing predictions, however, deterministic hypotheses can be abstracted as functions. We build upon Simon’s notion of structural equations in order to efficiently extract the (so-called) causal ordering between variables, implicit in a hypothesis structure (set of mathematical equations).\nWe show how to process the hypothesis predictive structure effectively through\noriginal algorithms for encoding it into a set of functional dependencies (fd’s) and then performing causal reasoning in terms of acyclic pseudo-transitive reasoning over fd’s. Such reasoning reveals important causal dependencies implicit in the hypothesis predictive data and guide our synthesis of a probabilistic database. Like in the field of graphical models in AI, such a probabilistic database should be normalized so that the uncertainty arisen from competing hypotheses is decomposed into factors and propagated properly onto predictive data by recovering its joint probability distribution through a lossless join. That is motivated as a design-theoretic principle for data-driven hypothesis management and predictive analytics.\nThe method is applicable to both quantitative and qualitative deterministic\nhypotheses and demonstrated in realistic use cases from computational science.\nResumo da Tese apresentada ao LNCC/MCT como parte dos requisitos necessários para a obtenção do grau de Doutor em Ciências (D.Sc.)\nGERÊNCIA DE HIPÓTESES CIENTÍFICAS DE LARGA-ESCALA\nCOMO DADOS INCERTOS E PROBABILÍSTICOS\nBernardo Gonçalves\nFevereiro, 2015\nOrientador: Fabio Porto, D.Sc.\nTendo em vista a mudança de paradigma que faz da ciência cada vez mais guiada por dados, nesta tese propomos um método para codificação e gerência de hipóteses cient́ıficas determińısticas de larga escala como dados incertos e probabiĺısticos.\nNa forma de equações matemáticas, hipóteses relacionam simetricamente as-\npectos do fenômeno de estudo. Para computação de predições, no entanto, hipóteses determińısticas podem ser abstráıdas como funções. Levamos adiante a noção de Simon de equações estruturais para extrair de forma eficiente a então chamada ordenação causal impĺıcita na estrutura de uma hipótese.\nMostramos como processar a estrutura preditiva de uma hipótese através de\nalgoritmos originais para sua codificação como um conjunto de dependências funcionais (df’s) e então realizamos inferência causal em termos de racioćınio aćıclico pseudo-transitivo sobre df’s. Tal racioćınio revela importantes dependências causais impĺıcitas nos dados preditivos da hipótese, que conduzem nossa śıntese do banco de dados probabiĺıstico. Como na área de modelos gráficos (IA), o banco de dados probabiĺıstico deve ser normalizado de tal forma que a incerteza oriunda de hipóteses alternativas seja decomposta em fatores e propagada propriamente recuperando sua distribuição de probabilidade conjunta via junção ‘lossless.’ Isso é motivado como um prinćıpio teórico de projeto para gerência e análise de hipóteses.\nO método proposto é aplicável a hipóteses determińısticas quantitativas e\nqualitativas e é demonstrado em casos reaĺısticos de ciência computacional.\nTable of Contents\n1 Introduction 1\n1.1 Problem Space and Specific Goals . . . . . . . . . . . . . . . . . . . 4 1.2 Thesis Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.3 Thesis Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . 13 1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2 Vision: Hypotheses as Data 15\n2.1 Running Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2 Hypothesis Encoding . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.3 Reasoning over FD’s . . . . . . . . . . . . . . . . . . . . . . . . . . 21 2.4 Uncertainty Introduction . . . . . . . . . . . . . . . . . . . . . . . . 22 2.5 Predictive Analytics . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2.7 Summary: Key Points . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3 Hypothesis Encoding 32\n3.1 Preliminaries: Structural Equations . . . . . . . . . . . . . . . . . . 32 3.2 The Problem of Causal Ordering . . . . . . . . . . . . . . . . . . . 36 3.3 Total Causal Mappings . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.4 The Encoding Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.5 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.7 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4 Causal Reasoning over FD’s 49\n4.1 Preliminaries: Armstrong’s Inference Rules . . . . . . . . . . . . . . 49\nTABLE OF CONTENTS x\n4.2 Acyclic Pseudo-Transitive Reasoning . . . . . . . . . . . . . . . . . 50 4.3 Equivalence with Causal Ordering . . . . . . . . . . . . . . . . . . . 55 4.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.5 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.6 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n5 Probabilistic Database Synthesis 61\n5.1 Preliminaries: U-Relations and Probabilistic WSA . . . . . . . . . . 61 5.2 Running Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.3 U-Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 5.4 U-Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 5.5 Design-Theoretic Properties . . . . . . . . . . . . . . . . . . . . . . 73 5.6 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 5.7 Summary of Results . . . . . . . . . . . . . . . . . . . . . . . . . . 78\n6 Applicability 79\n6.1 The Physiome Project as a Testbed . . . . . . . . . . . . . . . . . . 79 6.2 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 6.3 System Prototype . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 6.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95 6.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 6.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n7 Conclusions 102\n7.1 Revisiting the Research Questions . . . . . . . . . . . . . . . . . . . 102 7.2 Significance and Limitations . . . . . . . . . . . . . . . . . . . . . . 105 7.3 Open Problems and Future Work . . . . . . . . . . . . . . . . . . . 106 7.4 Final Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . 107\nTABLE OF CONTENTS xi\nBibliography 108\nAppendix\nA Detailed Proofs 116\nA.1 Proofs of Hypothesis Encoding . . . . . . . . . . . . . . . . . . . . . 116 A.2 Proofs of Causal Reasoning . . . . . . . . . . . . . . . . . . . . . . 119 A.3 Proofs of Probabilistic DB Synthesis . . . . . . . . . . . . . . . . . 126\nList of Figures\nFigure\n1.1 Multi-fold view of a deterministic scientific hypothesis. . . . . . . . 2 1.2 A view of the scientific method life cycle . . . . . . . . . . . . . . . 5 1.3 The usual data ingesture pipeline of simulation data management . 7 1.4 Design-theoretic pipeline for processing hypotheses as uncertain and\nprobabilistic data . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n2.1 Scientific hypotheses seen as alternative functions to predict data. . 16 2.2 Predictive analytics in a data-intensive hypothesis evaluation study. 18 2.3 Descriptive (textual) data of Example 1. . . . . . . . . . . . . . . . 19 2.4 ‘Big’ fact table H1 loaded with simulation raw data . . . . . . . . . 19 2.5 ‘Explanation’ relational table H0. . . . . . . . . . . . . . . . . . . . 23 2.6 Result set of query Q1 on simulation trial dataset for hypothesis H1. 24 2.7 U-relational predictive tables rendered by query using the fd’s. . . . 25 2.8 Analytics on predicted position s conditioned on observation. . . . . 26\n3.1 “Directed causal graphs” associated with the two systems. . . . . . . 33 3.2 Running Simon’s Causal Ordering Algorithm (COA) . . . . . . . . . 35 3.3 Directed causal graph Gϕ induced by mapping ϕ for structure S . . 36 3.4 Bipartite graph G of structure S from Example 3. . . . . . . . . . . 37 3.5 Another hypothesis structure example. . . . . . . . . . . . . . . . . 39 3.6 Complete matching M for structure S from Example 3. . . . . . . . 42 3.7 Encoded fd set Σ (cf. Alg. 3) for the structure from Example 3. . . . 44 3.8 Performance of hypothesis encoding (in logscale). . . . . . . . . . . 46\n4.1 Fd set Σ encoding (cf. Alg. 3) the structure of Fig. 3.2a and its\nfolding Σ# derived by Alg. 5. . . . . . . . . . . . . . . . . . . . . . 52\nLIST OF FIGURES xiii\n4.2 Fd set Σ encoding the structure of Fig. 3.2a and the folding Υ(Σ)#\nof its υ-projection. . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n4.3 Performance of acyclic causal reasoning over fd’s (logscale). . . . . . 59\n5.1 U-relation generated by the repair-key operation. . . . . . . . . . . 63 5.2 Fd sets encoded from the given structures Sk(Ek, Vk) for hypotheses\nk = 1..3 from Example 6. . . . . . . . . . . . . . . . . . . . . . . . . 65\n5.3 ‘Big’ fact table H3 of hypothesis k=3 from Example 6 loaded with\ntrial datasets identified by special attribute tid. . . . . . . . . . . . 65\n5.4 ‘Big’ fact table H3 of hypothesis k=3 from Example 6 with u-factors\n{b, d} and {p, r} emphasized (resp.) in colors green and red. . . . . 67\n5.5 Fd set Ω3 (compare with Σ3) and its folding Ω # 3 . . . . . . . . . . . . 68 5.6 U-factor projections rendered for hypothesis υ = 3. . . . . . . . . . 69 5.7 U-relations rendered for hypothesis υ = 3. . . . . . . . . . . . . . . 72\n6.1 Plot of hemoglobin oxygen saturation hypotheses . . . . . . . . . . 81 6.2 Descriptive (textual) data of Example 8. . . . . . . . . . . . . . . . 81 6.3 Fd set Σ28 of hypothesis υ=28. . . . . . . . . . . . . . . . . . . . . 82 6.4 Fd set Σ31 of hypothesis υ=31. . . . . . . . . . . . . . . . . . . . . 82 6.5 Fd set Σ32 of hypothesis υ=32. . . . . . . . . . . . . . . . . . . . . 82 6.6 Result set of hypothesis management query Q1. . . . . . . . . . . . 83 6.7 Results of analytical study on the hemoglobin phenomenon. . . . . 83 6.8 Plot of baroreflex hypothesis for Dahl SS Rat . . . . . . . . . . . . 84 6.9 Descriptive (textual) data of Example 9. . . . . . . . . . . . . . . . 84 6.10 Result set of hypothesis management query Q2. . . . . . . . . . . . 85 6.11 Results of analytical study on the baroreflex phenomenon. . . . . . 85 6.12 Fd set Σ1001 of hypothesis υ=1001. . . . . . . . . . . . . . . . . . . 86 6.13 Plot of myogenic behavior hypothesis . . . . . . . . . . . . . . . . . 87 6.14 Descriptive (textual) data of Example 10. . . . . . . . . . . . . . . . 87 6.15 Fd set Σ60 of hypothesis υ=60. . . . . . . . . . . . . . . . . . . . . 88 6.16 Fd set Σ89 of hypothesis υ=89. . . . . . . . . . . . . . . . . . . . . 88 6.17 Result set of hypothesis management query Q3. . . . . . . . . . . . 89 6.18 Results of analytics on the vessel’s myogenic behavior phenomenon. 89\nLIST OF FIGURES xiv\n6.19 Lynx-Hare population observed . . . . . . . . . . . . . . . . . . . . 91 6.20 Lynx-Hare population observed . . . . . . . . . . . . . . . . . . . . 91 6.21 Screenshots of this first prototype of the Υ-DB system. . . . . . . . 92 6.22 Descriptive (textual) data of Example 10. . . . . . . . . . . . . . . . 93 6.23 Fd set Σ1 of hypothesis υ=1. . . . . . . . . . . . . . . . . . . . . . 93 6.24 Fd set Σ2 of hypothesis υ=2. . . . . . . . . . . . . . . . . . . . . . 93 6.25 Fd set Σ3 of hypothesis υ=3. . . . . . . . . . . . . . . . . . . . . . 94 6.26 Results of analytics on the US population phenomenon. . . . . . . . 95 6.27 Results of analytics on the Hudson’s Bay lynx population phenomenon 95 6.28 Performance behavior of Υ-DB on a Physiome testbed scenario. . . 96 6.29 Physiome hypotheses used in the experiments. . . . . . . . . . . . . 97 6.30 Example of Boolean Network hypothesis. . . . . . . . . . . . . . . . 100 6.31 Example of Boolean Network model . . . . . . . . . . . . . . . . . . 101\nList of Tables\nTable\n1.1 Simulation data management vs. hypothesis data management. . . 5\nAcronyms\n• fd: functional dependency\n• p-DB: probabilistic database\n• p-WSA: probabilistic world set algebra\n• MayBMS: U-relational database management system\n• AI: Artificial Intelligence\n• GM: graphical models (e.g., Bayesian Networks)\n• ETL: extract, transform, load\n• OLAP: On-Line Analytical Processing\n• DW: Data Warehouse\n• SEM: structural equation model\n• COA: causal ordering algorithm\n• Sk: structure of hypothesis k • E : set of equations in a structure\n• V: set of variables appearing in the equations in a structure\n• V ars(f): set of variables appearing in equation f\n• ϕ : E → V: total causal mapping from equations to variables in a structure\n• Cϕ: set of causal dependencies • Gϕ: causal graph induced by ϕ • Hk: ‘big’ fact table of hypothesis k • H: set of hypothesis ‘big’ fact tables\n• Y `k : U-relation synthesized for hypothesis k • Y k: set of U-relations synthesized for hypothesis k • H0: relational ‘explanation’ table\nLIST OF TABLES xvii\n• Y0: U-relational ‘explanation’ table • φ: phenomenon identifier\n• υ: hypothesis identifier\n• Σ, Γ, ∆, Ω: fd sets\n• ΣB: subset of the closure of an fd set derived by pseudo-transitivity\n• Σ+: closure of an fd set\n• Σ#: the folding of an fd set\n• synthesis ‘4U’: synthesis for uncertainty\n• tid: hypothesis trial identifier\n• BCNF: Boyce-Codd Normal Form\n• MathML: Mathematical Markup Language\n• TCM: total causal mapping algorithm\n• lhs, rhs: left-hand side, right-hand side\n• sch(R): data columns of table R\n• ViDi: condition columns of a table\nChapter 1\nIntroduction\nIn view of the paradigm shift that makes science ever more data-driven [1], in this thesis we demonstrate that large deterministic scientific hypotheses can be effectively encoded and managed as a kind of uncertain and probabilistic data.\nDeterministic hypotheses can be formed as principles or ideas, then expressed\nmathematically and implemented in a program that is run to give their decisive form of data (see Fig. 1.1). Hypotheses can also be learned in large scale, as exhibited in the Eureqa project [2]. Examples of ‘structured deterministic hypotheses’ include tentative mathematical models in physics, engineering and economical sciences, or conjectured boolean networks in molecular biology and social sciences. These are important reasoning devices, as they are solved to generate valuable predictive data for decision making in science and increasingly in business as well.\nIn fact, we can refer nowadays to a broad, modern context of data science [3]\nand big data [4] in which the complexity and scale of so-called ‘data-driven’ problems require proper data management tools for the predicted data to be analyzed effectively. In this thesis, we pay attention to a quite general class of (tentative) computational science models,1 and we look at them in an original way as a distinguished kind of data source.\n1 ‘Computational science’ is (sic.) “a rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems” [5]. We may refer to non-stochastic, tentative computational science models throughout this text as ‘structured deterministic hypotheses.’\n2 Law of free fall\n“If a body falls from rest, its velocity at any point is proportional to the time it has been falling.”\n(i)\na(t) = −g v(t) = −gt + v0 s(t) = −(g/2)t2 + v0 t + s0\n(ii)\nIt is generally considered that computational science models, interpreted here\nas hypotheses to explain real-world phenomena, are of strategic relevance [5]. They are usually complex in that they may have hundreds to thousands of intertwined (coupled) variables and be computed along space, time or frequency domains in arbitrarily large scale. It is important to note the distinction between the structure and data levels. Consider, say, Lotka-Volterra’s model, which essentially consists in (Eqs. 1.1) two ordinary differential equations, complemented by seven subsidiary equations f1(t), f2(x0), f3(y0), f4(b), f5(p), f6(r), f7(d) to set the values of its domain variable t and (input) parameters x0, y0, b, p, r, d. ẋ = x(b− py)ẏ = y(rx− d) (1.1) In a sense, it can be said fairly simple, as it is characterized by a set E of equations and a set V of variables, sized |E| = |V| = 9. Yet, at the data level this model (cf. Chapter 2) can be made very large just by computing its predictions in a fine time resolution and/or along an extended time window.\nAs we shall see shortly, the technical challenges associated with this thesis\ninvolve (not only but) majorly the structure level where, e.g., such Lotka-Volterra model can be abstracted as a deterministic structure S(E ,V) with |S| = 18.2\n2 The structure length |S| is a measure of how dense the hypothesis structure is, comprising the total sum of the number of variables appearing in each equation.\n3 We are really concerned here with models whose structure S is in the order of |S| . 1M , and whose results (data!) shall be difficult to analyze by handicrafted practice. Note that the data level of a model can be set as large as wanted (set the domain resolution and/or extension accordingly), but it shall be necessarily large when its structure is itself large. By ‘large-scale hypotheses’ then we mean tentative deterministic models that are large at structure level.\nOverall, such class of hypotheses can be said to qualify to at least four of\nthe five v’s associated to the notion of big data:3 value, because of their role in advancing science and technology; volume, due to the large scale of modern scientific problems; variety, because of their structural heterogeneity, even when they refer to the same phenomena; and veracity, due to their uncertainty.\nThe idea of managing hypotheses ‘as data’ may sound intriguing and in fact\nit raises a number of research questions of both conceptual and technical nature.4\nWe start by outlining below the conceptual research questions.\nRQ1. How to define and encode hypotheses ‘as data’? What are the sources of\nuncertainty that may be present and should be considered?\nRQ2. How does hypotheses ‘as data’ relate with observational data or, likewise,\nphenomena ‘as data’ from a database perspective?\nRQ3. Does every piece of simulated data qualify as a scientific hypothesis? What\nis the difference between managing ‘simulation’ data from managing ‘hypotheses’ as data?\nRQ4. Is there available a proper (machine-readable) data format we can use to\nautomatically extract mathematically-expressed hypotheses from?\nIt has been a challenge of this thesis to provide reasonable answers to these questions, which are brought together into the vision of hypotheses ‘as data’ (we call it the Υ-DB vision) and its use case that we present in Chapter 2, and experiment with in realistic scenarios in Chapter 6.\n3 The ‘v’ of velocity may appear in connection with machine learning hypotheses, which we discuss in Chapter 6.\n4 We shall keep record of those questions and revisit them in §7.1."
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 4",
      "text" : "The Υ-DB vision formulates the problem of hypothesis encoding as a problem of probabilistic database design. A number of technical questions arise then.\nWe introduce now technical context, materials and methods identified and\nselected in this thesis as a basis to realize the Υ-DB vision in terms of probabilistic database design. We shall outline in the sequel the technical research questions to be answered by the core of the thesis.\n1.1. Problem Space and Specific Goals\nIt has been a goal of this thesis to investigate the capabilities of probabilistic\ndatabases to enable hypothesis data management as a particular case of simulation data management. In the sequel, we first characterize the use case of hypothesis data management and then formulate it in terms of probabilistic DB design."
    }, {
      "heading" : "1.1.1 Simulation data management",
      "text" : "Simulation laboratories provide scientists and engineers with very large, pos-\nsibly huge datasets that reconstruct phenomena of interest in high resolution. Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7]. A core motivation for the delivery of such data is enabling new insights and discoveries through hypothesis testing against observations. Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].\nIn fact, there is a pressing call for innovative technology to integrate (ob-\nserved) data and (simulated) theories in a unified framework [11, 12, 13]. The point has just been raised by leading neuroscientists in the context of the HBP, who are incisive on the compelling argument that massive simulation databases should be constrained by experimental data in corrective loops to test precise hypotheses [14, p. 28]. Fig. 1.2 shows a simplified view of the (data-driven) scientific method life cycle. It distinguishes the phases of exploratory analytics (context of"
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 5",
      "text" : "discovery) and predictive analytics (context of justification), and highlights the loop between hypothesis formulation and testing [15].\nSimulation data, being generated and tuned from a combination of theo-\nretical and empirical principles, has a distinctive feature to be considered when compared to data generated by high-throughput technology in large-scale scientific experiments. It has a pronounced uncertainty component that motivates the use case of hypothesis data management for predictive analytics [10]. Essential aspects of hypothesis data management can be described in contrast to simulation data management as follows — Table 1.1 summarizes our comparison.\n• Sample data. Hypothesis management shall not deal with the same volume\nof data as in simulation data management for exploratory analytics, but only samples of it. This is aligned, for example, with the architectural design of CERN’s particle-physics experiment and simulation ATLAS, where there are four tier/layers of data. The volume of data significantly decreases from (tier-0) the raw data to (tier-3) the data actually used for analyses such as hypothesis testing [8, p. 71-2]. Samples of raw simula-"
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 6",
      "text" : "tion data are to be selected for comparative studies involving competing hypotheses in the presence of evidence (sample observational data). This principle is also aligned with how data is delivered at model repositiories. Since observations are usually less available, only the fragment (sample) of the simulation data that matches in coordinates the (sample) of observations is required out of simulation results for comparative analysis. For instance, we show in §6.2.2 a predictive analytical study extracted from the Virtual Physiological Rat Project (VPR1001-M) comparing sample simulation data (heart rates) from a baroreflex model with observations on a Dahl SS rat strain.5 The simulation is originally set to produce predictions in the time resolution of t∆ = 0.01. But since the observational sample is only as fine as t∆ = 0.1, there is no gain in rendering a predicted sample with t∆ ≥ 0.1 for hypothesis testing. Note that such a ‘sampling’ does not incur in any additional uncertainty as typical of statistical sampling [16].\n• Claim-centered access pattern. In simulation data management the access\npattern is dimension-centered (e.g., based on selected space-time coordinates) and the data is denormalized for faster retrieval, as typical of Data Warehouses (DW’s) and OLAP applications.6 In particular, on account of the so-called ‘big table’ approach, each state of the modeled physical system is recorded in a large, single row of data. This is fairly reasonable for an Extract-Transform-Load (ETL) data ingesture pipeline characterized by batch-, incremental-only updates (see Fig. 1.3). Such a setting is in fact fit for exploratory analytics, as entire states of the simulated system shall be accessed at once (e.g., providing data to a visualization system). Altogether, data retrieval is critical and there is no risk of update anomalies. Hypothesis management, in contrast, should be centered on claims identified within the hypothesis structure w.r.t. available data dependencies. Since the focus is on resolving uncertainty for decision making (which\n5 http://virtualrat.org/computational-models/vpr1001/. 6 On-Line Analytical Processing, as distinguished from OLTP (On-Line Transaction Processing. The latter is meant for transaction processing of daily queries and updates in operational systems, while the former is for analytical queries in Data Warehouses (DW’s) that gather a lot of data collected from different sources for decision making."
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 7",
      "text" : "D1\nD2 . . . Dp\nsim⋃p i=1Ri\nETL\nferent from simulation data management.\nA key point that distinguishes hypothesis management is that a fact or unit of data is defined by its predictive content. That is, every clear-cut predicted fact (w.r.t.available data dependencies) is a claim. Accordingly, the data should be decomposed and organized for a claim-centered access pattern."
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 8",
      "text" : "To anticipate Chapter 2, the synthesis method we have developed in this\nthesis work for processing hypotheses as uncertain and probabilistic data comprises a design-theoretic pipeline (see Fig. 1.4) that extends the one shown in Fig. 1.3."
    }, {
      "heading" : "1.1.2 Probabilistic database design",
      "text" : "Probabilistic databases (p-DB’s) have evolved into mature technology in the\nlast decade with the emergence of new data models and query processing techniques [17]. One of the state-of-the-art probabilistic data models is the U-relational representation system with its probabilistic world-set algebra (p-WSA) implemented in MayBMS [18]. That is an elegant extension of the relational model we shall refer to in this thesis for the management of large-scale uncertain and probabilistic data.\nWe look at U-relations from the point of view of p-DB design, for which no\nformal design methodology has yet been proposed. Despite the advanced state of probabilistic data management techniques, a lack of methods for the systematic design of p-DBs may prevent wider adoption. The availability of design methods has been considered one of the key success factors for the rapid growth of applications in the field of Graphical Models (GM’s) [19], considered to inform research in p-DB’s [17, p. 14]. Analogously, we have proposed to distinguish methods for p-DB design in three classes [10]: (i) subjective construction, (ii) learning from"
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 9",
      "text" : "data, and (iii) synthesis from other kind of formal specification.\nThe first is the less systematic, as the user has to model for the data and\ncorrelations by steering all the p-DB construction process (MayBMS’ use cases [18], e.g., are illustrated that way). The second comprises analytical techniques to extract the data and learn correlations from external sources, possibly unstructured, into a p-DB under some ad-hoc schema. This is the prevalent one up to date, motivated by information extraction and data integration applications [17, p. 10- 3]. In this thesis we present a methodology of the third kind, as we extract data dependencies from some previously existing formal specification (the hypothesis mathematical structure) to synthesize a p-DB algorithmically. Such a type of construction method has been successful, e.g., for building Bayesian Networks [19]. To our knowledge, this thesis is the first synthesis method for p-DB design (cf. §5.6).\nWe shall develop means to extract the specification of a hypothesis and encode it into a U-relational DB for data-driven hypothesis management and analytics. That is, we shall flatten deterministic hypotheses into U-relations.\nThe synthesis method that we have developed for p-DB’s relies on the ex-\ntraction of functional dependencies (fd’s; cf. [20, 21, 22]) that are basic input to algorithmic synthesis.7 For an example of fd, consider relation FALL in Fig. 1.1. There holds an fd t → v s, meaning that values of attribute time t functionally determine values of both attributes velocity v and position s. More precisely, let µ and τ be any two tuples (rows) in an instance of relation (table) FALL. Then it satisfies fd t→ v s iff µ[t] = τ [t] implies µ[v s] = τ [v s]. In our illustrative relation FALL, that fd is, in particular, a key constraint, which means that (values of) t play the role of a key to (provide access to the values of) v and s in the relation.\nA related concept which is also a major one for us is that of normalization\n[20, 21, 22], viz., to ensure that the DB resulting from a design process bears some desirable properties which are associated with some notion of normal form (ibid.). For hypothesis management, the uncertainty has to be modeled and should be normalized so that the uncertainty of one claim may not be undesirably mixed\n7 In fact, it has been considered a critical failure in traditional DB design the lack of techniques to obtain important information such as fd’s in the real world [23, p. 62]."
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 10",
      "text" : "with the uncertainty of another claim. It is expected to involve a processing of the causal dependencies implicit in the given hypothesis structure. We shall introduce in detail such concepts in context when necessary."
    }, {
      "heading" : "1.1.3 Structural equations",
      "text" : "The flattening of the user mathematical models into hypothesis p-DB’s, nonetheless, is not straightforward. It has been a goal of this thesis to investigate proper abstractions on mathematical models in order to (partly) capture their semantics, viz., to an extent that is tailored for hypothesis management (as opposed to, say, model solving). We shall abstract mathematical models into intermediary artifacts that are amenable to be further encoded into fd’s.\nIn fact, given a system of equations with a set of variables appearing in\nthem, in a seminal article Simon introduced an asymmetrical, functional relation among variables that establishes a (so-called) causal ordering [24]. That became known as structural equation models (SEM’s) or just ‘structural equations’ (cf. also [25]). Along these lines, our goal is to extract the causal ordering implicit in the structure of a deterministic hypothesis into a set of fd’s that guides our synthesis of U-relational DB’s. As we shall see throughout this text,\nthe causal ordering we capture and process through fd’s provides causal dependencies implicit in the predictive data that are very useful information to decompose uncertainty for the sake of probabilistic modeling and reasoning."
    }, {
      "heading" : "1.1.4 Uncertainty Model",
      "text" : "In uncertain and probabilistic data management, there are essentially two sources of uncertainty: incompleteness (missing data), and multiplicity (inconsistent data).\nThe kind of uncertainty that is dealt with in this work is the multiplicity of hypothesis trial records identified to be targeted at the same phenomenon record. That is, the uncertainty arises from the existance of competing hypotheses. If multiple hypotheses and trials are inserted for the same phenomenon, the system interprets it as defining a probability distribution."
    }, {
      "heading" : "1.1. PROBLEM SPACE AND SPECIFIC GOALS 11",
      "text" : "Such a probability distribution (usually uniform) on the multiplicity of com-\npeting hypotheses is in accordance with probability theory under possible-worlds semantics [17, Ch. 1]. It is modeled into the U-relational data model and its p-WSA operators, and implemented into the MayBMS system as we shall see in §5.1.8 The conf() aggregate operator, for instance, in spite of the name, performs standard (non-Bayesian) probabilistic inference on such probability distribution. Eventually, however, there is a need to condition the initial probability distribution in the presence of observations. For the conditioning, then, we shall adopt Bayesian inference so that the prior probability distribution can be updated to a posterior.\nThe informal discussion of this section opens the way for a number of tech-\nnical research questions that we outline next.\nRQ5. Is there an algorithm to, given a SEM, efficiently extract its causal order-\ning? What are the computational properties of this problem?\nRQ6. What is the connection between SEM’s and fd’s? Can we devise an en-\ncoding scheme to ‘orient equations’ and then effectively transform one into the other with guarantees? Once we do it, what design-theoretic properties have such a set of fd’s?\nRQ7. Is such fd set ready to be used for p-DB schema synthesis as an encoding\nof the hypothesis causal structure? If not, what kind of further processing we have to do? Can we perform it efficiently by reasoning directly on the fd’s? How does it relate to the SEM’s causal ordering?\nRQ8. Is the uncertainty decomposition required for predictive analytics reducible\nto the structure level (fd processing), or do we need to process the simulated data to identify additional uncertainty factors? Finally, what properties are desirable for a p-DB schema targeted at hypothesis management? Are they ensured by this synthesis method?\nRQ9. Given all such a design-theoretic machinery to process hypotheses into\n(U-)relational DB’s, what properties can we detect on the hypotheses back\n8 Our own system of hypothesis management is to be delivered on top of the MayBMS backend."
    }, {
      "heading" : "1.2. THESIS STATEMENT 12",
      "text" : "at the conceptual level? Do we have now technical means to speak of hypotheses that are “good” in terms of principles of the philosophy of science?\nThe core of this thesis is devoted to answer these questions, and we shall accomplish it throughout Chapters 3, 4 and 5.\n1.2. Thesis Statement\nThe statement of this thesis is that it is possible to effectively encode and\nmanage large deterministic scientific hypotheses as uncertain and probabilistic data. Its key challenges are of both conceptual and technical nature. Conceptually, we provide core, non-obvious abstractions to define and encode hypotheses as data. Technically, we provide a number of algorithms that compose a designtheoretic pipeline to encode hypotheses as uncertain and probabilistic data, and verify their efficiency and correctness. The applicability and effectiveness of our method is demonstrated in realistic case studies in computational science.\nBesides, it is worthwhile highlighting some non-goals of this thesis.\nN1. Although we perform some sort of information extraction [26] for the ac-\nquisition of hypotheses from some model repositories on the web, it is very basic and ad-hoc in order to obtain a testbed for our method. That is, we are not proposing means for the systematic extraction of hypotheses from available sources. In fact we shall outline it in §7.3 as an important direction of future work.\nN2. We do not address solving computational models or numerical analytics\nin any sense. In fact we rely on the numerical solvers (implemented into tools that we use) as ‘transaction processing’ systems, load their computed data into a relational ‘big’ fact table and then render it into U-relational tables synthesized by our method. We do not deal with data visualization either in any sense.\nN3. The efficiency and scalability of query processing in p-DB’s, in particular U-\nrelational’s MayBMS and its p-WSA (which we rely on) is not addressed or"
    }, {
      "heading" : "1.3. THESIS CONTRIBUTIONS 13",
      "text" : "evaluated in this thesis. In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18]. All performance tests carried out in this thesis comprise our design-theoretic techniques for the encoding and synthesis of U-relational hypothesis databases.\nN4. In terms of uncertainty and statistical analysis, we stick to (i) process some\nwell-defined forms of multiplicity in the data which constitute the model of uncertainty dealt with in this work; then (ii) by relying on MayBMS we perform probabilistic inference; and (iii) eventually (at application level) we perform Bayesian inference and so that a posterior probability distribution is propagated through p-DB updates. We do not provide any additional form of uncertainty management. Rather, we manage the data extracted into the system (under user control) and process its uncertainty in terms of the specific sources of uncertainty recognized in Υ-DB (cf. Chapter 2).\n1.3. Thesis Contributions\nThe contributions of this thesis are outlined as follows."
    }, {
      "heading" : "1.3.1 Innovative Contributions",
      "text" : "This thesis presents the vision of hypotheses as data (and its use case) so-\ncalled Υ-DB vision. It has been published in the vision track of VLDB 2014 [10], for its (sic.) potentially high-impact visionary content. The innovative system of Υ-DB has been described in a ‘system prototype demonstration’ paper [28].9"
    }, {
      "heading" : "1.3.2 Technical Contributions",
      "text" : "This thesis presents specific technical developments over theΥ-DB vision. In\nshort, it shows how to encode deterministic hypotheses as uncertain and probabilistic data. Our detailed technical contributions (cf. Chapters 3, 4, and 5) are formulated into a formal method for the design of hypothesis p-DB’s which is described in a technical report [29].10 The method, together with our realistic testbed scenarios and performance evaluation, are yet to be published.\n9 Preliminary version available at CoRR abs/1411.7419. 10 Preliminary version available at CoRR abs/1411.5196."
    }, {
      "heading" : "1.4. THESIS OUTLINE 14",
      "text" : "1.4. Thesis Outline\nThe structure of the remainder of this thesis is outlined for reference.\nChapter 2. [Υ-DB Vision]. The research vision of hypotheses as (uncertain and probabilistic) data, the characterization of its use case, key points and technical challenges are presented.\nChapter 3. [Encoding]. The problem of encoding a hypothesis ‘as data’ given its formal specification (set of mathematical equations) is presented and addressed by an encoding scheme that transforms the equations into fd’s with guarantees in terms of preserving the hypothesis causal structure.\nChapter 4. [Causal Reasoning]. It is presented a technique for causal reasonig as acyclic pseudo-transitive reasoning over the encoded fd’s. It processes the hypothesis causal ordering to find the ‘first causes’ for each of its predictive variables.\nChapter 5. [p-DB Synthesis]. It is presented a technique to address the problem of uncertainty introduction and propagation for the transformation of hypotheses into U-relational databases. The synthesized U-database is shown to bear desirable properties for hypothesis management and predictive analytics.\nChapter 6. [Applicability]. A discussion of applicability, the implementation of the proposed techniques into a prototype system for test and demonstration of the vision realization through realistic case studies are presented.\nChapter 7. [Conclusions]. Research questions are revisited, and the significance and limitations of the thesis with directions to future work and final considerations are discussed.\nChapter 2\nVision: Hypotheses as Data\nHigh-throughput technology and large-scale scientific experiments provide\nscientists with empirical data that has to be extracted, transformed and loaded before it is ready for analysis [1]. In this vision we consider theoretical data, or data generated by simulation from deterministic scientific hypotheses, which also needs to be pre-processed to be analyzed.\nHypotheses as data. In view of the age of data-driven science, we consider\ndeterministic scientific hypotheses from a multi-fold point of view: formed as principles or learned in large scale,1 hypotheses are formulated mathematically and coded in a program that is run to give their decisive form of data (see Fig. 1.1).\nUncertain data. The semantic structure of relation FALL (Fig. 1.1), item\n(iv) can be expressed by the functional dependency (fd) t → v s. This is typical semantics assigned to empirical data in the design of experiment databases. A space-time dimension (like time t in our example) is used as a key to observables (like velocity v and position s). In empirical uncertainty, it is such “physical” dimension keys like t that may be violated, say, by alternative sensor readings.\nHypotheses, however, are tentative explanations of phenomena [15], which\ncharacterizes a different kind of uncertain data. In order to manage such theoretical uncertainty, we shall need two special attributes to compose, say, the epistemological dimension of keys to observables: φ, identifying the studied phenomena; and υ, identifying the hypotheses aimed at explaining them. That is, we shall leverage the semantics of relations like FALL to φ υ t→ v s. This leap is a core abstraction 1 As exhibited, e.g., in the Eureqa project [2].\n16\nin this vision of Υ-DB.\nPredictive data. Scientific hypotheses are tested by way of their predic-\ntions [15]. In the form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomenon. However, for computing predictions, deterministic hypotheses are applied asymmetrically as functions [30]. They take a given valuation over input variables (parameters) to produce values of output variables (predictions). By observing that, we shall seek a principled method to transform the (symmetric) mathematical equations of a hypothesis into (asymmetric) fd’s.\nBy looking at deterministic hypotheses as alternative functions to predict\ndata (see Fig. 2.1), in this vision we shall deal with two sources of uncertainty. Given a well-defined context with a set of alternative hypotheses aimed at explaining (providing predictions for) a selected phenomenon:\n• Theoretical uncertainty,2 comprises selecting the best tentative model\n(function) to produce (the best) data?\n• Empirical uncertainty,3 comprises, for each candidate model, what is the\n(parameter) input setting that calibrates it the best way for the selected phenomenon?\nNote that these two sources of uncertainty are intertwined in that one cannot\n‘clean’ one without cleaning the other — neither theory nor parameters are directly\n2 That is, multiplicity of hypothesis entries associated with a phenomenon. 3 That is, multiplicity of hypothesis trial entries associated with a phenomenon.\n17\nobservable, but only their joint results (the predictions) [15]. In this thesis we aim at providing means to support such kind of ‘integrated’ analytics.\nApplications. Big computational science research programs such as the\nHuman Brain Project,4 or Cardiovascular Mathematics,5 are highly-demanding applications challenged by such theoretical (big) data. Users need to analyze results of hundreds to thousands of data-intensive simulation trials.\nBesides, recent initiatives on web-based model repositories have been foster-\ning large-scale model integration, sharing and reproducibility in the computational sciences (e.g., [31, 32, 33]). They are growing reasonably fast on the web, (i) promoting some MathML-based standard for model specification, but (ii) with limited integrity and lack of support for rating/ranking competing models. For those two reasons, they provide a strong use case for our vision of hypothesis management. The Physiome project [33, 34], e.g., is planned to integrate several large deterministic models of human physiology — a fairly simple model of the human cardiovascular system, e.g., has about 600+ variables.\nAlso, there is a pressing call for deep predictive analytic tools to support users\nassessing what-if scenarios in business enterprises [35]. Deep predictive analytics are based on first principles (deterministic hypotheses) and go beyond descriptive analytics or shallow predictive analytics such as statistical forecasting (ibid.).6\nU-relations. All that ratifies that hypothesis management is a promising\nclass of applications for probabilistic DB’s. The vision of Υ-DB is currently set to be delivered on top of U-relations and probabilistic world-set algebra (p-WSA) [18]. These were developed in the influential MayBMS project.7 As implied by some of its design principles, viz., compositionality and the ability to introduce uncertainty, MayBMS’ query language fits well to hypothesis management. We shall look at it, as previously mentioned, from the point of view of a synthesis method for p-DB design. We shall particularly make use of the repair key operation, which gives\n4 http://www.humanbrainproject.eu/. 5 http://icerm.brown.edu/tw14-1-pdecm. 6 The concept of ‘deep’ predictive analytics is from Haas et al. [35], and is discussed in more detail in §2.6.1. 7 Project website: http://maybms.sourceforge.net/. MayBMS is as a backend extension of PostgreSQL. It offers all the traditional querying capabilities of the latter in addition to the uncertain and probabilistic’s."
    }, {
      "heading" : "2.1. RUNNING EXAMPLE 18",
      "text" : "rise to alternative worlds as maximal-subset repairs of an argument key.\nPredictive analytics tool. In database research (e.g., [36]), uncertainty\nis usually seen as an undesirable property that hinders data quality. We shall refer to U-relations and p-WSA as implemented in MayBMS, nonetheless, to show that the ability to introduce controlled uncertainty into an (otherwise complete) simulation dataset can be a tool for ‘deep’ predictive analytics on a set of competing or alternative hypotheses. Fig.2.2 shows such a scenario of hypotheses ‘as data’ compete to explain a phenomenon ‘as data.’\nAs a roadmap to most of the remainder of this chapter, we claim that if hy-\npotheses can be encoded and identified (see §2.2), and their uncertainty quantified by some probability distribution (see §2.4), then they can be rated/ranked and browsed by the user under selectivity criteria. Furthermore, their probabilities can be conditioned for possibly being re-ranked in the presence of evidence (see §2.5).\n2.1. Running Example\nLet us consider Example 1 for the presentation of the vision.\nExample 1 A research is conducted on the effects of gravity on a falling object in the Earth’s atmosphere. Scientists are uncertain about the precise object’s"
    }, {
      "heading" : "2.1. RUNNING EXAMPLE 19",
      "text" : "density and its predominant state as a fluid or a solid. Three hypotheses are then considered as alternative explanations of the fall (see Fig. 2.3). Due to parameter uncertainty, six simulation trials are run for H1, and four for H2 and H3 each. 2\nThe construction of Υ-DB, a Data Warehouse (DW), requires a simple user description of a research. That is, descriptive records of the phenomena and hypotheses dimensions (see Fig. 2.3) are to be inserted first such that basic referential constraints are satisfied by their associated datasets (fact tables). For instance, each one of the six trial datasets for hypothesis H1 shall reference its id υ = 1 as a foreign key from table HYPOTHESIS further in their synthesized relations.\nFig. 2.4 shows the ‘big’ fact table H1 for hypothesis υ=1 loaded with its trial\ndatasets for phenomenon φ =1. Although table H1 is denormalized for faster data retrieval as usual in DW’s, the extraction of the hypothesis equations allows to render it automatically since all variables must appear in some equation. Now we proceed to the hypothesis encoding and start to address research questions RQ1-4."
    }, {
      "heading" : "2.2. HYPOTHESIS ENCODING 20",
      "text" : "2.2. Hypothesis Encoding\nWe aim at extracting, for each hypothesis, a set of fd’s from its mathematical\nequations. Suppose we are given a set of equations of hypothesis H1 below, and let us examine the set Σ1 of fd’s we target at. 8\nH1. Law of free fall\na(t) = g v(t) = −gt + v0 s(t) = −(g/2)t2 + v0 t + s0\nΣ1 = { φ → g, φ → v0, φ → s0,\ng υ → a, g v0 t υ → v,\ng v0 s0 t υ → s }.\nIn order to derive Σ1 from the equations of H1, we focus on their implicit data dependencies and get rid of constants and possibly complex mathematical constructs. Equation v(t) =−gt+v0, e.g., written this way (roughly speaking), suggests that v is a prediction variable functionally dependent on t (the physical dimension), g and v0 (the parameters). Yet a dependency like g v0 t→ v may hold for infinitely many equations.9 In fact, we need a way to identify H1’s mathematical formulation precisely, i.e., an abstraction of its data-level semantics. This is achieved by introducing hypothesis id υ as a special attribute in the fd (see Σ1).\nThis is a data representation of a deterministic scientific hypothesis. It is built into an encoding scheme (see §3.4) that leverages the semantics of structural equations.\nThe other special attribute, the phenomenon id φ, is supposed to be a key to the values of parameters, i.e., determination of parameters is an empirical, phenomenondependent task. The fd φ → g v0 s0 is to be (expectedly) violated when the user 8 Recall that a rigorous presentation of the method to encode fd set Σ1 is due by Chapter 3. 9 Think of, say, how many polynomials satisfy that dependency ‘signature.’"
    }, {
      "heading" : "2.3. REASONING OVER FD’S 21",
      "text" : "is uncertain about the values of parameters. The same rationale applies to derive Σ2 =Σ3 from the equations of H2, H3 below. These, n.b., vary in structure w.r.t. H1 (e.g., they include parameter D, the object’s diameter).\nH2. Stokes’ law H3. Velocity-squared law a(t) = 0 a(t) = 0\nv(t) = − √ gD/ 4.6×10−4 v(t) = −gD2/ 3.29×10−6\ns(t) = −t √ gD/ 4.6×10−4+s0 s(t) = −(gD2/ 3.29×10−6) t+s0\nΣ2 = Σ3 = { φ → g, φ → D, φ → s0, φ → a,\ng D υ → v, g D s0 t υ → s }.\nThe key point here is that, if the hypothesis structure (set of equations) is\ngiven in a machine-readable format for mathematics, then the method to extract the hypothesis fd set from its equations can be carefully designed based on such hypothesis data representation abstraction. In fact, we shall explore W3C’s MathML as a format for hypothesis specification.10\n2.3. Reasoning over FD’s\nOnce each hypothesis fd set has been extracted, some reasoning is to be performed to discover implicit data dependencies. In fact, dependency theory is equipped with a formal system (cf. §4.1) for reasoning over fd sets like Σ1 and derive other fd’s in its closure Σ+1 . As we elaborate on in Chapter 4, we shall be particularly concerned with the pseudo-transitivity inference rule. Applied over fd’s {φ → g, g υ → a } ⊂ Σ1, for instance, it gives us φ υ → a . This inference allows us to observe that {g} is a ‘factor’ on the uncertainty of a, but {φ υ} should be a dimensional key constraint for values of a.\nIn fact, note that derived fd’s like 〈φ υ, a〉 ∈ Σ+1 , which should be a constraint\non values of a in H1, are (expectedly) violated in the presence of uncertainty: ob-\n10 http://www.w3.org/Math/."
    }, {
      "heading" : "2.4. UNCERTAINTY INTRODUCTION 22",
      "text" : "serve in Fig. 2.4 the multiplicity {32.0, 32.2} of values of a under the same pair (φ 7→ 1, υ 7→ 1), which should functionally determine them in H1. For that reason we admit a special attribute ‘trial id’ tid to be overimposed into H1 for a trivial repair, provisionally, until uncertainty can be introduced in a controlled way by synthesis ‘4U.’ It is meant to identify simulation trials and “pretend” certainty not to lose the integrity of the data. It is under this imposed certainty that the raw simulation trial data is safely loaded from files (see Fig. 2.4). Note, however, how ‘certainty’ is held at the expense of redundancy and, mostly important, opaqueness for predictive analytics (since tid isolates or hides the inconsistency w.r.t. to the violated constraints). This is until the next stage of the Υ-DB construction pipeline, when uncertainty is to be introduced in a controlled manner.\n2.4. Uncertainty Introduction\nBefore we proceed to the uncertainty introduction procedure, note in relation\nH1 (Fig. 2.4), that the predicted acceleration values a are such that an association between the hypothesis and a target phenomenon, viz., (φ 7→ 1, υ 7→ 1) is established. In fact, as of the insertion of each hypothesis trial dataset, the user must set for it a target phenomenon. This may be non-obvious but is quite convenient a design decision for the envisioned system of Υ-DB because hypotheses, as (abstract) universal statements [15], can only be derived predictions from (be empirically grounded) by assigning (callibrating) them onto some real-world phenomenon. This assignment is set at data entry time because in fact it only holds at the data level.11 It is to be recorded in an ‘explanation’ table named H0 by default (see Fig. 2.5, top), being provided with weights for establishing a prior probability distribution which (by user choice) may or may not be uniform.\nThe data transformation of ‘certain’ to ‘uncertain’ relations then starts with\nquery Q0, whose result set is materialized into U-relational table Y0 (see Fig. 2.5). As we introduce in detail in §5.1, U-relations have in their schema a set of pairs (Vi, Di) of condition columns [18] to map each discrete random variable xi created by the repair-key operation to one of its possible values (e.g., x0 7→ 1). The 11 Hypotheses are ‘universal’ by definition [15]. They (must) qualify for a class of different situated phenomena, while its predictive datasets must be very specific (for one specific situation)."
    }, {
      "heading" : "2.4. UNCERTAINTY INTRODUCTION 23",
      "text" : "world table W is internal to MayBMS’ and automatically stores their marginal probabilities. The formal semantics the repair-key operation is given in §5.1.\nQ0. create table Y0 as select φ, υ from (repair key φ in H0 weight by Conf);\nThe possible-world semantics of p-DB’s (cf. §5.1) can be seen as a gener-\nalization of data cleaning. In the context of p-DB’s [17], data cleaning does not have to be one-shot — which is more error-prone [37]. Rather, it can be carried out gradually, viz., by keeping all mutually inconsistent tuples under a probability distribution (ibid.) that can be updated in face of evidence until the probabilities of some tuples eventually tend to zero to be eliminated. This motivates Remark 1.\nRemark 1 Consider U-relational table Y0 (Fig. 2.5). Note that it abstracts the goal of a data-intensive hypothesis evaluation study, or the scientific method itself [15], as the repair of each φ as a key. That is, in Υ-DB users can develop their research directly upon data with support of query and update capabilities to rate/rank their hypotheses υ w.r.t. each φ, until the relationship r(φ, υ) is repaired to be a function f : Φ→ Υ from each phenomenon φ to its best explanation υ. 2\nGiven a ‘big’ fact table such as H1, we need to identify/group the correlated\ninput attributes under independent uncertainty units, viz., ‘u-factors,’ each one associated with a random variable.12 We illustrate that by means of query Q1, which materializes view Y1[g] for (let g = Zi) identified u-factor Zi ⊆ Z in H1[φ, Z].\n12 An attribute can be inferred ‘input’ (viz., a parameter) by means of fd reasoning (cf. §3.4)."
    }, {
      "heading" : "2.4. UNCERTAINTY INTRODUCTION 24",
      "text" : "Q1. create table Y1[g] as select U.φ, U.g from (repair key φ in\n(select φ, g, count(*) as Fr from H1 group by φ, g) weight by Fr) as U;\nThe result set of Q1 is stored in Y1[g], see Fig. 2.6. Note that the possible values of g are mapped to random variable x1, and that table H1 is considered source for a joint probability distribution (on the values of H1’s input parameters) which may not be uniform: we count the frequency Fr of each possible value of a u-factor Zi ⊆ Z (as done for g in Q1) and pass it as argument to the weight-by construct.\nSo far, we have presented informally the procedure of u-factorization. Now\nwe proceed to u-propagation — both are presented rigorously in Chapter 5. We consider g υ → a ∈ Σ1 again in order to synthesize predictive U-relation Y1[a]. Since a is functionally determined by υ and g only, and these are independent, we propagate their uncertainty onto a into Y1[a] by query Q2.\nQ2. create table Y1[a] as select H1.φ, H1.υ, H1.a from H1, Y0, Y1[g] as G\nwhere H1.φ=Y0.φ and H1.υ=Y0.υ and G.φ=H1.φ and G.g=H1.g;\nQuery Q′2 (not shown) then selects φ, υ and a from Yi[a] for each i = 1..3. The result sets of Q2 and Q ′ 2 (resp. Y1[a] and Y [a]) are shown in Fig. 2.7."
    }, {
      "heading" : "2.5. PREDICTIVE ANALYTICS 25",
      "text" : "Compare relations H1[a] and Y1[a]. By accounting for the correlations cap-\ntured in the fd g υ → a, we could propagate onto a the uncertainty coming from the hypothesis and the only parameter a is sensible to, thus precisely situating tuples of Y1[a] in the space of possible worlds. The same is done for predictive attributes v and s. In the end, Υ-DB shall be ready for predictive analytics, i.e., with all competing predictions as possible alternatives which are mutually inconsistent.\nA key point here is that all the synthesis process is amenable to algorithm\ndesign. Except for the user ‘research’ description, the Υ-DB construction is fully automated based on the hypothesis structure (set of equations) and the raw hypothesis trial data.\n2.5. Predictive Analytics\nUsers of Example 1, has to be able, say, to query phenomenon φ = 1 w.r.t.\npredicted position s at specific values of time t by considering all hypotheses υ admitted. That is illustrated by query Q3, which creates integrative table Y [s]; and by query Q4, which computes the confidence aggregate operation [18] for all s tuples where t = 3 (Fig. 2.8 shows Q4’s result, apart from column Posterior).\nThe confidence on each hypothesis for the specific prediction of Q4 is split\ndue to parameter uncertainty such that they sum up back to its total confidence. For H2 and H3, e.g., we have {g D s0 t υ → s} ⊂ Γ, where Γ = Σ2 = Σ3. Since g and D are the parameter uncertainty factors of s (s0 is certain), with 2 possible values (not shown) each, then there are only 2×2 = 4 possible s tuples for H2 and H3 each. Considering all hypotheses υ for the same phenomenon φ, the confidence"
    }, {
      "heading" : "2.5. PREDICTIVE ANALYTICS 26",
      "text" : "values sum up to one in accordance with the laws of probability.\nUsers can make informed decisions in light of such confidence aggregates,\nwhich are to be eventually conditioned in face of evidence (observed data). Example 2 features such kind of Bayesian conditioning for discrete random variables mapped to the possible values of predictive attributes (like position s) whose domain are continuous.\nExample 2 Suppose position s = 2250 feet is observed at t = 3 secs, with standard deviation σ = 20. Then, by applying Bayes’ theorem for normal mean with a discrete prior [16], Prior is updated to Posterior (see Fig. 2.8). 2"
    }, {
      "heading" : "2.6. RELATED WORK 27",
      "text" : "The procedure uses normal density function (2.1), with (say) σ = 20, to get\nthe likelihood f(y |µk) of each alternative prediction of s from Y [s] as mean µk given y at observed s = 2250. Then it applies Bayes’ rule (2.2) to get the posterior p(µk | y) [16].\nf(y |µk) = 1√\n2πσ2 e− 1 2σ2 (y−µk)2 (2.1)\np(µk | y) = f(y |µk) p(µk) / ∑n i=1 f(y |µi) p(µi) (2.2)\nIn the general case (cf. examples shown in Chapter 6), we actually have phenomenon ‘as data:’ a sample of independent observed values y1, ..., yn (e.g., Brazil’s population observed by census over the years). Then, the likelihood f(y1, ..., yn |µk)\nfor each competing trial µk, is computed as a product ∏n j=1 f(yj |µkj) of the single likelihoods f(yj |µkj) [16]. Bayes’ rule is then settled by (2.3) to compute the posterior p(µk | y1, ..., yn) given prior p(µk).\np(µk | y1, . . . , yn) = ∏n\nj=1 f(yj |µkj) p(µk) m∑ i=1 n∏ j=1 f(yj |µij) p(µi) (2.3)\nAs a result, the prior probability distribution assigned to u-factors via repair key\nis to be eventually conditioned on observed data. This is an applied Bayesian inference problem that translates into a p-DB update one to induce effects of posteriors back to table W . In a first prototype of the Υ-DB system (cf. 6.3), we accomplish it by performing Bayesian inference at application level and then applying p-WSA’s update (a variant of SQL’s update) into MayBMS. This solution is good enough to let us complete use case demonstrations of Υ-DB.13\n2.6. Related Work\nThe vision of managing hypotheses as data has some roots in Porto and\nSpaccapietra [38], who motivated a conceptual data model to support (the socalled) in silico science by means of a scientific model management system. We\n13 Cf. Chapter 6, and §6.3 in particular."
    }, {
      "heading" : "2.6. RELATED WORK 28",
      "text" : "discuss now the work we understand to be mostly related to our vision of datadriven hypothesis management and analytics."
    }, {
      "heading" : "2.6.1 Models-and-data",
      "text" : "Haas et al. [35] provide an original long-term perspective on the evolution of\ndatabase technology. They characterize the data typically managed by traditional DB systems as a record about the past, not a conclusion or an insight or a solution (ibid.). In the context of scientific databases, e.g., their position is suggestive that DB technology has been designed for empirical data, not the theoretical data generated by simulation from domain-specific principles or scientific hypotheses.\nThey recognize current DB technology to have raised the art of scalable\n‘descriptive’ analytics to a very high level; but point out, however, that nowadays (sic.) what enterprises really need is ‘prescriptive’ analytics to identify optimal business, policy, investment, and engineering decisions in the face of uncertainty. Such analytics, in turn, shall rest on deep ‘predictive’ analytics that go beyond mere statistical forecasting and are imbued with an understanding of the fundamental mechanisms that govern a system’s behavior, allowing what-if analyses [35]. In sum, there is a pressing call for deep predictive analytic tools in business enterprises as much as in science’s.\nIn comparison with the Υ-DB vision, Haas et al. are proposing a long-term\nmodels-and-data research program to pursue data management technology for deep predictive analytics. They discuss strategies to extend query engines for model execution within a (p-)DB. Along these lines, query optimization is understood as a more general problem with connections to algebraic solvers.\nOur framework in turn essentially comprises an abstraction and technique\nfor the encoding of hypotheses as data. It can be understood (in comparison) as putting models strictly into a flattened data perspective. For that reason it has been directly applicable by building upon recent work on p-DBs [17]. In principle, it can be integrated into, say, the OLAP layer of the models-and-data project."
    }, {
      "heading" : "2.6.2 Scientific simulation data",
      "text" : "As previsouly mentioned, science’s ETL is distinguished by its unfrequent, incremental-only updates and by having large raw files as data sources [8]. Challenges for"
    }, {
      "heading" : "2.6. RELATED WORK 29",
      "text" : "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics. The extreme scale of the raw data has motivated such non-conventional approaches for data exploration, viz., the ‘immersive’ query processing (move the program to the data) [6, 40], or ‘in situ’ query processing in the raw files [41, 42]. Both exploit the spatial structure of the data in their indexing schemes.\nThat line of research is motivated for equipping scientist end-users for an\nimmediate interaction with their very large simulation datasets.14 The NoDB approach, in particular, argues to eliminate such ETL phase (viz., the loading) for a direct access to data ‘in situ’ in the raw data files [42]. In fact, data exploration is a fundamental use case of data-driven science.\nNonetheless, being generated from first principles or learned deterministic\nhypotheses, simulation data has a pronounced uncertainty component that motivates a another use case, viz., the case of hypothesis management and predictive analytics [35, 10]. As we have motivated in §1.1, the latter requires probabilistic DB design for enabling uncertainty decomposition (factorization).\nHypothesis management shall not deal with the same volume of data as in\nsimulation data management for exploratory analytics, but samples of it (cf. Table 1.1 for a comparison). For instance, in CERN’s particle-physics experiment ATLAS there are four tier/layers of data management. The volume of data significantly decreases from the (tier-0) raw data to the (tier-3) data actually used for analyses such as hypothesis testing [8, p. 71-2].\nOverall, the overhead incurred in loading samples of raw simulation trial\ndatasets into a p-DB is justified for enabling a principled hypothesis evaluation and rating/ranking according to the scientific method."
    }, {
      "heading" : "2.6.3 Hypothesis encoding",
      "text" : "Our framework is comparable with Bioinformatics’ initiatives that address\nhypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses\n14 Sometimes phrased ‘here is my files, here is my queries, where are my results?’ [41]."
    }, {
      "heading" : "2.7. SUMMARY: KEY POINTS 30",
      "text" : "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.\nThe Robot Scientist relies on rule-based logic programming analytics to au-\ntomatically generate and test RDF-encoded hypotheses of the kind ‘gene G has function A’ against RDF-encoded empirical data [44]. HyBrow is likewise, but hypotheses are formulated by the user about biological events [45]. SWAN in turn disfavors analytic techniques for hypothesis evaluation and focus on descriptive aspects: hypotheses are high-level natural language statements retrieved from publications. Each ‘hypothesis’ is associated with lower-level ‘claims’ (both RDFencoded) that are meant to support it on the basis of some empirical evidence (RDF-encoded gene/protein data). In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].\nAll of them though, consist in some ad-hoc RDF encoding of sequence and\ngenome analysis hypotheses under varying levels of structure (viz., from ‘gene G has function A’ statements to free text). Our framework in turn consists in the U-relational encoding of hypotheses from mathematical equations, which is (to our knowledge) the first work on hypothesis relational encoding.\nFinally, as for hypothesis evaluation and comparison analytics, the Υ-DB\nvision is distinguished in terms of its Bayesian inference approach. The latter has been pointed out as a major direction for the improvement of the Bioinformatics’ initiatives just mentioned (cf. [43, p. 13]), and is in fact an influential model of decision making for hypothesis evaluation [15, p. 220].\n2.7. Summary: Key Points\nWe outline some key points in the Υ-DB vision:\n• ‘Structured deterministic hypotheses’ are encoded as theoretical data and\ndistinguished from empirical data by the introduction of an epistemological dimension into their semantic structure."
    }, {
      "heading" : "2.7. SUMMARY: KEY POINTS 31",
      "text" : "• Two sources of uncertainty are considered: theoretical uncertainty, origi-\nnating from competing hypotheses; and empirical uncertainty, derived from alternative simulation trials on each hypothesis for the same phenomenon.\n• A method to extract the structure of a hypothesis can be carefully designed\nbased on a hypothesis data representation and shall be reducible in terms of machine-readable format for mathematical modeling, viz., W3C’s MathML, which we shall adopt as a standard for hypothesis specification.\n• We have seen that the controlled introduction of uncertainty into simula-\ntion data is amenable to algorithm design and then reducible to a designtheoretic synthesis method for the construction of U-relational DB’s.\n• Simulation data can be modeled as hypothesis data whenever it is associ-\nated with a target phenomenon. As the same phenomenon may happen to be associated with many such hypotheses, the research activity can be modeled as a data cleaning problem in p-DB’s.\nEssentially, the vision of Υ-DB comprises a design-theoretic pipeline (Fig.\n1.4). For the insertion of a hypothesis k, we shall be given a MathML-compliant structure Sk together with its simulation trial datasets D`k in raw files (e.g., .mat, .csv). Then we apply an Extract-Transform-Load (ETL) automatic procedure to generate the hypothesis ‘big’ fact table Hk under the trial id’s.\nThe extracted equations are firstly encoded into fd’s. Then, at any time, as\nmany hypotheses may have been inserted into the system, the uncertainty introduction (U-intro) procedure can be applied to process the encoded fd’s and synthesize the ‘uncertain’ U-relations that are to be eventually conditioned on observations.\nNote, in Fig. 1.4, that the ETL procedure is operated in a ‘local’ view for\neach hypothesis k, while the U-intro procedure and the conditioning are operated in the ‘global’ view of all available hypotheses k = 1..n. The pipeline opens up four main tracks of technical research challenges from the ETL stage on, viz., (i) hypothesis encoding and (ii) causal reasoning over fd’s, (iii) p-DB synthesis and (iv) conditioning. We address in the sequel the three first track of challenges in depth. The problem of conditioning is outlined for further work in §7.3.\nChapter 3\nHypothesis Encoding\nIn this chapter we address the problem of hypothesis encoding. In §3.1 we\nintroduce notation and basic concepts of structural equations and the problem of causal ordering. In §3.2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon’s classical approach [24, 48] is intractable. In §3.3 then we build upon a less notorious approach of Nayak’s [49] and borrow an efficient algorithm for it that fits very well our use case for hypothesis encoding. In §3.4 we develop an encoding scheme that builds upon the idea of structural equations through an original abstraction of hypotheses ‘as data.’ In §3.5 we present experiments that attest how the encoding scheme works in practice for large hypotheses. In §3.6 we discuss related work. In §3.7 we summarize the results of this chapter.\n3.1. Preliminaries: Structural Equations\nGiven a system of mathematical equations involving a set of variables, to build a structural equation model (SEM) is, essentially, to establish a one-to-one mapping between equations and variables [24]. That shall enable further detecting the hidden asymmetry between variables, i.e., their causal ordering. For instance, Einstein’s famous equation E = mc2 states the equivalence of mass and energy, summarizing a theory that can be imputed two different asymmetries (for different applications), say, given a fixed amount of mass m = m0 (and recall c is a constant), predict the particle’s relativistic rest energy E; or given the particle’s rest energy, predict its mass or potential for nuclear fission."
    }, {
      "heading" : "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 33",
      "text" : "To stress the point, consider Newton’s second law F = ma in such a scalar\nsetting. The modeler can either use it to compute (predict), say, acceleration values given an amount of mass and different force intensities, or to predict force intensities given a fixed acceleration (e.g., for testing an engineered dynamometer). The point here is that Newton’s equation is not enough to derive predictions. That is, it has a number of variables |V| = 3, which is larger than |E| = 1. It must be completed with two more equations in order to qualify as an (applied) hypothesis ‘as data.’ Although usually it is interpreted an asymmetry towards a, technically, there is nothing in its semantics to suggest so.1 Compare the two systems given in Fig. 3.1.2 In sum, the causal ordering of any system of equations is not to be guessed, as it can be inferred. In this chapter we rely on previous work (mostly AI’s work, viz., [24, 49, 48]) and adapt it for the encoding of hypotheses into fd’s.\nDef. 1 A structure is a pair S(E ,V), where E is a set of equations over set V of variables, |E| ≤ |V|, such that:\n(a) In any subset of k equations of the structure, at least k different variables\nappear;\n(b) In any subset of k equations in which r variables appear, k ≤ r, if the\nvalues of any (r−k) variables are chosen arbitrarily, then the values of the 1 As the equality construct ‘=’ is used as a predicate, not an assignment operator. 2 We shall introduce the notion of ‘directed causal graphs’ shortly."
    }, {
      "heading" : "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 34",
      "text" : "remaining k variables can be determined uniquely — finding these unique values is a matter of solving the equations.\nDef. 2 Let S(E ,V) be a structure. We say that S is self-contained or complete if |E| = |V|.\nIn short, we are interested in systems of equations that are ‘structural’ (Def. 1) and ‘complete’ (Def. 2), viz., that has as many equations as variables and no subset of equations has fewer variables than equations.3\nComplete structures can be solved for unique sets of values of their variables.\nIn this work, however, we are not concerned with solving sets of mathematical equations at all, but with processing their causal ordering in view of U-relational DB design. Simon’s concept of causal ordering has its roots in econometrics studies (cf. [24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48]. In this thesis we translate the problem of causal ordering into the language of data dependencies, viz., into fd’s.\nDef. 3 Let S be a structure. We say that S is minimal if it is complete and there is no complete structure S ′⊂ S.\nDef. 4 The structure matrix AS of a structure S(E ,V), with f1, f2, . . . , fn ∈ E and x1, x2, . . . , xm ∈ V , is a n × m matrix of 1’s and 0’s in which entry aij is non-zero if variable xj appears in equation fi, and zero otherwise.\nElementary row operations (e.g., row multiplication by a constant) on the\nstructure matrix may hinder the structure’s causal ordering and then are not valid in general [24]. This also emphasizes that the problem of causal ordering is not about solving the system of mathematical equations of a structure, but identifying its hidden asymmetries.\nDef. 5 Let S(E ,V) be a complete structure. Then a total causal mapping over S is a bijection ϕ : E → V such that, for all f ∈ E , if ϕ(f) = x then x∈ V ars(f). 3 Also, we expect the systems of equations given as input to be ‘independent’ in the sense of Linear Algebra. In our context, that means systems that can only have non-redundant equations. In that case, if some subset of equations has fewer variables than equations, then the system must be ‘overconstrained.’"
    }, {
      "heading" : "3.1. PRELIMINARIES: STRUCTURAL EQUATIONS 35",
      "text" : "Simon has informally described an algorithm (cf. [24]) that, given a complete structure S(E ,V), can be used to compute a partial causal mapping ϕp from partitions on the set of equations to same-cardinality partitions on the set of variables. As shown by Dash and Druzdzel [48], the causal mapping returned by Simon’s (socalled) Causal Ordering Algorithm (COA) is not total when S has variables that are strongly coupled (because they can only be determined simultaneously). They also have shown that any total mapping ϕ over S must be consistent with COA’s partial mapping ϕp [48]. The latter is made partial by design (merge strongly coupled variables into partitions or clusters) in order to force its induced causal graph Gϕp to be acyclic. Algorithm 1, COAt, is a variant of Simon’sCOA adapted to illustrate our use case. It returns a total causal mapping ϕ, instead of a partial causal mapping. We illustrate it through Example 3 and Fig. 3.2.\nExample 3 Consider structure S(E ,V) whose matrix is shown in Fig. 3.2a. Note that S is complete, since |E|= |V|= 7, but not minimal. The set of all minimal subsets S ′⊂ S is Sc={ {f1}, {f2}, {f3} }. By eliminating the variables identified at recursive step k, a smaller structure T ⊂ S is derived. Compare the partial causal mapping eventually returned by COA, ϕp={ 〈{f1}, {x1}〉, 〈{f2}, {x2}〉, 〈{f3}, {x3}〉, 〈{f4, f5}, {x4, x5}〉, 〈{f6}, {x6}〉, 〈{f7}, {x7}〉 }, to the total causal mapping returned by COAt, ϕ ={〈f1, x1〉, 〈f2, x2〉, 〈f3, x3〉, 〈f4, x4〉, 〈f5, x5〉, 〈f6, x6〉, 〈f7, x7〉}."
    }, {
      "heading" : "3.2. THE PROBLEM OF CAUSAL ORDERING 36",
      "text" : "Algorithm 1 COAt as a variant of Simon’s COA.\n1: procedure COAt(S : structure over E and V) Require: S given is complete, i.e., |E| = |V| Ensure: Returns total causal mapping ϕ : E → V\n2: ϕ← ∅, Sc ← ∅ 3: for all minimal S ′ ⊂ S do 4: Sk ← Sk ∪ S ′ . store minimal structures S ′ found in S 5: V ′ ← S ′(V) 6: for all f ∈ S ′(E) do 7: x← any xa ∈ V ′ 8: ϕ← ϕ ∪ 〈f, x〉 9: V ′ ← V ′ \\ {x}\n10: T ← S \\ ⋃ S′∈Sk S ′ 11: if T 6= ∅ then 12: return ϕ ∪ COAt(T ) 13: return ϕ\nSince x4 and x5 are strongly coupled (see Fig.3.2b), COAt maps them arbitrarily (e.g., it could be f4 7→ x5, f5 7→ x4 instead). Such total mapping ϕ renders a cycle in the directed causal graph Gϕ induced by ϕ (see Fig.3.3). 2\nx1 x2 x3\nx4 x5\nx6 x7\nin a given structure (cf. line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in"
    }, {
      "heading" : "3.2. THE PROBLEM OF CAUSAL ORDERING 37",
      "text" : "Boolean matrices. Simon’s approach, however, as we shall see next, is not the only way to cope with the problem of causal ordering.\nIn fact, in order to study the computational properties of SEM’s and the\nproblem of causal ordering, we observe that any structure S(E ,V) satisfying Def. 1 can be modeled straightforwardly as a bipartite graph G = (V1 ∪ V2, E), where the set E of equations and the set V of variables are the disjoint vertex sets, i.e., V1 7→ E , V2 7→ V , and E 7→ S is the edge set connecting equations to the variables appearing in them. Fig. 3.4 shows the bipartite graph G corresponding to the structure given in Example 3 — for a comprehensive text on graph concepts and its related algorithmic problems, cf. Even [53].\nA biclique (or complete bipartite graph) is a bipartite graph G = (A∪B, E)\nsuch that for every two vertices a ∈ A, b ∈ B, we have (a, b) ∈ E [53]. Note that for balanced bicliques, i.e., when |A| = |B| = K, the degree deg(u) of any vertex u ∈ A ∪B must be deg(u) = |A| = |B| = K.\nRecent approaches to co-clustering problems (e.g., [54]) have come with the\nnotion of pseudo-biclique (also called ‘quasi-biclique’), which is a relaxation of the biclique concept to allow some less rigid notion of connectivity than the ‘complete connectivity’ required in a biclique. Now, recall that Simon’s COA(t) needs to find, at each recursive step, all minimal subsets S ′ ⊆ S. Theorem 1 situates this particular computational task in terms of its complexity, which for |E ′| = |V ′| ≥ 2 is equivalent to find, at each recursive step, the minimal-size ‘pseudo-bicliques’"
    }, {
      "heading" : "3.2. THE PROBLEM OF CAUSAL ORDERING 38",
      "text" : "(i.e., with the least K ≥ 2) in its corresponding bipartite graph (e.g., see Fig. 3.4). Here we take, in Def. 6, a specific notion of pseudo-biclique.\nDef. 6 Let G = (A∪B, E) be a bipartite graph. We say that G is a K-balanced pseudo-biclique if |A| = |B| = K with |E| ≥ 2K and, for all vertices u ∈ A∪B, deg(u) ≥ 2.\nNow we state (originally) the balanced pseudo-biclique problem (BPBP) as a decision problem as follows.\n(BPBP). Given a bipartite graph G = (V1 ∪ V2, E) and a positive integer K ≥ 2, does G contain a K-balanced pseudo-biclique?\nLemma 1 The balanced pseudo-biclique problem (BPBP) is NP-Complete.\nProof 1 We show (by restriction [55]) that the BPBP is a generalization of the balanced biclique problem (BBP), referred ‘balanced complete bipartite subgraph’ problem [55, GT24, p. 196], which is shown to be NP-Complete by means of a transformation from ‘clique’ [56, p. 446]. The restriction from BPBP to BBP (special case) is made by requiring (cf. Def. 6) either (a) |E| = K2 or (b) deg(u) = K, for K ≥ 2,4 which are equivalent ways of enforcing the inquired K-balanced pseudo-biclique to be a K-balanced biclique. 2\nWe introduce another hypothesis structure (see Fig. 3.5) to illustrate the correspondence between the pseudo-biclique property and COA’s algorithmic approach as elaborated in the proof of Theorem 1.\nTheorem 1 Let S(E ,V) be a complete structure. Then the extraction of its causal ordering by Simon’s COA(S) is NP-Hard.\nProof 2 We show that, at each recursive step k of COA, to find all non-trivial minimal subsets (i.e., |E ′| ≥ 2) translates into an optimization problem associated with the decision problem BPBP, which we know by Lemma 1 to be NP-Complete. See Appendix §A.1.1. 2 4 Note that clearly, for any positive integer K ≥ 2, we have for K2 ≥ 2K."
    }, {
      "heading" : "3.3. TOTAL CAUSAL MAPPINGS 39",
      "text" : "Nonetheless, the problem of causal ordering can be solved efficiently by means\nof a different, less notorious approach due to Nayak [49], which we introduce and build upon next.\n3.3. Total Causal Mappings\nThe problem of causal ordering can be solved in polynomial time by (i)\nfinding any total causal mapping ϕ : E → V over structure S given (cf. Def. 5); and then (ii) by computing the transitive closure C+ϕ of the set Cϕ (cf. Eq. 3.1) of direct causal dependencies induced by ϕ.\nCϕ= { (xa, xb) | there exists f ∈ E such that ϕ(f) = xb and xa ∈ V ars(f) } (3.1)\nDef. 7 Let S(E ,V) be a structure with variables xa, xb ∈ V , and ϕ a total causal mapping over S inducing set of direct causal dependencies Cϕ and its transitive closure C+ϕ . We say that (xb, xa) is a direct causal dependency in S if (xb, xa) ∈ Cϕ, and that (xb, xa) is a causal dependency in S if (xb, xa) ∈ C+ϕ .\nIn other words, (xa, xb) is in Cϕ iff xb direct and causally depends on xa,\ngiven the causal asymmetries induced by ϕ. Those notions open up an approach to causal reasoning that fits very well to our use case, which is aimed at encoding hypothesis structures into fd sets and then performing (symbolic) causal reasoning in terms of acyclic pseudo-transitive reasoning over fd’s (cf. Chapter 4).5 For it to be effective, nonetheless, we shall need to ensure some properties of total causal mappings first.\n5 Note that it differs from AI research (e.g., [48]) geared for reasoning over GM’s."
    }, {
      "heading" : "3.3. TOTAL CAUSAL MAPPINGS 40",
      "text" : "For a given structure S, there may be multiple total causal mappings over\nS (recall Example 3). But the causal ordering of S must be unique (see Fig. 3.3). Therefore, a question that arises is whether the transitive closure C+ϕ is the same for any total causal mapping ϕ over S. Proposition 1, originally from Nayak [49], ensures that is the case.\nProposition 1 Let S(E ,V) be a structure, and ϕ1 : E → V and ϕ2 : E → V be any two total causal mappings over S. Then C+ϕ1 = C + ϕ2 .\nProof 3 The proof is based on an argument from Nayak [49], which we present in arguably much clearer way (see Appendix, §A.1.2). Intuitively, it shows that if ϕ1 and ϕ2 differ on the variable an equation f is mapped to, then such variables, viz., ϕ1(f) and ϕ2(f), must be causally dependent on each other (strongly coupled). 2\nAnother issue is concerned with the precise conditions under which total\ncausal mappings exist (i.e., whether or not all variables in the equations can be causally determined). In fact, by Proposition 2, based on Nayak [49] apud. Hall [53, p. 135-7], we know that the existence condition holds iff the given structure is complete.\nBefore proceeding to it, let us refer to Even [53] to briefly introduce the\nadditional graph-theoretic concepts which are necessary here. A matching in a graph is a subset of edges such that no two edges in the matching share a common node. A matching is said maximum if no edge can be added to the matching (without hindering the matching property). Finally, a matching in a graph is said ‘perfect’ if every vertex is an end-point of some edge in the matching — in a bipartite graph, a perfect matching is said a complete matching.\nProposition 2 Let S(E ,V) be a structure. Then a total causal mapping ϕ : E → V over S exists iff S is complete.\nProof 4 We observe that a total causal mapping ϕ : E → V over S corresponds exactly to a complete matching M in a bipartite graph B = (V1 ∪ V2, E), where V1 7→ E , V2 7→ V , and E 7→ S. In fact, by Even apud. Hall’s theorem (cf. [53, 135-7]), we know that B has a complete matching iff (a) for every subset of vertices"
    }, {
      "heading" : "3.3. TOTAL CAUSAL MAPPINGS 41",
      "text" : "F ⊆ V1, we have |F | ≤ |E(F )|, where E(F ) is the set of all vertices connected to the vertices in F by edges in E; and (b) |V1| = |V2|. By Def. 1 (no subset of equations has fewer variables than equations), and Def. 2 (number of equations is the same as number of variables), it is easy to see that conditions (a) and (b) above hold iff S is a complete structure. 2\nThe problem of finding a maximum matching is a well-studied algorithmic\nproblem. In this thesis we adopt the Hopcroft-Karp algorithm [57], which is known to be polynomial-time, bounded by O( √ |V1|+ |V2| |E|).6 That is, we handle the problem of total causal mapping by (see Alg. 2) translating it to the problem of maximum matching in a bipartite graph (in linear time) and then applying the Hopcroft-Karp algorithm to get the matching and finally translate it back to the total causal mapping, as suggested by the proof of Proposition 2.\nAlgorithm 2 Find a total causal mapping for a given structure.\n1: procedure TCM(S : structure over E and V) Require: S given is a complete structure, i.e., |E| = |V| Ensure: Returns a total causal mapping ϕ\n2: B(V1 ∪ V2, E)← ∅ 3: ϕ← ∅ 4: for all 〈f,X〉 ∈ S do . translates the structure S to a bipartite graph B 5: V1 ← V1 ∪ {f} 6: for all x ∈ X do 7: V2 ← V2 ∪ {x} 8: E ← E ∪ {(f, x)} 9: M ← Hopcroft-Karp(B) . solves the maximum matching problem 10: for all (f, x) ∈M do . translates the matching to a total causal mapping 11: ϕ← ϕ ∪ {〈f, x〉} 12: return ϕ\nFig. 3.6 shows the complete matching found by the Hopcroft-Karp algorithm for the structure given in Example 3.\n6 The Hopcroft-Karp algorithm solves maximum matching in a bipartite graph efficiently as a problem of finding maximum flow in a network (cf. [53, p. 135-7], or [58, p. 664-9]).\nCorollary 1 summarizes the results we have so far.\nCorollary 1 Let S(E ,V) be a complete structure. Then a total causal mapping ϕ : E → V over S can be found by (Alg. 2) TCM in time that is bounded by\nO( √ |E| |S|).\nProof 5 Let B = (V1 ∪ V2, E) be the bipartite graph corresponding to complete structure S given to TCM, where V1 7→ E , V2 7→ V , and E 7→ S. The translation of S into B is done by a scan over it. This scan is of length |S| = |E|. Note that number |E| of edges rendered is precisely the length |S| of structure, where the denser the structure, the greater |S| is. The re-translation of the matching computed by internal procedure Hopcroft-Karp, in turn, is done at expense of |E| = |V| ≤ |S|. Thus, it is easy to see that TCM is dominated by the maximum matching algorithm\nHopcroft-Karp, which is known to be O( √ |V1|+ |V2| |E|), i.e., O( √ |E|+ |V| |S|).\nSince S is assumed complete, we have |E| = |V| then √ |E|+ |V| = √ 2 √ |E|.\nTherefore, TCM must have running time at most O( √ |E| |S|). 2\nRemark 2 Let S(E ,V) be a complete structure. Then we know (cf. Proposition 2) that a total causal mapping over S exists. Let it be defined ϕ , TCM(S). Then the causal ordering implicit in S shall be correctly extracted (cf. Proposition 1) by processing the causal dependencies induced by ϕ, as we show in Chapter 4. 2\nNow we are ready to accomplish the hypothesis encoding into fd’s, as we\nshow next."
    }, {
      "heading" : "3.4. THE ENCODING SCHEME 43",
      "text" : "3.4. The Encoding Scheme\nWe shall encode variables as relational attributes and map equations onto\nfd’s through total causal mappings. Let Z be a set of attribute symbols such that Z ' V , where S(E ,V) is a complete structure; and let φ, υ /∈ Z be two special attribute symbols kept to identify (resp.) phenomena and hypotheses. We are explicitly distinguishing symbols in Z, assigned by the user into structure S, from epistemological symbols φ and υ. Then we consider a sense of Simon’s into the nature of scientific modeling and interventions [24], summarized in Def. 8.\nDef. 8 Let S(E ,V) be a structure and x ∈ V be a variable. We say that x is exogenous if there exists an equation f ∈ E such that V ars(f) = {x}. In this case f can be written f(x) = 0, and must be mapped to x in any total causal mapping ϕ over S. We say that x is endogenous otherwise.\nRemark 3 introduces an interpretation of Def. 8 with a data dependency flavor.\nRemark 3 The values of exogenous variables (attributes) are to be determined empirically, outside of the system (proposed structure S). Such values are, therefore, dependent on the phenomenon id φ only. The values of endogenous variables (attributes) are in turn to be determined theoretically, within the system. They are dependent on the hypothesis id υ and shall be dependent on the phenomenon id φ as well indirectly. 2\nAs introduced in §2.2, the encoding scheme we are presenting here is not\nobvious. It goes beyond Simon’s structural equations to abstract the data-level semantics of mathematical deterministic hypotheses. Whereas Simon’s structural equations are able to represent only linear equations, our encoding scheme can represent non-linear equations and arbitrarily complex mathematical operators by means of its data representation of deterministic hypotheses.\nFor instance, take non-linear equation y = a x2 and suppose that, considering\nthe context of its complete system of equations, (Alg. 2) TCM maps it onto variable y. Then, by an abstraction of the equation semantics, we shall encode it into fd"
    }, {
      "heading" : "3.4. THE ENCODING SCHEME 44",
      "text" : "a x υ → y. That is, the hypothesis identifier υ captures the data-level semantics of the hypothesis equation.7\nWe encode complete structures into fd sets by means of (Alg. 3) h-encode.\nFig. 4.1 presents an fd set defined Σ , h-encode(S), encoding the same structure S from Example 3.\nAlgorithm 3 Hypothesis encoding.\n1: procedure h-encode(S : structure over E and V , D : domain variables) Require: S given is a complete structure, i.e., |E| = |V| Ensure: Returns a non-redundant fd set Σ\n2: Σ← ∅ 3: ϕ← TCM(S) 4: for all 〈f, x〉 ∈ ϕt do 5: Z ← X \\ {x}, where 〈f,X〉 ∈ S 6: if Z = ∅ or Z ⊆ D then . x is exogenous 7: if x /∈ D then . supress φ-fd for dimensions like time t 8: Σ← Σ ∪ 〈Z ∪ {φ}, x〉 9: else . x is endogenous\n10: Σ← Σ ∪ 〈Z ∪ {υ}, x〉 11: return Σ\nNow we study the design-theoretic properties of the encoded fd sets. We\nshall make use of the concept of ‘canonical’ fd sets (also called ‘minimal’ [20, p. 390]), see Def. 9.\nDef. 9 Let Σ be an fd set. We say that Σ is canonical if:\n7 Note that, without the hypothesis id, infinitely many equations fit the pattern a x→ y."
    }, {
      "heading" : "3.5. EXPERIMENTS 45",
      "text" : "(a) each fd in Σ has the form X→ A, where |A| = 1;\n(b) For no 〈X,A〉 ∈ Σ we have (Σ− {〈X,A〉})+ = Σ+;\n(c) for each fd X→ A in Σ, there is no Y ⊂ X such that (Σ\\{X→ A}∪{Y →\nA})+ = Σ+.\nFor an fd set satisfying such properties (Def. 9) individually, we say that it is (a) singleton-rhs, (b) non-redundant and (c) left-reduced. It is said to have an attribute A in X that is ‘extraneous’ w.r.t. Σ if it is not left-reduced (Def. 9-c) [22, p. 74]. Finally, an fd X→ Y in Σ is said trivial if Y ⊆ X. Note that the presence of a trivial fd in a an fd set is sufficient to make it redundant.\nTheorem 2 Let Σ be an fd set defined Σ , h-encode(S) for some complete structure S. Then Σ is non-redundant and singleton-rhs but may not be left-reduced (then may not be canonical).\nProof 6 We show that properties (a-b) of Def. 9 must hold for Σ produced by (Alg. 3) h-encode, but property (c) may not hold (i.e., encoded fd set Σ may not be left-reduced). See Appendix, §A.1.3. 2\nWe draw attention to the significance of Theorem 2, as it sheds light on a\nconnection between Simon’s complete structures [24] and fd sets [20]. In fact, we continue to elaborate on that connection in next chapter to handle causal ordering processing symbolically by causal reasoning over fd’s.\n3.5. Experiments\nFig. 3.8 shows the results of experiments we have carried out in order to study how effective the procedure of hypothesis encoding is in practice, in particular its behavior for hypotheses whose structure S has been randomly generated over orders of magnitude |S| ≈ 2k, to have length up to |S| ≈ 220 . 1M . The largest structure considered, with |S| ≈ 1M , has been generated to have exactly |E| = 2.5K."
    }, {
      "heading" : "3.6. RELATED WORK 46",
      "text" : "We have executed ten runs for each tested order of magnitude, and then\ntaken its mean running time in ms.8 The plot is shown in Fig. 3.8 in logscale base 2. In fact a a near-, sub-quadratic slope is expected for the curve structure length |S| × time.\nThese scalability results are compatible with the computational complexity of h-encode, which is (cf. Corollary 1) bounded by O( √ |E| |S|).9\n3.6. Related Work\nModeling physical and socio-economical systems as a set of equations is a traditional modeling approach, and a very large bulk of models exist up to date. Simon’s early work on structural equations and causal ordering comprises a specific notion of causality aimed at further contributing to the potential of such modeling approach (cf. [59]). It is meant for identifying influences among variables (or their values) that are implicit in the system model for enabling informed interventions. These may apply either to the system (phenomenon) under study, or to the model itself (say, when its predictions are not approximating observations very well).\nSignificant research effort has been devoted to causal modeling and reasoning\nin the past decades in both statistics and AI (cf. [25, 19]). The notion of causality used can be traced back to the early work of Simon’s and others in Econometrics.\n8 The experiments were performed on a 2.3GHZ/4GB Intel Core i5 running Mac OS X 10.6.8. 9 Note that, for any arbitrary structure S(E ,V), we have |E| ≤ |S| ≤ |E|2. So, in worst case (the densest structure possible) we have |S| = |E|2 and then can establish a time bound function of |E| only, viz., O(|E|2 √ |E|)."
    }, {
      "heading" : "3.6. RELATED WORK 47",
      "text" : "Nonetheless, there are two important differences to be emphasized:\n• Such work is majorly devoted to deal with (statistical) qualitative hypothe-\nses, not (deterministic) quantitative hypotheses;\n• The causal model is assumed as given or is derived from data, instead of\nbeing converted or synthesized from a set of equations.\nThese are both core differences that also apply to our work in comparison\nto the bulk of existing work in probabilistic DB’s. Our main point here, though, is to clarify the technical context and state of the art of the problem of causal ordering. A few works have been concerned with extracting a causal model out of some previous existing formal specification such as a set of equations. This is a reason why causal ordering has been an yet barely studied problem from the computational point of view.\nDash and Druzdzel revisit the problem and re-motivate it in light of modern\napplications [48]. First, they provide a formal description of how Simon’s COA gives a summary of the causal dependencies implicit in a given SEM. That is, in clustering the strongly coupled variables into a causal graph, COA provides a condensed representation of the causal model implicit in the given SEM. They show then that any valid total causal mapping produced for a given SEM must be consistent with COA’s partial causal mapping.\nYet, the serious problem is that the algorithm turns out to be intractable. In\nfact, no formal study of COA’s computational properties can yet be found in the literature. In this thesis we have obtained the (negative) hardness result that it is intractable, which turns out to be compatible with Nayak’s intuition (sic.) that it is a worst-case exponential time algorithm (cf. [60, p. 37]).\nInspired on Serrano and Gossard’s work on constraint modeling and reason-\ning [61], Nayak reports an approach that is provably quite effective to process the causal ordering: extract a total causal mapping and then compute the transitive closure of the direct causal dependencies. In this thesis we build upon it to perform causal reasoning in terms of a form of transitive reasoning. Such approach fits very well to our use case, viz., the synthesis of p-DB’s from fd’s. As we show"
    }, {
      "heading" : "3.7. SUMMARY OF RESULTS 48",
      "text" : "in Chapter 4, we process the causal ordering of a hypothesis structure (abstracted as a SEM) in terms of acyclic causal reasoning over fd’s and prove its correctness. This is enabled by the encoding scheme presented in this chapter.\n3.7. Summary of Results\nIn this chapter we have studied and developed an encoding scheme to process\nthe mathematical structure of a deterministic hypothesis into a set of fd’s towards the encoding of hypotheses ‘as data.’ Then we have studied the design-theoretic properties held by such an encoded fd set. We list the results achieved as follows.\n• By Theorem 1, we know (an original hardness result) that Simon’s ap-\nproach to process the causal ordering of a structure is intractable;\n• By building upon on the work of Simon [24] and Nayak [49] (cf. Proposi-\ntions 1 and 2), we have framed an approach to efficiently extract the basic information (a total causal mapping) for processing the causal ordering implicit in the mathematical structure of a deterministic hypothesis;\n• By Corollary 1, we know how to process the complete structure of a hypothesis into a total causal mapping in time that is bounded by O( √ |E| |S|).\nThat is, the machinery of hypothesis encoding is provably suitable for very large hypothesis structures.\n• By Theorem 2, which studies the design-theoretic properties of the encoded\nfd sets, we have unraveled the connection between Simon’s complete structures and fd sets to further explore it in next chapter.\n• We have performed experiments (cf. Fig. 3.8) to study how effective the\napproach is in practice, or how it scales for hypotheses whose structure S is randomly generated to have length up to the order of |S| . 1M .10\n10 The tests were up to this order only because of the hardware limitations of our experimental settings. In theory (cf. complexity time bounds), larger structures can be handled very efficiently.\nChapter 4\nCausal Reasoning over FD’s\nIn this chapter we present a technique to address the problem of causal ordering processing in order to enable the synthesis of U-relational DB’s. In §4.1 we introduce Armstrong’s classical inference system to reason over fd’s. In §4.2 we develop the core concept and algorithm of the folding of an fd set, as a method for acyclic causal reasoning over fd’s. In §4.3 we show its connections (equivalence) with causal reasoning. In §4.4 we present experiments on how the method behaves in practice. §4.5 we discuss related work. In §4.6 we conclude the chapter.\n4.1. Preliminaries: Armstrong’s Inference Rules\nAs usual notational conventions from the DB literature [20, 21], we write X, Y, Z to denote sets of relational attributes and A,B,C to denote singleton attribute sets. Also, we write XY as shorthand for X ∪ Y .\nFunctional dependency theory relies on Armstrong’s inference rules (or ax-\nioms) of (R0) reflexivity, (R1) augmentation and (R2) transitivity, which forms a sound and complete inference system for reasoning over fd’s [20]. From R0-R2 one can derive additional rules, viz., (R3) decomposition, (R4) union and (R5) pseudo-transitivity.\nR0. If Y ⊆ X, then X→ Y ; R1. If X→ Y , then XZ→ Y Z; R2. If X→ Y and Y → W, then X→ W ; R3. If X→ Y Z, then X→ Y and X→ Z; R4. If X→ Y and X→ Z, then X→ Y Z;"
    }, {
      "heading" : "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 50",
      "text" : "R5. If X→ Y and Y Z→ W, then XZ→ W .\nGiven an fd set Σ, one can obtain Σ+, the closure of Σ, by a finite application of rules R0-R5. We are concerned with reasoning over an fd set in order to process its implicit causal ordering. The latter, as we shall see in §4.3, can be performed in terms of (pseudo-)transitive reasoning. Note that R2 is a particular case of R5 when Z=∅, then we shall refer to R5 reasoning and understand R2 included. The next definition opens up a way to compute Σ+ very efficiently.\nLet Σ be an fd set on attributes U , with X ⊆ U . Then X+, the attribute\nclosure of X w.r.t. Σ, is the set of attributes A such that 〈X,A〉 ∈ Σ+. Bernstein has long given algorithm (Alg. 4) XClosure to compute X+. It is polynomial time in |Σ| · |U | (cf. [62]), where Σ and U are (resp.) the given fd set and the attribute set over which it is defined. A tighter time bound (linear time in |Σ| · |U |) is achievable as discussed further in Remark 4.\nAlgorithm 4 Attribute closure X+ (cf. [20, p. 388]).\n1: procedure XClosure(Σ: fd set, X : attribute set)\nRequire: Σ is an fd set, X is a non-empty attribute set Ensure: X+ is the attribute closure of X w.r.t. Σ\n2: size ← 0 3: Λ← ∅ 4: X+ ← X 5: while size < |X+| do 6: size ← |X+| 7: Σ← Σ \\∆ 8: for all 〈Y, Z〉 ∈ Σ do 9: if Y ⊆ X+ then\n10: ∆← ∆ ∪ {〈Y, Z〉} . consumes fd 11: X+ ← X+ ∪ Z 12: return X+\n4.2. Acyclic Pseudo-Transitive Reasoning\nAs discussed in the previous chapter, we shall process the causal ordering in terms of computing the transitive closure of each endogenous variable (predictive attribute). Before we proceed to that, we shall develop some machinery to reason over fd’s in terms of Armstrong’s rule R5 (pseudo-transitivity). We shall then"
    }, {
      "heading" : "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 51",
      "text" : "demonstrate the correspondence between this kind of reasoning with causal reasoning shortly in the sequel.\nDef. 10 Let Σ be a set of fd’s on attributes U. Then Σ., the pseudo-transitive closure of Σ, is the minimal set Σ. ⊇ Σ such that X→ Y is in ΣB, with XY ⊆ U , iff it can be derived from a finite (possibly empty) application of rule R5 over fd’s in Σ. In that case, we may write X .−→ Y and omit ‘w.r.t. Σ’ if it can be understood from the context.\nWe are in fact interested in a very specific proper subset of ΣB, say, a kernel\nof fd’s in ΣB that gives a “compact” representation of the causal ordering implicit in Σ. Note that, to characterize such special subset we shall need to be careful w.r.t. the presence of cycles in the causal ordering.\nDef. 11 Let Σ be a set of fd’s on attributes U, and 〈X,A〉 ∈ ΣBwith XA ⊆ U . We say that X→A is folded (w.r.t. Σ), and write X #−→ A, if it is non-trivial and for no Y ⊂ U with Y + X, we have Y → X and X 6→ Y in Σ+.\nThe intuition of Def. 11 is that an fd is folded when there is no sense in going\non with pseudo-transitive reasoning over it anymore (nothing new is discovered). Given an fd X→ A in fd set Σ, we shall be able to find some folded fd Z→ A by applying (R5) pseudo-transitivity as much as possible while ruling out cyclic or trivial fd’s in some clever way.\nDef. 12 Let Σ be an fd set on attributes U , and 〈X,A〉 ∈ Σ be an fd with XA ⊆ U . Then,\n(a) A#, the (attribute) folding of A (w.r.t.Σ) is an attribute set Z⊂ U such that Z #−→ A;\n(b) Accordingly, Σ#, the folding of Σ, is a proper subset Σ#⊂ ΣB such that\nan fd 〈Z,A〉 ∈ ΣB is in Σ# iff X #−→ A for some Z ⊂ U .\nExample 4 (continued). Fig. 4.1 shows an fd set Σ (left) and its folding Σ# (right). Note that the folding can be obtained by computing the attribute folding"
    }, {
      "heading" : "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 52",
      "text" : "for A in each fd X→ A in Σ. We illustrate below some reasoning steps to partially compute an attribute folding considering the subset of fd’s in Σ with φ /∈ X.\n1. x1 x2 x3 x5 υ → x4 [given] 2. x1 x3 x4 υ → x5 [given] 3. x5 υ → x7 [given] 4. x1 x3 x4 υ → x7 [R5 over (2), (3)] 5. x1 x2 x3 x5 υ → x7 [R5 over (1), (4)]"
    }, {
      "heading" : "6. ∴ x1 x2 x3 x4 υ → x7 [R5 over (2), (5)] .",
      "text" : "Note that (6) is still amenable to further application of R5, say over (1), to\nderive (7) x1 x2 x3 x5 υ→ x7. However, even though (1) and (6) have (resp.) the form (1) X → A and (6) Y → B with Y .−→ X, we have X .−→ Y as well which characterizes a cycle that fetches nothing into Y .1 In fact, if we consider only the fd’s 1-3 given, then (6) itself satisfies Def. 11 and then is folded. The same holds, e.g., for (1) by an empty application of R5. 2\nLemma 2 Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. If 〈X,A〉 ∈ Σ, then A#, the attribute folding of A (w.r.t. Σ) exists and is unique.\nProof 7 See Appendix, §A.2.1. 2 1 Note, when at the step deriving (6) by R5 over (2), (5), that such cycle was not yet formed."
    }, {
      "heading" : "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 53",
      "text" : "We give an original algorithm (Alg. 5) to compute the folding of an fd set. At its core there lies (Alg. 6) AFolding, which can be understood as a non-obvious variant of XClosure (cf. Alg. 4) designed for acyclic pseudo-transitivity reasoning. In order to compute the folding of attribute A in fd 〈X,A〉 ∈ Σ, algorithm AFolding can be seen as backtracing the causal ordering implicit in Σ towards A. Analogously, in terms of the directed graph Gϕ induced by the causal ordering (see Fig. 3.3), that would comprise graph traversal to identify the nodes x ∈ V that have xa ∈ V in their reachability, i.e., x xa. Rather, AFolding’s processing of the causal ordering is fully symbolic based on Armstrong’s rewrite rule R5.\nExample 5 Cyclicity in an fd set Σ may have the effect of making its folding Σ# to degenerate to Σ itself. For instance, consider Σ={A→ B, B→ A}. Note that Σ is canonical, and AFolding (w.r.t. Σ) is B given A, and A given B. That is, Σ#= Σ. 2\nAlgorithm 5 Folding of an fd set.\n1: procedure folding(Σ: fd set)\nRequire: Σ given encodes complete structure S Ensure: Returns fd set Σ#, the folding of Σ\n2: Σ# ← ∅ 3: for all 〈X, A〉 ∈ Σ do 4: Z ← AFolding(Σ, A) 5: Σ# ← Σ# ∪ 〈Z, A〉 6: return Σ#"
    }, {
      "heading" : "4.2. ACYCLIC PSEUDO-TRANSITIVE REASONING 54",
      "text" : "Algorithm 6 Folding of an attribute w.r.t. an fd set.\n1: procedure AFolding(Σ: fd set, A : attribute)\nRequire: Σ is parsimonious Ensure: Returns A#, the attribute folding of A (w.r.t. Σ)\n2: ∆← ∅ . consumed fd’s 3: Λ← ∅ . consumed attrs. 4: A? ← A . stores attrs. A is found to be ‘causally dependent’ on (cf. §4.3) 5: size ← 0 6: while size < |A?| do . halts when A(i+1) =A(i) 7: size ← |A?| 8: Σ← Σ \\∆ 9: for all 〈Y, B〉 ∈ Σ do 10: if B ∈ A? then 11: ∆← ∆ ∪ {〈Y, B〉} . consumes fd 12: A? ← A? ∪ Y 13: Λ← Λ ∪B . consumes attr. 14: for all C ∈ Y do 15: if C ∈ Λ and B ∈ X for 〈X,C〉 ∈ ∆ then . cyclic fd 16: Λ← Λ \\B . reingests it to simulate cyclic app. of R5 17: return A? \\ Λ\nTheorem 3 Let S(E ,V) be a complete structure, and Σ an fd set encoded given S. Now, let 〈X,A〉 ∈ Σ. Then AFolding(Σ, A) correctly computes A#, the attribute folding of A (w.r.t. Σ) in time O(|S|2).\nProof 8 For the proof roadmap, note that AFolding is monotone (size of A? can only increases) and terminates precisely when A(i+1) = A(i), where A(i) denotes the attributes in A? at step i of the outer loop. The folding A# of A at step i is A(i) \\ Λ(i). We shall prove by induction, given attribute A from fd X→ A in parsimonious Σ, that A? \\ Λ returned by AFolding(Σ, A) is the unique attribute folding A# of A. See Appendix, §A.2.2. 2\nRemark 4 Let Σ be an arbitrary fd set on attribute set U . Beeri and Bernstein gave a straightforward optimization to (Alg. 4) XClosure to make it linear in |Σ|·|U | (cf. [63, p. 43-5]), where |Σ| · |U | is the maximum length for a string encoding all the fd’s. Note that the actual length of such string in our case is exactly |S|. The"
    }, {
      "heading" : "4.3. EQUIVALENCE WITH CAUSAL ORDERING 55",
      "text" : "optimization mentioned applies likewise to (Alg. 6) AFolding.2 That is, AFolding can be implemented to run in linear time in |S|. 2\nCorollary 2 Let S(E ,V) be a complete structure, and Σ an fd set encoded given S. Then algorithm folding(Σ) correctly computes Σ#, the folding of Σ in time that is f(|S|) Θ(|E|), where f(|S|) is the time complexity of (Alg. 6) AFolding.\nProof 9 See Appendix, §A.2.3. 2\nFinally, it shall be convenient to come with a notion of parsimonious fd sets\n(see Def. 13), which is suggestive of a distinguishing feature of such mathematical information systems in comparison with arbitrary information systems.\nDef. 13 Let Σ be set of fd’s on attributes U . Then, we say that Σ is parsimonious if it is canonical and, for all fd’s 〈X,A〉 ∈ Σ with XA ⊆ U , there is no Y ⊂ U such that Y 6= X and 〈Y,A〉 ∈ Σ.\nProposition 3 then shall be useful further in connection with the concept of\nthe folding.\nProposition 3 Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Let Σ# be the folding of Σ, then Σ# is parsimonious.\nProof 10 See Appendix, §A.2.4. 2\n4.3. Equivalence with Causal Ordering\nNow we show the equivalence of acyclic pseudo-transitive reasoning with causal ordering processing. We start with Theorem 4, which establishes the equivalence between the notion of causal dependency and the fd encoding scheme presented in Chapter 3.\n2 We omit its tedious exposure here. In short, it shall require one more auxiliary data structure to keep track, for each fd not yet consumed, of how many attributes not yet consumed appear in its rhs."
    }, {
      "heading" : "4.3. EQUIVALENCE WITH CAUSAL ORDERING 56",
      "text" : "Theorem 4 Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Then xa, xb ∈ V are such that xb is causally dependent on xa, i.e., (xa, xb) ∈ C+ϕ iff there is some non-trivial fd 〈X,B〉 ∈ ΣB with A ∈ X, where B 7→ xb and A 7→ xa.\nProof 11 We prove the statement by induction. We consider first the ‘if’ direction, and then its ‘only if’ converse. See Appendix §A.2.5. 2\nDef. 14 then gives useful terminology for a neat concept towards our goal in\nthis chapter.\nDef. 14 Let S(E ,V) be a structure with variables xa, xb ∈ V , and ϕ a total causal mapping over S inducing set of direct causal dependencies Cϕ and its transitive closure C+ϕ . We say that xa is a first cause of xb in S if (xa, xb) ∈ C+ϕ and, for no x ∈ V , we have (x, xa) ∈ C+ϕ .\nProposition 4 connects the notion of first cause with those of exogenous and\nendogenous variables introduced in Chapter 3.\nProposition 4 Let S(E ,V) be a structure with variable x ∈ V . Then x can only be a first cause of some y ∈ V if x is exogenous. Accordingly, any variable y ∈ V can only have some first cause x ∈ V if it is endogenous.\nProof 12 Straightforward from definitions, see Appendix §A.2.6. 2\nNote that exogenous variables are encoded into fd’s X → A with φ ∈ X.\nSince the values of such variables are assigned “outside” the system (cf. Remark 3), they are devoid of indirect causal dependencies and then have no uncertainty except for their own. Thus, we have not to be concerned at all with processing the causal (uncertainty) chaining towards them. Our goal is rather find the first causes of the endogenous variables (predictive attributes).\nWe shall need then the terminology of Def. 15, and then we introduce Lemma\n3 paving the way to our goal.\nDef. 15 Let S be a structure, and Σ be a set of fd’s encoded over it. Then Υ(Σ), the υ-projection of Σ, is the subset of fd’s X → A such that υ ∈ X. Accordingly, Φ(Σ), the φ-projection of Σ, is the subset of fd’s X → A such that υ /∈ X."
    }, {
      "heading" : "4.3. EQUIVALENCE WITH CAUSAL ORDERING 57",
      "text" : "Fig. 4.2 illustrates the υ-projection of an fd set and the folding applied over\nsuch fd subset in order to compute the first causes of endogenous variables.\nLemma 3 Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Then a variable xa ∈ V can only be a first cause of some variable xb ∈ V , where 〈X,B〉 ∈ Σ, and B 7→ xb, A 7→ xa, if either (i) A ∈ X or (ii) A /∈ X but there is 〈Z,C〉 ∈ ΣBwith A ∈ Z and C ∈ X.\nProof 13 We prove the statement by construction out of Theorem 4, see Appendix §A.2.7. 2\nFinally, Theorem 5 further clarifies the purpose of the folding and its meaning\nin terms of causal ordering.\nTheorem 5 Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Now, let B be an attribute that encodes some variable xb ∈ V . If 〈X,B〉 ∈ Υ(Σ)#,3 then every first cause xa of xb (if any) is encoded by some attribute A ∈ X.\nProof 14 We show that the existance of a missing first cause xc of xb for folded X #−→ B, where B 7→ xb and C 7→ xc but C /∈ X leads to a contradiction. See Appendix §A.2.8. 2 3 Note that the folding is taken w.r.t. the υ-projection of Σ, then xb where B 7→ xb is an endogenous variable."
    }, {
      "heading" : "4.4. EXPERIMENTS 58",
      "text" : "Remark 5 Observe that, on the one hand, the goal of computing the transitive closure C+ϕ of a set of induced causal dependencies Cϕ is to derive the entire causal ordering of a given structure. The goal of folding, on the other hand, is not to discover all variables (attributes) a given variable (attribute) is causally dependent on, but only all of its first causes (if any). 2\nIn particular, the results just shown comprise a method to compute, for each\nendogenous variable (predictive attribute), all of its first causes. This is a core goal of the reasoning device developed in this chapter in order to enable the automatic synthesis of hypotheses as uncertain and probabilistic (U-relational) data.\n4.4. Experiments\nFig. 4.3 shows the results of experiments we have carried out in order to study how effective the causal reasoning over fd’s is in practice, in particular its behavior for hypotheses whose structure S has been randomly generated over orders of magnitude |S| ≈ 2k, to have length up to |S| ≈ 220 . 1M . The largest structure considered, with |S| ≈ 1M , has been generated to have exactly |E| = 2.5K. Like in the experiments of the previous chapter, we have executed ten runs for each tested order of magnitude, and then taken its mean running time in ms.4\nThe plot is shown in Fig. 3.8 is in logscale base 2. Notice the linear rate\nof growth across orders of magnitude (base 2) from 1K to 1M sized structures. For a growth factor of 2 in structure length (doubled), the time required by causal reasoning grows a factor of 2 (doubled as well). These scalability results are compatible with the computational complexity of folding, which is bounded by O(|S|2). Yet, that is a bit overestimated time bound as we see in the plot of Fig. 4.3.\n4.5. Related Work\nThe concept of fd set folding and the design of (Alg. 6) AFolding as a not quite\nobvious variant of XClosure, is an original approach to the problem of processing the causal ordering of a hypothesis via acyclic pseudo-transitive reasoning over fd’s. To the best of our knowledge, such a specific form of fd reasoning was an\n4 The experiments were performed on a 2.3GHZ/4GB Intel Core i5 running Mac OS X 10.6.8."
    }, {
      "heading" : "4.5. RELATED WORK 59",
      "text" : "yet unexplored problem in the database research literature (reasoning over fd’s is extensively covered in Maier [22]).\nRecent years have seen the emergence of some foundational work in causality\nin databases [64]. It is motivated for improving DB usability in terms of providing users with explanations to query answers (and non-answers). Essentially, the idea is borrowed from AI work on causality (cf. §3.6) to identify causal ordering between tuples. Given a query and its result set, the system should be able to explain to the user what tuples ‘caused’ that answer, or why possibly expected tuples are missing. That requires causal chain of tuples for a given query, which can be computationally expensive as the database instance can be very large. For conjunctive queries, the causality is said to be computed very efficiently [65].\nA more specific problem addressed by Kanagal et. al is the so-called sensitiv-\nity analysis [66], which is aimed at establishing a more refined connection between the query answer (output) and elements of the DB instance (input) for supporting user interventions. Instead of providing the user with causes and non-causes, the goal is to enable the user to know how changes in the input affect the output. This line of work is strongly related to the vision of ‘reverse data management’ [67].\nCausal reasoning in the presence of constraints (viz., fd’s) is an yet unex-\nplored topic, though called for as worth of future work by Meliou et al. [64, p. 3]. The fd’s are rich information that can be exploited for the sake of explanation and sensitivity analysis. Once they are available, it is intuitive that the search space of such problems shall be significantly reduced."
    }, {
      "heading" : "4.6. SUMMARY OF RESULTS 60",
      "text" : "In fact, our encoding of equations into fd’s captures the causal chain from\nexogenous (input) to endogenous (output) tuples in at schema level. Nonetheless our form of causal reasoning over fd’s is geared for hypothesis management and analytics, from a uncertainty management point of view. A concrete connection to causality in DB’s is not yet established.\n4.6. Summary of Results\nIn this chapter we have studied and developed a technique for acyclic causal\nreasoning over fd’s. We list the results achieved as follows.\n• We have developed principled concepts and a core algorithm, viz., (Alg.\n5) the folding, in order to perform acyclic pseudo-transitive reasoning over fd’s. This is towards an efficient method for causal reasoning, yet elegant as a database formalism for the systematic construction of hypothesis probabilistic DB’s.\n• We have given a reasonably tight time bound for the behavior of such\nreasoning device in terms of the structure given as input. We have established (cf. Theorem 3, Corollary 2) the time bound of O(|S|2) for the folding algorithm.\n• We have shown the correctness of the folding algorithm in connection with\ncausal reasoning (cf. Theorem 4, Theorem 5).\n• We have defined the core notion of first causes (cf. Def. 14, Proposi-\ntion 4), which is meant to guide the procedure of U-intro (Chapter 5) by precisely capturing the uncertainty factors on endogenous variables (predictive attributes). This is similar, yet markedly different from computing the transitive closure of causal dependencies (cf. Remark 5).\n• We have performed experiments (cf. Fig. 4.3) to study how effective the\napproach of causal reasoning over fd’s is in practice, or how it scales for hypotheses whose structure S is randomly generated to have length up to the order of |S| . 1M . The experiments show that the time bound, though already effective for very large structures, are a bit overestimated.\nChapter 5\nProbabilistic Database Synthesis\nIn this chapter we present a technique to synthesize hypothesis U-relations. At this stage of the pipeline, relational schema H is loaded with datasets computed from the hypotheses under alternative trials (input settings). The challenge is how to model or design its probabilistic version (i.e., render the U-relations Y ) so that it is suitable for data-driven hypothesis management and analytics.\nIn §5.1 we introduce U-relational DB’s. In §5.2 we present a running example\nto illustrate the uncertainty introduction procedure (U-intro in the pipeline, cf. Fig. 1.4). In §5.3 we present the technique to factorize the uncertainty present in the ‘big’ fact table in terms of the well-defined uncertainty factors. In §5.4 then we show how to propagate such uncertainty into the predictive attributes properly, i.e., based on their first causes detected as shown in Chapter 4. In §5.6, we discuss related work. Finally, in §5.7 we conclude the chapter. 1\n5.1. Preliminaries: U-Relations and Probabilistic WSA\nThree remarkable features of U-relations are: expressiveness (being closed under positive relational algebra queries); succinctness (efficient storage of a very large number of possible worlds through vertical decompositions to support attributelevel uncertainty); and efficient query processing (including confidence computation) [18].\nA U-relational database or U-database is a finite set of structures,\nW = { 〈R11, ..., R1m, p[1]〉, ..., 〈Rn1 , ..., Rnm, p[n] 〉 }, 1 We postpone the presentation of experiments on p-DB synthesis (U-intro as a whole) to §6.4."
    }, {
      "heading" : "5.1. PRELIMINARIES: U-RELATIONS AND PROBABILISTIC WSA 62",
      "text" : "of relations Ri1, ..., R i m and numbers 0 < p\n[i] ≤ 1 such that ∑\n1≤ i≤n p [i] = 1. An\nelement Ri1, ..., R i m, p [i] ∈W is a possible world, with p[i] being its probability [18].\nProbabilistic world-set algebra (p-WSA) consists of the operations of rela-\ntional algebra, an operation for computing tuple confidence conf, and the repairkey operation for introducing uncertainty — by giving rise to alternative worlds as maximal-subset repairs of an argument key [18].\nLet R`[U ] be a relation, and XA ⊆ U . For each possible world 〈R1, ..., Rm,\np〉 ∈ W , let A ∈ U contain only numerical values greater than zero and let R` satisfy the fd (U \\ A)→ U . Then, repair-key is:\nJrepair-keyX@A(R`)K(W) := { 〈R1, ..., R`, Rm, R̂` [U \\ A], p̂ 〉 } ,\nwhere 〈R1, ..., R`, Rm, p〉 ∈ W , R̂` is a maximal repair of fd X → U in R`, and p̂ = p · ∏ t∈R̂` t.B∑ s∈R` : s.X = t.X s.B . U-relations (cf. Fig. 5.1) have in their schema a set of pairs (Vi, Di) of condition columns (cf. [18]) to map each discrete random variable xi to one of its possible values (e.g., x1 7→1). The world table W stores their marginal probabilities (cf. the notion of pc-tables [17, Ch. 2]). For an illustration of the data transformation from certain to uncertain relations, consider query (5.1) in p-WSA’s extension of relational algebra, whose result set is materialized into U-relation Y0 as shown in (Fig. 5.1).\nY0 := πφ,υ(repair-keyφ@Conf(H0) ). (5.1)\nAlso, let R[ViDi | sch(R) ], S[Vj Dj | sch(S) ] be two U-relations, where R. ViDi is the union of all pairs of condition columns ViDi in R, then operations of selection Jσψ(R) K, projection J πZ(R) K, and product JR × SK issued in relational algebra are rewritten in positive relational algebra on U-relations:\nJσψ(R)K := σψ(R[ViDi | sch(R)]); JπZ(R) K := πViDi Z(R); JR×SK := π(R.ViDi ∪ S.ViDi)→V D ∪ sch(R)∪ sch(S)(R ./R.ViDi is consistent with S.Vj Dj S)."
    }, {
      "heading" : "5.2. RUNNING EXAMPLE 63",
      "text" : "If R and S have k and ` pairs of condition columns each, then JR × SK returns a U-relation with k + ` such pairs. If k = 0 or ` = 0 (or both), then R or S (or both) are classical relations, but the rewrite rules above apply accordingly. All that rewriting is parsimonious translation (sic. [18]): the number of algebraic operations does not increase and each of the operations selection, projection and product/join remains of the same kind. Query plans are hardly more complicated than the input queries. In fact, it has been verified hat off-the-shelf relational database query optimizers do well in practice.\nFor a comprehensive overview of U-relations and p-WSA we refer the reader\nto [18]. In this thesis we look at U-relations from the point of view of p-DB design, for which no methodology has yet been proposed. We are concerned in particular with hypothesis management applications [10].\n5.2. Running Example\nBefore proceeding, we consider Example 6, which is fairly representative\nto illustrate how to deal with correlations in the predictive data of deterministic hypotheses for the sake of suitable data-driven analytics.\nExample 6 We explore three slightly different theoretical models in population dynamics with applications in Ecology, Epidemics, Economics, etc: (5.2) Malthus’ model, (5.3) the logistic equation and (5.4) the Lotka-Volterra model. In practice, such equations are meant to be extracted from MathML-compliant XML files (cf. Chapter 6). For now, consider that the ordinary differential equation notation ‘ẋ’"
    }, {
      "heading" : "5.2. RUNNING EXAMPLE 64",
      "text" : "is read ‘variable x is a function of time t given initial condition x0.’\nẋ = bx (5.2)\nẋ = b(1− x/K)x (5.3)  ẋ = x(b− py)ẏ = y(rx− d) (5.4) The models are completed (by the user) with additional equations to provide the values of exogenous variables (or ‘input parameters’),2 e.g., x0 = 200, b = 10, such that we have SEM’s (resp.) Sk(Ek,Vk) for k = 1..3,\n• E1 ={ f1(t), f2(x0), f3(b), f4(x, t, x0, b) };\n• E2 ={f1(t), f2(x0), f3(K), f4(b), f5(x, t, x0, K, b)};\n• E3 ={ f1(t), f2(x0), f3(b), f4(p), f5(y0), f6(d), f7(r),\nf8(x, t, x0, b, p, y), f9(y, t, y0, d, r, x) }.\nFig. 5.2 shows the fd sets encoded from structures Sk given above.3 We also consider trial datasets for hypothesis υ=3 (viz., the Lotka-Volterra model), which are loaded into the ‘big’ fact table relation H3 ∈H as shown in Fig. 5.3. We admit a special attribute ‘trial id’ tid to keep hypothesis trials identified until uncertainty is introduced in a controlled way by p-DB synthesis (U-intro stage, cf. Fig. 1.4).\n2\n2 Given S(E ,V), it is actually a task of the encoding algorithm (viz., sub-procedure TCM) to infer, for each variable x ∈ V, whether it is exogenous or endogenous by means of the total causal mapping.\n3 Recall that domain variables like time t are informed to the encoding algorithm to suppress an fd φ→ t."
    }, {
      "heading" : "5.2. RUNNING EXAMPLE 65",
      "text" : "Given the ‘big’ fact table H3, p-DB synthesis has two main parts: process\nthe ‘empirical’ uncertainty present in the ‘big’ fact table and synthesize it out (decompose it) into independent u-factors (u-factorization); and then propagate it precisely into the predictive data (u-propagation)."
    }, {
      "heading" : "5.3. U-FACTORIZATION 66",
      "text" : "5.3. U-Factorization\nAs we have seen in §5.1, the repair-key operation allows one to create a discrete random variable in order to repair an argument key in a given relation. Our goal here is to devise a technique to perform such operation in a principled way for hypothesis management. It is a basic design principle to have exactly one random variable for each distinct uncertainty factor (‘u-factor’ for short), which requires carefully identifying the actual sources of uncertainty present in relations H .\nThe multiplicity of (competing) hypotheses is itself a standard one, viz., the\ntheoretical u-factor. Consider an ‘explanation’ table like H0 in Fig. 5.1, which stores (as foreign keys) all hypotheses available and their target phenomena. We can take such H0 as explanation table for the three hypotheses of Example 6. Then a discrete random variable V0 is defined into Y0 [V0D0 |φ υ ] by query formula (5.1). U-relation Y0 is considered standard in p-DB synthesis, as the repair of φ as a key in (standard) H0.\nHypotheses, nonetheless, are (abstract) ‘universal statements’ [15]. In order\nto produce a (concrete) valuation over their endogenous attributes (predictions), one has to inquire into some particular ‘situated’ phenomenon φ and tentatively assign a valuation over the exogenous attributes, which can be eventually tuned for a target φ. The multiplicity of such (competing) empirical estimations for a hypothesis k leads to Problem 1, viz., learning empirical u-factors for eachHk ∈H .\nProblem 1 Let Σk be an fd set encoded given hypothesis structure Sk, and Hk its ‘big’ fact table relation loaded with trial data. Now, let Z be the set of attributes encoding exogenous variables in Hk, then the problem of u-factor learning is:\n(1) to infer in Hk ‘casual’ fd’s φBi→ Bj, φBj→ Bi not in Σk (strong input\ncorrelations), where Bi, Bj ∈ Z;\n(2) to form maximal groups G1, ..., Gn ⊆ Z of attributes such that for all\nBi, Bj ∈ Ga, the casual fd’s φBi→ Bj and φBj→ Bi hold in Hk;\n(3) to pick, for each group Ga, any A ∈ Ga as a pivot representative and\ninsert φA→ B into an fd set Ωk for all B ∈ (Ga \\ A)."
    }, {
      "heading" : "5.3. U-FACTORIZATION 67",
      "text" : "U-factor learning is meant to process only the attributes Z ⊂ U from Hk[U ] that are inferred exogenous in the given hypothesis, i.e., for all A ∈ Z, there is an fd 〈X,A〉 ∈ Φ(Σk), where the latter is the φ-projection of Σk. Such attributes are then ‘officially’ unrelated. In fact, by ‘casual’ fd’s we mean correlations that, for a set of experimental trials, may occasionally show up in the trial input data; e.g., x0 ↔ y0 hold in H3, but not because x0 and y0 are related in principle (theory).\nFig. 5.4 helps to illustrate Problem 1 through the ‘big’ fact table. We\nemphasize u-factors {b, d} and {p, r} in colors green and red. Observe that values of b are strongly correlated (one-to-one) with values of d for φ = 1, just like p and r. Note also that {x0, y0} can be seen as a certain factor. From the user point of view, this is a record that reflects a common practice in computational science known as (parameter) sensibility analysis.\nProblem 1 is dominated by the (problem of) discovery of fd’s in a relation,\nwhich is not really a new problem (e.g., see [68]). We then keep focus on the synthesis method as a whole and omit our detailed u-factor-learning algorithm in"
    }, {
      "heading" : "5.3. U-FACTORIZATION 68",
      "text" : "particular.4 Its output, fd set Ωk, is then filled in (completed) with the υprojection Υ(Σk). For illustration consider hypothesis υ = 3 and its trial input data recorded in H3 in Fig. 5.3. We show its resulting fd set Ω3 in Fig. 5.5 (left), together with its folding Ω#k (right). The latter is then input to (Alg. 7) merge to get the final information necessary for the actual synthesis of U-relations, as captured in Def. 16. For an illustration of the merging of fd’s with equivalent left-hand sides, note in Fig. 5.5 (right) that φx0 b p t υ y ↔ φx0 b p t υ x holds in (Ω#3 ) +.\nDef. 16 Let Sk and Hk be the complete structure and ‘big’ fact table of hypothesis k, and Σk an fd set defined Σk , h-encode(Sk). Now, let Ωk , u-factor-learning(Hk, Φ(Σk) ) ∪ Υ(Σk), and Ω#k be the folding of Ωk. Finally, define Γk , merge( Ω#k ). Then we say that Γk is the u-factorization of Sk over Hk.\nAlgorithm 7 Merge fd’s with equivalent left-hand sides.\n1: procedure merge(Σ : fd set) 2: Ω← ∅ 3: for all 〈X,C〉 ∈ Σ do 4: if there is 〈Z,W 〉 ∈ Ω such that X ↔ Z holds in Σ+ then 5: Ω← Ω \\ 〈Z,W 〉 6: S ← X \\ Z 7: Ω← Ω ∪ 〈Z,WSC〉 . merges equivalent keys 8: else\n9: Ω← Ω ∪ 〈X,C〉 10: return Ω\n4 In short, we make use of relational algebra group-by operation and build a pruned lattice of attribute groups having the same number of rows under the grouping (similarly to [68])."
    }, {
      "heading" : "5.3. U-FACTORIZATION 69",
      "text" : "Remark 6 Let Γk be the u-factorization of structure Sk over ‘big’ fact table Hk. Then every fd in Γk encodes a clear-cut claim, either empirical, in Φ(Γk), or theoretical, in Υ(Γk). That is ensured by the merge algorithm, which groups fd’s in Ω#k with equivalent left-hand sides. 2\nWe are now able to employ a notion of u-factor decomposition formulated in\nDef. 17 into query formula (5.5) in p-WSA’s extension of relational algebra.\nDef. 17 Let Sk be the complete structure of hypothesis k, and Hk[U ] its ‘big’ fact table such that Γk is the u-factorization of Sk over Hk. Now, let Ga ⊂ U be a set of attributes Ga = AG such that, for all B ∈ G, an fd φA→ B exists in Φ(Γk). Then we define U-relation Y ik [ViDi |φAG ] by query formula (5.5), and say that Y ik is a u-factor projection of Hk.\nY ik := πφAG (repair-keyφ@ count ( γφ,A,G, count(∗)(Hk) ) ) (5.5)\nwhere γ is relational algebra’s grouping operator.\nThe synthesis of u-factor projections, in particular the application of repair-key (cf. Eq. 5.5), has an important consequence for the u-factorization Γk of Hk, viz., the introduction of new fd’s into Γ′k defined as follows (see Def. 18). We shall consider it (rather than Γk) to study design-theoretic properties of synthesized Y k in §5.5.\nDef. 18 Let Sk and Hk[U ] be (resp.) the complete structure and ‘big’ fact table of hypothesis k, and Γk be the u-factorization of Sk over Hk. Now, let Γ′k ,⋃ i∈ I{φ→ AiGi} ∪ Γk, where I indexes all u-factor projections Y ik [ViDi |φAiGi ] of Hk. We say that Γ ′ k is the repaired factorization of Sk over Hk."
    }, {
      "heading" : "5.4. U-PROPAGATION 70",
      "text" : "5.4. U-Propagation\nU-propagation is a central part of U-intro and the pipeline itself. Recall that all the machinery developed so far, from hypothesis encoding to causal reasoning to u-factorization is for enabling predictive analytics. Let us briefly reconstruct it.\nFor hypothesis structure Sk(E ,V), take any endogenous variable xc ∈ V en-\ncoded by attribute C 7→ xc. There should be exactly one fd 〈X,C〉 ∈ Υ(Σk)#. By Theorem 5, for every first cause xb of xc there is B ∈ X where B 7→ xb with xb ∈ V . Now, observe that when Ωk is rendered by u-factor learning, it is filled partly with fd’s from Υ(Σk), and partly with fd’s processed from Φ(Σk). This is to summarize exogenous variables into clear-cut independent u-factors. It means that, after u-factor learning, each first cause xb encoded by B shall be represented by some pivot attribute Ai which is, if not Ai = B itself, then occasionally strongly correlated to it (i.e., φAi → B and φB→ Ai hold in Hk).\nFurther then Ωk is subject to folding such that, if B ∈ X for 〈X,C〉 ∈ Υ(Ωk),\nnow we have Ai ∈ Z for 〈Z,C〉 ∈ Υ(Ω#k ). This processing from fd X → C (where X contains the first causes) into Z→ C (where Z contains only their pivot representatives) is meant for enabling an economical representation of uncertainty. Our running example is small, but such a principle is quite relevant for large-scale hypotheses (say, when |S| ≈ 1M). The correctness of such u-factor summarization shall be ensured by Proposition 3, which let us know that Ω#k is parsimonious then (by Def. 13) canonical, therefore (by Def. 9) left-reduced.\nNow, all fd’s in Υ(Ω#k ) have form Z → C and we are almost ready for u-\npropagation. Note that, as a result of u-factorization, each pivot attribute Ai ∈ Z is associated with random variable Vi from U-relation Y i k [ViDi |φAiGi ]. Then we shall use each Ai ∈ Z (from Z→ C) as a surrogate to Y ik in order to propagate factorized uncertainty into ‘predictive’ U-relations Y jk [Vj Dj |S T ] by a join formula. Attribute sets S and T are defined after merging fd’s in Ω#k with equivalent lhs to get Γk and pass it as argument for synthesis. We let S = Z \\W such that S contains the domain variables only (e.g., φ, υ, t). The pivot attributes in Z shall not be included in the data columns of Y jk , but leave their trace through the condition columns Vj Dj that annotate sch(Y j k ) as a repair of the key S→ T ."
    }, {
      "heading" : "5.4. U-PROPAGATION 71",
      "text" : "All that (cf. Def. 19) is abstracted into general p-WSA query formula (5.6),\nand employed in (Alg. 8 ) synthesize to accomplish u-propagation (Part II).\nDef. 19 Let Sk be the complete structure of hypothesis k, and Hk[U ] its ‘big’ fact table such that Γk is the u-factorization of Sk over Hk. Now, let Z→ T be an fd in Υ(Γk). Then we define U-relation Y j k [Vj Dj |S T ] by query formula (5.6), and say that Y jk is a predictive projection of Hk, where:\nY jk := πS, T (συ=k(Y0) ./ (./ i∈ I Y i k ) ./ Hk ) (5.6)\n(a) Y ik [ViDi |φAiGi ] is a u-factor projection of Hk; (a) we have i ∈ I if Ai ∈ Z; (c) we take S = Z \\W , where W is the set of all pivot attributes representing\nfirst causes in Z.\nAlgorithm 8 p-DB synthesis applied over folding fd set.\n1: procedure synthesize(Sk : structure, Hk : ‘big’ table, Y0 : explan. table) Require: Sk is complete Ensure: U-relations Y k returned are a BCNF, lossless decomposition of Hk\n2: Σk ← h-encode(Sk) 3: Ωk ← u-factor-learning( Φ(Σk), Hk ) ∪ Υ(Σk) 4: Γk ← merge( folding(Ωk) )\nPart I: U-factorization\n5: for all 〈φA,G〉 ∈ Φ(Γk) do . scans over the u-factors of hypothesis k 6: Y ik ← πφ,A,G (repair-keyφ@count( γφ,A,G, count(∗)(Hk) ) ) 7: Y k ← Y k ∪ Y ik\nPart II: U-propagation\n8: for all 〈Z, T 〉 ∈ Υ(Γk) do . scans over the claims of hypothesis k 9: W ← ∅ . prepares to keep track of u-factor pivot attributes 10: for all Y ik [φAiGi ] ∈ Y k do 11: if A ∈ Z then . A is a first cause of all B ∈ T 12: I = I ∪ {i} . indexes the u-factor projection 13: W ← W ∪ A . keeps track of u-factor’s pivot attribute 14: S ← Z \\W . removes u-factor pivot attributes 15: Y jk ← πS, T (συ=k(Y0) ./ (./ i∈ I Y ik ) ./ Hk ) 16: Y k ← Y k ∪ Y jk 17: return Y k"
    }, {
      "heading" : "5.4. U-PROPAGATION 72",
      "text" : "Fig. 5.7 shows the rendered U-relations for hypothesis k = 3 whose ‘big’\nfact table is shown in Fig. 5.3. Note that tid = 6 in H3 corresponds now to θ = { x1 7→ 3, x2 7→ 1, x3 7→ 3, x4 7→ 2 }, where θ defines a particular world in W whose probability is Pr(θ) ≈ .055. This value is derived from the marginal probabilities stored in world table W (see Fig. 5.7) as a result of the application of formulas Eq. 5.1 and Eq. 5.5.\nRemark 7 Observe that, although (Alg. 8) synthesize operates locally for each hypothesis k, the effects of p-DB synthesis (U-intro) in the pipeline are global on account of the (global) ‘explanation’ relation H0 (then U-relation Y0), e.g., see Fig. 5.7. In fact, the probability of each tuple (row), say, in U-relation Y jk with φ = p for hypothesis υ = k, is distributed among all the hypotheses ` 6= k that are keyed in Y0 under φ = p, i.e., all hypotheses that compete at φ = p. 2"
    }, {
      "heading" : "5.5. DESIGN-THEORETIC PROPERTIES 73",
      "text" : "U-relations rendered by p-DB synthesis are ready for querying. Typical\nqueries comprise the conf() aggregate operation, inquiring the probability (or confidence) for each tuple to ‘true’ in the probability space captured by the hypothesis competition. We illustrate queries in Chapter 6.\n5.5. Design-Theoretic Properties\nFor the U-intro procedure to be meaningful, we have to study design-theoretic properties of u-factor projections and prediction projections synthesized out of ‘big’ fact table Hk for the sake of predictive analytics. In particular, for the projections to be claim-centered, we submit that they should satisfy Boyce-Codd normal form (BCNF, cf. Def. 21) w.r.t. the repaired factorization Γ′k of Hk; and for them to be a correct decomposition of the uncertainty present in Hk, their join should be lossless (preserve the data in Hk, cf. Def. 22) w.r.t. Γ ′ k.\nNote that in this study we consider repaired factorization Γ′k (not u-factoriza-\ntion Γk), since it is the one which actually holds in Y k after key repairing."
    }, {
      "heading" : "5.5.1 Claim-Centered Decomposition",
      "text" : "As emphasized through Remark 6, every fd in u-factorization Γk is a claim (cf. Remark 6), then the same holds for repaired factorization Γ′k. Thus, for a claimcentered decomposition of ‘big’ fact table Hk, it is desirable that U-relational schema Y k that it satisfies BCNF w.r.t. Γ ′ k. BCNF (‘do not represent the same fact twice’ [21, p. 251]) is our notion of ‘good design’ for uncertainty decomposition in view of predictive analytics. This is to avoid the uncertainty of one claim to be undesirably mixed with the uncertainty of another claim.\nDef. 20 Let R[U ] be a relation scheme over set U of attributes, and Σ a set of fd’s. Then the projection of Σ onto R[U ], written πU(Σ), is the subset Σ ′ ⊆ Σ of fd’s X→ Z such that XZ ⊆ U .\nDef. 21 Let R[U ] be a relation scheme over set U of attributes, and Σ a set of fd’s on U . We say that:\n(a) R is in BCNF if, for all 〈X,A〉 ∈ Σ+ with A 6∈ X and XA⊆U , we have\nX→ U (i.e.,X is a superkey for R);"
    }, {
      "heading" : "5.5. DESIGN-THEORETIC PROPERTIES 74",
      "text" : "(b) A schema R is in BCNF if all of its schemes R1, ..., Rn ∈ R are in BCNF.\nExample 7 To illustrate the concept of BCNF, let us consider canonical fd set Σ = {A→ B, B → C } over attributes U = {A,B,C}, and a tentative schema containing a single relation R[ABC]. This relation is not in BCNF because, for one, B→ C violates it (C * B but B is not a superkey for R). 2\nObserve also that an overdecomposed schema may (trivially) satisfy BCNF.\nFor example, let Σ = {A → B, A → C} then by Def. 21 both schemas R = R1[ABC] and R ′ = {R1[AB], R2[AC]} are in BCNF w.r.t. Σ. The second, however, breaks data into two tables making their access more difficult than necessary since both B and C brings in information about A. That is, if the schema were to be synthesized over the fd’s in Σ, then it would be desirable to apply (R4) union or merge them before. Our point is that, if we target at a BCNF-satisfying schema, then it is also desirable for it to be the minimal-cardinality schema in BCNF.\nTheorem 6 guarantees the BCNF property w.r.t. Γ′k by design for every\nschema Y k rendered by (Alg. 8) synthesize over Γk.\nTheorem 6 Let Sk and Hk be (resp.) the complete structure and ‘big’ fact table of hypothesis k, and let Γ′k be the repaired factorization of Sk over Hk, and Y0 the ‘explanation’ table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize(Sk, Hk, Y0). Then Y k is in BCNF w.r.t. Γ′k and is minimal-cardinality.\nProof 15 We exploit the fact that the projection of (Γ′k) + onto u-factor projections and predictive projections define a disjoint partition of (Γ′k) + into its φ-projection Φ(Γ′k) + and υ-projection Υ(Γ′k) +. Since we know the form of fd’s in each of them, the search space for BCNF violations is significantly reduced. The minimality of |Y k| in turn comes from (Alg. 7) merge. See Appendix, §A.3.1. 2"
    }, {
      "heading" : "5.5.2 Correctness of Uncertainty Decomposition",
      "text" : "Recall from the preliminaries (cf. §5.1) that the U-relational equivalent of the relational product operation (main sub-operation of the join operation) has been introduced. Now, we provide the classical definition of a lossless join [20], i.e.,"
    }, {
      "heading" : "5.5. DESIGN-THEORETIC PROPERTIES 75",
      "text" : "when a decomposition of data from a relation into two or more relations is known to preserve the data in its original form by an application of the join.\nDef. 22 Let R[U ] be a (U-)relational schema synthesized into collection R =⋃n i=1Ri and let Σ be an fd set on attributes U . We say that R has a lossless join w.r.t. Σ if for every instance r of R[U ] satisfying Σ, we have r = ./ni=1 πRi(r).\nThe lossless join property is of interest to ensure that our decomposition of\nthe data from the ‘big’ fact table into u-factor projections preserves the data so that their join to ‘annotate’ the predictive projections when propagated by means of the U-relational join operation is correct. Theorem 7 guarantees that is the case.\nTheorem 7 Let Sk be the complete structure of hypothesis k, and Hk[U ] its ‘big’ fact table such that Γ′k is the repaired factorization of Sk over Hk and Y0 is the ‘explanation’ table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize(Sk, Hk, Y0). Then,\n(a) the join ./mi=1 Y i k [ViDi |φAiGi ] of any subset of the u-factor projections\nof Hk is lossless w.r.t. Γ ′ k.\n(b) any predictive projection Y jk [Vj Dj |S T ], result of a join of the theoretical\nu-factor Y0 [V0D0 |φ υ ] with the ‘big’ fact table Hk[U ] and in turn with u-factor projections Y ik [ViDi |φAiGi ], is lossless w.r.t. Γ′k.\nProof 16 We make use of a lemma from Ullman [20, p. 397], and then the proof comes straightforwardly. See Appendix, §A.3.2. 2\nRemark 8 The significance of Theorem 6 lies in that it guarantees the decomposition of uncertainty based on the causal ordering processing is in fact claim-centered as desirable for predictive analytics. Theorem 7 in turn is significant as it ensures that all the empirical uncertainty implicit in a hypothesis ‘big’ fact table Hk can be decomposed into u-factor projections that are (a) independent (not strongly correlated, cf. Problem 1), and (b) can be fully recovered by a join that is lossless w.r.t. repaired factorization Γ′k of structure Sk over Hk. This is essential to make sure that in u-propagation the composition of the required u-factors recovers the uncertainty associated with the predictive data. Since repaired factorization Γ′k is"
    }, {
      "heading" : "5.6. RELATED WORK 76",
      "text" : "known to be a correct processing of the causal ordering (cf. results of Chapter 4), altogether Theorem 7 guarantees that the first causes are joined together correctly towards the predictive variables influenced by them. 2\nAs we have seen, the p-DB synthesis technique presented here is essentially\ntargeted at design-theoretic properties. It is also motivated by computational performance, as uncertainty decomposition is desirable also to speed up probabilistic inference [17, p. 30-1]. In fact, the U-intro procedure is fully grounded in U-relations and p-WSA as implemented in the MayBMS system. Its computational performance is dominated by U-relational query processing. We present experimental studies on the U-intro procedure in §6.4, as they are designed from a applicability point of view. The goal is to provide some reference computational measures for prospective users.\n5.6. Related Work\nInformed on research on Graphical Models (GM) [69], Suciu et al. provide a\nstriking motivation for work on probabilistic database design [17, p.30-1]. In GM design, probability distributions on large sets of random variables are decomposed into factors of simpler probability functions, over small sets of these variables. The factors can be identified, e.g., by using a set of axioms (the so-called ‘graphoids’) for reasoning about the probabilistic independence of variables [70]. The same design principle (sic.) applies to p-DB’s [17]: the data should be decomposed into its simplest components so that only key constraints hold in a table (i.e., it is in BCNF). Attribute- and tuple-level correlations should guide the table decomposition into simpler tables. Ideally, the original table with its probability distribution can be recovered as a query (a view) from the decomposed tables. We have followed such principle in our claim-centered decomposition for predictive analytics.\nIn fact, a connection between database normalization theory and factor de-\ncomposition in Graphical Models (GM) has been discussed by Verma and Pearl [70], but has not been explored since then. To date, there is no formal design theory for p-DB’s [17]. A step in that direction is taken by Sarma et al. [71]. Their initiative revisits dependency theory in view of reformulating fd’s for uncertain schema"
    }, {
      "heading" : "5.6. RELATED WORK 77",
      "text" : "design [71]. Our work takes a different direction. We refer to classical dependency theory and U-relational operations (viz, its uncertainty-introduction operator) to construct p-DB’s systematically from scratch. We have focused on the extraction and processing of fd’s towards a factorized U-relational schema. The synthesized schema is ensured to be in BCNF and have a lossless join.\nDespite some major differences, our synthesis method builds upon the classi-\ncal theory of relational schema design by synthesis [62]. Classical design by synthesis [62] was once criticized due to its too strong ‘uniqueness’ of fd’s assumption [72, p. 443], as it reduces the problem of design to symbolic reasoning on fd’s, arguably neglecting semantic issues. Probabilistic design, however, has roots in statistical design so that the problem is less amenable to human factors. As we extract the dependencies from a formal specification, design by synthesis is doing nothing but translating seamlessly (into fd’s) the reduction made by the user herself in her tentative model for the studied phenomenon.\nThe last decade has seen significant research effort to make DB systems really\nusable [73]. Our design-by-synthesis framework can also be understood as a technique for user-friendly p-DB design. For instance, in comparison, the CRIUS system supports another kind of user-friendly DB design approach that provides users with a spreadsheet-like direct manipulation interface to increasingly add structure to their data [74]. Our dependency extraction and processing, instead, completely alleviates the user from the burden of data organization.\nAlso related to probabilistic DB design is the topic of conditioning a p-DB.\nIt has been firstly addressed by Koch and Olteanu motivated by data cleaning applications [75]. They have introduced the assert operation to implement, as in AI, a kind of knowledge compilation, viz., world elimination in face of constraints (e.g., FDs). For hypothesis management, nonetheless, we need to apply Bayes’ conditioning by asserting observed data, not constraints. In §2.5 we have presented an example that settles the kind of conditioning problem that is relevant to the Υ-DB vision. In Chapter 6 we present realistic use cases. We have addressed the problem at application level only in order to complete the realization of the vision in a real prototype system. The formulation of Bayes’ conditioning as an extension of, say, the U-relational data model is open to future work (cf. §7.3)."
    }, {
      "heading" : "5.7. SUMMARY OF RESULTS 78",
      "text" : "5.7. Summary of Results\nIn this chapter we have studied and developed our end-purpose technique for\nthe synthesis of a probabilistic DB geared for predictive analytics. It completes the pipeline (Fig. 1.4) so that conditioning can then be performed iteratively.\n• Algorithm 8 synthesize gives a general formulation of how to perform un-\ncertainty introduction from causal dependencies given in the form of fd’s.\n• By Problem 1, we have given a definition of uncertainty factor learning\nfrom data available in a given relation.\n• Remark 7 provides an example of the u-factor and predictive projections\nresulting from p-DB synthesis and their corresponding probability distributions stored in the world table;\n• By Remark 6 and Theorem 6, we have shown that U-relational schema Y k\nsynthesized over the fd’s processed by causal reasoning is in BCNF. That is, it is in fact a claim-centered decomposition as desirable for predictive analytics.\n• Theorem 7 ensures that such (uncertainty) decomposition is correct, as the\noriginal (probability distribution) ‘big’ fact table is fully recoverable by a lossless join.\nChapter 6\nApplicability\nIn this chapter we show the applicability of Υ-DB in real-world scenarios.\nWe present use cases in Computational Physiology extracted from the Physiome project.1 In §6.1 we introduce the Physiome project as providing a testbed for Υ-DB. Then in §6.2 we go through some Physiome case studies to show the construction of Υ-DB and its application for data-driven hypothesis management and analytics. In §6.3 we present a prototype of the Υ-DB system and demonstrate it through the running example introduced in §5.2. In §6.4 we present experiments on Physiome hypotheses. In §6.5 we provide a general discussion on the applicability of Υ-DB, its assumptions and scope. In §6.6 we conclude the chapter.\n6.1. The Physiome Project as a Testbed\nThe Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33]. It essentially comprises:\n• a curated repository of 380+ computational physiology models available\nonline for researchers;2\n• the Mathematical Modeling Language (MML) to allow models to be written\nin declarative form and then exported into a number of XML-compliant\n1 http://physiome.org. 2 The Physiome model repository is expanded to over 73K+ models by including models extracted from other sources (such as the EBML-EBI BioModels DB, the CellML Archive, and the Kegg Pathways DB) and converted to MML automatically.\n6.2. CASE STUDIES 80\ninteroperable formats;3\n• a problem-solving environment called JSim to allow researchers to code\ntheir MML models straightforwardly, run them under different parameter and solver settings and build customized data plots to see the results.\nFrom the point of view of Υ-DB, Physiome is an external data source that\nprovides a very interesting testbed with realistic scenarios. We extract Physiome models into Υ-DB by means of a wrapper we have implemented to read XMML files (JSim’s XML encoding of MML models). Simulation trial datasets are rendered by a parametrized UNIX script we have developed to invoke JSim automatically (in batch mode). Currently, Υ-DB’s Physiome wrapper is designed to read MAT files to load both the model input (parameter settings data) and its associated model output (computed predictive data) for each simulation trial.\nPhysiome does not keep records of phenomena in a repository, but it does\nhave observational data attached to some of the entries of the model repository. Such models appear in the filter ‘models with data,’ meaning that they have one or more observational datasets and plots showing how the model data fits to observations. We shall make use of model entries containing observational data in the realistic scenarios presented in this paper.\n6.2. Case Studies\nIn this section we present use cases extracted from the Physiome model repository.4"
    }, {
      "heading" : "6.2.1 Case: Hemoglobin Oxygen Saturation",
      "text" : "In this case we stress the potential of data-driven hypothesis analytics in comparison to handcrafted curve fitting (visual) analysis. We study three different hypotheses that perform “closely” visually when compared to their target phenomenon dataset, see Fig. 6.1. All of them have been empirically set as fit as possible to the observations (‘R1s1’ dataset) in their local view (in separate), and are now compared together in a global view.\n3 http://www.physiome.org/jsim/docs/MML_Intro.html. 4 http://www.physiome.org/Models/modelDB/."
    }, {
      "heading" : "6.2. CASE STUDIES 81",
      "text" : "Example 8 The resources of this example are shown in Fig. 6.2. We consider the Physiome model entries described in relation HYPOTHESIS, associated to the phenomenon described in relation PHENOMENON (cf. explanation relation H0). One single hypothesis trial (its best fit) is considered for each hypothesis. 2"
    }, {
      "heading" : "6.2. CASE STUDIES 82",
      "text" : "Encoding. The fd encoding of hypotheses υ ∈ {28, 31, 32} is shown (resp.) in Fig. 6.3, Fig. 6.4 and Fig. 6.5.\nSymbol Mappings. As we have seen, the insertion of hypothesis trial datasets requires users to specify a target phenomenon and the corresponding mappings from the hypothesis symbols to the target phenomenon symbols. In this use case, we have:\n• M287→1 = { pO2 7→ pO2, SHbO2 H 7→ SHbO2 };\n• M317→1 = { pO2 7→ pO2, SHbO2 Ad 7→ SHbO2 };\n• M327→1 = { pO2 7→ pO2, SHbO2 D 7→ SHbO2 };"
    }, {
      "heading" : "6.2. CASE STUDIES 83",
      "text" : "Hypothesis Management. Query Q1 illustrates the feature of hypothesis management for this case. We consider the user is interested in all SHbO2 predictions over a subset of the pO2 domain. Its result set is shown in Fig. 6.6.\nQ1. (select phi, upsilon, tid, “pO2”,“SHbO2 H”as SHbO2 from Y28 claim1\nwhere phi=1 and“pO2”>=20 and“pO2”<=40) union all (select phi, upsilon, tid, “pO2”,“SHbO2 Ad”as SHbO2 from Y31 claim1 where phi=1 and“pO2”>=20 and“pO2”<=40) union all (select phi, upsilon, tid, “pO2”,“SHbO2 D”as SHbO2 from Y32 claim1 where phi=1 and“pO2”>=20 and“pO2”<=40) order by“pO2”, upsilon, tid;\nHypothesis Analytics. Fig. 6.7 shows the results of analytics after conditoning the probability distribution in the presence of observations (‘R1s1’ dataset). The fact that hypothesis υ = 31 provides the best explanation for the studied phenomenon is enabled by the application of Bayesian inference as implemented within the Υ-DB system. The contribution of the Υ-DB methodology is to equip users with a tool for large-scale, data-driven hypothesis management and analytics."
    }, {
      "heading" : "6.2. CASE STUDIES 84",
      "text" : ""
    }, {
      "heading" : "6.2.2 Case: Baroreflex Dysfunction in Dahl SS Rat",
      "text" : "This case is extracted from the Virtual Physiological Rat project,5 Here we show the potential of data-driven hypothesis management and analytics for model tuning. Fig. 6.8 shows the best fit of a baroreflex model for an observational dataset acquired by experiment on Dahl SS rat [76]. We in turn use Υ-DB to carry out such hypothesis management and analytics. We generate by a parameter sweep script 1K trials and insert them into the database. A best fit is then selected automatically by Bayesian inference.\nExample 9 The resources of this example are shown in Fig. 6.9. We consider the single hypothesis entry described in relation HYPOTHESIS, and the phenomenon described in relation PHENOMENON. By parameter sweep, 1K trials are inserted into Υ-DB for management and analytics. 2\nEncoding. The fd encoding of hypothesis υ ∈ {1001} is shown in Fig. 6.12. 5 http://virtualrat.org/computational-models/."
    }, {
      "heading" : "6.2. CASE STUDIES 85",
      "text" : "Symbol Mappings. We consider that the user provides symbol mappings:\n• M10017→2 = { Time 7→ Time, HR 7→ HR };\nHypothesis Management. In query Q2 we consider that the user is interested in time instants where the heart rate is higher than a threshold, say, 300 beats/min. The result set is shown in Fig. 6.10.\nQ2. select phi, upsilon, tid, “Time”,“HR” from Y1001 claim1\nwhere phi=2 and“HR”>=300 order by“Time”, tid;\nHypothesis Analytics. Fig. 6.11 shows the results of analytics on phenomenon φ=2 after conditioning the probability distribution in the presence of observations (‘SSBN9 HR’ dataset). Since this case deals with model tuning, viz., 1K slightly different parameter settings, the trial ranking is decided by small differences in the posterior probability distribution (cf. Fig 6.11)."
    }, {
      "heading" : "6.2.3 Case: Myogenic Behavior of a Blood Vessel",
      "text" : "Computational models of physiology may account for diverse effects that take place at different levels of biological organization from the organ to the cellular"
    }, {
      "heading" : "6.2. CASE STUDIES 86",
      "text" : "and molecular levels [33]. Typically, a sophisticate model is developed incrementally by, say, adding detail into some previously existing model or extending its dimensionality (e.g., extending it from a stationary to a dynamic account of phenomena). In this case study (cf. Example 10) we consider alternative models of the myogenic behavior of a reference human blood vessel."
    }, {
      "heading" : "6.2. CASE STUDIES 87",
      "text" : "Example 10 (See Fig. 6.14). We consider the Physiome model entries displayed in relation HYPOTHESIS, and two phenomena (see relation PHENOMENON). One trial is considered for hypothesis υ = 60, and two for hypothesis υ = 89. 2\nEncoding. The fd encoding of hypotheses υ ∈ {60, 89} is shown (resp.) in Fig. 6.15 and Fig. 6.16.\nSymbol Mappings. We consider that the user provides symbol mappings:\n• M607→1 = { t 7→ Time, D 7→ Diameter };\n• M897→1 = { t 7→ Time, D 7→ Diameter };"
    }, {
      "heading" : "6.2. CASE STUDIES 88",
      "text" : "Hypothesis Management. Query Q3 illustrates the feature of hypothesis management for this case. The user selects all diameter predictions within the time interval t ∈ [100, 300] (cf. plot in Fig. 6.13). Its result set is shown in Fig. 6.17."
    }, {
      "heading" : "6.3. SYSTEM PROTOTYPE 89",
      "text" : "Q3. select phi, upsilon, tid, “t”,“D” from Y60 claim1\nwhere phi=3 and“t”>=100 and“t”<=300 union all select phi, upsilon, tid, “t”,“D” from Y89 claim1 where phi=3 and“t”>=100 and“t”<=300 order by“t”, upsilon, tid;\nHypothesis Analytics. Fig. 6.18 shows the results of analytics on phenomenon φ = 3 after conditoning the probability distribution in the presence of observations, viz., ‘Davis Sikes Fig3 Myo DigData’ dataset.\nIn this case study two tentative models have been considered under a uniform prior probability distribution which has been updated to a posterior distribution. Note that, even though hypothesis υ = 60 has its probability weight concentrated in a single trial, the Bayesian inference is able to indicate υ = 89 as the best explanation for φ = 3 and tid = 2, in particular, its best fit.\n6.3. System Prototype\nA first prototype of the Υ-DB system has been implemented as a Java web application, with the pipeline component in the server side on top of MayBMS (a backend"
    }, {
      "heading" : "6.3. SYSTEM PROTOTYPE 90",
      "text" : "extension of PostgreSQL). We have developed a demonstration of this prototype (cf. [28]), in which we go through the whole design-by-synthesis pipeline (Fig. 1.4) exploring use case scenarios. In this section we provide a brief demonstration of the system in the population dynamics scenario previously introduced in this thesis.\nThe demonstration unfolds in three phases. In the first phase, we show\nthe ETL process to give a sense of what the user has to do in terms of simple phenomena description, hypothesis naming and file upload to get her phenomena and hypotheses available in the system to be managed as data. In the second phase, we reproduce some typical queries of hypothesis management (like those shown in the previous section). In the third phase, we enter the hypothesis analytics module. The user chooses a phenomenon for a hypothesis evaluation study, and the system lists all the predictions with their probabilities under some selectivity criteria (e.g., population at year 1920). The predictions are ranked according to their probabilities, which are conditioned on the observational data available for the chosen phenomenon."
    }, {
      "heading" : "6.3.1 Demo Screenshots",
      "text" : "Fig. 6.21 shows screenshots of the system. Fig. 6.21(a) shows the research\nprojects currently available for a user. Figs. 6.21(b, c) show the ETL interfaces for phenomenon and hypothesis data definition (by synthesis), and then the insertion of hypothesis trial datasets, i.e., explanations of a hypothesis towards a target phenomenon. Fig. 6.21(d) shows the interface for basic hypothesis management by listing the predictions of a given simulation trial. Figs. 6.21(e, f) show two tabs of the hypothesis analytics module, viz., selection of observations and then viewing the corresponding alternative predictions ranked by their conditioned probabilities."
    }, {
      "heading" : "6.3.2 Demo Case: Population Dynamics",
      "text" : "In this case we refer to a well-known problem in Computational Science, viz., population dynamics scenarios, to demonstrate the Υ-DB system prototype. Fig. 6.19 shows census data collected from in the US from 1790 to 1990.6 Fig. 6.20 shows observational data collected from Hudson’s Bay from 1900 to 1920 on the Lynx-Hare population [77].\n6 Cf. https://www.census.gov/population/censusdata/table-4.pdf."
    }, {
      "heading" : "6.3. SYSTEM PROTOTYPE 91",
      "text" : ""
    }, {
      "heading" : "6.3. SYSTEM PROTOTYPE 92",
      "text" : ""
    }, {
      "heading" : "6.3. SYSTEM PROTOTYPE 93",
      "text" : "Example 11 (See Fig. 6.22). We consider the model entries displayed in relation HYPOTHESIS, and two phenomena (see relation PHENOMENON). For φ = 1, three trials are considered for hypothesis υ = 1 and six for hypothesis υ = 2. For φ = 2, in turn, two trials are considered for hypothesis υ = 1 and υ = 2, and six trials for hypothesis υ = 3. Note the data definition interfaces in Figs. 6.21(b, c). 2\nEncoding. The fd encoding of hypotheses υ ∈ {1, 2, 3} is shown (resp.) in Fig. 6.23, Fig. 6.24 and Fig. 6.25. See hypothesis structure processing in Fig. 6.21(c).\nSymbol Mappings. We consider that the user provides the following symbol mappings for (resp.) phenomena φ = 1 and φ = 2, see the interface for mapping symbols in Fig. 6.21(c)."
    }, {
      "heading" : "6.3. SYSTEM PROTOTYPE 94",
      "text" : "• M17→1 = { t 7→ Year, x 7→ Population };\n• M27→1 = { t 7→ Year, x 7→ Population };\n• M17→2 = { t 7→ Year, x 7→ Lynx };\n• M27→2 = { t 7→ Year, x 7→ Lynx };\n• M3 7→2 = { t 7→ Year, x 7→ Lynx };\nHypothesis Management. Query Q4 illustrates the feature of hypothesis management for this case. The user selects hypothesis υ=3 (the Lotka-Volterra model), and filters its available data for trial tid=6 on phenomenon φ = 2. Both the formbased query set-up and its result set are shown in Fig. 6.21(d).\nQ4. select“t”,“y”,“x” from Y3 claim1\nwhere upsilon=3 and phi=2 and tid=6 order by“t”;\nHypothesis Analytics. Fig. 6.26 and Fig. 6.27 show the results of analytics on (resp.) phenomena φ = 1 and φ = 2 after conditoning the probability distribution in the presence of (resp.) observational datasets ‘US-census’ and ‘Lynx-Hare.’ In the first one, the user verifies that hypothesis υ = 1 (the Malthusian model) is unlikely to be competitive with hypothesis υ = 2 (the Logistic equation) as an approximation of the US population dynamics from 1790 to 1990. That is, if the user knows her current trials are reasonable, then more trials on the Malthusian model hardly could outperform trials on the Logistic equation for the studied phenomenon."
    }, {
      "heading" : "6.4. EXPERIMENTS 95",
      "text" : "6.4. Experiments\nThe efficiency and scalability of the U-relational representation system and\nits p-WSA query algebra have been extensively demonstrated [27]. Υ-DB’s, as U-relational hypothesis DB’s, must therefore be as efficient and scalable as any arbitrary U-relational DB.\nIn these experiments (see Fig. 6.28) we provide some measures of perfor-\nmance of the method of Υ-DB in the particular context of our real-world Physiome testbed. Our purpose here is to provide a concrete feel on how efficient the Υ-DB methodology can be. However, most of these tests (the four graphs on the bottom in Fig. 6.28) involve the data level and then require more of the hardware. Our current experimental setup (personal computer)7 allows us to reach a scale\n7 These experiments were performed on a 2.3GHZ/4GB Intel Core i5 running Mac OS X 10.6.8 and MayBMS (a PostgreSQL 8.3.3 extension)."
    }, {
      "heading" : "6.4. EXPERIMENTS 96",
      "text" : "in which the uncertain data being processed in synthesis ‘4U’ is sized up to 1GB. For the two first graphs (XML extraction and encoding), we have collected the response time on the measure of interest over different structure lengths. Each one corresponds to a real Physiome hypothesis from the table of Fig. 6.29. The last hypothesis in that table, υ = 379, is used for the tests of the four last graphs in Fig. 6.28, viz., u-learning, u-factorization, u-propagation and conditioning. We have set different number of trials (ntrials) over it, each one having 1MB. The last test in each of such four graphs, with 1K trials, is processing 1GB of uncertain data at once and then fits the machine’s main memory. We interpret the performance results shown in these graphs as follows for each measure of performance.\n• Extraction. Some fluctuation may be due to practicalities of XML DOM\naccess methods. The point of this performance study is to have practical"
    }, {
      "heading" : "6.4. EXPERIMENTS 97",
      "text" : "HYPOTHESIS υ name |S| |E| 186 Regulatory Vessel 40 20 89 Myo Dyn Resp wFit 73 28 60 Myogenic Compliant Vessel 100 38 75 Baroreceptor Lu et al 2001 153 74 70 4-State Sarcomere Energetics 298 91 120 Comp four gen weibel lung 440 186 91 CardiopulmonaryMechanics 1132 412 93 CardiopulmonMechGasBloodExch 1593 525 153 HighlyIntegHuman 1624 538 154 HighlyIntHuman wIntervention 1919 634 379 Baroreflex SB CT 171 74"
    }, {
      "heading" : "6.5. DISCUSSION 98",
      "text" : "has negligible processing time w.r.t. u-propagation. The latter is the most expensive sub-procedure of the synthesis method.\n• Conditioning. The conditioning procedure is run for a selected phenomenon.\nIt is composed of four main parts. First, by operation conf() it performs a probabilistic inference sub-query on the proper predictive projection of the ‘big’ fact table of each hypothesis associated with the phenomenon. Second, it combines the results of each such sub-query through a union all query whose result set is a multi-hypothesis predictive table. Third, it loads the phenomenon observation sample data and the predictive data from the multi-hypothesis table into memory to apply Bayesian inference. Finally, the prior probability distribution of the predictive table is updated with the posterior and all the corresponding marginal probabilities are updated in their original U-relational tables. In our tests, this procedure is carried out over varying number of trials (ntrials). The total response times are shown in the last plot of Fig. 6.28.\nThis performance behavior is to be interpreted in the context of ETL in\nDW’s. Loading and setting up an Υ-DB has an overhead that shall be, though, much lower in high-performance machines. Such overhead is nonetheless justified for the use case of hypothesis management and analytics as opposed to simulation data management and exploratory analytics (cf. §2.6.2).\n6.5. Discussion\nWe have verified that the hypothesis ratings/rankings shown in §6.2 coincide\nwith the results (e.g., of model tuning) described in the Physiome model entries and their related publications. That validates the applicability of the Υ-DB methodology as a tool for data-driven analysis in such realistic scenarios.\nThe current practice in Computational Science for model evaluation and\ncomparison in the presence of observational data is somewhat handcrafted: model agreement is assessed either qualitatively by referring to curve shapes in data plots or quantitatively by means of ad-hoc scripts. The Υ-DB methodology offers a tool to perform data-driven hypothesis analytics semi-automatically directly in the"
    }, {
      "heading" : "6.5. DISCUSSION 99",
      "text" : "database under the support of its querying capabilities. It has, therefore, potential to be a step towards higher standards of reproducibility and scalability.\nRealistic assumptions. The core assumption of our framework is that the\nhypotheses are given in a formal specification which is encodable into a SEM that is complete (satisfies Defs. 1, 2). Also, as a semantic assumption which is standard in scientific modeling, we consider a one-to-one correspondence between real-world entities and variable/attribute symbols within a structure, and that all of them must appear in some of its equations/fd’s. For most science use cases involving deterministic models (if not all), such assumptions are quite reasonable. It can be a topic of future work (cf. §7.3) to explore business use cases as well.\nHypothesis learning. The (user) method for hypothesis formation is ir-\nrelevant to our framework, as long as the resulting hypothesis is encodable into a SEM. So, a promising use case is to incorporate machine learning methods into our framework to scale up the formation/extraction of hypotheses and evaluate them under the querying capabilities of a p-DB. Consider, e.g., learning the equations, say, from Eureqa [2].8\nQualitative hypotheses. The Υ-DB methodology is primarely motivated\nby computational science (usually involving differential equations). It is, however, applicable to qualitative deterministic models as well. Boolean Networks, e.g., consist in sets of functions f(x1, x2,.., xn), where f is a Boolean expression. For instance, Fig. 6.30 presents the system of Boolean equations of a tentative Boolean Network model for a plant hormone (Fig. 6.31) published in [78].9 The notation, e.g., SphK*, is read (just like an ordinary differential equation), ‘the next state value of variable SphK is given by the state value of variable ABA. The parameters in this kind of model are the variable initial conditions.\n8 http://creativemachines.cornell.edu/Eureqa. 9 Cf. http://atlas.bx.psu.edu/booleannet/booleannet.html."
    }, {
      "heading" : "6.5. DISCUSSION 100",
      "text" : ""
    }, {
      "heading" : "6.6. CONCLUSIONS 101",
      "text" : "Several kinds of dynamical system can be modeled in this formalism. Ap-\nplications have grown out of gene regulatory network to social network and stock market predictive analytics. Even if richer semantics is considered (e.g., fuzzy logics), our encoding method is applicable likewise, as long as the equations are still deterministic.\n6.6. Conclusions\nIn this chapter we have demonstrated and discussed the applicability of the Υ-DB methodology. We have referred to real-world use case scenarios derived from the Physiome research project. We have shown in some detail the process of building an Υ-DB with representative models from Physiome’s model repository. That qualitative assessment is followed by experiments that provide some concrete feel on the performance behavior of Υ-DB for models with up to 600+ mathematical variables.\nChapter 7\nConclusions\nIn this chapter we (§7.1) revisit the research questions addressed by this\nthesis, (§7.2) point out its significance and limitations, (§7.3) list open problems and topics for future work, and (§7.4) conclude with final considerations.\n7.1. Revisiting the Research Questions\nLet us now revisit the conceptual (RQ1-4) and technical (RQ5-9) research\nquestions.\nRQ1. How to define and encode hypotheses ‘as data’? What are the sources of\nuncertainty that may be present and should be considered?\nIn Chapter 2 we have provided core abstractions that compose the vision of hypotheses ‘as data,’ or the Υ-DB vision. The problem of hypothesis encoding has been defined and addressed further in Chapter 3. We have distinguished two main sources of uncertainty in our model of uncertainty for hypothesis management, viz., (i) theoretical uncertainty, as arising from competing hypotheses; and (ii) empirical uncertainty, as arising from the alternative trial datasets available for each hypothesis.\nRQ2. How does hypotheses ‘as data’ relate with observational data or, likewise,\nphenomena ‘as data’ from a database perspective?\nAlso in Chapter 2, we have presented a conceptual framework in which we have defined hypotheses ‘as data’ and shown how it can be compared against phenomena ‘as data.’ In fact, hypothesis management is really"
    }, {
      "heading" : "7.1. REVISITING THE RESEARCH QUESTIONS 103",
      "text" : "significant when it is possible to rate/rank hypotheses in the presence of (some partial piece of) evidence.\nRQ3. Does every piece of simulated data qualify as a scientific hypothesis? What\nis the difference between managing ‘simulation’ data from managing ‘hypotheses’ as data?\nEarly in Table 1.1, we provided a comparison between simulation data management and hypothesis data management. Furthermore, the scientific research process is abstracted in Chapter 2 as a well-defined problem of data cleaning. Hypotheses are seen from an applied science point of view, and then are reduced into data such that a piece of simulation data is considered a hypothesis whenever it is assigned to explain some specific phenomenon.\nRQ4. Is there available a proper (machine-readable) data format we can use to\nautomatically extract mathematically-expressed hypotheses from?\nWe anticipated in Chapter 2 the adoption of the XML data model as the general data format for extracting hypothesis specifications from. In particular, since we deal here with mathematical hypotheses, we refer to MathML as a standard for hypothesis specification. Concretely, in Chapter 6 we present use case demonstration scenarios for which we have developed a specific wrapper, viz., for the extraction of hypotheses specified in MML (Mathematical Modeling Language).\nRQ5. Is there an algorithm to, given a SEM, efficiently extract its causal order-\ning? What are the computational properties of this problem?\nAs shown in Chapter 3, Simon’s treatment of the problem of causal ordering given a SEM S(E ,V) is NP-Hard. In the same chapter, we have discussed this problem in detail and presented an effective, efficient algorithmic approach to the problem. The computational cost for the whole\nprocess of hypothesis encoding is bounded by O( √ |S||E|). Experiments show that the approach performs well in practice for large hypotheses."
    }, {
      "heading" : "7.1. REVISITING THE RESEARCH QUESTIONS 104",
      "text" : "RQ6. What is the connection between SEM’s and fd’s? Can we devise an en-\ncoding scheme to ‘orient equations’ and then effectively transform one into the other with guarantees? Once we do it, what design-theoretic properties have such a set of fd’s?\nAlso in Chapter 3, we have presented an algorithmic encoding scheme to transform a SEM into a set of fd’s with guarantees in terms of preserving the hypothesis causal structure. Our study of this problem has revealed some interesting properties of the resulting fd sets, in particular, that they are always ‘non-redundant’ and, in comparison with arbitrary information systems, more precise and economical in the sense that, for any given attribute, there is exacly one fd with it in its rhs.\nRQ7. Is such fd set ready to be used for p-DB schema synthesis as an encoding\nof the hypothesis causal structure? If not, what kind of further processing we have to do? Can we perform it efficiently by reasoning directly on the fd’s? How does it relate to the SEM’s causal ordering?\nAs we discuss in Chapter 4, the encoded fd set must be further processed to find the ‘first causes’ for each of its predictive variable. For addressing that, in Chapter 4 we have presented the concept of the folding of an fd set and an efficient algorithm to compute it. Also, we have shown the equivalence of such fd reasoning with causal ordering processing.\nRQ8. Is the uncertainty decomposition required for predictive analytics reducible\nto the structure level (fd processing), or do we need to process the simulated data to identify additional uncertainty factors? Finally, what properties are desirable for a p-DB schema targeted at hypothesis management? Are they ensured by this synthesis method?\nIn Chapter 5 we have presented a conceptual framework to address synthesis for uncertainty ‘4U.’ In particular, we have introduced the need to process, for each hypothesis, its trial datasets available, and presented an efficient algorithm to factorize and propagate the overall uncertainty present in a given hypothesis (as a competing explanation for a target"
    }, {
      "heading" : "7.2. SIGNIFICANCE AND LIMITATIONS 105",
      "text" : "phenomenon). Then we have motivated BCNF as a notion of “good” design w.r.t. the factorized fd set based on the folding concept, and the lossless join property as required for the correctness of uncertainty decomposition. We have shown that the synthesized p-DB schema bears both properties.\nRQ9. Given all such a design-theoretic machinery to process hypotheses into\n(U-)relational DB’s, what properties can we detect on the hypotheses back at the conceptual level? Do we have now technical means to speak of hypotheses that are “good” in terms of principles of the philosophy of science?\nEquipped with the design-theoretic machinery proposed in this thesis, we are able to, given a SEM, to automatically (1) extract its causal ordering, (2) detect its strongly coupled components and decide, for a given predictive projection, what are its associated u-factor projections (if any), and shall be able as well to (3) query the hypothesis ranking for a phenomenon of interest. All these are technical means to [15]: (1′) extract the hypothesis ‘empirical content’ and ‘predictive power;’ (2′) unravel its cohesiveness and how parsimonious it is in terms of the number of different claims or epistemological units carried within it, as well as its empirical grounding (‘first causes’); and finally, we shall be able to (3′) appraise it in face of competing or alternative explanations.\n7.2. Significance and Limitations\nThis thesis addresses the pressing call for large-scale, data-driven hypoth-\nesis management and analytics [35, 3, 10]. Some reasons that contribute for its significance are listed (cf. [10, 29, 28]).\n• Structured deterministic hypotheses are now shown to be encodable as\nuncertain and probabilistic (U-relational) data based on p-DB principles;\n∗ Study of the connection between SEM’s and fd’s, with contribution\nboth to computational properties of the causal ordering problem, and to causal reasoning over fd’s;"
    }, {
      "heading" : "7.3. OPEN PROBLEMS AND FUTURE WORK 106",
      "text" : "∗ First synthesis method for the construction of p-DB’s from some pre-\nvious existing formal specification.\n• Definition of a concrete use case of data-driven hypothesis management\nand analytics;\n∗ New class of applications introduced for p-DB’s;\n∗ Settled the problem of Bayes’ conditioning in p-DB’s.\nNow some limitations of the thesis are listed.\n• The Bayesian inference is implemented at application level, yet not formu-\nlated as a principled technical solution within research in p-DB’s.\n• The encoding scheme to transform the mathematical system of a hypoth-\nesis into a set of fd’s enabling the synthesis of the p-DB is applicable to structured deterministic models only, not stochastic ones.\n7.3. Open Problems and Future Work\nOpen problems and topics of future work are listed (no particular order).\n(1) The design of a dedicated algebraic operation for Bayes’ conditioning in\np-WSA.\n(2) Investigation of other data dependency formalisms (e.g., multi-valued de-\npendencies [20]), approximate fd’s [68], conditional fd’s [79]) to extend the scope of Υ-DB towards structured stochastic models.\n(3) Development of techniques for systematic hypothesis extraction as a well-\ndefined problem of (web) information extraction;\n(4) Investigation of business use case scenarios for data-driven decision making\non top of Υ-DB;\n(5) Definition of a machine learning use case scenario to industrialize hypothe-\nsis formation and assess Υ-DB’s performance feasibility in such a scenario;"
    }, {
      "heading" : "7.4. FINAL CONSIDERATIONS 107",
      "text" : "(6) Development of automatic data sampling techniques to leverage the data\ndefinition of both hypotheses and phenomena in Υ-DB from a statistical point of view.\n7.4. Final Considerations\nIn this thesis we have developed the vision of Υ-DB, which is essentially\nan abstraction of hypotheses as uncertain and probabilistic data. It comprises a design-theoretic methodology for the systematic construction and management of U-relational hypothesis DB’s. It is meant to provide a principled approach to enable scientists and engineers to manage and evaluate (rate/rank) large-scale scientific hypotheses as theoretical data. We have addressed some core technical challenges over the Υ-DB vision in order to properly encode deterministic hypotheses as uncertain and probabilistic data.\nAs envisioned by Jim Gray [1], the scientific method has been shifting towards\nbeing operated as a data-driven discipline which is rapidly gaining ground [3]. In this thesis we have strived for proposing some core principles and techniques for enabling data-driven hypothesis management and analytics, opening a promising line of research in both probabilistic databases and simulation data management."
    }, {
      "heading" : "Appendix A",
      "text" : "Detailed Proofs\nA.1. Proofs of Hypothesis Encoding\nA.1.1 Proof of Theorem 1 “Let S(E ,V) be a complete structure. Then the extraction of its causal ordering by Simon’s COA(S) is intractable (NP-Hard).”\nProof 17 We show that, at each recursive step of COA, to find all non-trivial minimal subsets (i.e., |E ′| ≥ 2) translates into an optimization problem associated with the decision problem BPBP, which we know by Lemma 1 to be NP-Complete.\nFirst, recall (Def. 2) that a structure S(E , V) is complete if |E| = |V|; e.g., for\nthe structure given in Fig. 3.5 (left), note (Def. 9) the minimal structure S ′(E ′, V ′), where E ′ = { f1, f2, f3 }. For non-trivial minimal structures, i.e., when |E ′| = K ≥ 2, it is easy to see that its corresponding bipartite graph G = (V ′1 ∪ V ′2 , E ′), where V1 7→ E , V2 7→ V and E 7→ S must have number of edges |E ′| ≥ 2K and, for all its vertices u ∈ V ′1 ∪ V ′2 , u must have deg(u) ≥ 2, i.e., G is a pseudo-biclique in accordance with Def. 6. That intuition is elaborated as follows.\nThe point is that, no matter how big is such structure S ′, its equations f ∈ E ′\nare such that |V ars(f)| ≥ 2 (as S ′ is non-trivial) and its variables can be grouped in local patterns from the sparsest kind to the densest. To construct an instance of the sparsest case, let S ′ be built by setting a first equation where its entry in the structure matrix AS has form (1, 1, 0 +) and then, for the next |E ′| − 2 equations, shift such pair of 1’s one position right w.r.t. the previous one. Then complete it with a last equation whose form is form (1, 0+, 1). That is, the structure is built\nA.1. PROOFS OF HYPOTHESIS ENCODING 117\nwith unique pairs of 1’s spread all over the structure. Then, deciding whether there is a minimal structure of size |E ′| = K corresponds exactly to BPBP. It is a special case (BBP), when such minimal structure is the densest possible, i.e., when AS has only 1’s and then its corresponding bipartite graph is a K-balanced biclique with |E ′| = K2, and deg(u) = K for all vertices u ∈ V ′1 ∪ V ′2 . For instance, see the minimal structure with E ′ = { f4, f5 } found at the second recursive step of COA in Fig. 3.2. 2\nA.1.2 Proof of Proposition 1 “Let S(E ,V) be a structure, and ϕ1 : E → V and ϕ2 : E → V be any two total causal mappings over S. Then C+1 = C+2 .”\nProof 18 The proof is based on an argument from Nayak [49], which we present here arguably much clearer. Intuitively, it shows that if ϕ1 and ϕ2 differ on the variable an equation f is mapped to, then such variables, viz., ϕ1(f) and ϕ2(f), must be causally dependent on each other (strongly coupled).\nTo show C+1 = C + 2 reduces to C + 1 ⊆ C+2 and C+2 ⊆ C+1 . We show the first\ncontainment, with the second being understood as following by symmetry. Closure operators are extensive, X ⊆ cl(X), and idempotent, cl(cl(X)) = cl(X). That is, if we have C1 ⊆ C+2 , then we shall have C+1 ⊆ (C+2 )+ and, by idempotence, C+1 ⊆ C+2 .\nThen it suffices to show that C1 ⊆ C+2 , i.e., for any (x′, x) ∈ C1, we must show\nthat (x′, x) ∈ C+2 as well. Observe by Def. 5 that both ϕ1 and ϕ2 are bijections, then, invertible functions. If ϕ−11 (x) = ϕ −1 2 (x), then we have (x ′, x) ∈ C2 and thus, trivially, (x′, x) ∈ C+2 . Else, ϕ1 and ϕ2 disagree in which equations they map onto x. But we show next, in any case, that we shall have (x′, x) ∈ C+2 .\nTake all equations g ∈ E ′ ⊆ E such that ϕ1(g) 6= ϕ2(g), and let n ≤ |E| be\nthe number of such ‘disagreed’ equations. Now, let f ∈ E ′ be such that its mapped variable is x = ϕ1(f). Construct a sequence of length 2n such that, s0 = ϕ1(f) = x and, for 1 ≤ i ≤ 2n, element si is defined si = ϕ2(ϕ−11 (si−1)). That is, we are defining the sequence such that, for each equation g ∈ E ′, its disagreed mappings ϕ1(g) = xa and ϕ2(g) = xb are such that ϕ1(g) is immediately followed by ϕ2(g). As xa, xb ∈ V ars(g), we have (xa, xb) ∈ C2 and, symmetrically, (xb, xa) ∈ C1.\nA.1. PROOFS OF HYPOTHESIS ENCODING 118\nThe sequence is of form s = 〈x, xf︸ ︷︷ ︸ f , . . . , xa, xb︸ ︷︷ ︸ g , . . . , x2n−1, x2n︸ ︷︷ ︸ h 〉.\nSince x must be in the codomain of ϕ2, we must have a repetition of x at\nsome point 2 ≤ k ≤ 2n in the sequence index, with sk = x and sk−1 = x′′ such that (x′′, x) ∈ C2. If x′′ = x′, then (x′, x) ∈ C2 and obviously (x′, x) ∈ C+2 . Else, note that xf must also be in the codomain of ϕ1, while x ′′ in the codomain of ϕ2. Let ` be the point in the sequence, 3 ≤ ` ≤ 2n−1, at which s` = xf = xa and s`+1 = xb for some xb such that (xf , xb) ∈ C2. It is easy to see that, either we have xb = x′′ or xb 6= x′′ but (xb, x′′) ∈ C+2 . Thus, by transitivity on such a causal chain, we must have (xf , x ′′) ∈ C+2 and eventually (xf , x) ∈ C+2 . Finally, since x′ ∈ V ars(f) and ϕ2(f) = xf , we have (x ′, xf ) ∈ C2 and, by transitivity, (x′, x) ∈ C+2 . 2\nA.1.3 Proof of Theorem 2. “Let Σ be an fd set defined Σ, h-encode(S) for some complete structure S. Then Σ is non-redundant and singleton-rhs but may not be left-reduced (then may not be canonical).”\nProof 19 We will show that properties (a-b) of Def. 9 hold for Σ produced by (Alg. 3) h-encode, but property (c) may not hold.\nAt initialization, the algorithm sets Σ=∅ and then inserts an fd 〈X,A〉 ∈ Σ\nfor each 〈f, x〉 ∈ ϕt scanned, where x 7→ A and X ∩ A = ∅. At termination, for all fd’s in Σ we obviously have |A| = 1 then property (a) holds. Also, note that ϕ : S→ V ars(S) is, by Def. 5, a bijection.\nNow, for property (b) not to hold there must be some fd 〈X,A〉 ∈ Σ that\nis redundant and then can be found in the closure of Γ = Σ \\ 〈X,A〉. By Lemma 4 (below), that can be the case only if A ⊆ X or there is 〈Y,A〉 ∈ Γ for some Y . But from X ∩A = ∅, we have A * X; and from ϕ being a bijection it follows that there can be no such fd in Γ. Thus it must be the case that Σ is non-redundant, i.e., property (b) holds.\nFinally, property (c) does not hold if there can be some fd 〈X,A〉 ∈ Σ with\nY ⊂ X such that Γ = Σ \\ 〈X,A〉 ∪ 〈Y,A〉 has the same closure as Σ. That is, if we may find 〈Y,A〉 ∈ Σ+. Now, pick structure S whose (3× 3) matrix As has rows (1, 0, 0), (1, 1, 0), (1, 1, 1) as an instance. Alg. 3 encodes it into Σ={φ→ x1, x1 υ→\nA.2. PROOFS OF CAUSAL REASONING 119\nx2, x1 x2 υ→ x3}. Let Y ={x1, υ}, and B={x2}. Note that x1 υ → x2 ∈ Σ can be written as 〈Y,B〉 ∈ Σ, and x1 x2 υ→ x3 ∈ Σ as 〈Y B,A〉 ∈ Σ. Now observe that 〈Y,A〉 ∈ Σ+ can be derived by R5 over 〈Y,B〉, 〈Y B,A〉 ∈ Σ, which is sufficient to show that property (c) may not hold. That is, B is “extraneous” in 〈Y B,A〉 ∈ Σ and can be removed from its lhs without loss of information to Σ. 2\nLemma 4 Let Σ be a (Def. 9-a) singleton-rhs fd set on attributes U . Then X→ A can only be in Σ+, where XA ⊆ U , if A ⊆ X or there is non-trivial 〈Y,A〉 ∈ Σ for some Y ⊂ U .\nProof 20 By Lemma 5 (below), we know that X→ A ∈ Σ+ iff A⊆ X+. We need to prove that if A* X and there is no Y → A in singleton-rhs Σ, then A* X+. But this is equivalent to show that (Alg. 4) XClosure gives only correct answers for X+ w.r.t. Σ, which is known (cf. theorem from Ullman [20, p. 389]). Note that XClosure(Σ, X) inserts A in X+ only if A ⊆ X or there is some fd 〈Y,A〉 ∈ Σ. 2\nLemma 5 Let Σ be an fd set. An fd X→ Y is in Σ+ iff Y ⊆ X+, where X+ is the attribute closure of X w.r.t. Σ.\nProof 21 This is from Ullman [20, p. 386]. Let Y =A1 ... An and suppose Y ⊆ X+. Then for each Ai, we have Ai ∈ X+ and, by the definition of X+, we must have 〈X,Ai〉 ∈ Σ+. Then it follows by (R4) union that X → Y is in Σ+ as well. Conversely, suppose 〈X, Y 〉 ∈ Σ+. Then, by (R3) decomposition we have 〈X,Ai〉 ∈ Σ+ for each Ai ∈ Y . 2\nA.2. Proofs of Causal Reasoning\nA.2.1 Proof of Lemma 2 “Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. If 〈X,A〉 ∈ Σ, then A#, the attribute folding of A (w.r.t. Σ) exists and is unique.”\nProof 22 The existance of A# is ensured by the degenerate case where X = A#, as X→ A is itself in ΣB by an empty application of R5. If X→ A is in fact folded w.r.t. Σ, then the folding of A exists. Else, it is not folded yet X→ A is non-trivial\nA.2. PROOFS OF CAUSAL REASONING 120\nbecause by Theorem 2 Σ is non-redundant. Then, by Def. 11 there must be some Y ⊆ U with Y + X such that Y → X is in Σ+ and X 6→ Y . By Def. 10, there is a finite application of R5 over fd’s in Σ to derive Y .−→ X. Then by R2∼R5 over X→ A, we have Y → A. Although there may be many such (intermediate) attribute sets Y ⊂ U along the transitive chaining satisfying the conditions above, we claim there is at least one that is a folding of A. Suppose not. Then, for all such Y ⊂ U , there is some Y ′ ⊂ U with Y ′ + Y such that Y ′ → Y and Y 6→ Y ′, leading to an infinite regress. Nonetheless, in so far as cycles are ruled out by force of Def. 11, then Σ+ must have an infinite number of fd’s. But Σ+ is finite, viz., bounded by 22|U | (cf. [21, p. 165]). . Therefore the folding of A must exist. Moreover, observe that Σ is encoded through ϕ, which is by Def. 5 a bijection. Then we have 〈X,A〉 ∈ Σ for exactly one attribute setX. Then, as a straightfoward follow-up of the rationale that led us to infer the folding existance, note that there must be a single chaining Y n .−→ ... .−→ Y 1 .−→ Y 0 .−→ X .−→ A. Again, as cycles are ruled out by force of Def. 11 and Σ+ is finite, then the folding of A is unique. 2\nA.2.2 Proof of Theorem 3 “Let S(E ,V) be a complete structure, and Σ an fd set encoded given S. Now, let 〈X,A〉 ∈ Σ. Then AFolding(Σ, A) correctly computes A#, the attribute folding of A (w.r.t. Σ) in time O(|S|2).”\nProof 23 For the proof roadmap, note that AFolding is monotone (size of A? can only increase) and terminates precisely when A(i+1) =A(i), where A(i) denotes the attributes in A? at step i of the outer loop. The folding A# of A at step i is A(i) \\ Λ(i). We shall prove by induction, given attribute A from fd X→ A in Σ, that A?\\ Λ returned by AFolding(Σ, A) is the unique attribute folding A# of A.\n(Base case). By Theorem 2, Σ is non-redundant with (then) non-trivial\n〈X,A〉 ∈ Σ for exactly one attribute set X, the algorithm always reaches step i = 1, which is our base case. Then X is placed in A(1) and A in Λ(1), and we have A(1) = XA and Λ(1) = A. Therefore, A(1) \\ Λ(1) = X, and in fact we have 〈X,A〉 ∈ ΣB by an empty application of R5. For it to be specifically in Σ#⊂ ΣB, it must be folded w.r.t. set ∆ of consumed fd’s at this step, viz., ∆(1) ={X→ A}. In fact, as the only fd in ∆(1), by Def. 11 it must be folded w.r.t. ∆(1), and we\nA.2. PROOFS OF CAUSAL REASONING 121\nhave A#= X at step i=1.\n(Induction). Now, let i = k, for k > 1, and assume that 〈A(k)\\ Λ(k), A〉 ∈\nΣ# ⊂ ΣB with A(k) 6= Λ(k). By Lemma 2 we know that A# = A(k) \\ Λ(k) is the unique folding of A at step i= k. For the inductive step, suppose Y is placed in A(k+1) and B in Λ(k+1) because 〈Y,B〉∈ Σ \\∆(k) and B ∈ A(k).\nSince B ∈ A(k) and B /∈ Λ(k) (it is yet just be consumed into Λ(k+1)), we can\nwrite (A(k)\\Λ(k)) = ZB for some Z 6= B, where (A(k) \\ Λ(k))︸ ︷︷ ︸ ZB → A is assumed in Σ#. Now, with the application of R5 consuming Y → B we have (A(k)Y B \\ Λ(k)B)︸ ︷︷ ︸ ZS → A, where S = Y \\ Λ(k). We claim that ZS→ A is folded w.r.t. ∆(k+1).\nSuppose not. Then by Def. 11 there must be some W + ZS such that\nW → ZS is in (∆(k+1))+ but ZS 6→ W . Since ZS 6= ∅, there must be some C ∈ ZS, i.e., C /∈ Λ(k+1). Note that, as W → ZS is in (∆(k+1))+, then by (R3) decomposition we have W → C in (∆(k+1))+ as well. But by Lemma 4 that can only be the case if there is some 〈T,C〉 ∈ ∆(k+1), which means C has been already consumed into Λ(k+1), though C /∈ Λ(k+1). . Finally, as for the time bound, note that in worst case, exactly one fd Y → B is consumed from Σ into ∆ for each step of the outer loop, where |Σ| = |E|. That is, let n = |E|, then n is decreased stepwise in arithmetic progression such that n+ (n−1) + . . .+ 1 = n (n−1)/2 scans are required overall, i.e., O(n2). Note also, however, that B may be the only symbol read at each such fd scan but in worst case at most |U | = |V| symbols are read. Thus our measure n should be actually overestimated n = |E| |V| = |S|. Therefore Alg. 6 is bounded by O(|S|2). 2\nA.2.3 Proof of Corollary 2 “Let S(E ,V) be a complete structure, and Σ an fd set encoded given S. Then algorithm folding(Σ) correctly computes Σ#, the folding of Σ in time that is f(|S|) Θ(|E|), where f(|S|) is the time complexity of (Alg. 6) AFolding.”\nProof 24 By Theorem 3, we know that sub-procedure (Alg. 6) AFolding is correct and terminates. Then (Alg. 5) folding necessarily inserts in Σ# (initialized empty) exactly one fd Z #−→ A for each fd X→ A in Σ scanned. Thus, at termination we have |Σ#|= |Σ|. Again, as AFolding is correct, we know Z is the unique folding of\nA.2. PROOFS OF CAUSAL REASONING 122\nA. Therefore it must be the case that Alg. 5 is correct. Finally, for the time bound, the algorithm iterates over each fd in Σ without having to read its symbols, and at each such step AFolding takes time that is f(n). Thus folding takes f(|S|) Θ(|E|). But we know from Theorem 3 and Remark 4 that f(|S|) ∈ O(|S|), then it takes O(|S| |E|). 2\nA.2.4 Proof of Proposition 3 “Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Let Σ# be the folding of Σ, then Σ# is parsimonious.”\nProof 25 By Lemma 2 we know that, for each fd 〈X,A〉 ∈ Σ, the attribute folding Z of A such that Z #−→ A exists and is unique. That is, for no Y 6= Z we have Y #−→ A. Thus Σ# , folding(Σ) automatically satisfies Def. 13, as long as we show it is canonical (cf. Def. 9).\nMoreover, by Theorem 2 we know that Σ is both non-redundant and singleton-\nrhs. Now, consider by Lemma 2 that AFolding builds a bijection mapping each 〈X,A〉 ∈ Σ to exactly one 〈Z,A〉 ∈ Σ# such that Z #−→ A. Since Σ is singleton-rhs, it is obvious that Σ# is as well and covers all attributes in the rhs of fd’s in Σ. Also, the bijection implies |Σ#| = |Σ|. Since Σ is non-redundant and has exactly one fd with each attribute A in its rhs, then by Lemma 4 so is Σ#.\nFinally we will show that unlike Σ, its folding Σ# must be left-reduced.\nSuppose not. Then for some fd Z→ A in Σ# there is S ⊂ Z such that non-trivial S→ A is in (Σ#)+. Since Z→ A is the only fd in Σ#with A in its rhs and S→ A is non-trivial, we must have S M−→ Z M−→ A.\nNow, suppose S→ A is not folded. Then there is W + S such that W→ S\nis in (Σ#)+ but S 6→ W . Note that W 6= Z, as W + S. Also, W → S and S→ Zimplies W → Z by (R5) transitivity. Note also that S 6→ W and S→ Z implies Z 6→ W . But Z → A is assumed folded. . That is, S → A must be folded. Then we have both S→ A and Z→ A folded, though S 6= Z. That is, the attribute folding of A is not unique, even though we know by Lemma 2 that it must be unique. . Thus Σ#must be left-reduced, altogether, therefore, parsimonious. 2\nA.2. PROOFS OF CAUSAL REASONING 123\nA.2.5 Proof of Theorem 4 “Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Then xa, xb ∈ V are such that xb is causally dependent on xa, i.e., (xa, xb) ∈ C+ϕ iff there is some non-trivial fd 〈X,B〉 ∈ ΣB with A ∈ X, where B 7→ xb and A 7→ xa.”\nProof 26 We prove the statement by induction. We consider first the ‘if’ direction, and then its ‘only if’ converse. (Base case). Let 〈X,B〉 ∈ Σ be some fd with A ∈ X, where B 7→ xb and A 7→ xa. By Theorem 2, it is non-trivial and then by default (i.e., an empty application of R5) it is in ΣB. But as X → B is in Σ, (Alg. 3) h-encode ensures that there is exactly one equation f ∈ E such that ϕ(f) = xb and xa ∈ V ars(f) where B 7→ xb and A 7→ xa. Then by force of Eq. 3.1 we must have (xa, xb) ∈ Cϕ. Thus, we obviously have (xa, xb) ∈ C+ϕ as well.\n(Induction). Now, recall Armstrong’s (R5) pseudo-transitivity rule adapted\nhere for the particular case of singleton-rhs fd sets, viz., if Y → C and CZ → B, then Y Z → B. By the inductive hypothesis, take any two non-trivial fd’s 〈Y,C〉, 〈CZ,B〉 ∈ ΣB with B /∈ Y and assume that the causal dependency property holds for their attributes that encode variables. That is, let D ∈ Y and E ∈ Z, where D 7→ xd and E 7→ xe for xd, xe ∈ V such that (xd, xc), (xe, xb), (xc, xb) ∈ C+ϕ . Note that both Y → C and CZ → B are non-trivial, then C /∈ Y , B /∈ Z and B 6= C. Moreover, B /∈ Y has been assumed such that the fd 〈Y Z,B〉 ∈ ΣB to be derived by R5 over Y → C and CZ→ B is also non-trivial to satisfy the condition of the theorem. Now, it is easy to see that the property holds likewise for nontrivial fd 〈Y Z,B〉 ∈ ΣB. In fact, (xd, xc), (xc, xb) ∈ C+ϕ implies (xd, xb) ∈ C+ϕ and also by the inductive hypothesis we have (xe, xb) ∈ C+ϕ . That is, for either some D ∈ Y or some E ∈ Z, we must have (xd, xb), (xe, xb) ∈ C+ϕ .\nThe converse ‘only if’ direction can be shown by a symmetrical inductive\nargument. That is, for the base case suppose (xa, xb) ∈ Cϕ. Then, by Eq. 3.1 we know there is some f ∈ E such that ϕ(f) = xb and xa ∈ V ars(f). Moreover, in that case (Alg. 3) h-encode ensures there must be some non-trivial fd 〈X,B〉 ∈ Σ with A ∈ X where B 7→ xb and A 7→ xa. Thus by an empty application of R5 we\nA.2. PROOFS OF CAUSAL REASONING 124\nhave 〈X,B〉 ∈ ΣB. The inductive step shows the property still holds for arbitrary causal dependencies in C+ϕ . 2\nA.2.6 Proof of Proposition 4 “Let S(E ,V) be a structure with variable x ∈ V. Then x can only be a first cause of some y ∈ V if x is exogenous. Accordingly, any variable y ∈ V can only have some first cause x ∈ V if it is endogenous.”\nProof 27 The proof is straightforward from definitons. For the first statement, suppose by contradiction that x ∈ V is not exogenous but is a first cause of some y ∈ V . By Def. 5, ϕ is bijective then there is some f ∈ E such that ϕ(f) = x. Moreover, as x is not exogenous then by Def. 8 it must be endogenous. In other words, there must be some xa ∈ V such that xa 6= x ∈ V ars(f) and then by Eq. 3.1 we have (xa, x) ∈ Cϕ hence (xa, x) ∈ C+ϕ . However, as x is a first cause, by Def. 14 there can be no y ∈ V such that (y, x) ∈ C+ϕ . . Now, a symmetrical argument proves the second statement. Also by contradiction take a variable y ∈ V that is not endogenous and suppose it has some first cause x ∈ V . As variable y is not endogenous then by Def. 8 it must be exogenous. In other words, there must be f ∈ E such that V ars(f) = {y}. Thus for any total causal mapping ϕ over S, we must have ϕ(f) = y and, for no x ∈ V , we have (x, y) ∈ Cϕ. Therefore it is not possible to derive (x, y) ∈ C+ϕ for some x ∈ V . But as y has some first cause x ∈ V by assumption, we must have (x, y) ∈ C+ϕ . . 2\nA.2.7 Proof of Lemma 3 “Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Then a variable xa ∈ V can only be a first cause of some variable xb ∈ V, where 〈X,B〉 ∈ Σ, and B 7→ xb, A 7→ xa, if either (i) A ∈ X or (ii) A /∈ X but there is 〈Z,C〉 ∈ ΣB with A ∈ Z and C ∈ X.”\nProof 28 We prove the statement by construction out of Theorem 4.\nBy Def. 14, one of the conditions for xa to be a first cause of xb is that\n(xa, xb) ∈ C+ϕ . Moreover, by Theorem 4 we know that (xa, xb) ∈ C+ϕ can only hold if there is some non-trivial fd 〈Z,B〉 ∈ ΣB with A ∈ Z, where B 7→ xb and A 7→ xa. Now, by Def. 5 ϕ is bijective then there is 〈X,B〉 ∈ Σ. Moreover, since\nA.2. PROOFS OF CAUSAL REASONING 125\nΣ is parsimonious, X→ B is the only fd in Σ with B in its rhs. So let A /∈ X. Then we know X 6= Z hence X→ B cannot be the fd required by Theorem 4.\nThen such fd Z .−→ B with A ∈ Z can only exist if derived by some finite\napplication of R5. That is, there must be some 〈Z,C〉 ∈ Σ with A ∈ Z such that X = CW for some W and then R5 can be applied over 〈Z,C〉, 〈CW,B〉 ∈ Σ to get non-trivial 〈ZW,B〉 ∈ ΣB where A ∈ Z.\nNow, it is easy to see that when such fd Z→ C with A ∈ Z does not exists\nin ΣB (the second condition of the lemma), then obviously Z→ C cannot exist in Σ to satisfy the requirement imposed by Theorem 4. That is, (xa, xb) /∈ C+ϕ . 2\nA.2.8 Proof of Theorem 5 “Let S(E ,V) be a complete structure, ϕ a total causal mapping over S and Σ an fd set encoded through ϕ given S. Now, let B be an attribute that encodes some variable xb ∈ V. If 〈X,B〉 ∈ Υ(Σ)#,1 then every first cause xa of xb (if any) is encoded by some attribute A ∈ X.”\nProof 29 We show that the existance of a missing first cause xc of xb for folded X #−→ B, where B 7→ xb and C 7→ xc but C /∈ X leads to a contradiction.\nSuppose, by contradiction, that there is some missing first cause xc ∈ V of\nxb, where C 7→ xc and C /∈ X. Then, by Lemma 3, since variable xc is a first cause of variable xb, it must be exogenous and, for 〈Y,B〉 ∈ Υ(Σ) either (i) C ∈ Y or (ii) C /∈ Y but there is 〈Z,D〉 ∈ Υ(Σ)B with C ∈ Z and some D ∈ Y .\nIn the first case (i), since xc is exogenous and Σ is parsimonious, we have\n〈φ,C〉 ∈ Σ but by Def. 15 there can be no W→ C in the υ-projection Υ(Σ) of Σ. That is, C cannot be ‘consumed’ by R5 and then 〈Y,B〉 ∈ Υ(Σ) with C ∈ Y implies that, for any 〈W,B〉 ∈ Υ(Σ)B, we must have C ∈ W . However, by assumption we have 〈X,B〉 ∈ Υ(Σ)# then, by Def. 12, 〈X,B〉 ∈ Υ(Σ)B, yet C /∈ X. . In the second case (ii), observe that C ∈ Z and D ∈ Y , and let Y = DS. Then by R5 over Z→ D and DS→ B we get 〈ZS,B〉 ∈ Υ(Σ)B, where C ∈ Z. Well, either ZS → B is folded or it is not, rendering two cases for analysis. If ZS→ B is folded, then both ZS→ B and X → B are folded. But as Υ(Σ) is 1 Note that the folding is taken w.r.t. the υ-projection of Σ, then xb where B 7→ xb is an endogenous variable.\nA.3. PROOFS OF PROBABILISTIC DB SYNTHESIS 126\nparsimonious, then by Lemma 2 the folding of B must be unique. Therefore we must have ZS = X, with C ∈ Z but C /∈ X. . Else, assume ZS→ B is not folded. Then by Def. 11 there is some W with W + ZS such that non-trivial W→ ZS is in Υ(Σ)+ and ZS 6→ W .\nHowever, as C ∈ Z and W → ZS, by (R3) decomposition we must have\nW → C in Υ(Σ)+, either with C ∈ W or with C /∈ W and then W → C is nontrivial. But we know the latter cannot be the case by the same argument used in the first case (i), viz., xc is exogenous with C 7→ xc and Σ is parsimonious. That is, we must have C ∈ W . Furthermore, as W → ZS in Υ(Σ)+, then by R5 over ZS→ B we get 〈W,B〉 ∈ Υ(Σ)B with C ∈ W . Now it is easy to see that the same situation recurs to W→ B. If it is not folded, eventually for some T we will have T .−→ W .−→ B with C ∈ T , where T→ B will be folded just like X→ B. That is, by (Lemma 2) the uniqueness of the folding of B, we will have T = X with C ∈ T and C /∈ X. . 2\nA.3. Proofs of Probabilistic DB Synthesis\nA.3.1 Proof of Theorem 6 “Let Sk and Hk be (resp.) the complete structure and ‘big’ fact table of hypothesis k, and let Γ′k be the repaired factorization of Sk over Hk, and Y0 the ‘explanation’ table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize4u(Sk, Hk, Y0). Then Y k is in BCNF w.r.t. Γ′k and is minimalcardinality.”\nProof 30 Let Y ik [ViDi |φAiGi ] and Y j k [Vj Dj |S T ] be (resp.) any u-factor projection and predictive projection of Hk. Note that all fd’s in Γ ′ k are either in Φ(Γ′k) of form φAi → B or φ → B, or in Υ(Γ′k) of form A1A2 ... A` S→ T with υ ∈ S. We must show that no fd in (Γ′k)+ can violate Y ik or Y j k . It is easy to see that the projection (cf. Def. 20) of non-trivial fd’s in Φ(Γ′k) + onto Y jk is empty, just like the projection of non-trivial fd’s in Υ(Γ′k) + onto Y ik .\nFor the u-factor projections, note by Def. 21 that for any fd X → C in\n(Γ′k) + to violate BCNF in Y ik [ViDi |φAiGi ], it must be non-trivial (C 6∈ X) with XC ⊆ φAiGi but X 6→ φAiGi (that is, X is not a superkey for Y ik ). Note that we\nA.3. PROOFS OF PROBABILISTIC DB SYNTHESIS 127\nhave both φAi→ B and φ→ AiB in (Γ′k)+ for any B ∈ Gi, but both φAi and φ are superkeys for Y ik . Also, that there can be no non-trivial fd’s 〈X,C〉 ∈ Φ(Γ′k)+ with φ 6∈ X, and by the definition of Problem 1 we know that AiGi is a maximal group. So, for any non-trivial 〈X,C〉 ∈ Φ(Γ′k)+, X must be a superkey for Y ik . Thus no u-factor projection can be subject of BCNF violation w.r.t. Γ′k.\nNow, for predictive projection Y jk [Vj Dj |S T ] let us reconstruct the process\ntowards deriving 〈S, T 〉 ∈ Υ(Γ′k)B. Note that it is derived by synthesize4u simulating ` applications of R5 over 〈φ,Ai〉, 〈A1A2 ...A` S, T 〉 ∈ Γ′k for 0 ≤ i ≤ `. Note also that (i) no cyclic fd’s can be involved in such R5 applications, as they must always be over an fd in Φ(Γ′k); and (ii) as a result of (Alg. 7) merge, A1A2 ...A` S→ T was the only non-trivial fd in the projection πA1 A2 ...A` ST (Γ ′ k). Thus, the only nontrivial fd’s in the projection πST ( (Γ ′ k) ) + in addition to S→ T itself must be of form S→ C rendered out of (R3) decomposition from it for all C ∈ T . In any such fd’s, we have S as a superkey for Y jk . Therefore no predictive projection can be subject of BCNF violation w.r.t. Γ′k.\nFor the minimality note, as a consequence of (Alg. 7) merge, any two\nschemes Y pk [XZ], Y q k [VW ] are rendered by synthesize4u into Y k iff we have fd’s 〈X,Z〉, 〈V,W 〉 ∈ (Γ′k)+ and X 6↔ V , i.e., it is not the case that both X→ V and V → X hold in (Γ′k)+. Now, to prove that Y k is minimal-cardinality, we have to find that merging any such pair of arbitrary schemes shall hinder BCNF in Y k. In fact, take Y ′k := Y k \\ (Y p k [XZ] ∪ Y q k [VW ]) ∪ Y `k [XZVW ]. As X 6↔ V , then neither X nor V can be a superkey for Y `k , which therefore cannot be in BCNF. 2\nA.3.2 Proof of Theorem 7 “Let Sk be the complete structure of hypothesis k, and Hk[U ] its ‘big’ fact table such that Γ′k is the repaired factorization of Sk over Hk and Y0 is the ‘explanation’ table where hypothesis k is recorded. Now, let Y k be a U-relational schema defined Y k , synthesize4u(Sk, Hk, Y0). Then,\n(a) the join ./mi=1 Y i k [ViDi |φAiGi ] of any subset of the u-factor projections\nof Hk is lossless w.r.t. Γ ′ k.\n(b) any predictive projection Y jk [Vj Dj |S T ], result of a join of the theoretical\nu-factor Y0 [V0D0 |φ υ ] with the ‘big’ fact table Hk[U ] and in turn with\nA.3. PROOFS OF PROBABILISTIC DB SYNTHESIS 128\nu-factor projections Y ik [ViDi |φAiGi ], is lossless w.r.t. Γ′k.”\nProof 31 For item (a), by Lemma 6, we know that any pair Y ik [ViDi |φAiGi ], Y jk [Vj Dj |φAj Gj ] of u-factor projections of Hk will have a lossless join w.r.t. Γ′k iff (φAiGi ∩ φAj Gj)→ (φAiGi \\ φAj Gj) or (φAiGi ∩ φAj Gj)→ (φAj Gj \\ φAiGi) hold in (Γ ′ k) +. By Def. 17, we know that (φAiGi ∩ φAj Gj) = {φ}, and φAiGi \\ φAj Gj = AiGi. In fact φ→ AiGi is a repaired fd in Γ′k, therefore Y ik and Y jk have a lossless join. Now, since the join is an associative operation [20, p. 62], and as we have chosen Y ik and Y j k arbitrarily, then clearly any subset of the u-factor projections must have a lossless join.\nFor item (b), for any predictive projection Y jk [Vj Dj |S T ] take the join ./\nY ik [ViDi |φAiGi ] of u-factor projections such that, for all Ai ∈ AiGi, we have Ai ∈ W ⊂ Z where S = Z \\W and 〈Z, T 〉 ∈ Γ′k. That is, Ai is a pivot attribute representing a first cause of some C ∈ T . By item (a), we know that such join is lossless.\nWe must show that the join ./ Y ik with ‘big’ fact table Hk[U ] is also lossless.\nBy Lemma 6, that is the case iff (φAiGi ∩ U) → (φAiGi \\ U) or (φAiGi ∩ U) → (U \\ φAj Gj) hold in (Γ′k)+. In fact, we have (φAiGi ∩ U) = φAiGi and (φAiGi \\ U) = ∅ such that φAiGi → ∅ is trivially in (Γ′k)+.\nFinally, the join of theoretical u-factor Y0 [V0D0 |φ υ ] with big fact table\nHk[U ] must be lossless likewise. In fact, note that (φ υ ∩ U) = φ υ, and (φ υ\\U) = ∅. Then also trivially we have φ υ→ ∅, which is in (Γ′k)+ as well. Since the join is commutative [20, p. 62], the order of application is irrelevant therefore the join of all joins examined above taken together must be lossless. 2\nLemma 6 Let Σ be a set of fd’s on attributes U , and Ri[S], Rj[T ] ∈ R[U ] be relation schemes with ST ⊆ U ; and let πST (Σ) be the projection of Σ onto ST . Then Ri[S] and Rj[T ] have a lossless join w.r.t. πST (Σ) iff (S ∩ T ) → (S \\ T ) or (S ∩ T )→ (T \\ S) hold in πST (Σ)+.\nProof 32 See Ullman [20, p. 397]. 2"
    } ],
    "references" : [ {
      "title" : "The fourth paradigm: Dataintensive scientific discovery",
      "author" : [ "T. HEY", "S. TANSLEY", "K. TOLLE" ],
      "venue" : "Microsoft Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Distilling free-form natural laws from experimental data",
      "author" : [ "M. SCHMIDT", "H. LIPSON" ],
      "venue" : "Science, Washington, v. 324,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Data science and prediction",
      "author" : [ "V. DHAR" ],
      "venue" : "Communications of the ACM, v. 56,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Big data and its technical challenges",
      "author" : [ "H.V. JAGADISH", "J. GEHRKE", "A. LABRINIDIS", "Y. PAPAKONSTANTI- NOU", "J.M. PATEL", "R. RAMAKRISHNAN", "C. SHAHABI" ],
      "venue" : "Communications of the ACM, v. 57,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Computational Science: Ensuring America’s competitiveness",
      "author" : [ "M.R. BENIOFF", "E.D. (Eds.). LAZOWSKA" ],
      "venue" : "PITAC (US President’s Information Technology Advisory Committee),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Data exploration of turbulence simulations using a database cluster",
      "author" : [ "E. PERLMAN", "R. BURNS", "Y. LI", "C. MENEVEAU" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "The Blue Brain Project",
      "author" : [ "H. MARKRAM" ],
      "venue" : "Nature Reviews Neuroscience,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Managing scientific data",
      "author" : [ "A. AILAMAKI", "V. KANTERE", "D. DASH" ],
      "venue" : "Comm. ACM, v. 53,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Scientific data management at the Johns Hopkins Institute for  BIBLIOGRAPHY  109 Data Intensive Engineering and Science",
      "author" : [ "Y. AHMAD", "R. BURNS", "M. KAZHDAN", "C. MENEVEAU", "A. SZALAY", "A. TERZIS" ],
      "venue" : "SIGMOD Record, v. 39,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Beyond big data",
      "author" : [ "J.B. CUSHING" ],
      "venue" : "Computing in Science & Engineering, v. 15,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Point: Hypotheses first",
      "author" : [ "R. WEINBERG" ],
      "venue" : "Nature, London, v. 464,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Counterpoint: Data first",
      "author" : [ "T. GOLUB" ],
      "venue" : "Nature, London, v. 464,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Where is the brain in the Human Brain Project",
      "author" : [ "Y. FRÉGNAC", "G. LAURENT" ],
      "venue" : "Nature, London,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "A historical introduction to the philosophy of science",
      "author" : [ "J. LOSEE" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2001
    }, {
      "title" : "Introduction to bayesian statistics",
      "author" : [ "W.M. BOLSTAD" ],
      "venue" : "2nd. ed. Wiley- Interscience,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "MayBMS: A system for managing large uncertain and probabilistic databases",
      "author" : [ "C. KOCH" ],
      "venue" : "Managing and Mining Uncertain Data, Chapter",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Principles of Databases and Knowledge-Base Systems",
      "author" : [ "J. ULLMAN" ],
      "venue" : "Computer Science Press,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1988
    }, {
      "title" : "Theory of relational databases",
      "author" : [ "D. MAIER" ],
      "venue" : "Computer Science Press,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1983
    }, {
      "title" : "A call to arms: Revisiting database design",
      "author" : [ "A. BADIA", "D. LEMIRE" ],
      "venue" : "SIG- MOD Record, v. 40,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Causal ordering and identifiability",
      "author" : [ "H. SIMON" ],
      "venue" : "Studies in Econometric Methods,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1953
    }, {
      "title" : "Causality: Models, reasoning, and inference",
      "author" : [ "J. PEARL" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2000
    }, {
      "title" : "A survey of web information extraction systems",
      "author" : [ "C.-H. CHANG", "M. KAYED", "M.R. GIRGIS", "K. SHAALAN" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering, v. 18,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Fast and simple relational processing of uncertain data",
      "author" : [ "L. ANTOVA", "T. JANSEN", "C. KOCH", "D. OLTEANU" ],
      "venue" : "Proc. of IEEE ICDE",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2008
    }, {
      "title" : "Υ-DB: A system for datadriven hypothesis management and analytics",
      "author" : [ "B. GONCALVES", "F.C. SILVA", "F. PORTO" ],
      "venue" : "Technical report, LNCC,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2015
    }, {
      "title" : "Design-theoretic encoding of deterministic hypotheses as constraints and correlations in U-relational databases",
      "author" : [ "B. GONCALVES", "F. PORTO" ],
      "venue" : "Technical report, LNCC,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    }, {
      "title" : "Cause and counterfactual",
      "author" : [ "H. SIMON", "N. RESCHER" ],
      "venue" : "Philosophy of Science, v. 33,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1966
    }, {
      "title" : "ModelDB: A database to support computational neuroscience",
      "author" : [ "M. HINES", "T. MORSE", "M. MIGLIORE", "N. CARNEVALE", "G. SHEPHERD" ],
      "venue" : "J. Comput. Neurosci., v. 17,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2004
    }, {
      "title" : "BioModels Database: A repository of mathematical models of biological processes",
      "author" : [ "V. CHELLIAH", "C. LAIBE", "N. Le Novère" ],
      "venue" : "Method. Mol. Biol.,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "Integration from proteins to organs: the Physiome Project",
      "author" : [ "P.J. HUNTER", "T.K. BORG" ],
      "venue" : "Nat. Rev. Mol. Cell. Biol.,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2003
    }, {
      "title" : "Strategies for the Physiome Project",
      "author" : [ "J.B. BASSINGTHWAIGHTE" ],
      "venue" : "Ann. Biomed. Eng., v",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2000
    }, {
      "title" : "Data cleaning: Problems and current approaches",
      "author" : [ "E. RAHM", "H. Hai Do" ],
      "venue" : "IEEE Data Engineering Bulletin, v. 23,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2001
    }, {
      "title" : "Modeling and querying possible repairs in duplicate detection",
      "author" : [ "G. BESKALES", "M.A. SOLIMAN", "I.F. ILYAS", "S. BEN-DAVID" ],
      "venue" : "PVLDB, v. 2,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2009
    }, {
      "title" : "Data model for scientific models and hypotheses",
      "author" : [ "F. PORTO", "S. SPACAPPIETRA" ],
      "venue" : "The evolution of Conceptual Modeling,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2011
    }, {
      "title" : "Data-driven Neuroscience: Enabling breakthroughs via innovative data management",
      "author" : [ "A. STOUGIANNIS", "F. TAUHEED", "M. PAVLOVIC", "T. HEINIS", "A. AILA- MAKI" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2013
    }, {
      "title" : "I/O streaming evaluation of batch queries for data-intensive computational turbulence",
      "author" : [ "K. KANOV", "E.A. PERLMAN", "R.C. BURNS", "Y. AHMAD", "A.S. SZALAY" ],
      "venue" : null,
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2011
    }, {
      "title" : "NoDB: Efficient query execution on raw data",
      "author" : [ "I. ALAGIANNIS", "R. BOROVICA", "M. BRANCO", "S. IDREOS", "A. AILAMAKI" ],
      "venue" : "files. In: Proc. of ACM SIGMOD",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2012
    }, {
      "title" : "Representation of research hypotheses",
      "author" : [ "L. SOLDATOVA", "A. RZHETSKY" ],
      "venue" : "J. Biomed. Sem.,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2011
    }, {
      "title" : "The automation of science",
      "author" : [ "KING", "R. D" ],
      "venue" : "Science, Washington, v. 324,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2009
    }, {
      "title" : "HyBrow: a prototype system for computer-aided hypothesis evaluation",
      "author" : [ "S RACUNAS" ],
      "venue" : "Bioinformatics, v. 20,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2004
    }, {
      "title" : "SWAN: A distributed knowledge infrastructure for alzheimer disease research",
      "author" : [ "GAO Y" ],
      "venue" : "J. Web Semantics,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2006
    }, {
      "title" : "Hypotheses, evidence and relationships: The HypER approach for representing scientific knowledge claims",
      "author" : [ "A de WAARD" ],
      "venue" : "ISWC Proc. of Workshop on Semantic Web Applications in Scientific Discourse",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2009
    }, {
      "title" : "A note on the correctness of the causal ordering algorithm",
      "author" : [ "D. DASH", "M.J. DRUZDZEL" ],
      "venue" : "Artif. Intell., v. 172,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2008
    }, {
      "title" : "Causality in Bayesian belief networks",
      "author" : [ "M.J. DRUZDZEL", "H.A. SIMON" ],
      "venue" : "Proc. of Int. Conf. on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1993
    }, {
      "title" : "Biclustering algorithms for biological data analysis: A survey",
      "author" : [ "S.C. MADEIRA", "A.L. OLIVEIRA" ],
      "venue" : "IEEE Transactions on Computational Biology and Bioinformatics,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2004
    }, {
      "title" : "Information-theoretic coclustering",
      "author" : [ "I.S. DHILLON", "S. MALLELA", "D.S. MODHA" ],
      "venue" : "Proc. of ACM SIGKDD",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2003
    }, {
      "title" : "An efficient algorithm for solving pseudo clique enumeration problem",
      "author" : [ "UNO T" ],
      "venue" : "Algorithmica, v. 56,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 2010
    }, {
      "title" : "Computers and intractability: A guide to the theory of NP-completeness",
      "author" : [ "M.R. GAREY", "D.S. JOHNSON" ],
      "venue" : "1st. ed., Series of Books in the Mathematical Sciences",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 1979
    }, {
      "title" : "The NP-completeness column: an ongoing guide",
      "author" : [ "D.S. JOHNSON" ],
      "venue" : "J. Algorithms, v. 8,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 1987
    }, {
      "title" : "An n algorithm for maximum matchings in bipartite graphs",
      "author" : [ "J.E. HOPCROFT", "R.M. KARP" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 1973
    }, {
      "title" : "On the definition of the causal relation",
      "author" : [ "H. SIMON" ],
      "venue" : "The Journal of Philosophy,",
      "citeRegEx" : "59",
      "shortCiteRegEx" : "59",
      "year" : 1952
    }, {
      "title" : "Automated modelling of physical systems",
      "author" : [ "P.P. NAYAK" ],
      "venue" : null,
      "citeRegEx" : "60",
      "shortCiteRegEx" : "60",
      "year" : 1996
    }, {
      "title" : "Constraint management in conceptual design. In: Knowledge Based Expert Systems in Engineering: Planning and Design",
      "author" : [ "D. SERRANO", "D.C. GOSSARD" ],
      "venue" : "Computational Mechanics Publications,",
      "citeRegEx" : "61",
      "shortCiteRegEx" : "61",
      "year" : 1987
    }, {
      "title" : "Synthesizing third normal form relations from functional dependencies",
      "author" : [ "P. BERNSTEIN" ],
      "venue" : "ACM Trans. on Database Systems,",
      "citeRegEx" : "62",
      "shortCiteRegEx" : "62",
      "year" : 1976
    }, {
      "title" : "Computational problems related to the design of normal form relational schemas",
      "author" : [ "C. BEERI", "P. BERNSTEIN" ],
      "venue" : "ACM Trans. on Database Systems, v. 4,",
      "citeRegEx" : "63",
      "shortCiteRegEx" : "63",
      "year" : 1979
    }, {
      "title" : "Causality in databases",
      "author" : [ "A MELIOU" ],
      "venue" : "IEEE Data Eng. Bull., v. 33,",
      "citeRegEx" : "64",
      "shortCiteRegEx" : "64",
      "year" : 2010
    }, {
      "title" : "The complexity of causality and responsibility for query answers and non-answers",
      "author" : [ "A. MELIOU", "W. GATTERBAUER", "K.F. MOORE", "D. SUCIU" ],
      "venue" : "PVLDB, v. 4,",
      "citeRegEx" : "65",
      "shortCiteRegEx" : "65",
      "year" : 2010
    }, {
      "title" : "Sensitivity analysis and explanations for robust query evaluation in probabilistic databases",
      "author" : [ "B. KANAGAL", "J. LI", "A. DESHPANDE" ],
      "venue" : "Proc. of ACM SIGMOD",
      "citeRegEx" : "66",
      "shortCiteRegEx" : "66",
      "year" : 2011
    }, {
      "title" : "Reverse data management",
      "author" : [ "A. MELIOU", "W. GATTERBAUER", "D. SUCIU" ],
      "venue" : "PVLDB, v. 4,",
      "citeRegEx" : "67",
      "shortCiteRegEx" : "67",
      "year" : 2011
    }, {
      "title" : "TANE: An efficient algorithm for discovering functional and approximate dependencies",
      "author" : [ "Y HUHTALA" ],
      "venue" : "Computer Journal, v. 42,",
      "citeRegEx" : "68",
      "shortCiteRegEx" : "68",
      "year" : 1999
    }, {
      "title" : "Modeling and reasoning with Bayesian Networks",
      "author" : [ "A. DARWICHE" ],
      "venue" : null,
      "citeRegEx" : "69",
      "shortCiteRegEx" : "69",
      "year" : 2009
    }, {
      "title" : "Causal Networks: Semantics and expressiveness",
      "author" : [ "J.P. TOM S. VERMA" ],
      "venue" : "Proc. of the 4th Conf. on Uncertainty in Artificial Intelligence (UAI’88). North-Holland Publishing Co.,",
      "citeRegEx" : "70",
      "shortCiteRegEx" : "70",
      "year" : 1988
    }, {
      "title" : "Schema design for uncertain databases",
      "author" : [ "A. Das Sarma", "J. ULLMAN", "J. WIDOM" ],
      "venue" : "Proc. of AMW",
      "citeRegEx" : "71",
      "shortCiteRegEx" : "71",
      "year" : 2007
    }, {
      "title" : "The decomposition versus synthetic approach to relational database design",
      "author" : [ "R. FAGIN" ],
      "venue" : "Proc. of VLDB",
      "citeRegEx" : "72",
      "shortCiteRegEx" : "72",
      "year" : 1977
    }, {
      "title" : "Making database systems usable",
      "author" : [ "JAGADISH", "H. V" ],
      "venue" : "In: SIGMOD",
      "citeRegEx" : "73",
      "shortCiteRegEx" : "73",
      "year" : 2007
    }, {
      "title" : "CRIUS: User-friendly database design",
      "author" : [ "L. QIAN", "K. LEFEVRE", "H.V. JAGADISH" ],
      "venue" : "PVLDB, v. 4,",
      "citeRegEx" : "74",
      "shortCiteRegEx" : "74",
      "year" : 2010
    }, {
      "title" : "Conditioning probabilistic databases",
      "author" : [ "C. KOCH", "D. OLTEANU" ],
      "venue" : "PVLDB, v. 1,",
      "citeRegEx" : "75",
      "shortCiteRegEx" : "75",
      "year" : 2008
    }, {
      "title" : "Identifying physiological origins of baroreflex dysfunction in salt-sensitive hypertension in the Dahl SS rat",
      "author" : [ "S.M. BUGENHAGEN", "A.W.J. COWLEY", "D.A. BEARD" ],
      "venue" : "Physiological Genomics,",
      "citeRegEx" : "76",
      "shortCiteRegEx" : "76",
      "year" : 2010
    }, {
      "title" : "The ten-year cycle in numbers of the lynx in Canada",
      "author" : [ "C. ELTON", "M. NICHOLSON" ],
      "venue" : "Journal of Animal Ecology, v. 11,",
      "citeRegEx" : "77",
      "shortCiteRegEx" : "77",
      "year" : 1942
    }, {
      "title" : "Predicting essential components of signal transduction networks: A dynamic model of guard cell abscisic acid signaling",
      "author" : [ "S. LI", "S.M. ASSMANN", "R. ALBERT" ],
      "venue" : "PLOS Biology,",
      "citeRegEx" : "78",
      "shortCiteRegEx" : "78",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In view of the paradigm shift that makes science ever more data-driven [1], in this thesis we demonstrate that large deterministic scientific hypotheses can be effectively encoded and managed as a kind of uncertain and probabilistic data.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "Hypotheses can also be learned in large scale, as exhibited in the Eureqa project [2].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 2,
      "context" : "In fact, we can refer nowadays to a broad, modern context of data science [3] and big data [4] in which the complexity and scale of so-called ‘data-driven’ problems require proper data management tools for the predicted data to be analyzed effectively.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "In fact, we can refer nowadays to a broad, modern context of data science [3] and big data [4] in which the complexity and scale of so-called ‘data-driven’ problems require proper data management tools for the predicted data to be analyzed effectively.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : ") “a rapidly growing multidisciplinary field that uses advanced computing capabilities to understand and solve complex problems” [5].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "It is generally considered that computational science models, interpreted here as hypotheses to explain real-world phenomena, are of strategic relevance [5].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : "Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7].",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "Notorious examples are the John Hopkins Turbulance Databases [6], and the Human Brain Project (HBP) neuroscience simulation datasets [7].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].",
      "startOffset" : 215,
      "endOffset" : 221
    }, {
      "referenceID" : 8,
      "context" : "Nonetheless, while the use case for exploratory analytics is currently well understood and many of its challenges have already been coped with so that high-resolution simulation data is increasingly more accessible [8, 9], only very recently, as part of this thesis work, the use case of hypothesis management has been taken into account for predictive analytics [10].",
      "startOffset" : 215,
      "endOffset" : 221
    }, {
      "referenceID" : 9,
      "context" : "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].",
      "startOffset" : 137,
      "endOffset" : 149
    }, {
      "referenceID" : 10,
      "context" : "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].",
      "startOffset" : 137,
      "endOffset" : 149
    }, {
      "referenceID" : 11,
      "context" : "In fact, there is a pressing call for innovative technology to integrate (observed) data and (simulated) theories in a unified framework [11, 12, 13].",
      "startOffset" : 137,
      "endOffset" : 149
    }, {
      "referenceID" : 13,
      "context" : "discovery) and predictive analytics (context of justification), and highlights the loop between hypothesis formulation and testing [15].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 14,
      "context" : "Note that such a ‘sampling’ does not incur in any additional uncertainty as typical of statistical sampling [16].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "One of the state-of-the-art probabilistic data models is the U-relational representation system with its probabilistic world-set algebra (p-WSA) implemented in MayBMS [18].",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 15,
      "context" : "The first is the less systematic, as the user has to model for the data and correlations by steering all the p-DB construction process (MayBMS’ use cases [18], e.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 16,
      "context" : "[20, 21, 22]) that are basic input to algorithmic synthesis.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 17,
      "context" : "[20, 21, 22]) that are basic input to algorithmic synthesis.",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 16,
      "context" : "A related concept which is also a major one for us is that of normalization [20, 21, 22], viz.",
      "startOffset" : 76,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "A related concept which is also a major one for us is that of normalization [20, 21, 22], viz.",
      "startOffset" : 76,
      "endOffset" : 88
    }, {
      "referenceID" : 19,
      "context" : "In fact, given a system of equations with a set of variables appearing in them, in a seminal article Simon introduced an asymmetrical, functional relation among variables that establishes a (so-called) causal ordering [24].",
      "startOffset" : 218,
      "endOffset" : 222
    }, {
      "referenceID" : 20,
      "context" : "also [25]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "Although we perform some sort of information extraction [26] for the acquisition of hypotheses from some model repositories on the web, it is very basic and ad-hoc in order to obtain a testbed for our method.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 15,
      "context" : "In fact, the performance of U-relations and p-WSA has been extensively evaluated and shown to be effective [27, 18].",
      "startOffset" : 107,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "The innovative system of Υ-DB has been described in a ‘system prototype demonstration’ paper [28].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 24,
      "context" : "Chapters 3, 4, and 5) are formulated into a formal method for the design of hypothesis p-DB’s which is described in a technical report [29].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "High-throughput technology and large-scale scientific experiments provide scientists with empirical data that has to be extracted, transformed and loaded before it is ready for analysis [1].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 13,
      "context" : "Hypotheses, however, are tentative explanations of phenomena [15], which characterizes a different kind of uncertain data.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 1,
      "context" : ", in the Eureqa project [2].",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "Scientific hypotheses are tested by way of their predictions [15].",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "However, for computing predictions, deterministic hypotheses are applied asymmetrically as functions [30].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 13,
      "context" : "observable, but only their joint results (the predictions) [15].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : ", [31, 32, 33]).",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 27,
      "context" : ", [31, 32, 33]).",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 28,
      "context" : ", [31, 32, 33]).",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 28,
      "context" : "The Physiome project [33, 34], e.",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 29,
      "context" : "The Physiome project [33, 34], e.",
      "startOffset" : 21,
      "endOffset" : 29
    }, {
      "referenceID" : 15,
      "context" : "The vision of Υ-DB is currently set to be delivered on top of U-relations and probabilistic world-set algebra (p-WSA) [18].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 30,
      "context" : ", [36]), uncertainty is usually seen as an undesirable property that hinders data quality.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 13,
      "context" : "This may be non-obvious but is quite convenient a design decision for the envisioned system of Υ-DB because hypotheses, as (abstract) universal statements [15], can only be derived predictions from (be empirically grounded) by assigning (callibrating) them onto some real-world phenomenon.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "1, U-relations have in their schema a set of pairs (Vi, Di) of condition columns [18] to map each discrete random variable xi created by the repair-key operation to one of its possible values (e.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "The 11 Hypotheses are ‘universal’ by definition [15].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 31,
      "context" : "In the context of p-DB’s [17], data cleaning does not have to be one-shot — which is more error-prone [37].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "Note that it abstracts the goal of a data-intensive hypothesis evaluation study, or the scientific method itself [15], as the repair of each φ as a key.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "That is illustrated by query Q3, which creates integrative table Y [s]; and by query Q4, which computes the confidence aggregate operation [18] for all s tuples where t = 3 (Fig.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 14,
      "context" : "Then, by applying Bayes’ theorem for normal mean with a discrete prior [16], Prior is updated to Posterior (see Fig.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 14,
      "context" : "2) to get the posterior p(μk | y) [16].",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : ", yn |μk) for each competing trial μk, is computed as a product ∏n j=1 f(yj |μkj) of the single likelihoods f(yj |μkj) [16].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 32,
      "context" : "The vision of managing hypotheses as data has some roots in Porto and Spaccapietra [38], who motivated a conceptual data model to support (the socalled) in silico science by means of a scientific model management system.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "2 Scientific simulation data As previsouly mentioned, science’s ETL is distinguished by its unfrequent, incremental-only updates and by having large raw files as data sources [8].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 5,
      "context" : "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics.",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 33,
      "context" : "enabling an efficient access to high-resolution, raw simulation data have been documented from both supercomputing,[6] and database research viewpoints;[39] and pointed as key to the use case of exploratory analytics.",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 5,
      "context" : ", the ‘immersive’ query processing (move the program to the data) [6, 40], or ‘in situ’ query processing in the raw files [41, 42].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 34,
      "context" : ", the ‘immersive’ query processing (move the program to the data) [6, 40], or ‘in situ’ query processing in the raw files [41, 42].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 35,
      "context" : ", the ‘immersive’ query processing (move the program to the data) [6, 40], or ‘in situ’ query processing in the raw files [41, 42].",
      "startOffset" : 122,
      "endOffset" : 130
    }, {
      "referenceID" : 35,
      "context" : ", the loading) for a direct access to data ‘in situ’ in the raw data files [42].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 36,
      "context" : "3 Hypothesis encoding Our framework is comparable with Bioinformatics’ initiatives that address hypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses 14 Sometimes phrased ‘here is my files, here is my queries, where are my results?’ [41].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 37,
      "context" : "3 Hypothesis encoding Our framework is comparable with Bioinformatics’ initiatives that address hypothesis encoding into the RDF data model [43]: (i) the Robot Scientist [44] is a knowledge-base system (KBS) for automated generation and testing of hypotheses 14 Sometimes phrased ‘here is my files, here is my queries, where are my results?’ [41].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 38,
      "context" : "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 39,
      "context" : "about what genes encode enzymes in the yeast organism; (ii) HyBrow [45] is a KBS for scientists to test their hypotheses about events of the galactose metabolism also of the yeast organism; and (iii) SWAN [46] is a KBS for scientists to share hypotheses on possible causes of the Alzheimer disease.",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 37,
      "context" : "The Robot Scientist relies on rule-based logic programming analytics to automatically generate and test RDF-encoded hypotheses of the kind ‘gene G has function A’ against RDF-encoded empirical data [44].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 38,
      "context" : "HyBrow is likewise, but hypotheses are formulated by the user about biological events [45].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 39,
      "context" : "In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 40,
      "context" : "In particular, SWAN [46] differs from the former in that each hypothesis is unstructured, being then more related to efforts on the retrieval of textual claims from the narrative fabric of scientific reports [47].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 19,
      "context" : "2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon’s classical approach [24, 48] is intractable.",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 41,
      "context" : "2 we study the problem of extracting the causal ordering implicit in the structure of a deterministic hypothesis and show that Simon’s classical approach [24, 48] is intractable.",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 19,
      "context" : "Given a system of mathematical equations involving a set of variables, to build a structural equation model (SEM) is, essentially, to establish a one-to-one mapping between equations and variables [24].",
      "startOffset" : 197,
      "endOffset" : 201
    }, {
      "referenceID" : 19,
      "context" : ", [24, 49, 48]) and adapt it for the encoding of hypotheses into fd’s.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 41,
      "context" : ", [24, 49, 48]) and adapt it for the encoding of hypotheses into fd’s.",
      "startOffset" : 2,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 42,
      "context" : "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 20,
      "context" : "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 41,
      "context" : "[24]) and has been taken further in AI with a flavor of Graphical Models (GMs) [50, 25, 48].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 19,
      "context" : ", row multiplication by a constant) on the structure matrix may hinder the structure’s causal ordering and then are not valid in general [24].",
      "startOffset" : 137,
      "endOffset" : 141
    }, {
      "referenceID" : 19,
      "context" : "[24]) that, given a complete structure S(E ,V), can be used to compute a partial causal mapping φp from partitions on the set of equations to same-cardinality partitions on the set of variables.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 41,
      "context" : "As shown by Dash and Druzdzel [48], the causal mapping returned by Simon’s (socalled) Causal Ordering Algorithm (COA) is not total when S has variables that are strongly coupled (because they can only be determined simultaneously).",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 41,
      "context" : "They also have shown that any total mapping φ over S must be consistent with COA’s partial mapping φp [48].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 43,
      "context" : "line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 44,
      "context" : "line 3) is a hard problem that can only be addressed heuristically as a problem of co-clustering (also called biclustering [51, 52]) in",
      "startOffset" : 123,
      "endOffset" : 131
    }, {
      "referenceID" : 45,
      "context" : ", [54]) have come with the notion of pseudo-biclique (also called ‘quasi-biclique’), which is a relaxation of the biclique concept to allow some less rigid notion of connectivity than the ‘complete connectivity’ required in a biclique.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 46,
      "context" : "Proof 1 We show (by restriction [55]) that the BPBP is a generalization of the balanced biclique problem (BBP), referred ‘balanced complete bipartite subgraph’ problem [55, GT24, p.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 41,
      "context" : ", [48]) geared for reasoning over GM’s.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 48,
      "context" : "In this thesis we adopt the Hopcroft-Karp algorithm [57], which is known to be polynomial-time, bounded by O( √ |V1|+ |V2| |E|).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "Then we consider a sense of Simon’s into the nature of scientific modeling and interventions [24], summarized in Def.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 19,
      "context" : "We draw attention to the significance of Theorem 2, as it sheds light on a connection between Simon’s complete structures [24] and fd sets [20].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 16,
      "context" : "We draw attention to the significance of Theorem 2, as it sheds light on a connection between Simon’s complete structures [24] and fd sets [20].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 49,
      "context" : "[59]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[25, 19]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 41,
      "context" : "Dash and Druzdzel revisit the problem and re-motivate it in light of modern applications [48].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 51,
      "context" : "Inspired on Serrano and Gossard’s work on constraint modeling and reasoning [61], Nayak reports an approach that is provably quite effective to process the causal ordering: extract a total causal mapping and then compute the transitive closure of the direct causal dependencies.",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 19,
      "context" : "• By building upon on the work of Simon [24] and Nayak [49] (cf.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 16,
      "context" : "As usual notational conventions from the DB literature [20, 21], we write X, Y, Z to denote sets of relational attributes and A,B,C to denote singleton attribute sets.",
      "startOffset" : 55,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "Functional dependency theory relies on Armstrong’s inference rules (or axioms) of (R0) reflexivity, (R1) augmentation and (R2) transitivity, which forms a sound and complete inference system for reasoning over fd’s [20].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 52,
      "context" : "[62]), where Σ and U are (resp.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "yet unexplored problem in the database research literature (reasoning over fd’s is extensively covered in Maier [22]).",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 54,
      "context" : "Recent years have seen the emergence of some foundational work in causality in databases [64].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 55,
      "context" : "For conjunctive queries, the causality is said to be computed very efficiently [65].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 56,
      "context" : "al is the so-called sensitivity analysis [66], which is aimed at establishing a more refined connection between the query answer (output) and elements of the DB instance (input) for supporting user interventions.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 57,
      "context" : "This line of work is strongly related to the vision of ‘reverse data management’ [67].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "Three remarkable features of U-relations are: expressiveness (being closed under positive relational algebra queries); succinctness (efficient storage of a very large number of possible worlds through vertical decompositions to support attributelevel uncertainty); and efficient query processing (including confidence computation) [18].",
      "startOffset" : 331,
      "endOffset" : 335
    }, {
      "referenceID" : 15,
      "context" : ", R i m, p [i] ∈W is a possible world, with p being its probability [18].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 15,
      "context" : "Probabilistic world-set algebra (p-WSA) consists of the operations of relational algebra, an operation for computing tuple confidence conf, and the repairkey operation for introducing uncertainty — by giving rise to alternative worlds as maximal-subset repairs of an argument key [18].",
      "startOffset" : 280,
      "endOffset" : 284
    }, {
      "referenceID" : 15,
      "context" : "[18]) to map each discrete random variable xi to one of its possible values (e.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[18]): the number of algebraic operations does not increase and each of the operations selection, projection and product/join remains of the same kind.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "For a comprehensive overview of U-relations and p-WSA we refer the reader to [18].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "Hypotheses, nonetheless, are (abstract) ‘universal statements’ [15].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 58,
      "context" : ", see [68]).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 58,
      "context" : "4 In short, we make use of relational algebra group-by operation and build a pruned lattice of attribute groups having the same number of rows under the grouping (similarly to [68]).",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 16,
      "context" : "Now, we provide the classical definition of a lossless join [20], i.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 59,
      "context" : "Informed on research on Graphical Models (GM) [69], Suciu et al.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 60,
      "context" : ", by using a set of axioms (the so-called ‘graphoids’) for reasoning about the probabilistic independence of variables [70].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 60,
      "context" : "In fact, a connection between database normalization theory and factor decomposition in Graphical Models (GM) has been discussed by Verma and Pearl [70], but has not been explored since then.",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 61,
      "context" : "[71].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 61,
      "context" : "design [71].",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 52,
      "context" : "Despite some major differences, our synthesis method builds upon the classical theory of relational schema design by synthesis [62].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 52,
      "context" : "Classical design by synthesis [62] was once criticized due to its too strong ‘uniqueness’ of fd’s assumption [72, p.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 63,
      "context" : "The last decade has seen significant research effort to make DB systems really usable [73].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 64,
      "context" : "For instance, in comparison, the CRIUS system supports another kind of user-friendly DB design approach that provides users with a spreadsheet-like direct manipulation interface to increasingly add structure to their data [74].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 65,
      "context" : "It has been firstly addressed by Koch and Olteanu motivated by data cleaning applications [75].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 29,
      "context" : "The Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33].",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 28,
      "context" : "The Physiome project is an initiative to seriously address the problems of reproducibility, model integration and sharing in Computational Physiology [34, 33].",
      "startOffset" : 150,
      "endOffset" : 158
    }, {
      "referenceID" : 66,
      "context" : "8 shows the best fit of a baroreflex model for an observational dataset acquired by experiment on Dahl SS rat [76].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 66,
      "context" : "(source: [76]).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 28,
      "context" : "and molecular levels [33].",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 23,
      "context" : "[28]), in which we go through the whole design-by-synthesis pipeline (Fig.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 67,
      "context" : "20 shows observational data collected from Hudson’s Bay from 1900 to 1920 on the Lynx-Hare population [77].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 22,
      "context" : "The efficiency and scalability of the U-relational representation system and its p-WSA query algebra have been extensively demonstrated [27].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : ", learning the equations, say, from Eureqa [2].",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 68,
      "context" : "31) published in [78].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 68,
      "context" : "Example of Boolean Network model (source: [78]).",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "All these are technical means to [15]: (1′) extract the hypothesis ‘empirical content’ and ‘predictive power;’ (2′) unravel its cohesiveness and how parsimonious it is in terms of the number of different claims or epistemological units carried within it, as well as its empirical grounding (‘first causes’); and finally, we shall be able to (3′) appraise it in face of competing or alternative explanations.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "This thesis addresses the pressing call for large-scale, data-driven hypothesis management and analytics [35, 3, 10].",
      "startOffset" : 105,
      "endOffset" : 116
    }, {
      "referenceID" : 24,
      "context" : "[10, 29, 28]).",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 23,
      "context" : "[10, 29, 28]).",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 16,
      "context" : ", multi-valued dependencies [20]), approximate fd’s [68], conditional fd’s [79]) to extend the scope of Υ-DB towards structured stochastic models.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 58,
      "context" : ", multi-valued dependencies [20]), approximate fd’s [68], conditional fd’s [79]) to extend the scope of Υ-DB towards structured stochastic models.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "As envisioned by Jim Gray [1], the scientific method has been shifting towards being operated as a data-driven discipline which is rapidly gaining ground [3].",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 2,
      "context" : "As envisioned by Jim Gray [1], the scientific method has been shifting towards being operated as a data-driven discipline which is rapidly gaining ground [3].",
      "startOffset" : 154,
      "endOffset" : 157
    } ],
    "year" : 2015,
    "abstractText" : "of Thesis presented to LNCC/MCT in partial fulfillment of the requirements for the degree of Doctor of Sciences (D.Sc.) MANAGING LARGE-SCALE SCIENTIFIC HYPOTHESES AS UNCERTAIN AND PROBABILISTIC DATA Bernardo Gonçalves February 2015 Advisor: Fabio Porto, D.Sc. In view of the paradigm shift that makes science ever more data-driven, in this thesis we propose a synthesis method for encoding and managing large-scale deterministic scientific hypotheses as uncertain and probabilistic data. In the form of mathematical equations, hypotheses symmetrically relate aspects of the studied phenomena. For computing predictions, however, deterministic hypotheses can be abstracted as functions. We build upon Simon’s notion of structural equations in order to efficiently extract the (so-called) causal ordering between variables, implicit in a hypothesis structure (set of mathematical equations). We show how to process the hypothesis predictive structure effectively through original algorithms for encoding it into a set of functional dependencies (fd’s) and then performing causal reasoning in terms of acyclic pseudo-transitive reasoning over fd’s. Such reasoning reveals important causal dependencies implicit in the hypothesis predictive data and guide our synthesis of a probabilistic database. Like in the field of graphical models in AI, such a probabilistic database should be normalized so that the uncertainty arisen from competing hypotheses is decomposed into factors and propagated properly onto predictive data by recovering its joint probability distribution through a lossless join. That is motivated as a design-theoretic principle for data-driven hypothesis management and predictive analytics. The method is applicable to both quantitative and qualitative deterministic hypotheses and demonstrated in realistic use cases from computational science. Resumo da Tese apresentada ao LNCC/MCT como parte dos requisitos necessários para a obtenção do grau de Doutor em Ciências (D.Sc.) GERÊNCIA DE HIPÓTESES CIENTÍFICAS DE LARGA-ESCALA COMO DADOS INCERTOS E PROBABILÍSTICOS Bernardo Gonçalves Fevereiro, 2015 Orientador: Fabio Porto, D.Sc. Tendo em vista a mudança de paradigma que faz da ciência cada vez mais guiada por dados, nesta tese propomos um método para codificação e gerência de hipóteses cient́ıficas determińısticas de larga escala como dados incertos e probabiĺısticos. Na forma de equações matemáticas, hipóteses relacionam simetricamente aspectos do fenômeno de estudo. Para computação de predições, no entanto, hipóteses determińısticas podem ser abstráıdas como funções. Levamos adiante a noção de Simon de equações estruturais para extrair de forma eficiente a então chamada ordenação causal impĺıcita na estrutura de uma hipótese. Mostramos como processar a estrutura preditiva de uma hipótese através de algoritmos originais para sua codificação como um conjunto de dependências funcionais (df’s) e então realizamos inferência causal em termos de racioćınio aćıclico pseudo-transitivo sobre df’s. Tal racioćınio revela importantes dependências causais impĺıcitas nos dados preditivos da hipótese, que conduzem nossa śıntese do banco de dados probabiĺıstico. Como na área de modelos gráficos (IA), o banco de dados probabiĺıstico deve ser normalizado de tal forma que a incerteza oriunda de hipóteses alternativas seja decomposta em fatores e propagada propriamente recuperando sua distribuição de probabilidade conjunta via junção ‘lossless.’ Isso é motivado como um prinćıpio teórico de projeto para gerência e análise de hipóteses. O método proposto é aplicável a hipóteses determińısticas quantitativas e qualitativas e é demonstrado em casos reaĺısticos de ciência computacional.",
    "creator" : "LaTeX with hyperref package"
  }
}