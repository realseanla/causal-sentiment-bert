{"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-May-2017", "title": "Learning Structured Text Representations", "abstract": "In this paper, we focus on learning structure-aware document representations from data without recourse to a discourse parser or additional annotations. Drawing inspiration from recent efforts to empower neural networks with a structural bias, we propose a model that can encode a document while automatically inducing rich structural dependencies. Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases. Experimental evaluation across different tasks and datasets shows that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.", "histories": [["v1", "Thu, 25 May 2017 14:54:07 GMT  (162kb,D)", "http://arxiv.org/abs/1705.09207v1", null], ["v2", "Thu, 20 Jul 2017 23:14:58 GMT  (176kb,D)", "http://arxiv.org/abs/1705.09207v2", "Comments: 10 pages; typos corrected"], ["v3", "Thu, 14 Sep 2017 20:42:25 GMT  (208kb,D)", "http://arxiv.org/abs/1705.09207v3", "Accepted by TACL"]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["yang liu", "mirella lapata"], "accepted": false, "id": "1705.09207"}
