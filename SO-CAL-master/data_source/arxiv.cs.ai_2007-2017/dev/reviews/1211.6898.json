{"conference": "NIPS", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Nov-2012", "title": "On the Use of Non-Stationary Policies for Stationary Infinite-Horizon Markov Decision Processes", "abstract": "We consider infinite-horizon stationary $\\gamma$-discounted Markov Decision Processes, for which it is known that there exists a stationary optimal policy. Using Value and Policy Iteration with some error $\\epsilon$ at each iteration, it is well-known that one can compute stationary policies that are $\\frac{2\\gamma}{(1-\\gamma)^2}\\epsilon$-optimal. After arguing that this guarantee is tight, we develop variations of Value and Policy Iteration for computing non-stationary policies that can be up to $\\frac{2\\gamma}{1-\\gamma}\\epsilon$-optimal, which constitutes a significant improvement in the usual situation when $\\gamma$ is close to 1. Surprisingly, this shows that the problem of \"computing near-optimal non-stationary policies\" is much simpler than that of \"computing near-optimal stationary policies\".", "histories": [["v1", "Thu, 29 Nov 2012 12:54:58 GMT  (16kb)", "http://arxiv.org/abs/1211.6898v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["bruno scherrer", "boris lesner"], "accepted": true, "id": "1211.6898"}
