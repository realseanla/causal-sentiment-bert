Recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- have/VBP proven/VBN to/TO be/VB powerful/JJ models/NNS in/IN problems/NNS involving/VBG sequential/JJ data/NNS ./.
Recently/RB ,/, RNNs/NNS have/VBP been/VBN augmented/VBN with/IN "/`` attention/NN "/'' mechanisms/NNS which/WDT allow/VBP the/DT network/NN to/TO focus/VB on/IN different/JJ parts/NNS of/IN an/DT input/NN sequence/NN when/WRB computing/VBG their/PRP$ output/NN ./.
We/PRP propose/VBP a/DT simplified/JJ model/NN of/IN attention/NN which/WDT is/VBZ applicable/JJ to/TO feed/VB -/HYPH forward/RP neural/JJ networks/NNS and/CC demonstrate/VBP that/IN it/PRP can/MD solve/VB some/DT long/JJ -/HYPH term/NN memory/NN problems/NNS (/-LRB- specifically/RB ,/, those/DT where/WRB temporal/JJ order/NN does/VBZ n't/RB matter/VB )/-RRB- ./.
In/IN fact/NN ,/, we/PRP show/VBP empirically/RB that/IN our/PRP$ model/NN can/MD solve/VB these/DT problems/NNS for/IN sequence/NN lengths/NNS which/WDT are/VBP both/RB longer/RBR and/CC more/RBR widely/RB varying/VBG than/IN the/DT best/JJS results/NNS attained/VBN with/IN RNNs/NNS ./.
