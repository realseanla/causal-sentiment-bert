In/IN this/DT paper/NN we/PRP propose/VBP a/DT neural/JJ network/NN model/NN with/IN a/DT novel/JJ Sequential/JJ Attention/NN layer/NN that/WDT extends/VBZ soft/JJ attention/NN by/IN assigning/VBG weights/NNS to/IN words/NNS in/IN an/DT input/NN sequence/NN in/IN a/DT way/NN that/WDT takes/VBZ into/IN account/NN not/RB just/RB how/WRB well/RB that/DT word/NN matches/VBZ a/DT query/NN ,/, but/CC how/WRB well/RB surrounding/VBG words/NNS match/NN ./.
We/PRP evaluate/VBP this/DT approach/NN on/IN the/DT task/NN of/IN reading/NN comprehension/NN (/-LRB- Who/WP did/VBD What/WP and/CC CNN/NNP datasets/NNS )/-RRB- and/CC show/VBP that/IN it/PRP dramatically/RB improves/VBZ a/DT strong/JJ baseline/NN like/IN the/DT Stanford/NNP Reader/NNP ./.
The/DT resulting/VBG model/NN is/VBZ competitive/JJ with/IN the/DT state/NN of/IN the/DT art/NN ./.
