We/PRP propose/VBP an/DT algorithm/NN for/IN meta/NN -/HYPH learning/NN that/WDT is/VBZ model/NN -/HYPH agnostic/JJ ,/, in/IN the/DT sense/NN that/IN it/PRP is/VBZ compatible/JJ with/IN any/DT model/NN trained/VBN with/IN gradient/NN descent/NN and/CC applicable/JJ to/IN a/DT variety/NN of/IN different/JJ learning/NN problems/NNS ,/, including/VBG classification/NN ,/, regression/NN ,/, and/CC reinforcement/NN learning/NN ./.
The/DT goal/NN of/IN meta/NN -/HYPH learning/NN is/VBZ to/TO train/VB a/DT model/NN on/IN a/DT variety/NN of/IN learning/VBG tasks/NNS ,/, such/JJ that/IN it/PRP can/MD solve/VB new/JJ learning/NN tasks/NNS using/VBG only/RB a/DT small/JJ number/NN of/IN training/NN samples/NNS ./.
In/IN our/PRP$ approach/NN ,/, the/DT parameters/NNS of/IN the/DT model/NN are/VBP explicitly/RB trained/VBN such/JJ that/IN a/DT small/JJ number/NN of/IN gradient/NN steps/NNS with/IN a/DT small/JJ amount/NN of/IN training/NN data/NNS from/IN a/DT new/JJ task/NN will/MD produce/VB good/JJ generalization/NN performance/NN on/IN that/DT task/NN ./.
In/IN effect/NN ,/, our/PRP$ method/NN trains/VBZ the/DT model/NN to/TO be/VB easy/JJ to/TO fine/JJ -/HYPH tune/NN ./.
We/PRP demonstrate/VBP that/IN this/DT approach/NN leads/VBZ to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN a/DT few/JJ -/HYPH shot/NN image/NN classification/NN benchmark/NN ,/, produces/VBZ good/JJ results/NNS on/IN few/JJ -/HYPH shot/NN regression/NN ,/, and/CC accelerates/VBZ fine/JJ -/HYPH tuning/NN for/IN policy/NN gradient/NN reinforcement/NN learning/VBG with/IN neural/JJ network/NN policies/NNS ./.
