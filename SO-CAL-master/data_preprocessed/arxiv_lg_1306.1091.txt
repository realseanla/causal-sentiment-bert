Recent/JJ work/NN showed/VBD that/IN denoising/VBG auto/NN -/HYPH encoders/NNS can/MD be/VB interpreted/VBN as/IN generative/JJ models/NNS ./.
We/PRP generalize/VBP these/DT results/NNS to/IN arbitrary/JJ parametrizations/NNS that/WDT learn/VBP to/TO reconstruct/VB their/PRP$ input/NN and/CC where/WRB noise/NN is/VBZ injected/VBN ,/, not/RB just/RB in/IN input/NN ,/, but/CC also/RB in/IN intermediate/JJ computations/NNS ./.
We/PRP show/VBP that/IN under/IN reasonable/JJ assumptions/NNS (/-LRB- the/DT parametrization/NN is/VBZ rich/JJ enough/RB to/TO provide/VB a/DT consistent/JJ estimator/NN ,/, and/CC it/PRP prevents/VBZ the/DT learner/NN from/IN just/RB copying/VBG its/PRP$ input/NN in/IN output/NN and/CC producing/VBG a/DT dirac/NN output/NN distribution/NN )/-RRB- ,/, such/JJ models/NNS are/VBP consistent/JJ estimators/NNS of/IN the/DT data/NNS generating/VBG distributions/NNS ,/, and/CC that/IN they/PRP define/VBP the/DT estimated/VBN distribution/NN through/IN a/DT Markov/NNP chain/NN that/WDT consists/VBZ at/IN each/DT step/NN in/IN re-injecting/VBG sampled/VBN reconstructions/NNS as/IN a/DT sequence/NN of/IN inputs/NNS into/IN the/DT unfolded/VBN computational/JJ graph/NN ./.
As/IN a/DT consequence/NN ,/, one/PRP can/MD define/VB deep/JJ architectures/NNS similar/JJ to/IN deep/JJ Boltzmann/NNP machines/NNS in/IN that/DT units/NNS are/VBP stochastic/JJ ,/, that/IN the/DT model/NN can/MD learn/VB to/TO generate/VB a/DT distribution/NN similar/JJ to/IN its/PRP$ training/NN distribution/NN ,/, that/IN it/PRP can/MD easily/RB handle/VB missing/VBG inputs/NNS ,/, but/CC without/IN the/DT troubling/JJ problem/NN of/IN intractable/JJ partition/NN function/NN and/CC intractable/JJ inference/NN as/IN stumbling/VBG blocks/NNS for/IN both/DT training/NN and/CC using/VBG these/DT models/NNS ./.
In/IN particular/JJ ,/, we/PRP argue/VBP that/IN if/IN the/DT underlying/VBG latent/JJ variables/NNS of/IN a/DT graphical/JJ model/NN form/VBP a/DT highly/RB multi-modal/JJ posterior/JJ (/-LRB- given/VBN the/DT input/NN )/-RRB- ,/, none/NN of/IN the/DT currently/RB known/VBN training/NN methods/NNS can/MD appropriately/RB deal/VB with/IN this/DT multi-modality/NN (/-LRB- when/WRB the/DT number/NN modes/NNS is/VBZ much/RB greater/JJR than/IN the/DT number/NN of/IN MCMC/NN samples/NNS one/CD is/VBZ willing/JJ to/TO perform/VB ,/, and/CC when/WRB the/DT structure/NN of/IN the/DT posterior/JJ can/MD not/RB be/VB easily/RB approximated/VBN by/IN some/DT tractable/JJ variational/JJ approximation/NN )/-RRB- ./.
In/IN contrast/NN ,/, the/DT proposed/VBN models/NNS can/MD simply/RB be/VB trained/VBN by/IN back/RB -/HYPH propagating/VBG the/DT reconstruction/NN error/NN (/-LRB- seen/VBN as/IN log/NN -/HYPH likelihood/NN of/IN reconstruction/NN )/-RRB- into/IN the/DT parameters/NNS ,/, benefiting/VBG from/IN the/DT power/NN and/CC ease/NN of/IN training/NN recently/RB demonstrated/VBD for/IN deep/JJ supervised/JJ networks/NNS with/IN dropout/NN noise/NN ./.
