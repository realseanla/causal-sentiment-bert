We/PRP propose/VBP a/DT conceptually/RB simple/JJ and/CC lightweight/JJ framework/NN for/IN deep/JJ reinforcement/NN learning/VBG that/IN uses/VBZ asynchronous/JJ gradient/NN descent/NN for/IN optimization/NN of/IN deep/JJ neural/JJ network/NN controllers/NNS ./.
We/PRP present/VBP asynchronous/JJ variants/NNS of/IN four/CD standard/JJ reinforcement/NN learning/VBG algorithms/NNS and/CC show/VBP that/IN parallel/JJ actor/NN -/HYPH learners/NNS have/VBP a/DT stabilizing/VBG effect/NN on/IN training/NN allowing/VBG all/DT four/CD methods/NNS to/TO successfully/RB train/VB neural/JJ network/NN controllers/NNS ./.
The/DT best/JJS performing/VBG method/NN ,/, an/DT asynchronous/JJ variant/NN of/IN actor/NN -/HYPH critic/NN ,/, surpasses/VBZ the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN on/IN the/DT Atari/NNP domain/NN while/IN training/NN for/IN half/PDT the/DT time/NN on/IN a/DT single/JJ multi-core/JJ CPU/NN instead/RB of/IN a/DT GPU/NNP ./.
Furthermore/RB ,/, we/PRP show/VBP that/IN asynchronous/JJ actor/NN -/HYPH critic/NN succeeds/VBZ on/IN a/DT wide/JJ variety/NN of/IN continuous/JJ motor/NN control/NN problems/NNS as/RB well/RB as/IN on/IN a/DT new/JJ task/NN involving/VBG finding/VBG rewards/NNS in/IN random/JJ 3D/NN mazes/NNS using/VBG a/DT visual/JJ input/NN ./.
