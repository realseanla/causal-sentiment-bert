In/IN large/JJ scale/NN machine/NN learning/NN and/CC data/NNS mining/NN problems/NNS with/IN high/JJ feature/NN dimensionality/NN ,/, the/DT Euclidean/JJ distance/NN between/IN data/NNS points/NNS can/MD be/VB uninformative/JJ ,/, and/CC Distance/NNP Metric/NNP Learning/NNP (/-LRB- DML/NNP )/-RRB- is/VBZ often/RB desired/VBN to/TO learn/VB a/DT proper/JJ similarity/NN measure/NN (/-LRB- using/VBG side/NN information/NN such/JJ as/IN example/NN data/NNS pairs/NNS being/VBG similar/JJ or/CC dissimilar/JJ )/-RRB- ./.
However/RB ,/, high/JJ dimensionality/NN and/CC large/JJ volume/NN of/IN pairwise/JJ constraints/NNS in/IN modern/JJ big/JJ data/NNS can/MD lead/VB to/IN prohibitive/JJ computational/JJ cost/NN for/IN both/CC the/DT original/JJ DML/NNP formulation/NN in/IN Xing/NNP et/FW al./FW (/-LRB- 2002/CD )/-RRB- and/CC later/RB extensions/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT distributed/VBN algorithm/NN for/IN DML/NNP ,/, and/CC a/DT large/JJ -/HYPH scale/NN implementation/NN on/IN a/DT parameter/NN server/NN architecture/NN ./.
Our/PRP$ approach/NN builds/VBZ on/IN a/DT parallelizable/JJ reformulation/NN of/IN Xing/NNP et/FW al./FW (/-LRB- 2002/CD )/-RRB- ,/, and/CC an/DT asynchronous/JJ stochastic/JJ gradient/NN descent/NN optimization/NN procedure/NN ./.
To/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ distributed/VBN solution/NN to/IN DML/NNP ,/, and/CC we/PRP show/VBP that/IN ,/, on/IN a/DT system/NN with/IN 256/CD CPU/NN cores/NNS ,/, our/PRP$ program/NN is/VBZ able/JJ to/TO complete/VB a/DT DML/NNP task/NN on/IN a/DT dataset/NN with/IN 1/CD million/CD data/NNS points/NNS ,/, 22/CD -/SYM thousand/CD features/NNS ,/, and/CC 200/CD million/CD labeled/VBN data/NNS pairs/NNS ,/, in/IN 15/CD hours/NNS ;/: and/CC the/DT learned/VBN metric/JJ shows/NNS great/JJ effectiveness/NN in/IN properly/RB measuring/VBG distances/NNS ./.
