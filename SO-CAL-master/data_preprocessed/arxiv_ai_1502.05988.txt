In/IN multi-label/JJ classification/NN ,/, the/DT main/JJ focus/NN has/VBZ been/VBN to/TO develop/VB ways/NNS of/IN learning/VBG the/DT underlying/VBG dependencies/NNS between/IN labels/NNS ,/, and/CC to/TO take/VB advantage/NN of/IN this/DT at/IN classification/NN time/NN ./.
Developing/VBG better/JJR feature/NN -/HYPH space/NN representations/NNS has/VBZ been/VBN predominantly/RB employed/VBN to/TO reduce/VB complexity/NN ,/, e.g./FW ,/, by/IN eliminating/VBG non-helpful/JJ feature/NN attributes/NNS from/IN the/DT input/NN space/NN prior/JJ to/IN (/-LRB- or/CC during/IN )/-RRB- training/NN ./.
This/DT is/VBZ an/DT important/JJ task/NN ,/, since/IN many/JJ multi-label/JJ methods/NNS typically/RB create/VBP many/JJ different/JJ copies/NNS or/CC views/NNS of/IN the/DT same/JJ input/NN data/NNS as/IN they/PRP transform/VBP it/PRP ,/, and/CC considerable/JJ memory/NN can/MD be/VB saved/VBN by/IN taking/VBG advantage/NN of/IN redundancy/NN ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN a/DT proper/JJ development/NN of/IN the/DT feature/NN space/NN can/MD make/VB labels/NNS less/RBR interdependent/JJ and/CC easier/JJR to/TO model/VB and/CC predict/VB at/IN inference/NN time/NN ./.
For/IN this/DT task/NN we/PRP use/VBP a/DT deep/JJ learning/NN approach/NN with/IN restricted/VBN Boltzmann/NNP machines/NNS ./.
We/PRP present/VBP a/DT deep/JJ network/NN that/WDT ,/, in/IN an/DT empirical/JJ evaluation/NN ,/, outperforms/VBZ a/DT number/NN of/IN competitive/JJ methods/NNS from/IN the/DT literature/NN
