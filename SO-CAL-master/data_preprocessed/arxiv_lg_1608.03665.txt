High/JJ demand/NN for/IN computation/NN resources/NNS severely/RB hinders/VBZ deployment/NN of/IN large/JJ -/HYPH scale/NN Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNN/NN )/-RRB- in/IN resource/NN constrained/VBN devices/NNS ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT Structured/VBN Sparsity/NN Learning/NN (/-LRB- SSL/NN )/-RRB- method/NN to/TO regularize/VB the/DT structures/NNS (/-LRB- i.e./FW ,/, filters/NNS ,/, channels/NNS ,/, filter/NN shapes/NNS ,/, and/CC layer/NN depth/NN )/-RRB- of/IN DNNs/NNS ./.
SSL/NN can/MD :/: (/-LRB- 1/LS )/-RRB- learn/VB a/DT compact/JJ structure/NN from/IN a/DT bigger/JJR DNN/NN to/TO reduce/VB computation/NN cost/NN ;/: (/-LRB- 2/LS )/-RRB- obtain/VB a/DT hardware/NN -/HYPH friendly/JJ structured/JJ sparsity/NN of/IN DNN/NNP to/TO efficiently/RB accelerate/VB the/DT DNNs/NNS evaluation/NN ./.
Experimental/JJ results/NNS show/VBP that/IN SSL/NN achieves/VBZ on/IN average/JJ 5.1/CD x/NN and/CC 3.1/CD x/SYM speedups/NNS of/IN convolutional/JJ layer/NN computation/NN of/IN AlexNet/NNP against/IN CPU/NN and/CC GPU/NNP ,/, respectively/RB ,/, with/IN off/RB -/HYPH the/DT -/HYPH shelf/NN libraries/NNS ./.
These/DT speedups/NNS are/VBP about/RB twice/RB speedups/NNS of/IN non-structured/JJ sparsity/NN ;/: (/-LRB- 3/LS )/-RRB- regularize/VB the/DT DNN/NNP structure/NN to/TO improve/VB classification/NN accuracy/NN ./.
The/DT results/NNS show/VBP that/IN for/IN CIFAR/NN -/HYPH 10/CD ,/, regularization/NN on/IN layer/NN depth/NN can/MD reduce/VB 20/CD layers/NNS of/IN a/DT Deep/JJ Residual/JJ Network/NN (/-LRB- ResNet/NNP )/-RRB- to/IN 18/CD layers/NNS while/IN improve/VB the/DT accuracy/NN from/IN 91.25/CD percent/NN to/IN 92.60/CD percent/NN ,/, which/WDT is/VBZ still/RB slightly/RB higher/JJR than/IN that/DT of/IN original/JJ ResNet/NNP with/IN 32/CD layers/NNS ./.
For/IN AlexNet/NNP ,/, structure/NN regularization/NN by/IN SSL/NNP also/RB reduces/VBZ the/DT error/NN by/IN around/IN ~/SYM 1/CD percent/NN ./.
