Although/IN the/DT problem/NN of/IN automatic/JJ video/NN summarization/NN has/VBZ recently/RB received/VBN a/DT lot/NN of/IN attention/NN ,/, the/DT problem/NN of/IN creating/VBG a/DT video/NN summary/NN that/WDT also/RB highlights/VBZ elements/NNS relevant/JJ to/IN a/DT search/NN query/NN has/VBZ been/VBN less/RBR studied/VBN ./.
We/PRP address/VBP this/DT problem/NN by/IN posing/VBG query/NN -/HYPH relevant/JJ summarization/NN as/IN a/DT video/NN frame/NN subset/NN selection/NN problem/NN ,/, which/WDT lets/VBZ us/PRP optimise/VB for/IN summaries/NNS which/WDT are/VBP simultaneously/RB diverse/JJ ,/, representative/NN of/IN the/DT entire/JJ video/NN ,/, and/CC relevant/JJ to/IN a/DT text/NN query/NN ./.
We/PRP quantify/VBP relevance/NN by/IN measuring/VBG the/DT distance/NN between/IN frames/NNS and/CC queries/NNS in/IN a/DT common/JJ textual/JJ -/HYPH visual/JJ semantic/JJ embedding/NN space/NN induced/VBN by/IN a/DT neural/JJ network/NN ./.
In/IN addition/NN ,/, we/PRP extend/VBP the/DT model/NN to/TO capture/VB query/NN -/HYPH independent/JJ properties/NNS ,/, such/JJ as/IN frame/NN quality/NN ./.
We/PRP compare/VBP our/PRP$ method/NN against/IN previous/JJ state/NN of/IN the/DT art/NN on/IN textual/JJ -/HYPH visual/JJ embeddings/NNS for/IN thumbnail/JJ selection/NN and/CC show/VBP that/IN our/PRP$ model/NN outperforms/VBZ them/PRP on/IN relevance/NN prediction/NN ./.
Furthermore/RB ,/, we/PRP introduce/VBP a/DT new/JJ dataset/NN ,/, annotated/VBN with/IN diversity/NN and/CC query/NN -/HYPH specific/JJ relevance/NN labels/NNS ./.
On/IN this/DT dataset/NN ,/, we/PRP train/VBP and/CC test/VBP our/PRP$ complete/JJ model/NN for/IN video/NN summarization/NN and/CC show/VBP that/IN it/PRP outperforms/VBZ standard/JJ baselines/NNS such/JJ as/IN Maximal/JJ Marginal/JJ Relevance/NN ./.
