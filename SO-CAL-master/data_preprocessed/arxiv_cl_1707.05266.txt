In/IN this/DT study/NN ,/, we/PRP introduce/VBP a/DT new/JJ approach/NN for/IN learning/VBG language/NN models/NNS by/IN training/VBG them/PRP to/TO estimate/VB word/NN -/HYPH context/NN pointwise/NN mutual/JJ information/NN (/-LRB- PMI/NN )/-RRB- ,/, and/CC then/RB deriving/VBG the/DT desired/VBN conditional/JJ probabilities/NNS from/IN PMI/NNP at/IN test/NN time/NN ./.
Specifically/RB ,/, we/PRP show/VBP that/IN with/IN minor/JJ modifications/NNS to/IN word2vec/NN 's/POS algorithm/NN ,/, we/PRP get/VBP principled/JJ language/NN models/NNS that/WDT are/VBP closely/RB related/VBN to/IN the/DT well/NN -/HYPH established/VBN Noise/NN Contrastive/JJ Estimation/NN (/-LRB- NCE/NN )/-RRB- based/VBN language/NN models/NNS ./.
A/DT compelling/JJ aspect/NN of/IN our/PRP$ approach/NN is/VBZ that/IN our/PRP$ models/NNS are/VBP trained/VBN with/IN the/DT same/JJ simple/JJ negative/JJ sampling/NN objective/NN function/NN that/WDT is/VBZ commonly/RB used/VBN in/IN word2vec/NN to/TO learn/VB word/NN embeddings/NNS ./.
