We/PRP present/VBP weight/NN normalization/NN :/: a/DT reparameterization/NN of/IN the/DT weight/NN vectors/NNS in/IN a/DT neural/JJ network/NN that/WDT decouples/VBZ the/DT length/NN of/IN those/DT weight/NN vectors/NNS from/IN their/PRP$ direction/NN ./.
By/IN reparameterizing/VBG the/DT weights/NNS in/IN this/DT way/NN we/PRP improve/VBP the/DT conditioning/NN of/IN the/DT optimization/NN problem/NN and/CC we/PRP speed/VBP up/RP convergence/NN of/IN stochastic/JJ gradient/NN descent/NN ./.
Our/PRP$ reparameterization/NN is/VBZ inspired/VBN by/IN batch/NN normalization/NN but/CC does/VBZ not/RB introduce/VB any/DT dependencies/NNS between/IN the/DT examples/NNS in/IN a/DT minibatch/NN ./.
This/DT means/VBZ that/IN our/PRP$ method/NN can/MD also/RB be/VB applied/VBN successfully/RB to/IN recurrent/JJ models/NNS such/JJ as/IN LSTMs/NNPS and/CC to/IN noise/NN -/HYPH sensitive/JJ applications/NNS such/JJ as/IN deep/JJ reinforcement/NN learning/VBG or/CC generative/JJ models/NNS ,/, for/IN which/WDT batch/NN normalization/NN is/VBZ less/RBR well/RB suited/JJ ./.
Although/IN our/PRP$ method/NN is/VBZ much/JJ simpler/JJR ,/, it/PRP still/RB provides/VBZ much/JJ of/IN the/DT speed/NN -/HYPH up/NN of/IN full/JJ batch/NN normalization/NN ./.
In/IN addition/NN ,/, the/DT computational/JJ overhead/NN of/IN our/PRP$ method/NN is/VBZ lower/JJR ,/, permitting/VBG more/JJR optimization/NN steps/NNS to/TO be/VB taken/VBN in/IN the/DT same/JJ amount/NN of/IN time/NN ./.
We/PRP demonstrate/VBP the/DT usefulness/NN of/IN our/PRP$ method/NN on/IN applications/NNS in/IN supervised/JJ image/NN recognition/NN ,/, generative/JJ modelling/NN ,/, and/CC deep/JJ reinforcement/NN learning/NN ./.
