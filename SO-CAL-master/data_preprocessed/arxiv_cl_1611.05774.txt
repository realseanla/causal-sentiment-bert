Recurrent/JJ neural/JJ network/NN grammars/NNS (/-LRB- RNNG/NN )/-RRB- are/VBP a/DT recently/RB proposed/VBN probabilistic/JJ generative/JJ modeling/NN family/NN for/IN natural/JJ language/NN ./.
They/PRP show/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN language/NN modeling/NN and/CC parsing/VBG performance/NN ./.
We/PRP investigate/VBP what/WP information/NN they/PRP learn/VBP ,/, from/IN a/DT linguistic/JJ perspective/NN ,/, through/IN various/JJ ablations/NNS to/IN the/DT model/NN and/CC the/DT data/NNS ,/, and/CC by/IN augmenting/VBG the/DT model/NN with/IN an/DT attention/NN mechanism/NN (/-LRB- GA/NN -/HYPH RNNG/NN )/-RRB- to/TO enable/VB closer/JJR inspection/NN ./.
We/PRP find/VBP that/IN explicit/JJ modeling/NN of/IN composition/NN is/VBZ crucial/JJ for/IN achieving/VBG the/DT best/JJS performance/NN ./.
Through/IN the/DT attention/NN mechanism/NN ,/, we/PRP find/VBP that/IN headedness/NN plays/VBZ a/DT central/JJ role/NN in/IN phrasal/JJ representation/NN (/-LRB- with/IN the/DT model/NN 's/POS latent/JJ attention/NN largely/RB agreeing/VBG with/IN predictions/NNS made/VBN by/IN hand/NN -/HYPH crafted/VBN rules/NNS ,/, albeit/IN with/IN some/DT important/JJ differences/NNS )/-RRB- ./.
By/IN training/VBG grammars/NNS without/IN non-terminal/JJ labels/NNS ,/, we/PRP find/VBP that/IN phrasal/JJ representations/NNS depend/VBP minimally/RB on/IN non-terminals/NNS ,/, providing/VBG support/NN for/IN the/DT endocentricity/NN hypothesis/NN ./.
