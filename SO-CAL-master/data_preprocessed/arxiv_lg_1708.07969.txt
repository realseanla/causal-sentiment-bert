In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ 3D/NN -/HYPH RecGAN/NN approach/NN ,/, which/WDT reconstructs/VBZ the/DT complete/JJ 3D/NN structure/NN of/IN a/DT given/VBN object/NN from/IN a/DT single/JJ arbitrary/JJ depth/NN view/NN using/VBG generative/JJ adversarial/JJ networks/NNS ./.
Unlike/IN the/DT existing/VBG work/NN which/WDT typically/RB requires/VBZ multiple/JJ views/NNS of/IN the/DT same/JJ object/NN or/CC class/NN labels/NNS to/TO recover/VB the/DT full/JJ 3D/NN geometry/NN ,/, the/DT proposed/VBN 3D/NN -/HYPH RecGAN/NN only/RB takes/VBZ the/DT voxel/NNP grid/NN representation/NN of/IN a/DT depth/NN view/NN of/IN the/DT object/NN as/IN input/NN ,/, and/CC is/VBZ able/JJ to/TO generate/VB the/DT complete/JJ 3D/NN occupancy/NN grid/NN by/IN filling/VBG in/RP the/DT occluded/VBN //HYPH missing/JJ regions/NNS ./.
The/DT key/JJ idea/NN is/VBZ to/TO combine/VB the/DT generative/JJ capabilities/NNS of/IN autoencoders/NNS and/CC the/DT conditional/JJ Generative/NNP Adversarial/NNP Networks/NNP (/-LRB- GAN/NNP )/-RRB- framework/NN ,/, to/TO infer/VB accurate/JJ and/CC fine/JJ -/HYPH grained/JJ 3D/JJ structures/NNS of/IN objects/NNS in/IN high/JJ -/HYPH dimensional/JJ voxel/NN space/NN ./.
Extensive/JJ experiments/NNS on/IN large/JJ synthetic/JJ datasets/NNS show/VBP that/IN the/DT proposed/VBN 3D/NN -/HYPH RecGAN/NN significantly/RB outperforms/VBZ the/DT state/NN of/IN the/DT art/NN in/IN single/JJ view/NN 3D/JJ object/NN reconstruction/NN ,/, and/CC is/VBZ able/JJ to/TO reconstruct/VB unseen/JJ types/NNS of/IN objects/NNS ./.
Our/PRP$ code/NN and/CC data/NNS are/VBP available/JJ at/IN :/:
