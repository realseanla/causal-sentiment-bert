Many/JJ current/JJ natural/JJ language/NN processing/NN applications/NNS for/IN social/JJ media/NNS rely/VBP on/IN representation/NN learning/NN and/CC utilize/VB pre-trained/JJ word/NN embeddings/NNS ./.
There/RB currently/RB exist/VBP several/JJ publicly/RB -/HYPH available/JJ ,/, pre-trained/JJ sets/NNS of/IN word/NN embeddings/NNS ,/, but/CC they/PRP contain/VBP few/JJ or/CC no/DT emoji/NN representations/NNS even/RB as/IN emoji/NN usage/NN in/IN social/JJ media/NNS has/VBZ increased/VBN ./.
In/IN this/DT paper/NN we/PRP release/VBP emoji2vec/NN ,/, pre-trained/JJ embeddings/NNS for/IN all/DT Unicode/NNP emoji/NN which/WDT are/VBP learned/VBN from/IN their/PRP$ description/NN in/IN the/DT Unicode/NNP emoji/NN standard/NN ./.
The/DT resulting/VBG emoji/NN embeddings/NNS can/MD be/VB readily/RB used/VBN in/IN downstream/JJ social/JJ natural/JJ language/NN processing/NN applications/NNS alongside/IN word2vec/NN ./.
We/PRP demonstrate/VBP ,/, for/IN the/DT downstream/JJ task/NN of/IN sentiment/NN analysis/NN ,/, that/IN emoji/NN embeddings/NNS learned/VBN from/IN short/JJ descriptions/NNS outperforms/VBZ a/DT skip/VB -/HYPH gram/NN model/NN trained/VBN on/IN a/DT large/JJ collection/NN of/IN tweets/NNS ,/, while/IN avoiding/VBG the/DT need/NN for/IN contexts/NNS in/IN which/WDT emoji/NN need/VBP to/TO appear/VB frequently/RB in/IN order/NN to/TO estimate/VB a/DT representation/NN ./.
