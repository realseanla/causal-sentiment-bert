Generic/JJ text/NN embeddings/NNS are/VBP successfully/RB used/VBN in/IN a/DT variety/NN of/IN tasks/NNS ./.
However/RB ,/, they/PRP are/VBP often/RB learnt/VBN by/IN capturing/VBG the/DT co-occurrence/NN structure/NN from/IN pure/JJ text/NN corpora/NNS ,/, resulting/VBG in/IN limitations/NNS of/IN their/PRP$ ability/NN to/TO generalize/VB ./.
In/IN this/DT paper/NN ,/, we/PRP explore/VBP models/NNS that/WDT incorporate/VBP visual/JJ information/NN into/IN the/DT text/NN representation/NN ./.
Based/VBN on/IN comprehensive/JJ ablation/NN studies/NNS ,/, we/PRP propose/VBP a/DT conceptually/RB simple/JJ ,/, yet/RB well/RB performing/VBG architecture/NN ./.
It/PRP outperforms/VBZ previous/JJ multimodal/JJ approaches/NNS on/IN a/DT set/NN of/IN well/RB established/VBN benchmarks/NNS ./.
We/PRP also/RB improve/VBP the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS for/IN image/NN -/HYPH related/VBN text/NN datasets/NNS ,/, using/VBG orders/NNS of/IN magnitude/NN less/RBR data/NNS ./.
