Generative/JJ Adversarial/JJ Networks/NNS have/VBP emerged/VBN as/IN an/DT effective/JJ technique/NN for/IN estimating/VBG data/NNS distributions/NNS ./.
The/DT basic/JJ setup/NN consists/VBZ of/IN two/CD deep/JJ networks/NNS playing/VBG against/IN each/DT other/JJ in/IN a/DT zero/CD -/HYPH sum/NN game/NN setting/NN ./.
However/RB ,/, it/PRP is/VBZ not/RB understood/VBN if/IN the/DT networks/NNS reach/VBP an/DT equilibrium/NN eventually/RB and/CC what/WP dynamics/NNS makes/VBZ this/DT possible/JJ ./.
The/DT current/JJ GAN/NNP training/NN procedure/NN ,/, which/WDT involves/VBZ simultaneous/JJ gradient/NN descent/NN ,/, lacks/VBZ a/DT clear/JJ game/NN -/HYPH theoretic/JJ justification/NN in/IN the/DT literature/NN ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP regret/NN minimization/NN as/IN a/DT technique/NN to/TO reach/VB equilibrium/NN in/IN games/NNS and/CC use/VB this/DT to/TO motivate/VB the/DT use/NN of/IN simultaneous/JJ GD/NNP in/IN GANs/NNP ./.
In/IN addition/NN ,/, we/PRP present/VBP a/DT hypothesis/NN that/IN mode/NN collapse/NN ,/, which/WDT is/VBZ a/DT common/JJ occurrence/NN in/IN GAN/NNP training/NN ,/, happens/VBZ due/IN to/IN the/DT existence/NN of/IN spurious/JJ local/JJ equilibria/NNS in/IN non-convex/JJ games/NNS ./.
Motivated/VBN by/IN these/DT insights/NNS ,/, we/PRP develop/VBP an/DT algorithm/NN called/VBN DRAGAN/NNP that/WDT is/VBZ fast/RB ,/, simple/JJ to/TO implement/VB and/CC achieves/VBZ competitive/JJ performance/NN in/IN a/DT stable/JJ fashion/NN across/IN different/JJ architectures/NNS ,/, datasets/NNS (/-LRB- MNIST/NNP ,/, CIFAR/NNP -/HYPH 10/CD ,/, and/CC CelebA/NN )/-RRB- ,/, and/CC divergence/NN measures/NNS with/IN almost/RB no/DT hyperparameter/NN tuning/NN ./.
