We/PRP consider/VBP log/NN -/HYPH supermodular/JJ models/NNS on/IN binary/JJ variables/NNS ,/, which/WDT are/VBP probabilistic/JJ models/NNS with/IN negative/JJ log/NN -/HYPH densities/NNS which/WDT are/VBP submodular/JJ ./.
These/DT models/NNS provide/VBP probabilistic/JJ interpretations/NNS of/IN common/JJ combinatorial/JJ optimization/NN tasks/NNS such/JJ as/IN image/NN segmentation/NN ./.
In/IN this/DT paper/NN ,/, we/PRP focus/VBP primarily/RB on/IN parameter/NN estimation/NN in/IN the/DT models/NNS from/IN known/VBN upper/JJ -/HYPH bounds/NNS on/IN the/DT intractable/JJ log/NN -/HYPH partition/NN function/NN ./.
We/PRP show/VBP that/IN the/DT bound/VBN based/VBN on/IN separable/JJ optimization/NN on/IN the/DT base/NN polytope/NN of/IN the/DT submodular/JJ function/NN is/VBZ always/RB inferior/JJ to/IN a/DT bound/VBN based/VBN on/IN "/`` perturb/VB -/HYPH and/CC -/HYPH MAP/NN "/'' ideas/NNS ./.
Then/RB ,/, to/TO learn/VB parameters/NNS ,/, given/VBN that/IN our/PRP$ approximation/NN of/IN the/DT log/NN -/HYPH partition/NN function/NN is/VBZ an/DT expectation/NN (/-LRB- over/IN our/PRP$ own/JJ randomization/NN )/-RRB- ,/, we/PRP use/VBP a/DT stochastic/JJ subgradient/NN technique/NN to/TO maximize/VB a/DT lower/JJR -/HYPH bound/VBN on/IN the/DT log/NN -/HYPH likelihood/NN ./.
This/DT can/MD also/RB be/VB extended/VBN to/IN conditional/JJ maximum/JJ likelihood/NN ./.
We/PRP illustrate/VBP our/PRP$ new/JJ results/NNS in/IN a/DT set/NN of/IN experiments/NNS in/IN binary/JJ image/NN denoising/NN ,/, where/WRB we/PRP highlight/VBP the/DT flexibility/NN of/IN a/DT probabilistic/JJ model/NN to/TO learn/VB with/IN missing/VBG data/NNS ./.
