We/PRP introduce/VBP a/DT neural/JJ machine/NN translation/NN model/NN that/WDT views/VBZ the/DT input/NN and/CC output/NN sentences/NNS as/IN sequences/NNS of/IN characters/NNS rather/RB than/IN words/NNS ./.
Since/IN word/NN -/HYPH level/NN information/NN provides/VBZ a/DT crucial/JJ source/NN of/IN bias/NN ,/, our/PRP$ input/NN model/NN composes/VBZ representations/NNS of/IN character/NN sequences/NNS into/IN representations/NNS of/IN words/NNS (/-LRB- as/IN determined/VBN by/IN whitespace/NN boundaries/NNS )/-RRB- ,/, and/CC then/RB these/DT are/VBP translated/VBN using/VBG a/DT joint/JJ attention/NN //HYPH translation/NN model/NN ./.
In/IN the/DT target/NN language/NN ,/, the/DT translation/NN is/VBZ modeled/VBN as/IN a/DT sequence/NN of/IN word/NN vectors/NNS ,/, but/CC each/DT word/NN is/VBZ generated/VBN one/CD character/NN at/IN a/DT time/NN ,/, conditional/JJ on/IN the/DT previous/JJ character/NN generations/NNS in/IN each/DT word/NN ./.
As/IN the/DT representation/NN and/CC generation/NN of/IN words/NNS is/VBZ performed/VBN at/IN the/DT character/NN level/NN ,/, our/PRP$ model/NN is/VBZ capable/JJ of/IN interpreting/VBG and/CC generating/VBG unseen/JJ word/NN forms/NNS ./.
A/DT secondary/JJ benefit/NN of/IN this/DT approach/NN is/VBZ that/IN it/PRP alleviates/VBZ much/RB of/IN the/DT challenges/NNS associated/VBN with/IN preprocessing/NN //HYPH tokenization/NN of/IN the/DT source/NN and/CC target/NN languages/NNS ./.
We/PRP show/VBP that/IN our/PRP$ model/NN can/MD achieve/VB translation/NN results/NNS that/WDT are/VBP on/IN par/NN with/IN conventional/JJ word/NN -/HYPH based/VBN models/NNS ./.
