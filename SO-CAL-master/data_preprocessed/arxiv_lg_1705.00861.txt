Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNNs/NNS )/-RRB- have/VBP provably/RB enhanced/VBN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN Neural/JJ Machine/NN Translation/NN (/-LRB- NMT/NN )/-RRB- with/IN their/PRP$ capability/NN in/IN modeling/NN complex/JJ functions/NNS and/CC capturing/VBG complex/JJ linguistic/JJ structures/NNS ./.
However/RB NMT/NNP systems/NNS with/IN deep/JJ architecture/NN in/IN their/PRP$ encoder/NN or/CC decoder/NN RNNs/NNS often/RB suffer/VBP from/IN severe/JJ gradient/NN diffusion/NN due/IN to/IN the/DT non-linear/JJ recurrent/JJ activations/NNS ,/, which/WDT often/RB make/VBP the/DT optimization/NN much/RB more/RBR difficult/JJ ./.
To/TO address/VB this/DT problem/NN we/PRP propose/VBP novel/JJ linear/JJ associative/JJ units/NNS (/-LRB- LAU/NN )/-RRB- to/TO reduce/VB the/DT gradient/NN propagation/NN length/NN inside/IN the/DT recurrent/JJ unit/NN ./.
Different/JJ from/IN conventional/JJ approaches/NNS (/-LRB- LSTM/NNP unit/NN and/CC GRU/NN )/-RRB- ,/, LAUs/NNP utilizes/VBZ linear/JJ associative/JJ connections/NNS between/IN input/NN and/CC output/NN of/IN the/DT recurrent/JJ unit/NN ,/, which/WDT allows/VBZ unimpeded/JJ information/NN flow/NN through/IN both/DT space/NN and/CC time/NN direction/NN ./.
The/DT model/NN is/VBZ quite/RB simple/JJ ,/, but/CC it/PRP is/VBZ surprisingly/RB effective/JJ ./.
Our/PRP$ empirical/JJ study/NN on/IN Chinese/JJ -/HYPH English/JJ translation/NN shows/VBZ that/IN our/PRP$ model/NN with/IN proper/JJ configuration/NN can/MD improve/VB by/IN 11.7/CD BLEU/NN upon/IN Groundhog/NNP and/CC the/DT best/JJS reported/VBN results/NNS in/IN the/DT same/JJ setting/NN ./.
On/IN WMT14/NNP English/NNP -/HYPH German/NNP task/NN and/CC a/DT larger/JJR WMT14/NN English/NNP -/HYPH French/NNP task/NN ,/, our/PRP$ model/NN achieves/VBZ comparable/JJ results/NNS with/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
