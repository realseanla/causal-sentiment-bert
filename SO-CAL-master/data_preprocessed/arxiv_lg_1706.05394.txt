We/PRP examine/VBP the/DT role/NN of/IN memorization/NN in/IN deep/JJ learning/NN ,/, drawing/VBG connections/NNS to/IN capacity/NN ,/, generalization/NN ,/, and/CC adversarial/JJ robustness/NN ./.
While/IN deep/JJ networks/NNS are/VBP capable/JJ of/IN memorizing/VBG noise/NN data/NNS ,/, our/PRP$ results/NNS suggest/VBP that/IN they/PRP tend/VBP to/TO prioritize/VB learning/NN simple/JJ patterns/NNS first/RB ./.
In/IN our/PRP$ experiments/NNS ,/, we/PRP expose/VBP qualitative/JJ differences/NNS in/IN gradient/NN -/HYPH based/VBN optimization/NN of/IN deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- on/IN noise/NN vs./FW real/JJ data/NNS ./.
We/PRP also/RB demonstrate/VBP that/IN for/IN appropriately/RB tuned/VBN explicit/JJ regularization/NN (/-LRB- e.g./FW ,/, dropout/NN )/-RRB- we/PRP can/MD degrade/VB DNN/NN training/NN performance/NN on/IN noise/NN datasets/NNS without/IN compromising/VBG generalization/NN on/IN real/JJ data/NNS ./.
Our/PRP$ analysis/NN suggests/VBZ that/IN the/DT notions/NNS of/IN effective/JJ capacity/NN which/WDT are/VBP dataset/NN independent/JJ are/VBP unlikely/JJ to/TO explain/VB the/DT generalization/NN performance/NN of/IN deep/JJ networks/NNS when/WRB trained/VBN with/IN gradient/NN based/VBN methods/NNS because/IN training/NN data/NNS itself/PRP plays/VBZ an/DT important/JJ role/NN in/IN determining/VBG the/DT degree/NN of/IN memorization/NN ./.
