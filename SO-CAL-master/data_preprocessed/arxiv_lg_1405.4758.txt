We/PRP consider/VBP stochastic/JJ multi-armed/JJ bandit/NN problems/NNS where/WRB the/DT expected/VBN reward/NN is/VBZ a/DT Lipschitz/NNP function/NN of/IN the/DT arm/NN ,/, and/CC where/WRB the/DT set/NN of/IN arms/NNS is/VBZ either/CC discrete/JJ or/CC continuous/JJ ./.
For/IN discrete/JJ Lipschitz/NNP bandits/NNS ,/, we/PRP derive/VBP asymptotic/JJ problem/NN specific/JJ lower/JJR bounds/NNS for/IN the/DT regret/NN satisfied/VBN by/IN any/DT algorithm/NN ,/, and/CC propose/VB OSLB/NNP and/CC CKL/NNP -/HYPH UCB/NNP ,/, two/CD algorithms/NNS that/WDT efficiently/RB exploit/VBP the/DT Lipschitz/NNP structure/NN of/IN the/DT problem/NN ./.
In/IN fact/NN ,/, we/PRP prove/VBP that/IN OSLB/NNP is/VBZ asymptotically/RB optimal/JJ ,/, as/IN its/PRP$ asymptotic/JJ regret/NN matches/VBZ the/DT lower/JJR bound/JJ ./.
The/DT regret/NN analysis/NN of/IN our/PRP$ algorithms/NNS relies/VBZ on/IN a/DT new/JJ concentration/NN inequality/NN for/IN weighted/JJ sums/NNS of/IN KL/NNP divergences/NNS between/IN the/DT empirical/JJ distributions/NNS of/IN rewards/NNS and/CC their/PRP$ true/JJ distributions/NNS ./.
For/IN continuous/JJ Lipschitz/NNP bandits/NNS ,/, we/PRP propose/VBP to/IN first/JJ discretize/NN the/DT action/NN space/NN ,/, and/CC then/RB apply/VB OSLB/NN or/CC CKL/NN -/HYPH UCB/NN ,/, algorithms/NNS that/WDT provably/RB exploit/VBP the/DT structure/NN efficiently/RB ./.
This/DT approach/NN is/VBZ shown/VBN ,/, through/IN numerical/JJ experiments/NNS ,/, to/TO significantly/RB outperform/VB existing/VBG algorithms/NNS that/WDT directly/RB deal/VBP with/IN the/DT continuous/JJ set/NN of/IN arms/NNS ./.
Finally/RB the/DT results/NNS and/CC algorithms/NNS are/VBP extended/VBN to/IN contextual/JJ bandits/NNS with/IN similarities/NNS ./.
