Deep/JJ Gaussian/JJ processes/NNS (/-LRB- DGPs/NNS )/-RRB- are/VBP multi-layer/JJ hierarchical/JJ generalisations/NNS of/IN Gaussian/JJ processes/NNS (/-LRB- GPs/NNS )/-RRB- and/CC are/VBP formally/RB equivalent/JJ to/IN neural/JJ networks/NNS with/IN multiple/JJ ,/, infinitely/RB wide/RB hidden/JJ layers/NNS ./.
DGPs/NNS are/VBP nonparametric/JJ probabilistic/JJ models/NNS and/CC as/IN such/JJ are/VBP arguably/RB more/RBR flexible/JJ ,/, have/VBP a/DT greater/JJR capacity/NN to/TO generalise/VB ,/, and/CC provide/VB better/JJR calibrated/VBN uncertainty/NN estimates/NNS than/IN alternative/JJ deep/JJ models/NNS ./.
This/DT paper/NN develops/VBZ a/DT new/JJ approximate/JJ Bayesian/JJ learning/NN scheme/NN that/WDT enables/VBZ DGPs/NNS to/TO be/VB applied/VBN to/IN a/DT range/NN of/IN medium/NN to/IN large/JJ scale/NN regression/NN problems/NNS for/IN the/DT first/JJ time/NN ./.
The/DT new/JJ method/NN uses/VBZ an/DT approximate/JJ Expectation/NN Propagation/NN procedure/NN and/CC a/DT novel/NN and/CC efficient/JJ extension/NN of/IN the/DT probabilistic/JJ backpropagation/NN algorithm/NN for/IN learning/NN ./.
We/PRP evaluate/VBP the/DT new/JJ method/NN for/IN non-linear/JJ regression/NN on/IN eleven/CD real/JJ -/HYPH world/NN datasets/NNS ,/, showing/VBG that/IN it/PRP always/RB outperforms/VBZ GP/NNP regression/NN and/CC is/VBZ almost/RB always/RB better/JJR than/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN deterministic/JJ and/CC sampling/NN -/HYPH based/VBN approximate/JJ inference/NN methods/NNS for/IN Bayesian/JJ neural/JJ networks/NNS ./.
As/IN a/DT by/IN -/HYPH product/NN ,/, this/DT work/NN provides/VBZ a/DT comprehensive/JJ analysis/NN of/IN six/CD approximate/JJ Bayesian/JJ methods/NNS for/IN training/NN neural/JJ networks/NNS ./.
