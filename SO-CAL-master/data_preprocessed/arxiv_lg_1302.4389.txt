We/PRP consider/VBP the/DT problem/NN of/IN designing/VBG models/NNS to/IN leverage/NN a/DT recently/RB introduced/VBN approximate/JJ model/NN averaging/VBG technique/NN called/VBN dropout/NN ./.
We/PRP define/VBP a/DT simple/JJ new/JJ model/NN called/VBN maxout/NN (/-LRB- so/RB named/VBN because/IN its/PRP$ {/-LRB- \/SYM em/PRP out/RP }/-RRB- put/NN is/VBZ the/DT max/NN of/IN a/DT set/NN of/IN inputs/NNS ,/, and/CC because/IN it/PRP is/VBZ a/DT natural/JJ companion/NN to/IN dropout/NN )/-RRB- designed/VBN to/TO both/CC facilitate/VB optimization/NN by/IN dropout/NN and/CC improve/VB the/DT accuracy/NN of/IN dropout/NN 's/POS fast/JJ approximate/JJ model/NN averaging/VBG technique/NN ./.
We/PRP empirically/RB verify/VBP that/IN the/DT model/NN successfully/RB accomplishes/VBZ both/DT of/IN these/DT tasks/NNS ./.
We/PRP use/VBP maxout/NN and/CC dropout/NN to/TO demonstrate/VB state/NN of/IN the/DT art/NN classification/NN performance/NN on/IN four/CD benchmark/NN datasets/NNS :/: MNIST/NNP ,/, CIFAR/NNP -/HYPH 10/CD ,/, CIFAR/NN -/HYPH 100/CD ,/, and/CC SVHN/NNP ./.
