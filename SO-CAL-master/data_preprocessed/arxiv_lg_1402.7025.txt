When/WRB dealing/VBG with/IN datasets/NNS containing/VBG a/DT billion/CD instances/NNS or/CC with/IN simulations/NNS that/WDT require/VBP a/DT supercomputer/NN to/TO execute/VB ,/, computational/JJ resources/NNS become/VBP part/NN of/IN the/DT equation/NN ./.
We/PRP can/MD improve/VB the/DT efficiency/NN of/IN learning/NN and/CC inference/NN by/IN exploiting/VBG their/PRP$ inherent/JJ statistical/JJ nature/NN ./.
We/PRP propose/VBP algorithms/NNS that/WDT exploit/VBP the/DT redundancy/NN of/IN data/NNS relative/JJ to/IN a/DT model/NN by/IN subsampling/VBG data/NNS -/HYPH cases/NNS for/IN every/DT update/NN and/CC reasoning/NN about/IN the/DT uncertainty/NN created/VBN in/IN this/DT process/NN ./.
In/IN the/DT context/NN of/IN learning/NN we/PRP propose/VBP to/TO test/VB for/IN the/DT probability/NN that/IN a/DT stochastically/RB estimated/VBN gradient/NN points/VBZ more/JJR than/IN 180/CD degrees/NNS in/IN the/DT wrong/JJ direction/NN ./.
In/IN the/DT context/NN of/IN MCMC/NNP sampling/NN we/PRP use/VBP stochastic/JJ gradients/NNS to/TO improve/VB the/DT efficiency/NN of/IN MCMC/NNP updates/NNS ,/, and/CC hypothesis/NN tests/NNS based/VBN on/IN adaptive/JJ mini-batches/NNS to/TO decide/VB whether/IN to/TO accept/VB or/CC reject/VB a/DT proposed/VBN parameter/NN update/NN ./.
Finally/RB ,/, we/PRP argue/VBP that/IN in/IN the/DT context/NN of/IN likelihood/NN free/JJ MCMC/NN one/CD needs/VBZ to/TO store/VB all/PDT the/DT information/NN revealed/VBN by/IN all/DT simulations/NNS ,/, for/IN instance/NN in/IN a/DT Gaussian/JJ process/NN ./.
We/PRP conclude/VBP that/IN Bayesian/JJ methods/NNS will/MD remain/VB to/TO play/VB a/DT crucial/JJ role/NN in/IN the/DT era/NN of/IN big/JJ data/NNS and/CC big/JJ simulations/NNS ,/, but/CC only/RB if/IN we/PRP overcome/VBP a/DT number/NN of/IN computational/JJ challenges/NNS ./.
