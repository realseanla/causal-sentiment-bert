The/DT scale/NN of/IN modern/JJ datasets/NNS necessitates/VBZ the/DT development/NN of/IN efficient/JJ distributed/VBN optimization/NN methods/NNS for/IN machine/NN learning/NN ./.
We/PRP present/VBP a/DT general/JJ -/HYPH purpose/NN framework/NN for/IN the/DT distributed/VBN environment/NN ,/, CoCoA/NN ,/, that/WDT has/VBZ an/DT efficient/JJ communication/NN scheme/NN and/CC is/VBZ applicable/JJ to/IN a/DT wide/JJ variety/NN of/IN problems/NNS in/IN machine/NN learning/NN and/CC signal/NN processing/NN ./.
We/PRP extend/VBP the/DT framework/NN to/TO cover/VB general/JJ non-strongly/JJ convex/NN regularizers/NNS ,/, including/VBG L1/NN -/HYPH regularized/VBN problems/NNS like/IN lasso/NN ,/, sparse/JJ logistic/JJ regression/NN ,/, and/CC elastic/JJ net/JJ regularization/NN ,/, and/CC show/VB how/WRB earlier/JJR work/NN can/MD be/VB derived/VBN as/IN a/DT special/JJ case/NN ./.
We/PRP provide/VBP convergence/NN guarantees/NNS for/IN the/DT class/NN of/IN convex/NN regularized/VBN loss/NN minimization/NN objectives/NNS ,/, leveraging/VBG a/DT novel/JJ approach/NN in/IN handling/VBG non-strongly/JJ convex/NN regularizers/NNS and/CC non-smooth/JJ loss/NN functions/NNS ./.
The/DT resulting/VBG framework/NN has/VBZ markedly/RB improved/VBN performance/NN over/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS ,/, as/IN we/PRP illustrate/VBP with/IN an/DT extensive/JJ set/NN of/IN experiments/NNS on/IN real/JJ distributed/VBN datasets/NNS ./.
