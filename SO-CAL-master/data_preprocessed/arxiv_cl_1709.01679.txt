This/DT study/NN addresses/VBZ the/DT problem/NN of/IN identifying/VBG the/DT meaning/NN of/IN unknown/JJ words/NNS or/CC entities/NNS in/IN a/DT discourse/NN with/IN respect/NN to/IN the/DT word/NN embedding/VBG approaches/NNS used/VBN in/IN neural/JJ language/NN models/NNS ./.
We/PRP proposed/VBD a/DT method/NN for/IN on/IN -/HYPH the/DT -/HYPH fly/NN construction/NN and/CC exploitation/NN of/IN word/NN embeddings/NNS in/IN both/CC the/DT input/NN and/CC output/NN layers/NNS of/IN a/DT neural/JJ model/NN by/IN tracking/VBG contexts/NNS ./.
This/DT extends/VBZ the/DT dynamic/JJ entity/NN representation/NN used/VBN in/IN Kobayashi/NNP et/FW al./FW (/-LRB- 2016/CD )/-RRB- and/CC incorporates/VBZ a/DT copy/NN mechanism/NN proposed/VBN independently/RB by/IN Gu/NNP et/FW al./FW (/-LRB- 2016/CD )/-RRB- and/CC Gulcehre/NNP et/FW al./FW (/-LRB- 2016/CD )/-RRB- ./.
In/IN addition/NN ,/, we/PRP construct/VBP a/DT new/JJ task/NN and/CC dataset/NN called/VBN Anonymized/NNP Language/NNP Modeling/VBG for/IN evaluating/VBG the/DT ability/NN to/TO capture/VB word/NN meanings/NNS while/IN reading/VBG ./.
Experiments/NNS conducted/VBN using/VBG our/PRP$ novel/JJ dataset/NN show/VBP that/IN the/DT proposed/VBN variant/NN of/IN RNN/NNP language/NN model/NN outperformed/VBD the/DT baseline/NN model/NN ./.
Furthermore/RB ,/, the/DT experiments/NNS also/RB demonstrate/VBP that/IN dynamic/JJ updates/NNS of/IN an/DT output/NN layer/NN help/VB a/DT model/NN predict/VBP reappearing/VBG entities/NNS ,/, whereas/IN those/DT of/IN an/DT input/NN layer/NN are/VBP effective/JJ to/TO predict/VB words/NNS following/VBG reappearing/VBG entities/NNS ./.
