We/PRP present/VBP a/DT generative/JJ model/NN of/IN images/NNS based/VBN on/IN layering/NN ,/, in/IN which/WDT image/NN layers/NNS are/VBP individually/RB generated/VBN ,/, then/RB composited/VBN from/IN front/NN to/TO back/RB ./.
We/PRP are/VBP thus/RB able/JJ to/TO factor/VB the/DT appearance/NN of/IN an/DT image/NN into/IN the/DT appearance/NN of/IN individual/JJ objects/NNS within/IN the/DT image/NN ---/, and/CC additionally/RB for/IN each/DT individual/JJ object/NN ,/, we/PRP can/MD factor/VB content/NN from/IN pose/NN ./.
Unlike/IN prior/JJ work/NN on/IN layered/JJ models/NNS ,/, we/PRP learn/VBP a/DT shape/NN prior/JJ for/IN each/DT object/NN //HYPH layer/NN ,/, allowing/VBG the/DT model/NN to/TO tease/VB out/RP which/WDT object/NN is/VBZ in/IN front/NN by/IN looking/VBG for/IN a/DT consistent/JJ shape/NN ,/, without/IN needing/VBG access/NN to/IN motion/NN cues/NNS or/CC any/DT labeled/VBN data/NNS ./.
We/PRP show/VBP that/IN ordinary/JJ stochastic/JJ gradient/NN variational/JJ bayes/NNS (/-LRB- SGVB/NN )/-RRB- ,/, which/WDT optimizes/VBZ our/PRP$ fully/RB differentiable/JJ lower/JJR -/HYPH bound/VBN on/IN the/DT log/NN -/HYPH likelihood/NN ,/, is/VBZ sufficient/JJ to/TO learn/VB an/DT interpretable/JJ representation/NN of/IN images/NNS ./.
Finally/RB we/PRP present/JJ experiments/NNS demonstrating/VBG the/DT effectiveness/NN of/IN the/DT model/NN for/IN inferring/VBG foreground/NN and/CC background/NN objects/NNS in/IN images/NNS ./.
