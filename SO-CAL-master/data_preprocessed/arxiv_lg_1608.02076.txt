We/PRP develop/VBP a/DT novel/JJ bi-directional/JJ attention/NN model/NN for/IN dependency/NN parsing/VBG ,/, which/WDT learns/VBZ to/TO agree/VB on/IN headword/JJ predictions/NNS from/IN the/DT forward/JJ and/CC backward/JJ parsing/VBG directions/NNS ./.
The/DT parsing/VBG procedure/NN for/IN each/DT direction/NN is/VBZ formulated/VBN as/IN sequentially/RB querying/VBG the/DT memory/NN component/NN that/WDT stores/VBZ continuous/JJ headword/NN embeddings/NNS ./.
The/DT proposed/VBN parser/NN makes/VBZ use/NN of/IN soft/JJ headword/NN embeddings/NNS ,/, allowing/VBG the/DT model/NN to/TO implicitly/RB capture/VB high/JJ -/HYPH order/NN parsing/VBG history/NN without/IN dramatically/RB increasing/VBG the/DT computational/JJ complexity/NN ./.
We/PRP conduct/VBP experiments/NNS on/IN English/NNP ,/, Chinese/NNP ,/, and/CC 12/CD other/JJ languages/NNS from/IN the/DT CoNLL/NNP 2006/CD shared/VBD task/NN ,/, showing/VBG that/IN the/DT proposed/VBN model/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN unlabeled/JJ attachment/NN scores/NNS on/IN 7/CD languages/NNS ./.
