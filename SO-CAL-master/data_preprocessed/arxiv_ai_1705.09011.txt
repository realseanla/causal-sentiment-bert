We/PRP propose/VBP a/DT probabilistic/JJ framework/NN for/IN domain/NN adaptation/NN that/WDT blends/VBZ both/CC generative/JJ and/CC discriminative/JJ modeling/NN in/IN a/DT principled/JJ way/NN ./.
By/IN maximizing/VBG both/CC the/DT marginal/JJ and/CC the/DT conditional/JJ log/NN -/HYPH likelihoods/NNS ,/, models/NNS derived/VBN from/IN this/DT framework/NN can/MD use/VB both/DT labeled/VBN instances/NNS from/IN the/DT source/NN domain/NN as/RB well/RB as/IN unlabeled/JJ instances/NNS from/IN both/DT source/NN and/CC target/NN domains/NNS ./.
Under/IN this/DT framework/NN ,/, we/PRP show/VBP that/IN the/DT popular/JJ reconstruction/NN loss/NN of/IN autoencoder/NN corresponds/VBZ to/IN an/DT upper/JJ bound/VBN of/IN the/DT negative/JJ marginal/JJ log/NN -/HYPH likelihoods/NNS of/IN unlabeled/JJ instances/NNS ,/, where/WRB marginal/JJ distributions/NNS are/VBP given/VBN by/IN proper/JJ kernel/NN density/NN estimations/NNS ./.
This/DT provides/VBZ a/DT way/NN to/TO interpret/VB the/DT empirical/JJ success/NN of/IN autoencoders/NNS in/IN domain/NN adaptation/NN and/CC semi-supervised/VBN learning/NN ./.
We/PRP instantiate/VBP our/PRP$ framework/NN using/VBG neural/JJ networks/NNS ,/, and/CC build/VB a/DT concrete/JJ model/NN ,/, DAuto/NNP ./.
Empirically/RB ,/, we/PRP demonstrate/VBP the/DT effectiveness/NN of/IN DAuto/NNP on/IN text/NN ,/, image/NN and/CC speech/NN datasets/NNS ,/, showing/VBG that/IN it/PRP outperforms/VBZ related/JJ competitors/NNS when/WRB domain/NN adaptation/NN is/VBZ possible/JJ ./.
