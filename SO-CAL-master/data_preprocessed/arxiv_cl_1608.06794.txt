Distributional/JJ models/NNS are/VBP derived/VBN from/IN co-occurrences/NNS in/IN a/DT corpus/NN ,/, where/WRB only/RB a/DT small/JJ proportion/NN of/IN all/DT possible/JJ plausible/JJ co-occurrences/NNS will/MD be/VB observed/VBN ./.
This/DT results/VBZ in/IN a/DT very/RB sparse/JJ vector/NN space/NN ,/, requiring/VBG a/DT mechanism/NN for/IN inferring/VBG missing/VBG knowledge/NN ./.
Most/JJS methods/NNS face/VBP this/DT challenge/NN in/IN ways/NNS that/WDT render/VBP the/DT resulting/VBG word/NN representations/NNS uninterpretable/JJ ,/, with/IN the/DT consequence/NN that/WDT semantic/JJ composition/NN becomes/VBZ hard/JJ to/TO model/VB ./.
In/IN this/DT paper/NN we/PRP explore/VBP an/DT alternative/NN which/WDT involves/VBZ explicitly/RB inferring/VBG unobserved/JJ co-occurrences/NNS using/VBG the/DT distributional/JJ neighbourhood/NN ./.
We/PRP show/VBP that/IN distributional/JJ inference/NN improves/VBZ sparse/JJ word/NN representations/NNS on/IN several/JJ word/NN similarity/NN benchmarks/NNS and/CC demonstrate/VBP that/IN our/PRP$ model/NN is/VBZ competitive/JJ with/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN for/IN adjective/NN -/HYPH noun/NN ,/, noun/NN -/HYPH noun/NN and/CC verb/VB -/HYPH object/NN compositions/NNS while/IN being/VBG fully/RB interpretable/JJ ./.
