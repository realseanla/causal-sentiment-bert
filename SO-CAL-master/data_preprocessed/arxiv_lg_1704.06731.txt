We/PRP propose/VBP Batch/NN -/HYPH Expansion/NN Training/NN (/-LRB- BET/NN )/-RRB- ,/, a/DT framework/NN for/IN running/VBG a/DT batch/NN optimizer/NN on/IN a/DT gradually/RB expanding/VBG dataset/NN ./.
As/IN opposed/VBN to/IN stochastic/JJ approaches/NNS ,/, batches/NNS do/VBP not/RB need/VB to/TO be/VB resampled/VBN i.i.d./NN at/IN every/DT iteration/NN ,/, thus/RB making/VBG BET/NN more/RBR resource/NN efficient/JJ in/IN a/DT distributed/VBN setting/NN ,/, and/CC when/WRB disk/NN -/HYPH access/NN is/VBZ constrained/VBN ./.
Moreover/RB ,/, BET/NN can/MD be/VB easily/RB paired/VBN with/IN most/JJS batch/NN optimizers/NNS ,/, does/VBZ not/RB require/VB any/DT parameter/NN -/HYPH tuning/NN ,/, and/CC compares/VBZ favorably/RB to/IN existing/VBG stochastic/JJ and/CC batch/NN methods/NNS ./.
We/PRP show/VBP that/IN when/WRB the/DT batch/NN size/NN grows/VBZ exponentially/RB with/IN the/DT number/NN of/IN outer/JJ iterations/NNS ,/, BET/NN achieves/VBZ optimal/JJ $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- 1/CD //SYM \/SYM epsilon/SYM )/-RRB- $/$ data/NNS -/HYPH access/NN convergence/NN rate/NN for/IN strongly/RB convex/JJ objectives/NNS ./.
