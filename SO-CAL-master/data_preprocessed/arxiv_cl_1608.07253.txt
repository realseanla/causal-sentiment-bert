We/PRP introduce/VBP a/DT novel/JJ latent/NN vector/NN space/NN model/NN that/WDT jointly/RB learns/VBZ the/DT latent/JJ representations/NNS of/IN words/NNS ,/, e-commerce/NN products/NNS and/CC a/DT mapping/NN between/IN the/DT two/CD without/IN the/DT need/NN for/IN explicit/JJ annotations/NNS ./.
The/DT power/NN of/IN the/DT model/NN lies/VBZ in/IN its/PRP$ ability/NN to/TO directly/RB model/VB the/DT discriminative/JJ relation/NN between/IN products/NNS and/CC a/DT particular/JJ word/NN ./.
We/PRP compare/VBP our/PRP$ method/NN to/IN existing/VBG latent/NN vector/NN space/NN models/NNS (/-LRB- LSI/NNP ,/, LDA/NNP and/CC word2vec/NN )/-RRB- and/CC evaluate/VB it/PRP as/IN a/DT feature/NN in/IN a/DT learning/NN to/IN rank/NN setting/NN ./.
Our/PRP$ latent/NN vector/NN space/NN model/NN achieves/VBZ its/PRP$ enhanced/VBN performance/NN as/IN it/PRP learns/VBZ better/JJR product/NN representations/NNS ./.
Furthermore/RB ,/, the/DT mapping/NN from/IN words/NNS to/TO products/NNS and/CC the/DT representations/NNS of/IN words/NNS benefit/VBP directly/RB from/IN the/DT errors/NNS propagated/VBN back/RB from/IN the/DT product/NN representations/NNS during/IN parameter/NN estimation/NN ./.
We/PRP provide/VBP an/DT in/IN -/HYPH depth/NN analysis/NN of/IN the/DT performance/NN of/IN our/PRP$ model/NN and/CC analyze/VB the/DT structure/NN of/IN the/DT learned/VBN representations/NNS ./.
