Vanishing/VBG (/-LRB- and/CC exploding/VBG )/-RRB- gradients/NNS effect/NN is/VBZ a/DT common/JJ problem/NN for/IN recurrent/JJ neural/JJ networks/NNS with/IN nonlinear/JJ activation/NN functions/NNS which/WDT use/VBP backpropagation/NN method/NN for/IN calculation/NN of/IN derivatives/NNS ./.
Deep/JJ feedforward/NN neural/JJ networks/NNS with/IN many/JJ hidden/JJ layers/NNS also/RB suffer/VBP from/IN this/DT effect/NN ./.
In/IN this/DT paper/NN we/PRP propose/VBP a/DT novel/JJ universal/JJ technique/NN that/WDT makes/VBZ the/DT norm/NN of/IN the/DT gradient/NN stay/NN in/IN the/DT suitable/JJ range/NN ./.
We/PRP construct/VBP a/DT way/NN to/TO estimate/VB a/DT contribution/NN of/IN each/DT training/NN example/NN to/IN the/DT norm/NN of/IN the/DT long/JJ -/HYPH term/NN components/NNS of/IN the/DT target/NN function/NN s/POS gradient/NN ./.
Using/VBG this/DT subroutine/NN we/PRP can/MD construct/VB mini-batches/NNS for/IN the/DT stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- training/NN that/WDT leads/VBZ to/IN high/JJ performance/NN and/CC accuracy/NN of/IN the/DT trained/VBN network/NN even/RB for/IN very/RB complex/JJ tasks/NNS ./.
We/PRP provide/VBP a/DT straightforward/JJ mathematical/JJ estimation/NN of/IN minibatch/NN s/POS impact/NN on/IN for/IN the/DT gradient/NN norm/NN and/CC prove/VB its/PRP$ correctness/NN theoretically/RB ./.
To/TO check/VB our/PRP$ framework/NN experimentally/RB we/PRP use/VBP some/DT special/JJ synthetic/JJ benchmarks/NNS for/IN testing/NN RNNs/NNS on/IN ability/NN to/TO capture/VB long/RB -/HYPH term/NN dependencies/NNS ./.
Our/PRP$ network/NN can/MD detect/VB links/NNS between/IN events/NNS in/IN the/DT (/-LRB- temporal/JJ )/-RRB- sequence/NN at/IN the/DT range/NN approx./IN 100/CD and/CC longer/JJR ./.
