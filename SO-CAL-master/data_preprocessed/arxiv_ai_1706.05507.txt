Adaptive/JJ gradient/NN methods/NNS have/VBP become/VBN recently/RB very/RB popular/JJ ,/, in/IN particular/JJ as/IN they/PRP have/VBP been/VBN shown/VBN to/TO be/VB useful/JJ in/IN the/DT training/NN of/IN deep/JJ neural/JJ networks/NNS ./.
In/IN this/DT paper/NN we/PRP have/VBP analyzed/VBN RMSProp/NNP ,/, originally/RB proposed/VBN for/IN the/DT training/NN of/IN deep/JJ neural/JJ networks/NNS ,/, in/IN the/DT context/NN of/IN online/JJ convex/NN optimization/NN and/CC show/VB $/$ \/CD sqrt/NN {/-LRB- T/NN }/-RRB- $/$ -/HYPH type/NN regret/NN bounds/NNS ./.
Moreover/RB ,/, we/PRP propose/VBP two/CD variants/NNS SC/NN -/HYPH Adagrad/NN and/CC SC/NN -/HYPH RMSProp/NN for/IN which/WDT we/PRP show/VBP logarithmic/JJ regret/NN bounds/NNS for/IN strongly/RB convex/JJ functions/NNS ./.
Finally/RB ,/, we/PRP demonstrate/VBP in/IN the/DT experiments/NNS that/IN these/DT new/JJ variants/NNS outperform/VBP other/JJ adaptive/JJ gradient/NN techniques/NNS or/CC stochastic/JJ gradient/NN descent/NN in/IN the/DT optimization/NN of/IN strongly/RB convex/JJ functions/NNS as/RB well/RB as/IN in/IN training/NN of/IN deep/JJ neural/JJ networks/NNS ./.
