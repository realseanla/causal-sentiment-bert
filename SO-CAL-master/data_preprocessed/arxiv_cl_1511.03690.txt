In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT model/NN which/WDT takes/VBZ as/RB input/VB a/DT corpus/NN of/IN images/NNS with/IN relevant/JJ spoken/VBN captions/NNS and/CC finds/VBZ a/DT correspondence/NN between/IN the/DT two/CD modalities/NNS ./.
We/PRP employ/VBP a/DT pair/NN of/IN convolutional/JJ neural/JJ networks/NNS to/TO model/VB visual/JJ objects/NNS and/CC speech/NN signals/NNS at/IN the/DT word/NN level/NN ,/, and/CC tie/VB the/DT networks/NNS together/RB with/IN an/DT embedding/NN and/CC alignment/NN model/NN which/WDT learns/VBZ a/DT joint/JJ semantic/JJ space/NN over/IN both/DT modalities/NNS ./.
We/PRP evaluate/VBP our/PRP$ model/NN using/VBG image/NN search/NN and/CC annotation/NN tasks/NNS on/IN the/DT Flickr8k/NN dataset/NN ,/, which/WDT we/PRP augmented/VBD by/IN collecting/VBG a/DT corpus/NN of/IN 40,000/CD spoken/VBN captions/NNS using/VBG Amazon/NNP Mechanical/NNP Turk/NNP ./.
