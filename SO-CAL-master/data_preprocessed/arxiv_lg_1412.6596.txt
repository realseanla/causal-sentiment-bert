Current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN deep/JJ learning/NN systems/NNS for/IN visual/JJ object/NN recognition/NN and/CC detection/NN use/VBP purely/RB supervised/JJ training/NN with/IN regularization/NN such/JJ as/IN dropout/NN to/TO avoid/VB overfitting/NN ./.
The/DT performance/NN depends/VBZ critically/RB on/IN the/DT amount/NN of/IN labeled/VBN examples/NNS ,/, and/CC in/IN current/JJ practice/NN the/DT labels/NNS are/VBP assumed/VBN to/TO be/VB unambiguous/JJ and/CC accurate/JJ ./.
However/RB ,/, this/DT assumption/NN often/RB does/VBZ not/RB hold/VB ;/: e.g./FW in/IN recognition/NN ,/, class/NN labels/NNS may/MD be/VB missing/VBG ;/: in/IN detection/NN ,/, objects/NNS in/IN the/DT image/NN may/MD not/RB be/VB localized/VBN ;/: and/CC in/IN general/JJ ,/, the/DT labeling/NN may/MD be/VB subjective/JJ ./.
In/IN this/DT work/NN we/PRP propose/VBP a/DT generic/JJ way/NN to/TO handle/VB noisy/JJ and/CC incomplete/JJ labeling/NN by/IN augmenting/VBG the/DT prediction/NN objective/NN with/IN a/DT notion/NN of/IN consistency/NN ./.
We/PRP consider/VBP a/DT prediction/NN consistent/JJ if/IN the/DT same/JJ prediction/NN is/VBZ made/VBN given/VBN similar/JJ percepts/NNS ,/, where/WRB the/DT notion/NN of/IN similarity/NN is/VBZ between/IN deep/JJ network/NN features/VBZ computed/VBN from/IN the/DT input/NN data/NNS ./.
In/IN experiments/NNS we/PRP demonstrate/VBP that/IN our/PRP$ approach/NN yields/VBZ substantial/JJ robustness/NN to/IN label/NN noise/NN on/IN several/JJ datasets/NNS ./.
On/IN MNIST/JJ handwritten/JJ digits/NNS ,/, we/PRP show/VBP that/IN our/PRP$ model/NN is/VBZ robust/JJ to/IN label/NN corruption/NN ./.
On/IN the/DT Toronto/NNP Face/NNP Database/NNP ,/, we/PRP show/VBP that/IN our/PRP$ model/NN handles/VBZ well/RB the/DT case/NN of/IN subjective/JJ labels/NNS in/IN emotion/NN recognition/NN ,/, achieving/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS ,/, and/CC can/MD also/RB benefit/VB from/IN unlabeled/JJ face/NN images/NNS with/IN no/DT modification/NN to/IN our/PRP$ method/NN ./.
On/IN the/DT ILSVRC2014/NN detection/NN challenge/NN data/NNS ,/, we/PRP show/VBP that/IN our/PRP$ approach/NN extends/VBZ to/IN very/RB deep/JJ networks/NNS ,/, high/JJ resolution/NN images/NNS and/CC structured/VBN outputs/NNS ,/, and/CC results/VBZ in/IN improved/VBN scalable/JJ detection/NN ./.
