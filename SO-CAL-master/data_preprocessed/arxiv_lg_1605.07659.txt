We/PRP consider/VBP empirical/JJ risk/NN minimization/NN for/IN large/JJ -/HYPH scale/NN datasets/NNS ./.
We/PRP introduce/VBP Ada/NNP Newton/NNP as/IN an/DT adaptive/JJ algorithm/NN that/WDT uses/VBZ Newton/NNP 's/POS method/NN with/IN adaptive/JJ sample/NN sizes/NNS ./.
The/DT main/JJ idea/NN of/IN Ada/NNP Newton/NNP is/VBZ to/TO increase/VB the/DT size/NN of/IN the/DT training/NN set/VBN by/IN a/DT factor/NN larger/JJR than/IN one/CD in/IN a/DT way/NN that/IN the/DT minimization/NN variable/NN for/IN the/DT current/JJ training/NN set/NN is/VBZ in/IN the/DT local/JJ neighborhood/NN of/IN the/DT optimal/JJ argument/NN of/IN the/DT next/JJ training/NN set/NN ./.
This/DT allows/VBZ to/TO exploit/VB the/DT quadratic/JJ convergence/NN property/NN of/IN Newton/NNP 's/POS method/NN and/CC reach/VB the/DT statistical/JJ accuracy/NN of/IN each/DT training/NN set/VBN with/IN only/RB one/CD iteration/NN of/IN Newton/NNP 's/POS method/NN ./.
We/PRP show/VBP theoretically/RB and/CC empirically/RB that/DT Ada/NNP Newton/NNP can/MD double/VB the/DT size/NN of/IN the/DT training/NN set/VBN in/IN each/DT iteration/NN to/TO achieve/VB the/DT statistical/JJ accuracy/NN of/IN the/DT full/JJ training/NN set/VBN with/IN about/RB two/CD passes/NNS over/IN the/DT dataset/NN ./.
