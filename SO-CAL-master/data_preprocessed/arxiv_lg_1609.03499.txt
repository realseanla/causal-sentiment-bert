This/DT paper/NN introduces/VBZ WaveNet/NNP ,/, a/DT deep/JJ neural/JJ network/NN for/IN generating/VBG raw/JJ audio/NN waveforms/NNS ./.
The/DT model/NN is/VBZ fully/RB probabilistic/JJ and/CC autoregressive/JJ ,/, with/IN the/DT predictive/JJ distribution/NN for/IN each/DT audio/JJ sample/NN conditioned/VBN on/IN all/DT previous/JJ ones/NNS ;/: nonetheless/RB we/PRP show/VBP that/IN it/PRP can/MD be/VB efficiently/RB trained/VBN on/IN data/NNS with/IN tens/NNS of/IN thousands/NNS of/IN samples/NNS per/IN second/NN of/IN audio/NN ./.
When/WRB applied/VBN to/IN text/NN -/HYPH to/IN -/HYPH speech/NN ,/, it/PRP yields/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN ,/, with/IN human/JJ listeners/NNS rating/VBG it/PRP as/IN significantly/RB more/JJR natural/JJ sounding/VBG than/IN the/DT best/JJS parametric/JJ and/CC concatenative/JJ systems/NNS for/IN both/DT English/NNP and/CC Mandarin/NNP ./.
A/DT single/JJ WaveNet/NNP can/MD capture/VB the/DT characteristics/NNS of/IN many/JJ different/JJ speakers/NNS with/IN equal/JJ fidelity/NN ,/, and/CC can/MD switch/VB between/IN them/PRP by/IN conditioning/VBG on/IN the/DT speaker/NN identity/NN ./.
When/WRB trained/VBN to/TO model/VB music/NN ,/, we/PRP find/VBP that/IN it/PRP generates/VBZ novel/JJ and/CC often/RB highly/RB realistic/JJ musical/JJ fragments/NNS ./.
We/PRP also/RB show/VBP that/IN it/PRP can/MD be/VB employed/VBN as/IN a/DT discriminative/JJ model/NN ,/, returning/VBG promising/JJ results/NNS for/IN phoneme/NN recognition/NN ./.
