In/IN this/DT paper/NN ,/, we/PRP focus/VBP on/IN learning/VBG structure/NN -/HYPH aware/JJ document/NN representations/NNS from/IN data/NNS without/IN recourse/NN to/IN a/DT discourse/NN parser/NN or/CC additional/JJ annotations/NNS ./.
Drawing/VBG inspiration/NN from/IN recent/JJ efforts/NNS to/TO empower/VB neural/JJ networks/NNS with/IN a/DT structural/JJ bias/NN ,/, we/PRP propose/VBP a/DT model/NN that/WDT can/MD encode/VB a/DT document/NN while/IN automatically/RB inducing/VBG rich/JJ structural/JJ dependencies/NNS ./.
Specifically/RB ,/, we/PRP embed/VBP a/DT differentiable/JJ non-projective/JJ parsing/VBG algorithm/NN into/IN a/DT neural/JJ model/NN and/CC use/VB attention/NN mechanisms/NNS to/TO incorporate/VB the/DT structural/JJ biases/NNS ./.
Experimental/JJ evaluation/NN across/IN different/JJ tasks/NNS and/CC datasets/NNS shows/VBZ that/IN the/DT proposed/VBN model/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN document/NN modeling/NN tasks/NNS while/IN inducing/VBG intermediate/JJ structures/NNS which/WDT are/VBP both/DT interpretable/JJ and/CC meaningful/JJ ./.
