We/PRP derive/VBP an/DT equation/NN for/IN temporal/JJ difference/NN learning/NN from/IN statistical/JJ principles/NNS ./.
Specifically/RB ,/, we/PRP start/VBP with/IN the/DT variational/JJ principle/NN and/CC then/RB bootstrap/NN to/TO produce/VB an/DT updating/NN rule/NN for/IN discounted/VBN state/NN value/NN estimates/NNS ./.
The/DT resulting/VBG equation/NN is/VBZ similar/JJ to/IN the/DT standard/JJ equation/NN for/IN temporal/JJ difference/NN learning/NN with/IN eligibility/NN traces/NNS ,/, so/RB called/VBN TD/NNP (/-LRB- lambda/NN )/-RRB- ,/, however/RB it/PRP lacks/VBZ the/DT parameter/NN alpha/NN that/WDT specifies/VBZ the/DT learning/NN rate/NN ./.
In/IN the/DT place/NN of/IN this/DT free/JJ parameter/NN there/EX is/VBZ now/RB an/DT equation/NN for/IN the/DT learning/NN rate/NN that/WDT is/VBZ specific/JJ to/IN each/DT state/NN transition/NN ./.
We/PRP experimentally/RB test/VB this/DT new/JJ learning/NN rule/NN against/IN TD/NN (/-LRB- lambda/NN )/-RRB- and/CC find/VB that/IN it/PRP offers/VBZ superior/JJ performance/NN in/IN various/JJ settings/NNS ./.
Finally/RB ,/, we/PRP make/VBP some/DT preliminary/JJ investigations/NNS into/IN how/WRB to/TO extend/VB our/PRP$ new/JJ temporal/JJ difference/NN algorithm/NN to/IN reinforcement/NN learning/NN ./.
To/TO do/VB this/DT we/PRP combine/VBP our/PRP$ update/NN equation/NN with/IN both/CC Watkins/NNP '/POS Q/NN (/-LRB- lambda/NN )/-RRB- and/CC Sarsa/NN (/-LRB- lambda/NN )/-RRB- and/CC find/VB that/IN it/PRP again/RB offers/VBZ superior/JJ performance/NN without/IN a/DT learning/NN rate/NN parameter/NN ./.
