Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNNs/NNS )/-RRB- ,/, and/CC specifically/RB a/DT variant/NN with/IN Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- ,/, are/VBP enjoying/VBG renewed/VBN interest/NN as/IN a/DT result/NN of/IN successful/JJ applications/NNS in/IN a/DT wide/JJ range/NN of/IN machine/NN learning/NN problems/NNS that/WDT involve/VBP sequential/JJ data/NNS ./.
However/RB ,/, while/IN LSTMs/NNS provide/VBP exceptional/JJ results/NNS in/IN practice/NN ,/, the/DT source/NN of/IN their/PRP$ performance/NN and/CC their/PRP$ limitations/NNS remain/VBP rather/RB poorly/RB understood/VBN ./.
Using/VBG character/NN -/HYPH level/NN language/NN models/NNS as/IN an/DT interpretable/JJ testbed/NN ,/, we/PRP aim/VBP to/IN bridge/NN this/DT gap/NN by/IN providing/VBG a/DT comprehensive/JJ analysis/NN of/IN their/PRP$ representations/NNS ,/, predictions/NNS and/CC error/NN types/NNS ./.
In/IN particular/JJ ,/, our/PRP$ experiments/NNS reveal/VBP the/DT existence/NN of/IN interpretable/JJ cells/NNS that/WDT keep/VBP track/NN of/IN long/JJ -/HYPH range/NN dependencies/NNS such/JJ as/IN line/NN lengths/NNS ,/, quotes/NNS and/CC brackets/NNS ./.
Moreover/RB ,/, an/DT extensive/JJ analysis/NN with/IN finite/JJ horizon/NN n/NN -/HYPH gram/NN models/NNS suggest/VBP that/IN these/DT dependencies/NNS are/VBP actively/RB discovered/VBN and/CC utilized/VBN by/IN the/DT networks/NNS ./.
Finally/RB ,/, we/PRP provide/VBP detailed/JJ error/NN analysis/NN that/WDT suggests/VBZ areas/NNS for/IN further/JJ study/NN ./.
