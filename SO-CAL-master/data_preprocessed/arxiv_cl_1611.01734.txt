While/IN deep/JJ learning/NN parsing/VBG approaches/NNS have/VBP proven/VBN very/RB successful/JJ at/IN finding/VBG the/DT structure/NN of/IN sentences/NNS ,/, most/JJS neural/JJ dependency/NN parsers/NNS use/VBP neural/JJ networks/NNS only/RB for/IN feature/NN extraction/NN ,/, and/CC then/RB use/VB those/DT features/NNS in/IN traditional/JJ parsing/VBG algorithms/NNS ./.
In/IN contrast/NN ,/, this/DT paper/NN builds/VBZ off/IN recent/JJ work/NN using/VBG general/JJ -/HYPH purpose/NN neural/JJ network/NN components/NNS ,/, training/VBG an/DT attention/NN mechanism/NN over/IN an/DT LSTM/NN to/TO attend/VB to/IN the/DT head/NN of/IN the/DT phrase/NN ./.
We/PRP get/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS for/IN standard/JJ dependency/NN parsing/VBG benchmarks/NNS ,/, achieving/VBG 95.44/CD percent/NN UAS/NNS and/CC 93.76/CD percent/NN LAS/NN on/IN the/DT PTB/NNP dataset/NN ,/, 0.8/CD percent/NN and/CC 1.0/CD percent/NN improvement/NN ,/, respectively/RB ,/, over/IN Andor/NNP et/FW al./FW (/-LRB- 2016/CD )/-RRB- ./.
In/IN addition/NN to/IN proposing/VBG a/DT new/JJ parsing/VBG architecture/NN using/VBG dimensionality/NN reduction/NN and/CC biaffine/NN interactions/NNS ,/, we/PRP examine/VBP simple/JJ hyperparameter/NN choices/NNS that/WDT had/VBD a/DT profound/JJ influence/NN on/IN the/DT model/NN 's/POS performance/NN ,/, such/JJ as/IN reducing/VBG the/DT value/NN of/IN beta2/NN in/IN the/DT Adam/NNP optimization/NN algorithm/NN ./.
