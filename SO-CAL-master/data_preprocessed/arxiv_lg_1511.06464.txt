Recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- are/VBP notoriously/RB difficult/JJ to/TO train/VB ./.
When/WRB the/DT eigenvalues/NNS of/IN the/DT hidden/VBN to/IN hidden/JJ weight/NN matrix/NN deviate/VB from/IN absolute/JJ value/NN 1/CD ,/, optimization/NN becomes/VBZ difficult/JJ due/IN to/IN the/DT well/RB studied/VBN issue/NN of/IN vanishing/VBG and/CC exploding/VBG gradients/NNS ,/, especially/RB when/WRB trying/VBG to/TO learn/VB long/JJ -/HYPH term/NN dependencies/NNS ./.
To/TO circumvent/VB this/DT problem/NN ,/, we/PRP propose/VBP a/DT new/JJ architecture/NN that/WDT learns/VBZ a/DT unitary/JJ weight/NN matrix/NN ,/, with/IN eigenvalues/NNS of/IN absolute/JJ value/NN exactly/RB 1/CD ./.
The/DT challenge/NN we/PRP address/VBP is/VBZ that/IN of/IN parametrizing/VBG unitary/JJ matrices/NNS in/IN a/DT way/NN that/WDT does/VBZ not/RB require/VB expensive/JJ computations/NNS (/-LRB- such/JJ as/IN eigendecomposition/NN )/-RRB- after/IN each/DT weight/NN update/NN ./.
We/PRP construct/VBP an/DT expressive/JJ unitary/JJ weight/NN matrix/NN by/IN composing/VBG several/JJ structured/JJ matrices/NNS that/WDT act/VBP as/IN building/NN blocks/NNS with/IN parameters/NNS to/TO be/VB learned/VBN ./.
Optimization/NN with/IN this/DT parameterization/NN becomes/VBZ feasible/JJ only/RB when/WRB considering/VBG hidden/JJ states/NNS in/IN the/DT complex/JJ domain/NN ./.
We/PRP demonstrate/VBP the/DT potential/NN of/IN this/DT architecture/NN by/IN achieving/VBG state/NN of/IN the/DT art/NN results/NNS in/IN several/JJ hard/JJ tasks/NNS involving/VBG very/RB long/RB -/HYPH term/NN dependencies/NNS ./.
