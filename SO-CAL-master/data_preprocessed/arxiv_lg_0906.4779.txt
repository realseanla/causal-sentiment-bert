Learning/VBG in/IN probabilistic/JJ models/NNS is/VBZ often/RB severely/RB hampered/VBN by/IN the/DT general/JJ intractability/NN of/IN the/DT normalization/NN factor/NN and/CC its/PRP$ derivatives/NNS ./.
Here/RB we/PRP propose/VBP a/DT new/JJ learning/NN technique/NN that/WDT obviates/VBZ the/DT need/NN to/TO compute/VB an/DT intractable/JJ normalization/NN factor/NN or/CC sample/NN from/IN the/DT equilibrium/NN distribution/NN of/IN the/DT model/NN ./.
This/DT is/VBZ achieved/VBN by/IN establishing/VBG dynamics/NNS that/WDT would/MD transform/VB the/DT observed/VBN data/NNS distribution/NN into/IN the/DT model/NN distribution/NN ,/, and/CC then/RB setting/VBG as/IN the/DT objective/NN the/DT minimization/NN of/IN the/DT initial/JJ flow/NN of/IN probability/NN away/RB from/IN the/DT data/NNS distribution/NN ./.
Score/NN matching/NN ,/, minimum/JJ velocity/NN learning/NN ,/, and/CC certain/JJ forms/NNS of/IN contrastive/JJ divergence/NN are/VBP shown/VBN to/TO be/VB special/JJ cases/NNS of/IN this/DT learning/NN technique/NN ./.
We/PRP demonstrate/VBP the/DT application/NN of/IN minimum/JJ probability/NN flow/NN learning/NN to/IN parameter/NN estimation/NN in/IN Ising/NN models/NNS ,/, deep/JJ belief/NN networks/NNS ,/, multivariate/JJ Gaussian/NNP distributions/NNS and/CC a/DT continuous/JJ model/NN with/IN a/DT highly/RB general/JJ energy/NN function/NN defined/VBN as/IN a/DT power/NN series/NN ./.
In/IN the/DT Ising/NNP model/NN case/NN ,/, minimum/JJ probability/NN flow/NN learning/NN outperforms/VBZ current/JJ state/NN of/IN the/DT art/NN techniques/NNS by/IN approximately/RB two/CD orders/NNS of/IN magnitude/NN in/IN learning/NN time/NN ,/, with/IN comparable/JJ error/NN in/IN the/DT recovered/VBN parameters/NNS ./.
It/PRP is/VBZ our/PRP$ hope/NN that/IN this/DT technique/NN will/MD alleviate/VB existing/VBG restrictions/NNS on/IN the/DT classes/NNS of/IN probabilistic/JJ models/NNS that/WDT are/VBP practical/JJ for/IN use/NN ./.
