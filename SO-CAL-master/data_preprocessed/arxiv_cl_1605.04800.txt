This/DT paper/NN describes/VBZ the/DT submission/NN of/IN the/DT AMU/NNP (/-LRB- Adam/NNP Mickiewicz/NNP University/NNP )/-RRB- team/NN to/IN the/DT Automatic/JJ Post-Editing/NN (/-LRB- APE/NN )/-RRB- task/NN of/IN WMT/NNP 2016/CD ./.
We/PRP explore/VBP the/DT application/NN of/IN neural/JJ translation/NN models/NNS to/IN the/DT APE/NN problem/NN and/CC achieve/VB good/JJ results/NNS by/IN treating/VBG different/JJ models/NNS as/IN components/NNS in/IN a/DT log/NN -/HYPH linear/JJ model/NN ,/, allowing/VBG for/IN multiple/JJ inputs/NNS (/-LRB- the/DT MT/NN -/HYPH output/NN and/CC the/DT source/NN )/-RRB- that/WDT are/VBP decoded/VBN to/IN the/DT same/JJ target/NN language/NN (/-LRB- post-edited/JJ translations/NNS )/-RRB- ./.
A/DT simple/JJ string/NN -/HYPH matching/VBG penalty/NN integrated/VBN within/IN the/DT log/NN -/HYPH linear/JJ model/NN can/MD be/VB used/VBN to/TO control/VB for/IN higher/JJR faithfulness/NN with/IN regard/NN to/IN the/DT to/IN -/HYPH be-corrected/VBN machine/NN translation/NN input/NN ./.
Our/PRP$ submission/NN outperforms/VBZ the/DT uncorrected/JJ baseline/NN on/IN the/DT unseen/JJ test/NN set/VBN by/IN -3.2/CD percent/NN TER/NN and/CC 5.5/CD percent/NN BLEU/NN ./.
