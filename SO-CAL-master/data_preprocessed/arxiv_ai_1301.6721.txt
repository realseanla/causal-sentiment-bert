Reactive/JJ (/-LRB- memoryless/JJ )/-RRB- policies/NNS are/VBP sufficient/JJ in/IN completely/RB observable/JJ Markov/NNP decision/NN processes/NNS (/-LRB- MDPs/NNS )/-RRB- ,/, but/CC some/DT kind/NN of/IN memory/NN is/VBZ usually/RB necessary/JJ for/IN optimal/JJ control/NN of/IN a/DT partially/RB observable/JJ MDP/NN ./.
Policies/NNS with/IN finite/JJ memory/NN can/MD be/VB represented/VBN as/IN finite/JJ -/HYPH state/NN automata/NN ./.
In/IN this/DT paper/NN ,/, we/PRP extend/VBP Baird/NNP and/CC Moore/NNP 's/POS VAPS/NNP algorithm/NN to/IN the/DT problem/NN of/IN learning/VBG general/JJ finite/NN -/HYPH state/NN automata/NN ./.
Because/IN it/PRP performs/VBZ stochastic/JJ gradient/NN descent/NN ,/, this/DT algorithm/NN can/MD be/VB shown/VBN to/TO converge/VB to/IN a/DT locally/RB optimal/JJ finite/NN -/HYPH state/NN controller/NN ./.
We/PRP provide/VBP the/DT details/NNS of/IN the/DT algorithm/NN and/CC then/RB consider/VB the/DT question/NN of/IN under/IN what/WP conditions/NNS stochastic/JJ gradient/NN descent/NN will/MD outperform/VB exact/JJ gradient/NN descent/NN ./.
We/PRP conclude/VBP with/IN empirical/JJ results/NNS comparing/VBG the/DT performance/NN of/IN stochastic/JJ and/CC exact/JJ gradient/NN descent/NN ,/, and/CC showing/VBG the/DT ability/NN of/IN our/PRP$ algorithm/NN to/TO extract/VB the/DT useful/JJ information/NN contained/VBN in/IN the/DT sequence/NN of/IN past/JJ observations/NNS to/TO compensate/VB for/IN the/DT lack/NN of/IN observability/NN at/IN each/DT time/NN -/HYPH step/NN ./.
