Recovering/VBG a/DT low/JJ -/HYPH rank/NN tensor/NN from/IN incomplete/JJ information/NN is/VBZ a/DT recurring/VBG problem/NN in/IN signal/NN processing/NN and/CC machine/NN learning/NN ./.
The/DT most/RBS popular/JJ convex/NN relaxation/NN of/IN this/DT problem/NN minimizes/VBZ the/DT sum/NN of/IN the/DT nuclear/JJ norms/NNS of/IN the/DT unfoldings/NNS of/IN the/DT tensor/NN ./.
We/PRP show/VBP that/IN this/DT approach/NN can/MD be/VB substantially/RB suboptimal/JJ :/: reliably/RB recovering/VBG a/DT K/NN -/HYPH way/NN tensor/NN of/IN length/NN n/NN and/CC Tucker/NNP rank/NN r/NN from/IN Gaussian/JJ measurements/NNS requires/VBZ $/$ \/SYM Omega/NN (/-LRB- rn/NNP ^/SYM {/-LRB- K/NN -/HYPH 1/CD }/-RRB- )/-RRB- $/$ observations/NNS ./.
In/IN contrast/NN ,/, a/DT certain/JJ (/-LRB- intractable/JJ )/-RRB- nonconvex/JJ formulation/NN needs/VBZ only/RB $/$ O/UH (/-LRB- r/NN ^/SYM K/NN nrK/NN )/-RRB- $/$ observations/NNS ./.
We/PRP introduce/VBP a/DT very/RB simple/JJ ,/, new/JJ convex/NN relaxation/NN ,/, which/WDT partially/RB bridges/VBZ this/DT gap/NN ./.
Our/PRP$ new/JJ formulation/NN succeeds/VBZ with/IN $/$ O/UH (/-LRB- r/NN ^/SYM {/-LRB- \/SYM lfloor/NN K/NN //HYPH 2/CD \/SYM rfloor/NN }/-RRB- n/NN ^/SYM {/-LRB- \/SYM lceil/NN K/NN //HYPH 2/CD \/SYM rceil/NN }/-RRB- )/-RRB- $/$ observations/NNS ./.
While/IN these/DT results/NNS pertain/VBP to/IN Gaussian/JJ measurements/NNS ,/, simulations/NNS strongly/RB suggest/VBP that/IN the/DT new/JJ norm/NN also/RB outperforms/VBZ the/DT sum/NN of/IN nuclear/JJ norms/NNS for/IN tensor/NN completion/NN from/IN a/DT random/JJ subset/NN of/IN entries/NNS ./.
Our/PRP$ lower/JJR bounds/NNS for/IN the/DT sum/NN -/HYPH of/IN -/HYPH nuclear/JJ -/HYPH norm/NN model/NN follow/VB from/IN a/DT new/JJ result/NN on/IN simultaneously/RB structured/VBN models/NNS ,/, which/WDT may/MD be/VB of/IN independent/JJ interest/NN for/IN matrix/NN and/CC vector/NN recovery/NN problems/NNS ./.
