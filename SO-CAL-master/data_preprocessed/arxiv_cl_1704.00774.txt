Increasing/VBG the/DT capacity/NN of/IN recurrent/JJ neural/JJ networks/NNS (/-LRB- RNN/NN )/-RRB- usually/RB involves/VBZ augmenting/VBG the/DT size/NN of/IN the/DT hidden/JJ layer/NN ,/, resulting/VBG in/IN a/DT significant/JJ increase/NN of/IN computational/JJ cost/NN ./.
An/DT alternative/NN is/VBZ the/DT recurrent/JJ neural/JJ tensor/NN network/NN (/-LRB- RNTN/NN )/-RRB- ,/, which/WDT increases/VBZ capacity/NN by/IN employing/VBG distinct/JJ hidden/JJ layer/NN weights/NNS for/IN each/DT vocabulary/NN word/NN ./.
The/DT disadvantage/NN of/IN RNTNs/NNS is/VBZ that/IN memory/NN usage/NN scales/NNS linearly/RB with/IN vocabulary/NN size/NN ,/, which/WDT can/MD reach/VB millions/NNS for/IN word/NN -/HYPH level/NN language/NN models/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP restricted/VBN recurrent/JJ neural/JJ tensor/NN networks/NNS (/-LRB- r/NN -/HYPH RNTN/NN )/-RRB- which/WDT reserve/VBP distinct/JJ hidden/JJ layer/NN weights/NNS for/IN frequent/JJ vocabulary/NN words/NNS while/IN sharing/VBG a/DT single/JJ set/NN of/IN weights/NNS for/IN infrequent/JJ words/NNS ./.
Perplexity/NN evaluations/NNS using/VBG the/DT Penn/NNP Treebank/NNP corpus/NN show/VBP that/IN r/NN -/HYPH RNTNs/NNS improve/VBP language/NN model/NN performance/NN over/IN standard/JJ RNNs/NNS using/VBG only/RB a/DT small/JJ fraction/NN of/IN the/DT parameters/NNS of/IN unrestricted/JJ RNTNs/NNS ./.
