We/PRP examine/VBP a/DT new/JJ form/NN of/IN smooth/JJ approximation/NN to/IN the/DT zero/CD one/CD loss/NN in/IN which/WDT learning/NN is/VBZ performed/VBN using/VBG a/DT reformulation/NN of/IN the/DT widely/RB used/VBN logistic/JJ function/NN ./.
Our/PRP$ approach/NN is/VBZ based/VBN on/IN using/VBG the/DT posterior/JJ mean/NN of/IN a/DT novel/JJ generalized/VBN Beta/NN -/HYPH Bernoulli/NN formulation/NN ./.
This/DT leads/VBZ to/IN a/DT generalized/VBN logistic/JJ function/NN that/WDT approximates/VBZ the/DT zero/CD one/CD loss/NN ,/, but/CC retains/VBZ a/DT probabilistic/JJ formulation/NN conferring/VBG a/DT number/NN of/IN useful/JJ properties/NNS ./.
The/DT approach/NN is/VBZ easily/RB generalized/VBN to/IN kernel/NN logistic/JJ regression/NN and/CC easily/RB integrated/VBN into/IN methods/NNS for/IN structured/JJ prediction/NN ./.
We/PRP present/VBP experiments/NNS in/IN which/WDT we/PRP learn/VBP such/JJ models/NNS using/VBG an/DT optimization/NN method/NN consisting/VBG of/IN a/DT combination/NN of/IN gradient/NN descent/NN and/CC coordinate/VB descent/NN using/VBG localized/VBN grid/NN search/NN so/RB as/IN to/TO escape/VB from/IN local/JJ minima/NN ./.
Our/PRP$ experiments/NNS indicate/VBP that/IN optimization/NN quality/NN is/VBZ improved/VBN when/WRB learning/VBG meta/NN -/HYPH parameters/NNS are/VBP themselves/PRP optimized/VBN using/VBG a/DT validation/NN set/NN ./.
Our/PRP$ experiments/NNS show/VBP improved/VBN performance/NN relative/JJ to/IN widely/RB used/VBN logistic/JJ and/CC hinge/VBP loss/NN methods/NNS on/IN a/DT wide/JJ variety/NN of/IN problems/NNS ranging/VBG from/IN standard/JJ UC/NNP Irvine/NNP and/CC libSVM/NNP evaluation/NN datasets/NNS to/IN product/NN review/NN predictions/NNS and/CC a/DT visual/JJ information/NN extraction/NN task/NN ./.
We/PRP observe/VBP that/IN the/DT approach/NN :/: 1/LS )/-RRB- is/VBZ more/RBR robust/JJ to/IN outliers/NNS compared/VBN to/IN the/DT logistic/JJ and/CC hinge/VBP losses/NNS ;/: 2/LS )/-RRB- outperforms/VBZ comparable/JJ logistic/JJ and/CC max/JJ margin/NN models/NNS on/IN larger/JJR scale/NN benchmark/NN problems/NNS ;/: 3/LS )/-RRB- when/WRB combined/VBN with/IN Gaussian/JJ -/HYPH Laplacian/JJ mixture/NN prior/RB on/IN parameters/NNS the/DT kernelized/VBN version/NN of/IN our/PRP$ formulation/NN yields/NNS sparser/JJR solutions/NNS than/IN Support/NN Vector/NNP Machine/NNP classifiers/NNS ;/: and/CC 4/LS )/-RRB- when/WRB integrated/VBN into/IN a/DT probabilistic/JJ structured/JJ prediction/NN technique/NN our/PRP$ approach/NN provides/VBZ more/RBR accurate/JJ probabilities/NNS yielding/VBG improved/VBN inference/NN and/CC increasing/VBG information/NN extraction/NN performance/NN ./.
