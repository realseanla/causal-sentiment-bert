We/PRP consider/VBP a/DT crowdsourcing/NN model/NN in/IN which/WDT $/$ n/NN $/$ workers/NNS are/VBP asked/VBN to/TO rate/VB the/DT quality/NN of/IN $/$ n/NN $/$ items/NNS previously/RB generated/VBN by/IN other/JJ workers/NNS ./.
An/DT unknown/JJ set/NN of/IN $/$ \/CD alpha/NN n/NN $/$ workers/NNS generate/VBP reliable/JJ ratings/NNS ,/, while/IN the/DT remaining/VBG workers/NNS may/MD behave/VB arbitrarily/RB and/CC possibly/RB adversarially/RB ./.
The/DT manager/NN of/IN the/DT experiment/NN can/MD also/RB manually/RB evaluate/VB the/DT quality/NN of/IN a/DT small/JJ number/NN of/IN items/NNS ,/, and/CC wishes/VBZ to/TO curate/VB together/RB almost/RB all/DT of/IN the/DT high/JJ -/HYPH quality/NN items/NNS with/IN at/IN most/RBS an/DT $/$ \/CD epsilon/CD $/$ fraction/NN of/IN low/JJ -/HYPH quality/NN items/NNS ./.
Perhaps/RB surprisingly/RB ,/, we/PRP show/VBP that/IN this/DT is/VBZ possible/JJ with/IN an/DT amount/NN of/IN work/NN required/VBN of/IN the/DT manager/NN ,/, and/CC each/DT worker/NN ,/, that/DT does/VBZ not/RB scale/VB with/IN $/$ n/NN $/$ :/: the/DT dataset/NN can/MD be/VB curated/VBN with/IN $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- \/SYM Big/JJ (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM beta/NN \/SYM alpha/NN ^/SYM 3/CD \/SYM epsilon/CD ^/SYM 4/CD }/-RRB- \/SYM Big/JJ )/-RRB- $/$ ratings/NNS per/IN worker/NN ,/, and/CC $/$ \/CD tilde/NN {/-LRB- O/NN }/-RRB- \/SYM Big/JJ (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM beta/NN \/SYM epsilon/NN ^/SYM 2/CD }/-RRB- \/SYM Big/JJ )/-RRB- $/$ ratings/NNS by/IN the/DT manager/NN ,/, where/WRB $/$ \/SYM beta/NN $/$ is/VBZ the/DT fraction/NN of/IN high/JJ -/HYPH quality/NN items/NNS ./.
Our/PRP$ results/NNS extend/VBP to/IN the/DT more/RBR general/JJ setting/NN of/IN peer/NN prediction/NN ,/, including/VBG peer/NN grading/NN in/IN online/JJ classrooms/NNS ./.
