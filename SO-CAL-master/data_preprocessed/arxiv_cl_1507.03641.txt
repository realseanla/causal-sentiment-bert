This/DT paper/NN describes/VBZ a/DT parsing/VBG model/NN that/WDT combines/VBZ the/DT exact/JJ dynamic/JJ programming/NN of/IN CRF/NNP parsing/VBG with/IN the/DT rich/JJ nonlinear/JJ featurization/NN of/IN neural/JJ net/JJ approaches/NNS ./.
Our/PRP$ model/NN is/VBZ structurally/RB a/DT CRF/NNP that/WDT factors/NNS over/IN anchored/VBN rule/NN productions/NNS ,/, but/CC instead/RB of/IN linear/JJ potential/JJ functions/NNS based/VBN on/IN sparse/JJ features/NNS ,/, we/PRP use/VBP nonlinear/JJ potentials/NNS computed/VBN via/IN a/DT feedforward/JJ neural/JJ network/NN ./.
Because/IN potentials/NNS are/VBP still/RB local/JJ to/IN anchored/VBN rules/NNS ,/, structured/VBN inference/NN (/-LRB- CKY/NN )/-RRB- is/VBZ unchanged/JJ from/IN the/DT sparse/JJ case/NN ./.
Computing/NNP gradients/NNS during/IN learning/NN involves/VBZ backpropagating/VBG an/DT error/NN signal/NN formed/VBN from/IN standard/JJ CRF/NNP sufficient/JJ statistics/NNS (/-LRB- expected/VBN rule/NN counts/NNS )/-RRB- ./.
Using/VBG only/RB dense/JJ features/NNS ,/, our/PRP$ neural/JJ CRF/NNP already/RB exceeds/VBZ a/DT strong/JJ baseline/NN CRF/NNP model/NN (/-LRB- Hall/NNP et/FW al./FW ,/, 2014/CD )/-RRB- ./.
In/IN combination/NN with/IN sparse/JJ features/NNS ,/, our/PRP$ system/NN achieves/VBZ 91.1/CD F1/NN on/IN section/NN 23/CD of/IN the/DT Penn/NNP Treebank/NNP ,/, and/CC more/RBR generally/RB outperforms/VBZ the/DT best/JJS prior/JJ single/JJ parser/NN results/NNS on/IN a/DT range/NN of/IN languages/NNS ./.
