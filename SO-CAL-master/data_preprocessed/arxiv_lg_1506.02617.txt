We/PRP revisit/VBP the/DT choice/NN of/IN SGD/NNP for/IN training/VBG deep/JJ neural/JJ networks/NNS by/IN reconsidering/VBG the/DT appropriate/JJ geometry/NN in/IN which/WDT to/TO optimize/VB the/DT weights/NNS ./.
We/PRP argue/VBP for/IN a/DT geometry/NN invariant/JJ to/IN rescaling/VBG of/IN weights/NNS that/WDT does/VBZ not/RB affect/VB the/DT output/NN of/IN the/DT network/NN ,/, and/CC suggest/VBP Path/NN -/HYPH SGD/NN ,/, which/WDT is/VBZ an/DT approximate/JJ steepest/JJS descent/NN method/NN with/IN respect/NN to/IN a/DT path-wise/JJ regularizer/NN related/VBN to/IN max/NN -/HYPH norm/NN regularization/NN ./.
Path/NN -/HYPH SGD/NN is/VBZ easy/JJ and/CC efficient/JJ to/TO implement/VB and/CC leads/VBZ to/IN empirical/JJ gains/NNS over/IN SGD/NNP and/CC AdaGrad/NNP ./.
