We/PRP consider/VBP an/DT agent/NN interacting/VBG with/IN an/DT environment/NN in/IN a/DT single/JJ stream/NN of/IN actions/NNS ,/, observations/NNS ,/, and/CC rewards/NNS ,/, with/IN no/DT reset/NN ./.
This/DT process/NN is/VBZ not/RB assumed/VBN to/TO be/VB a/DT Markov/NNP Decision/NN Process/NN (/-LRB- MDP/NN )/-RRB- ./.
Rather/RB ,/, the/DT agent/NN has/VBZ several/JJ representations/NNS (/-LRB- mapping/VBG histories/NNS of/IN past/JJ interactions/NNS to/IN a/DT discrete/JJ state/NN space/NN )/-RRB- of/IN the/DT environment/NN with/IN unknown/JJ dynamics/NNS ,/, only/RB some/DT of/IN which/WDT result/VBP in/IN an/DT MDP/NN ./.
The/DT goal/NN is/VBZ to/TO minimize/VB the/DT average/JJ regret/NN criterion/NN against/IN an/DT agent/NN who/WP knows/VBZ an/DT MDP/NN representation/NN giving/VBG the/DT highest/JJS optimal/JJ reward/NN ,/, and/CC acts/VBZ optimally/RB in/IN it/PRP ./.
Recent/JJ regret/NN bounds/NNS for/IN this/DT setting/NN are/VBP of/IN order/NN $/$ O/UH (/-LRB- T/NN ^/SYM {/-LRB- 2/3/CD }/-RRB- )/-RRB- $/$ with/IN an/DT additive/JJ term/NN constant/JJ yet/RB exponential/JJ in/IN some/DT characteristics/NNS of/IN the/DT optimal/JJ MDP/NN ./.
We/PRP propose/VBP an/DT algorithm/NN whose/WP$ regret/NN after/IN $/$ T$/CD time/NN steps/NNS is/VBZ $/$ O/UH (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ ,/, with/IN all/DT constants/NNS reasonably/RB small/JJ ./.
This/DT is/VBZ optimal/JJ in/IN $/$ T$/CD since/IN $/$ O/UH (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ is/VBZ the/DT optimal/JJ regret/NN in/IN the/DT setting/NN of/IN learning/NN in/IN a/DT (/-LRB- single/JJ discrete/JJ )/-RRB- MDP/NN ./.
