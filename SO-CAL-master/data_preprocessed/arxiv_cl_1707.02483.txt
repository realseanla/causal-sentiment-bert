The/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN named/VBN entity/NN recognition/NN (/-LRB- NER/NN )/-RRB- systems/NNS are/VBP supervised/JJ machine/NN learning/NN models/NNS that/WDT require/VBP large/JJ amounts/NNS of/IN manually/RB annotated/VBN data/NNS to/TO achieve/VB high/JJ accuracy/NN ./.
However/RB ,/, annotating/VBG NER/NN data/NNS by/IN human/JJ is/VBZ expensive/JJ and/CC time/NN -/HYPH consuming/VBG ,/, and/CC can/MD be/VB quite/RB difficult/JJ for/IN a/DT new/JJ language/NN ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP two/CD weakly/RB supervised/JJ approaches/NNS for/IN cross-lingual/JJ NER/NN with/IN no/DT human/JJ annotation/NN in/IN a/DT target/NN language/NN ./.
The/DT first/JJ approach/NN is/VBZ to/TO create/VB automatically/RB labeled/VBN NER/NN data/NNS for/IN a/DT target/NN language/NN via/IN annotation/NN projection/NN on/IN comparable/JJ corpora/NNS ,/, where/WRB we/PRP develop/VBP a/DT heuristic/NN scheme/NN that/WDT effectively/RB selects/VBZ good/JJ -/HYPH quality/NN projection/NN -/HYPH labeled/VBN data/NNS from/IN noisy/JJ data/NNS ./.
The/DT second/JJ approach/NN is/VBZ to/TO project/VB distributed/VBN representations/NNS of/IN words/NNS (/-LRB- word/NN embeddings/NNS )/-RRB- from/IN a/DT target/NN language/NN to/IN a/DT source/NN language/NN ,/, so/IN that/IN the/DT source/NN -/HYPH language/NN NER/NN system/NN can/MD be/VB applied/VBN to/IN the/DT target/NN language/NN without/IN re-training/VBG ./.
We/PRP also/RB design/VB two/CD co-decoding/JJ schemes/NNS that/WDT effectively/RB combine/VBP the/DT outputs/NNS of/IN the/DT two/CD projection/NN -/HYPH based/VBN approaches/NNS ./.
We/PRP evaluate/VBP the/DT performance/NN of/IN the/DT proposed/VBN approaches/NNS on/IN both/CC in/IN -/HYPH house/NN and/CC open/JJ NER/NN data/NNS for/IN several/JJ target/NN languages/NNS ./.
The/DT results/NNS show/VBP that/IN the/DT combined/VBN systems/NNS outperform/VBP three/CD other/JJ weakly/RB supervised/JJ approaches/NNS on/IN the/DT CoNLL/NN data/NNS ./.
