Stochastic/JJ convex/NN optimization/NN is/VBZ a/DT basic/JJ and/CC well/RB studied/VBN primitive/JJ in/IN machine/NN learning/NN ./.
It/PRP is/VBZ well/RB known/VBN that/IN convex/NN and/CC Lipschitz/NNP functions/NNS can/MD be/VB minimized/VBN efficiently/RB using/VBG Stochastic/NNP Gradient/NNP Descent/NNP (/-LRB- SGD/NNP )/-RRB- ./.
The/DT Normalized/JJ Gradient/NN Descent/NN (/-LRB- NGD/NN )/-RRB- algorithm/NN ,/, is/VBZ an/DT adaptation/NN of/IN Gradient/NNP Descent/NNP ,/, which/WDT updates/NNS according/VBG to/IN the/DT direction/NN of/IN the/DT gradients/NNS ,/, rather/RB than/IN the/DT gradients/NNS themselves/PRP ./.
In/IN this/DT paper/NN we/PRP analyze/VBP a/DT stochastic/JJ version/NN of/IN NGD/NNP and/CC prove/VB its/PRP$ convergence/NN to/IN a/DT global/JJ minimum/NN for/IN a/DT wider/JJR class/NN of/IN functions/NNS :/: we/PRP require/VBP the/DT functions/NNS to/TO be/VB quasi-convex/NN and/CC locally/RB -/HYPH Lipschitz/NNP ./.
Quasi-convexity/NN broadens/VBZ the/DT con/NN -/HYPH cept/NN of/IN unimodality/NN to/IN multidimensions/NNS and/CC allows/VBZ for/IN certain/JJ types/NNS of/IN saddle/NN points/NNS ,/, which/WDT are/VBP a/DT known/VBN hurdle/NN for/IN first/JJ -/HYPH order/NN optimization/NN methods/NNS such/JJ as/IN gradient/NN decent/JJ ./.
Locally/RB -/HYPH Lipschitz/JJ functions/NNS are/VBP only/RB required/VBN to/TO be/VB Lipschitz/NNP in/IN a/DT small/JJ region/NN around/IN the/DT optimum/JJ ./.
This/DT assumption/NN circumvents/VBZ gradient/NN explosion/NN ,/, which/WDT is/VBZ another/DT known/VBN hurdle/NN for/IN gradient/NN descent/NN variants/NNS ./.
Interestingly/RB ,/, unlike/IN the/DT vanilla/NN SGD/NNP algorithm/NN ,/, the/DT stochastic/JJ normalized/VBN gradient/NN descent/NN algorithm/NN provably/RB requires/VBZ a/DT minimal/JJ minibatch/NN size/NN ./.
