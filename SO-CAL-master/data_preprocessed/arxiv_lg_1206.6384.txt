We/PRP describe/VBP novel/JJ subgradient/NN methods/NNS for/IN a/DT broad/JJ class/NN of/IN matrix/NN optimization/NN problems/NNS involving/VBG nuclear/JJ norm/NN regularization/NN ./.
Unlike/IN existing/VBG approaches/NNS ,/, our/PRP$ method/NN executes/VBZ very/RB cheap/JJ iterations/NNS by/IN combining/VBG low/JJ -/HYPH rank/NN stochastic/JJ subgradients/NNS with/IN efficient/JJ incremental/JJ SVD/NN updates/NNS ,/, made/VBN possible/JJ by/IN highly/RB optimized/VBN and/CC parallelizable/JJ dense/JJ linear/JJ algebra/NN operations/NNS on/IN small/JJ matrices/NNS ./.
Our/PRP$ practical/JJ algorithms/NNS always/RB maintain/VBP a/DT low/JJ -/HYPH rank/NN factorization/NN of/IN iterates/NNS that/WDT can/MD be/VB conveniently/RB held/VBN in/IN memory/NN and/CC efficiently/RB multiplied/VBN to/TO generate/VB predictions/NNS in/IN matrix/NN completion/NN settings/NNS ./.
Empirical/JJ comparisons/NNS confirm/VBP that/IN our/PRP$ approach/NN is/VBZ highly/RB competitive/JJ with/IN several/JJ recently/RB proposed/VBN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN solvers/NNS for/IN such/JJ problems/NNS ./.
