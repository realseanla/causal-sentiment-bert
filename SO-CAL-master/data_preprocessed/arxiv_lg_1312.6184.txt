Currently/RB ,/, deep/JJ neural/JJ networks/NNS are/VBP the/DT state/NN of/IN the/DT art/NN on/IN problems/NNS such/JJ as/IN speech/NN recognition/NN and/CC computer/NN vision/NN ./.
In/IN this/DT extended/JJ abstract/NN ,/, we/PRP show/VBP that/IN shallow/JJ feed/NN -/HYPH forward/JJ networks/NNS can/MD learn/VB the/DT complex/JJ functions/NNS previously/RB learned/VBN by/IN deep/JJ nets/NNS and/CC achieve/VB accuracies/NNS previously/RB only/RB achievable/JJ with/IN deep/JJ models/NNS ./.
Moreover/RB ,/, the/DT shallow/JJ neural/JJ nets/NNS can/MD learn/VB these/DT deep/JJ functions/NNS using/VBG a/DT total/JJ number/NN of/IN parameters/NNS similar/JJ to/IN the/DT original/JJ deep/JJ model/NN ./.
We/PRP evaluate/VBP our/PRP$ method/NN on/IN TIMIT/NNP phoneme/NN recognition/NN task/NN and/CC are/VBP able/JJ to/TO train/VB shallow/JJ fully/RB -/HYPH connected/VBN nets/NNS that/WDT perform/VBP similarly/RB to/IN complex/NN ,/, well/RB -/HYPH engineered/VBN ,/, deep/JJ convolutional/JJ architectures/NNS ./.
Our/PRP$ success/NN in/IN training/NN shallow/JJ neural/JJ nets/NNS to/TO mimic/VB deeper/JJR models/NNS suggests/VBZ that/IN there/EX probably/RB exist/VBP better/JJR algorithms/NNS for/IN training/NN shallow/JJ feed/NN -/HYPH forward/JJ nets/NNS than/IN those/DT currently/RB available/JJ ./.
