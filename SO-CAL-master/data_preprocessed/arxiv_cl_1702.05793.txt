This/DT work/NN presents/VBZ a/DT systematic/JJ theoretical/JJ and/CC empirical/JJ comparison/NN of/IN the/DT major/JJ algorithms/NNS that/WDT have/VBP been/VBN proposed/VBN for/IN learning/VBG Harmonic/NNP and/CC Optimality/NNP Theory/NNP grammars/NNS (/-LRB- HG/NN and/CC OT/NNP ,/, respectively/RB )/-RRB- ./.
By/IN comparing/VBG learning/VBG algorithms/NNS ,/, we/PRP are/VBP also/RB able/JJ to/TO compare/VB the/DT closely/RB related/VBN OT/NNP and/CC HG/NN frameworks/NNS themselves/PRP ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT additional/JJ expressivity/NN of/IN the/DT HG/NN framework/NN over/IN OT/NNP affords/VBZ performance/NN gains/NNS in/IN the/DT task/NN of/IN predicting/VBG the/DT surface/NN word/NN order/NN of/IN Czech/JJ sentences/NNS ./.
We/PRP compare/VBP the/DT perceptron/NN with/IN the/DT classic/JJ Gradual/JJ Learning/NN Algorithm/NN (/-LRB- GLA/NN )/-RRB- ,/, which/WDT learns/VBZ OT/NNP grammars/NNS ,/, as/RB well/RB as/IN the/DT popular/JJ Maximum/NNP Entropy/NNP model/NN ./.
In/IN addition/NN to/IN showing/VBG that/IN the/DT perceptron/NN is/VBZ theoretically/RB appealing/JJ ,/, our/PRP$ work/NN shows/VBZ that/IN the/DT performance/NN of/IN the/DT HG/NN model/NN it/PRP learns/VBZ approaches/NNS that/IN of/IN the/DT upper/JJ bound/VBN in/IN prediction/NN accuracy/NN on/IN a/DT held/VBN out/RP test/NN set/NN and/CC that/IN it/PRP is/VBZ capable/JJ of/IN accurately/RB modeling/VBG observed/VBN variation/NN ./.
