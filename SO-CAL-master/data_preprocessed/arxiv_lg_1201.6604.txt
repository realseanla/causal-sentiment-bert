We/PRP present/VBP an/DT implementation/NN of/IN model/NN -/HYPH based/VBN online/JJ reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- for/IN continuous/JJ domains/NNS with/IN deterministic/JJ transitions/NNS that/WDT is/VBZ specifically/RB designed/VBN to/TO achieve/VB low/JJ sample/NN complexity/NN ./.
To/TO achieve/VB low/JJ sample/NN complexity/NN ,/, since/IN the/DT environment/NN is/VBZ unknown/JJ ,/, an/DT agent/NN must/MD intelligently/RB balance/VB exploration/NN and/CC exploitation/NN ,/, and/CC must/MD be/VB able/JJ to/TO rapidly/RB generalize/VB from/IN observations/NNS ./.
While/IN in/IN the/DT past/NN a/DT number/NN of/IN related/JJ sample/NN efficient/JJ RL/NN algorithms/NNS have/VBP been/VBN proposed/VBN ,/, to/TO allow/VB theoretical/JJ analysis/NN ,/, mainly/RB model/NN -/HYPH learners/NNS with/IN weak/JJ generalization/NN capabilities/NNS were/VBD considered/VBN ./.
Here/RB ,/, we/PRP separate/VBP function/NN approximation/NN in/IN the/DT model/NN learner/NN (/-LRB- which/WDT does/VBZ require/VB samples/NNS )/-RRB- from/IN the/DT interpolation/NN in/IN the/DT planner/NN (/-LRB- which/WDT does/VBZ not/RB require/VB samples/NNS )/-RRB- ./.
For/IN model/NN -/HYPH learning/NN we/PRP apply/VBP Gaussian/JJ processes/NNS regression/NN (/-LRB- GP/NNP )/-RRB- which/WDT is/VBZ able/JJ to/TO automatically/RB adjust/VB itself/PRP to/IN the/DT complexity/NN of/IN the/DT problem/NN (/-LRB- via/IN Bayesian/JJ hyperparameter/NN selection/NN )/-RRB- and/CC ,/, in/IN practice/NN ,/, often/RB able/JJ to/TO learn/VB a/DT highly/RB accurate/JJ model/NN from/IN very/RB little/JJ data/NNS ./.
In/IN addition/NN ,/, a/DT GP/NNP provides/VBZ a/DT natural/JJ way/NN to/TO determine/VB the/DT uncertainty/NN of/IN its/PRP$ predictions/NNS ,/, which/WDT allows/VBZ us/PRP to/TO implement/VB the/DT "/`` optimism/NN in/IN the/DT face/NN of/IN uncertainty/NN "/'' principle/NN used/VBN to/TO efficiently/RB control/VB exploration/NN ./.
Our/PRP$ method/NN is/VBZ evaluated/VBN on/IN four/CD common/JJ benchmark/NN domains/NNS ./.
