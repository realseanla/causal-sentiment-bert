We/PRP describe/VBP and/CC analyze/VBP a/DT new/JJ algorithm/NN for/IN agnostically/RB learning/VBG kernel/NN -/HYPH based/VBN halfspaces/NNS with/IN respect/NN to/IN the/DT \/SYM emph/NN {/-LRB- zero/CD -/SYM one/CD }/-RRB- loss/NN function/NN ./.
Unlike/IN most/JJS previous/JJ formulations/NNS which/WDT rely/VBP on/IN surrogate/JJ convex/NN loss/NN functions/NNS (/-LRB- e.g./FW hinge/NN -/HYPH loss/NN in/IN SVM/NN and/CC log/NN -/HYPH loss/NN in/IN logistic/JJ regression/NN )/-RRB- ,/, we/PRP provide/VBP finite/JJ time/NN //, sample/NN guarantees/NNS with/IN respect/NN to/IN the/DT more/RBR natural/JJ zero/CD -/HYPH one/CD loss/NN function/NN ./.
The/DT proposed/VBN algorithm/NN can/MD learn/VB kernel/NN -/HYPH based/VBN halfspaces/NNS in/IN worst/JJS -/HYPH case/NN time/NN $/$ \/CD poly/NN (/-LRB- \/SYM exp/NN (/-LRB- L/NN \/SYM log/NN (/-LRB- L/NN //HYPH \/SYM epsilon/NN )/-RRB- )/-RRB- )/-RRB- $/$ ,/, for/IN $/$ \/CD emph/NN {/-LRB- any/DT }/-RRB- $/$ distribution/NN ,/, where/WRB $/$ L$/CD is/VBZ a/DT Lipschitz/NNP constant/JJ (/-LRB- which/WDT can/MD be/VB thought/VBN of/IN as/IN the/DT reciprocal/JJ of/IN the/DT margin/NN )/-RRB- ,/, and/CC the/DT learned/VBN classifier/NN is/VBZ worse/JJR than/IN the/DT optimal/JJ halfspace/NN by/IN at/IN most/RBS $/$ \/CD epsilon/CD $/$ ./.
We/PRP also/RB prove/VBP a/DT hardness/NN result/NN ,/, showing/VBG that/IN under/IN a/DT certain/JJ cryptographic/JJ assumption/NN ,/, no/DT algorithm/NN can/MD learn/VB kernel/NN -/HYPH based/VBN halfspaces/NNS in/IN time/NN polynomial/JJ in/IN $/$ L$/CD ./.
