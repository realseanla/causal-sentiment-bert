Our/PRP$ world/NN can/MD be/VB succinctly/RB and/CC compactly/RB described/VBN as/IN structured/VBN scenes/NNS of/IN objects/NNS and/CC relations/NNS ./.
A/DT typical/JJ room/NN ,/, for/IN example/NN ,/, contains/VBZ salient/JJ objects/NNS such/JJ as/IN tables/NNS ,/, chairs/NNS and/CC books/NNS ,/, and/CC these/DT objects/NNS typically/RB relate/VBP to/IN each/DT other/JJ by/IN their/PRP$ underlying/VBG causes/NNS and/CC semantics/NNS ./.
This/DT gives/VBZ rise/NN to/IN correlated/VBN features/NNS ,/, such/JJ as/IN position/NN ,/, function/NN and/CC shape/NN ./.
Humans/NNS exploit/VBP knowledge/NN of/IN objects/NNS and/CC their/PRP$ relations/NNS for/IN learning/VBG a/DT wide/JJ spectrum/NN of/IN tasks/NNS ,/, and/CC more/RBR generally/RB when/WRB learning/VBG the/DT structure/NN underlying/VBG observed/VBN data/NNS ./.
In/IN this/DT work/NN ,/, we/PRP introduce/VBP relation/NN networks/NNS (/-LRB- RNs/NNS )/-RRB- -/: a/DT general/JJ purpose/NN neural/JJ network/NN architecture/NN for/IN object/NN -/HYPH relation/NN reasoning/NN ./.
We/PRP show/VBP that/IN RNs/NNS are/VBP capable/JJ of/IN learning/VBG object/NN relations/NNS from/IN scene/NN description/NN data/NNS ./.
Furthermore/RB ,/, we/PRP show/VBP that/IN RNs/NNS can/MD act/VB as/IN a/DT bottleneck/NN that/WDT induces/VBZ the/DT factorization/NN of/IN objects/NNS from/IN entangled/JJ scene/NN description/NN inputs/NNS ,/, and/CC from/IN distributed/VBN deep/JJ representations/NNS of/IN scene/NN images/NNS provided/VBN by/IN a/DT variational/JJ autoencoder/NN ./.
The/DT model/NN can/MD also/RB be/VB used/VBN in/IN conjunction/NN with/IN differentiable/JJ memory/NN mechanisms/NNS for/IN implicit/JJ relation/NN discovery/NN in/IN one/CD -/HYPH shot/NN learning/NN tasks/NNS ./.
Our/PRP$ results/NNS suggest/VBP that/IN relation/NN networks/NNS are/VBP a/DT potentially/RB powerful/JJ architecture/NN for/IN solving/VBG a/DT variety/NN of/IN problems/NNS that/WDT require/VBP object/NN relation/NN reasoning/NN ./.
