We/PRP propose/VBP a/DT general/JJ information/NN -/HYPH theoretic/JJ approach/NN called/VBN Seraph/NNP (/-LRB- SEmi-supervised/VBN metRic/JJ leArning/NN Paradigm/NN with/IN Hyper/JJ -/HYPH sparsity/NN )/-RRB- for/IN metric/JJ learning/NN that/WDT does/VBZ not/RB rely/VB upon/IN the/DT manifold/JJ assumption/NN ./.
Given/VBN the/DT probability/NN parameterized/VBN by/IN a/DT Mahalanobis/NNP distance/NN ,/, we/PRP maximize/VBP the/DT entropy/NN of/IN that/DT probability/NN on/IN labeled/VBN data/NNS and/CC minimize/VB it/PRP on/IN unlabeled/JJ data/NNS following/VBG entropy/NN regularization/NN ,/, which/WDT allows/VBZ the/DT supervised/JJ and/CC unsupervised/JJ parts/NNS to/TO be/VB integrated/VBN in/IN a/DT natural/JJ and/CC meaningful/JJ way/NN ./.
Furthermore/RB ,/, Seraph/NNP is/VBZ regularized/VBN by/IN encouraging/VBG a/DT low/JJ -/HYPH rank/NN projection/NN induced/VBN from/IN the/DT metric/JJ ./.
The/DT optimization/NN of/IN Seraph/NNP is/VBZ solved/VBN efficiently/RB and/CC stably/RB by/IN an/DT EM/NN -/HYPH like/JJ scheme/NN with/IN the/DT analytical/JJ E-Step/NN and/CC convex/NN M/NN -/HYPH Step/NN ./.
Experiments/NNS demonstrate/VBP that/IN Seraph/NNP compares/VBZ favorably/RB with/IN many/JJ well/RB -/HYPH known/VBN global/JJ and/CC local/JJ metric/JJ learning/NN methods/NNS ./.
