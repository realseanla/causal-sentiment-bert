Current/JJ work/NN in/IN lexical/JJ distributed/VBN representations/NNS maps/VBZ each/DT word/NN to/IN a/DT point/NN vector/NN in/IN low/JJ -/HYPH dimensional/JJ space/NN ./.
Mapping/VBG instead/RB to/IN a/DT density/NN provides/VBZ many/JJ interesting/JJ advantages/NNS ,/, including/VBG better/JJR capturing/NN uncertainty/NN about/IN a/DT representation/NN and/CC its/PRP$ relationships/NNS ,/, expressing/VBG asymmetries/NNS more/RBR naturally/RB than/IN dot/NN product/NN or/CC cosine/NN similarity/NN ,/, and/CC enabling/VBG more/JJR expressive/JJ parameterization/NN of/IN decision/NN boundaries/NNS ./.
This/DT paper/NN advocates/VBZ for/IN density/NN -/HYPH based/VBN distributed/VBN embeddings/NNS and/CC presents/VBZ a/DT method/NN for/IN learning/VBG representations/NNS in/IN the/DT space/NN of/IN Gaussian/JJ distributions/NNS ./.
We/PRP compare/VBP performance/NN on/IN various/JJ word/NN embedding/NN benchmarks/NNS ,/, investigate/VB the/DT ability/NN of/IN these/DT embeddings/NNS to/TO model/VB entailment/NN and/CC other/JJ asymmetric/JJ relationships/NNS ,/, and/CC explore/VB novel/JJ properties/NNS of/IN the/DT representation/NN ./.
