We/PRP propose/VBP a/DT novel/JJ method/NN for/IN speeding/VBG up/RP stochastic/JJ optimization/NN algorithms/NNS via/IN sketching/VBG methods/NNS ,/, which/WDT recently/RB became/VBD a/DT powerful/JJ tool/NN for/IN accelerating/VBG algorithms/NNS for/IN numerical/JJ linear/JJ algebra/NN ./.
We/PRP revisit/VBP the/DT method/NN of/IN conditioning/NN for/IN accelerating/VBG first/RB -/HYPH order/NN methods/NNS and/CC suggest/VBP the/DT use/NN of/IN sketching/VBG methods/NNS for/IN constructing/VBG a/DT cheap/JJ conditioner/NN that/WDT attains/VBZ a/DT significant/JJ speedup/NN with/IN respect/NN to/IN the/DT Stochastic/NNP Gradient/NNP Descent/NNP (/-LRB- SGD/NNP )/-RRB- algorithm/NN ./.
While/IN our/PRP$ theoretical/JJ guarantees/NNS assume/VBP convexity/NN ,/, we/PRP discuss/VBP the/DT applicability/NN of/IN our/PRP$ method/NN to/TO deep/RB neural/JJ networks/NNS ,/, and/CC experimentally/RB demonstrate/VBP its/PRP$ merits/NNS ./.
