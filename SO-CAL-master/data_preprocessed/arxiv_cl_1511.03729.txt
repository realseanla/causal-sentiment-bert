In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT novel/JJ method/NN to/TO incorporate/VB corpus/NN -/HYPH level/NN discourse/NN information/NN into/IN language/NN modelling/NN ./.
We/PRP call/VBP this/DT larger/JJR -/HYPH context/NN language/NN model/NN ./.
We/PRP introduce/VBP a/DT late/JJ fusion/NN approach/NN to/IN a/DT recurrent/JJ language/NN model/NN based/VBN on/IN long/JJ short/JJ -/HYPH term/NN memory/NN units/NNS (/-LRB- LSTM/NN )/-RRB- ,/, which/WDT helps/VBZ the/DT LSTM/NNP unit/NN keep/VB intra-sentence/JJ dependencies/NNS and/CC inter-sentence/NN dependencies/NNS separate/VBP from/IN each/DT other/JJ ./.
Through/IN the/DT evaluation/NN on/IN three/CD corpora/NNS (/-LRB- IMDB/NNP ,/, BBC/NNP ,/, and/CC PennTree/NNP Bank/NNP )/-RRB- ,/, we/PRP demon/NN -/HYPH strate/NN that/IN the/DT proposed/VBN model/NN improves/VBZ perplexity/NN significantly/RB ./.
In/IN the/DT experi/NN -/HYPH ments/NNS ,/, we/PRP evaluate/VBP the/DT proposed/VBN approach/NN while/IN varying/VBG the/DT number/NN of/IN context/NN sentences/NNS and/CC observe/VBP that/IN the/DT proposed/VBN late/JJ fusion/NN is/VBZ superior/JJ to/IN the/DT usual/JJ way/NN of/IN incorporating/VBG additional/JJ inputs/NNS to/IN the/DT LSTM/NNP ./.
By/IN analyzing/VBG the/DT trained/VBN larger/JJR -/HYPH context/NN language/NN model/NN ,/, we/PRP discover/VBP that/IN content/JJ words/NNS ,/, including/VBG nouns/NNS ,/, adjec/NN -/HYPH tives/NNS and/CC verbs/NNS ,/, benefit/NN most/RBS from/IN an/DT increasing/VBG number/NN of/IN context/NN sentences/NNS ./.
This/DT analysis/NN suggests/VBZ that/IN larger/JJR -/HYPH context/NN language/NN model/NN improves/VBZ the/DT unconditional/JJ language/NN model/NN by/IN capturing/VBG the/DT theme/NN of/IN a/DT document/NN better/RBR and/CC more/RBR easily/RB ./.
