Unlabeled/JJ data/NNS is/VBZ often/RB used/VBN to/TO learn/VB representations/NNS which/WDT can/MD be/VB used/VBN to/TO supplement/VB baseline/NN features/NNS in/IN a/DT supervised/JJ learner/NN ./.
For/IN example/NN ,/, for/IN text/NN applications/NNS where/WRB the/DT words/NNS lie/VBP in/IN a/DT very/RB high/JJ dimensional/JJ space/NN (/-LRB- the/DT size/NN of/IN the/DT vocabulary/NN )/-RRB- ,/, one/PRP can/MD learn/VB a/DT low/JJ rank/NN "/'' dictionary/NN "/'' by/IN an/DT eigen/NN -/HYPH decomposition/NN of/IN the/DT word/NN co-occurrence/NN matrix/NN (/-LRB- e.g./FW using/VBG PCA/NN or/CC CCA/NN )/-RRB- ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT new/JJ spectral/JJ method/NN based/VBN on/IN CCA/NN to/TO learn/VB an/DT eigenword/NN dictionary/NN ./.
Our/PRP$ improved/VBN procedure/NN computes/VBZ two/CD set/NN of/IN CCAs/NNS ,/, the/DT first/JJ one/CD between/IN the/DT left/JJ and/CC right/JJ contexts/NNS of/IN the/DT given/VBN word/NN and/CC the/DT second/JJ one/NN between/IN the/DT projections/NNS resulting/VBG from/IN this/DT CCA/NN and/CC the/DT word/NN itself/PRP ./.
We/PRP prove/VBP theoretically/RB that/IN this/DT two/CD -/HYPH step/NN procedure/NN has/VBZ lower/JJR sample/NN complexity/NN than/IN the/DT simple/JJ single/JJ step/NN procedure/NN and/CC also/RB illustrate/VBP the/DT empirical/JJ efficacy/NN of/IN our/PRP$ approach/NN and/CC the/DT richness/NN of/IN representations/NNS learned/VBN by/IN our/PRP$ Two/CD Step/NN CCA/NN (/-LRB- TSCCA/NN )/-RRB- procedure/NN on/IN the/DT tasks/NNS of/IN POS/NN tagging/NN and/CC sentiment/NN classification/NN ./.
