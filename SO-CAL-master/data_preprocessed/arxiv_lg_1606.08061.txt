An/DT important/JJ class/NN of/IN problems/NNS involves/VBZ training/NN deep/JJ neural/JJ networks/NNS with/IN sparse/JJ prediction/NN targets/NNS of/IN very/RB high/JJ dimension/NN D/NN ./.
These/DT occur/VBP naturally/RB in/IN e.g./FW neural/JJ language/NN models/NNS or/CC the/DT learning/NN of/IN word/NN -/HYPH embeddings/NNS ,/, often/RB posed/VBN as/IN predicting/VBG the/DT probability/NN of/IN next/JJ words/NNS among/IN a/DT vocabulary/NN of/IN size/NN D/NN (/-LRB- e.g./FW 200,000/CD )/-RRB- ./.
Computing/NNP the/DT equally/RB large/JJ ,/, but/CC typically/RB non-sparse/JJ D/NN -/HYPH dimensional/JJ output/NN vector/NN from/IN a/DT last/JJ hidden/JJ layer/NN of/IN reasonable/JJ dimension/NN d/NN (/-LRB- e.g./FW 500/CD )/-RRB- incurs/VBZ a/DT prohibitive/JJ O/NN (/-LRB- Dd/NN )/-RRB- computational/JJ cost/NN for/IN each/DT example/NN ,/, as/IN does/VBZ updating/VBG the/DT $/$ D/LS \/SYM times/NNS d/NN $/$ output/NN weight/NN matrix/NN and/CC computing/VBG the/DT gradient/NN needed/VBN for/IN backpropagation/NN to/IN previous/JJ layers/NNS ./.
While/IN efficient/JJ handling/NN of/IN large/JJ sparse/JJ network/NN inputs/NNS is/VBZ trivial/JJ ,/, the/DT case/NN of/IN large/JJ sparse/JJ targets/NNS is/VBZ not/RB ,/, and/CC has/VBZ thus/RB so/RB far/RB been/VBN sidestepped/VBN with/IN approximate/JJ alternatives/NNS such/JJ as/IN hierarchical/JJ softmax/NN or/CC sampling/NN -/HYPH based/VBN approximations/NNS during/IN training/NN ./.
In/IN this/DT work/NN we/PRP develop/VBP an/DT original/JJ algorithmic/JJ approach/NN which/WDT ,/, for/IN a/DT family/NN of/IN loss/NN functions/NNS that/WDT includes/VBZ squared/VBN error/NN and/CC spherical/JJ softmax/NN ,/, can/MD compute/VB the/DT exact/JJ loss/NN ,/, gradient/NN update/NN for/IN the/DT output/NN weights/NNS ,/, and/CC gradient/NN for/IN backpropagation/NN ,/, all/DT in/IN $/$ O/UH (/-LRB- d/NN ^/SYM {/-LRB- 2/CD }/-RRB- )/-RRB- $/$ per/IN example/NN instead/RB of/IN $/$ O/UH (/-LRB- Dd/NN )/-RRB- $/$ ,/, remarkably/RB without/IN ever/RB computing/VBG the/DT D/NN -/HYPH dimensional/JJ output/NN ./.
The/DT proposed/VBN algorithm/NN yields/VBZ a/DT speedup/NN of/IN up/RB to/IN $/$ D/NN //HYPH 4d/NN $/$ i.e./FW two/CD orders/NNS of/IN magnitude/NN for/IN typical/JJ sizes/NNS ,/, for/IN that/DT critical/JJ part/NN of/IN the/DT computations/NNS that/WDT often/RB dominates/VBZ the/DT training/NN time/NN in/IN this/DT kind/NN of/IN network/NN architecture/NN ./.
