Recent/JJ work/NN on/IN sequence/NN to/IN sequence/NN translation/NN using/VBG Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNNs/NNS )/-RRB- based/VBN on/IN Long/JJ Short/JJ Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- architectures/NNS has/VBZ shown/VBN great/JJ potential/NN for/IN learning/VBG useful/JJ representations/NNS of/IN sequential/JJ data/NNS ./.
These/DT architectures/NNS ,/, using/VBG one/CD recurrent/JJ neural/JJ network/NN to/TO encode/VB sequences/NNS into/IN fixed/VBN -/HYPH length/NN representations/NNS ,/, and/CC one/CD or/CC more/JJR network/NN (/-LRB- s/AFX )/-RRB- to/IN decode/NN representations/NNS into/IN new/JJ sequences/NNS have/VBP the/DT advantages/NNS of/IN being/VBG modular/JJ ,/, while/IN also/RB allowing/VBG modules/NNS to/TO be/VB jointly/RB trained/VBN ./.
A/DT one/CD -/HYPH to/IN -/HYPH many/JJ encoder/NN -/HYPH decoder/NN (/-LRB- s/AFX )/-RRB- scheme/NN allows/VBZ for/IN a/DT single/JJ encoder/NN to/TO provide/VB representations/NNS serving/VBG multiple/JJ purposes/NNS ./.
In/IN our/PRP$ case/NN ,/, we/PRP present/VBP an/DT LSTM/NNP encoder/NN network/NN able/JJ to/TO produce/VB representations/NNS used/VBN by/IN two/CD decoders/NNS :/: one/CD that/WDT reconstructs/VBZ ,/, and/CC one/CD that/WDT classifies/VBZ if/IN the/DT training/NN sequence/NN has/VBZ a/DT labelling/NN ./.
This/DT allows/VBZ the/DT network/NN to/TO learn/VB representations/NNS that/WDT are/VBP useful/JJ for/IN both/DT discriminative/JJ and/CC generative/JJ tasks/NNS at/IN the/DT same/JJ time/NN ./.
We/PRP show/VBP how/WRB this/DT paradigm/NN is/VBZ very/RB well/RB suited/JJ for/IN semi-supervised/JJ learning/NN with/IN sequences/NNS ./.
We/PRP test/VBP our/PRP$ proposed/VBN approach/NN on/IN an/DT action/NN recognition/NN task/NN using/VBG motion/NN capture/NN (/-LRB- MOCAP/NN )/-RRB- sequences/NNS and/CC show/VBP that/IN semi-supervised/JJ feature/NN learning/NN can/MD improve/VB movement/NN classification/NN ./.
