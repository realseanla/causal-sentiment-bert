Scalable/JJ and/CC effective/JJ exploration/NN remains/VBZ a/DT key/JJ challenge/NN in/IN reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- ./.
While/IN there/EX are/VBP methods/NNS with/IN optimality/NN guarantees/NNS in/IN the/DT setting/NN of/IN discrete/JJ state/NN and/CC action/NN spaces/NNS ,/, these/DT methods/NNS can/MD not/RB be/VB applied/VBN in/IN high/JJ -/HYPH dimensional/JJ deep/JJ RL/NN scenarios/NNS ./.
As/IN such/JJ ,/, most/RBS contemporary/JJ RL/NNP relies/VBZ on/IN simple/JJ heuristics/NNS such/JJ as/IN epsilon/NN -/HYPH greedy/JJ exploration/NN or/CC adding/VBG Gaussian/JJ noise/NN to/IN the/DT controls/NNS ./.
This/DT paper/NN introduces/VBZ Variational/NNP Information/NNP Maximizing/NNP Exploration/NNP (/-LRB- VIME/NNP )/-RRB- ,/, an/DT exploration/NN strategy/NN based/VBN on/IN maximization/NN of/IN information/NN gain/NN about/IN the/DT agent/NN 's/POS belief/NN of/IN environment/NN dynamics/NNS ./.
We/PRP propose/VBP a/DT practical/JJ implementation/NN ,/, using/VBG variational/JJ inference/NN in/IN Bayesian/JJ neural/JJ networks/NNS which/WDT efficiently/RB handles/VBZ continuous/JJ state/NN and/CC action/NN spaces/NNS ./.
VIME/NNP modifies/VBZ the/DT MDP/NNP reward/NN function/NN ,/, and/CC can/MD be/VB applied/VBN with/IN several/JJ different/JJ underlying/JJ RL/NN algorithms/NNS ./.
We/PRP demonstrate/VBP that/IN VIME/NNP achieves/VBZ significantly/RB better/JJR performance/NN compared/VBN to/IN heuristic/NN exploration/NN methods/NNS across/IN a/DT variety/NN of/IN continuous/JJ control/NN tasks/NNS and/CC algorithms/NNS ,/, including/VBG tasks/NNS with/IN very/RB sparse/JJ rewards/NNS ./.
