Since/IN its/PRP$ introduction/NN ,/, Word2Vec/NN and/CC its/PRP$ variants/NNS are/VBP widely/RB used/VBN to/TO learn/VB semantics/NNS -/HYPH preserving/VBG representations/NNS of/IN words/NNS or/CC entities/NNS in/IN an/DT embedding/NN space/NN ,/, which/WDT can/MD be/VB used/VBN to/TO produce/VB state/NN -/HYPH of/IN -/HYPH art/NN results/NNS for/IN various/JJ Natural/NNP Language/NNP Processing/NNP tasks/NNS ./.
Existing/VBG implementations/NNS aim/VBP to/TO learn/VB efficiently/RB by/IN running/VBG multiple/JJ threads/NNS in/IN parallel/NN while/IN operating/VBG on/IN a/DT single/JJ model/NN in/IN shared/VBN memory/NN ,/, ignoring/VBG incidental/JJ memory/NN update/NN collisions/NNS ./.
We/PRP show/VBP that/IN these/DT collisions/NNS can/MD degrade/VB the/DT efficiency/NN of/IN parallel/JJ learning/NN ,/, and/CC propose/VB a/DT straightforward/JJ caching/NN strategy/NN that/WDT improves/VBZ the/DT efficiency/NN by/IN a/DT factor/NN of/IN 4/CD ./.
