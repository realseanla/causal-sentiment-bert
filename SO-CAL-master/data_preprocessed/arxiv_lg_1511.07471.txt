We/PRP consider/VBP the/DT emphatic/JJ temporal/JJ -/HYPH difference/NN (/-LRB- TD/NN )/-RRB- algorithm/NN ,/, ETD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- ,/, for/IN learning/VBG the/DT value/NN functions/NNS of/IN stationary/JJ policies/NNS in/IN a/DT discounted/VBN ,/, finite/JJ state/NN and/CC action/NN Markov/NNP decision/NN process/NN ./.
The/DT ETD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- algorithm/NN was/VBD recently/RB proposed/VBN by/IN Sutton/NNP ,/, Mahmood/NNP ,/, and/CC White/NNP to/TO solve/VB a/DT long/JJ -/HYPH standing/NN divergence/NN problem/NN of/IN the/DT standard/JJ TD/NNP algorithm/NN when/WRB it/PRP is/VBZ applied/VBN to/IN off/RB -/HYPH policy/NN training/NN ,/, where/WRB data/NNS from/IN an/DT exploratory/JJ policy/NN are/VBP used/VBN to/TO evaluate/VB other/JJ policies/NNS of/IN interest/NN ./.
The/DT almost/RB sure/JJ convergence/NN of/IN ETD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- has/VBZ been/VBN proved/VBN in/IN our/PRP$ recent/JJ work/NN under/IN general/JJ off/IN -/HYPH policy/NN training/NN conditions/NNS ,/, but/CC for/IN a/DT narrow/JJ range/NN of/IN diminishing/VBG stepsize/NN ./.
In/IN this/DT paper/NN we/PRP present/VBP convergence/NN results/NNS for/IN constrained/VBN versions/NNS of/IN ETD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- with/IN constant/JJ stepsize/NN and/CC with/IN diminishing/VBG stepsize/NN from/IN a/DT broad/JJ range/NN ./.
Our/PRP$ results/NNS characterize/VBP the/DT asymptotic/JJ behavior/NN of/IN the/DT trajectory/NN of/IN iterates/NNS produced/VBN by/IN those/DT algorithms/NNS ,/, and/CC are/VBP derived/VBN by/IN combining/VBG key/JJ properties/NNS of/IN ETD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- with/IN powerful/JJ convergence/NN theorems/NNS from/IN the/DT weak/JJ convergence/NN methods/NNS in/IN stochastic/JJ approximation/NN theory/NN ./.
For/IN the/DT case/NN of/IN constant/JJ stepsize/NN ,/, in/IN addition/NN to/IN analyzing/VBG the/DT behavior/NN of/IN the/DT algorithms/NNS in/IN the/DT limit/NN as/IN the/DT stepsize/JJ parameter/NN approaches/VBZ zero/CD ,/, we/PRP also/RB analyze/VBP their/PRP$ behavior/NN for/IN a/DT fixed/VBN stepsize/NN and/CC bound/VBD the/DT deviations/NNS of/IN their/PRP$ averaged/JJ iterates/NNS from/IN the/DT desired/VBN solution/NN ./.
These/DT results/NNS are/VBP obtained/VBN by/IN exploiting/VBG the/DT weak/JJ Feller/NNP property/NN of/IN the/DT Markov/NNP chains/NNS associated/VBN with/IN the/DT algorithms/NNS ,/, and/CC by/IN using/VBG ergodic/JJ theorems/NNS for/IN weak/JJ Feller/NNP Markov/NNP chains/NNS ,/, in/IN conjunction/NN with/IN the/DT convergence/NN results/VBZ we/PRP get/VBP from/IN the/DT weak/JJ convergence/NN methods/NNS ./.
Besides/IN ETD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- ,/, our/PRP$ analysis/NN also/RB applies/VBZ to/IN the/DT off/NN -/HYPH policy/NN TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- algorithm/NN ,/, when/WRB the/DT divergence/NN issue/NN is/VBZ avoided/VBN by/IN setting/VBG $/$ \/CD lambda/NN $/$ sufficiently/RB large/JJ ./.
