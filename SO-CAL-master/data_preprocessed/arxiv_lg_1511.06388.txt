Neural/JJ word/NN representations/NNS have/VBP proven/VBN useful/JJ in/IN Natural/NNP Language/NNP Processing/NNP (/-LRB- NLP/NNP )/-RRB- tasks/NNS due/IN to/IN their/PRP$ ability/NN to/TO efficiently/RB model/VB complex/JJ semantic/JJ and/CC syntactic/JJ word/NN relationships/NNS ./.
However/RB ,/, most/JJS techniques/NNS model/NN only/RB one/CD representation/NN per/IN word/NN ,/, despite/IN the/DT fact/NN that/IN a/DT single/JJ word/NN can/MD have/VB multiple/JJ meanings/NNS or/CC "/`` senses/NNS "/'' ./.
Some/DT techniques/NNS model/NN words/NNS by/IN using/VBG multiple/JJ vectors/NNS that/WDT are/VBP clustered/VBN based/VBN on/IN context/NN ./.
However/RB ,/, recent/JJ neural/JJ approaches/NNS rarely/RB focus/VBP on/IN the/DT application/NN to/IN a/DT consuming/VBG NLP/NN algorithm/NN ./.
Furthermore/RB ,/, the/DT training/NN process/NN of/IN recent/JJ word/NN -/HYPH sense/NN models/NNS is/VBZ expensive/JJ relative/JJ to/IN single/JJ -/HYPH sense/NN embedding/NN processes/NNS ./.
This/DT paper/NN presents/VBZ a/DT novel/JJ approach/NN which/WDT addresses/VBZ these/DT concerns/NNS by/IN modeling/VBG multiple/JJ embeddings/NNS for/IN each/DT word/NN based/VBN on/IN supervised/JJ disambiguation/NN ,/, which/WDT provides/VBZ a/DT fast/JJ and/CC accurate/JJ way/NN for/IN a/DT consuming/VBG NLP/NN model/NN to/TO select/VB a/DT sense/NN -/HYPH disambiguated/VBN embedding/NN ./.
We/PRP demonstrate/VBP that/IN these/DT embeddings/NNS can/MD disambiguate/VB both/DT contrastive/JJ senses/NNS such/JJ as/IN nominal/JJ and/CC verbal/JJ senses/NNS as/RB well/RB as/IN nuanced/JJ senses/NNS such/JJ as/IN sarcasm/NN ./.
We/PRP further/RB evaluate/VB Part/NN -/HYPH of/IN -/HYPH Speech/NNP disambiguated/VBD embeddings/NNS on/IN neural/JJ dependency/NN parsing/VBG ,/, yielding/VBG a/DT greater/JJR than/IN 8/CD percent/NN average/JJ error/NN reduction/NN in/IN unlabeled/JJ attachment/NN scores/NNS across/IN 6/CD languages/NNS ./.
