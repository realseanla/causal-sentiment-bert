In/IN order/NN for/IN robots/NNS to/TO operate/VB effectively/RB in/IN homes/NNS and/CC workplaces/NNS ,/, they/PRP must/MD be/VB able/JJ to/TO manipulate/VB the/DT articulated/JJ objects/NNS common/JJ to/IN environments/NNS built/VBN for/IN and/CC by/IN humans/NNS ./.
Previous/JJ work/NN learns/VBZ kinematic/JJ models/NNS that/WDT prescribe/VBP this/DT manipulation/NN from/IN visual/JJ demonstrations/NNS ./.
Lingual/NNP signals/NNS ,/, such/JJ as/IN natural/JJ language/NN descriptions/NNS and/CC instructions/NNS ,/, offer/VBP a/DT complementary/JJ means/NNS of/IN conveying/VBG knowledge/NN of/IN such/JJ manipulation/NN models/NNS and/CC are/VBP suitable/JJ to/IN a/DT wide/JJ range/NN of/IN interactions/NNS (/-LRB- e.g./FW ,/, remote/JJ manipulation/NN )/-RRB- ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT multimodal/JJ learning/NN framework/NN that/WDT incorporates/VBZ both/CC visual/JJ and/CC lingual/JJ information/NN to/TO estimate/VB the/DT structure/NN and/CC parameters/NNS that/WDT define/VBP kinematic/JJ models/NNS of/IN articulated/JJ objects/NNS ./.
The/DT visual/JJ signal/NN takes/VBZ the/DT form/NN of/IN an/DT RGB/NN -/HYPH D/NN image/NN stream/NN that/WDT opportunistically/RB captures/VBZ object/NN motion/NN in/IN an/DT unprepared/JJ scene/NN ./.
Accompanying/VBG natural/JJ language/NN descriptions/NNS of/IN the/DT motion/NN constitute/VB the/DT lingual/NN signal/NN ./.
We/PRP present/VBP a/DT probabilistic/JJ language/NN model/NN that/WDT uses/VBZ word/NN embeddings/NNS to/TO associate/VB lingual/JJ verbs/NNS with/IN their/PRP$ corresponding/VBG kinematic/JJ structures/NNS ./.
By/IN exploiting/VBG the/DT complementary/JJ nature/NN of/IN the/DT visual/JJ and/CC lingual/JJ input/NN ,/, our/PRP$ method/NN infers/VBZ correct/JJ kinematic/JJ structures/NNS for/IN various/JJ multiple/JJ -/HYPH part/NN objects/NNS on/IN which/WDT the/DT previous/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ,/, visual/JJ -/HYPH only/JJ system/NN fails/VBZ ./.
We/PRP evaluate/VBP our/PRP$ multimodal/JJ learning/NN framework/NN on/IN a/DT dataset/NN comprised/VBN of/IN a/DT variety/NN of/IN household/NN objects/NNS ,/, and/CC demonstrate/VBP a/DT 36/CD percent/NN improvement/NN in/IN model/NN accuracy/NN over/IN the/DT vision/NN -/HYPH only/JJ baseline/NN ./.
