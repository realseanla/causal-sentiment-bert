We/PRP present/VBP a/DT new/JJ method/NN to/TO reduce/VB the/DT variance/NN of/IN stochastic/JJ versions/NNS of/IN the/DT BFGS/NNP optimization/NN method/NN ,/, applied/VBD to/IN the/DT optimization/NN of/IN a/DT class/NN of/IN smooth/JJ strongly/RB convex/JJ functions/NNS ./.
Although/IN Stochastic/NNP Gradient/NNP Descent/NNP (/-LRB- SGD/NNP )/-RRB- is/VBZ a/DT popular/JJ method/NN to/TO solve/VB this/DT kind/NN of/IN problem/NN ,/, its/PRP$ convergence/NN rate/NN is/VBZ sublinear/JJ as/IN it/PRP is/VBZ in/IN fact/NN limited/VBN by/IN the/DT noisy/JJ approximation/NN of/IN the/DT true/JJ gradient/NN ./.
In/IN order/NN to/TO recover/VB a/DT high/JJ convergence/NN rate/NN ,/, one/CD has/VBZ to/TO pick/VB an/DT appropriate/JJ step/NN -/HYPH size/NN or/CC explicitly/RB reduce/VB the/DT variance/NN of/IN the/DT approximate/JJ gradients/NNS ./.
Another/DT limiting/VBG factor/NN of/IN SGD/NNP is/VBZ that/IN it/PRP ignores/VBZ the/DT curvature/NN of/IN the/DT objective/JJ function/NN that/WDT can/MD help/VB greatly/RB speed/VB up/RP convergence/NN ./.
Stochastic/JJ variants/NNS of/IN BFGS/NNP that/WDT include/VBP curvature/NN have/VBP shown/VBN good/JJ empirical/JJ performance/NN but/CC suffer/VB from/IN the/DT same/JJ noise/NN effects/NNS as/IN SGD/NNP ./.
We/PRP here/RB propose/VBP a/DT new/JJ algorithm/NN V/NN ITE/NN that/WDT uses/VBZ an/DT existing/VBG technique/NN to/TO reduce/VB this/DT variance/NN while/IN allowing/VBG a/DT constant/JJ step/NN -/HYPH size/NN to/TO be/VB used/VBN ./.
We/PRP show/VBP that/IN the/DT expected/VBN objective/JJ value/NN converges/VBZ to/IN the/DT optimum/JJ at/IN a/DT geometric/JJ rate/NN ./.
We/PRP experimentally/RB demonstrate/VBP improved/VBN convergence/NN rate/NN on/IN diverse/JJ stochastic/JJ optimization/NN problems/NNS ./.
