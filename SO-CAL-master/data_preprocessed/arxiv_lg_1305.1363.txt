AUC/NNP is/VBZ an/DT important/JJ performance/NN measure/NN and/CC many/JJ algorithms/NNS have/VBP been/VBN devoted/VBN to/IN AUC/NNP optimization/NN ,/, mostly/RB by/IN minimizing/VBG a/DT surrogate/JJ convex/NN loss/NN on/IN a/DT training/NN data/NNS set/VBN ./.
In/IN this/DT work/NN ,/, we/PRP focus/VBP on/IN one/CD -/HYPH pass/NN AUC/NN optimization/NN that/WDT requires/VBZ only/RB going/VBG through/IN the/DT training/NN data/NNS once/RB without/IN storing/VBG the/DT entire/JJ training/NN dataset/NN ,/, where/WRB conventional/JJ online/JJ learning/NN algorithms/NNS can/MD not/RB be/VB applied/VBN directly/RB because/IN AUC/NNP is/VBZ measured/VBN by/IN a/DT sum/NN of/IN losses/NNS defined/VBN over/IN pairs/NNS of/IN instances/NNS from/IN different/JJ classes/NNS ./.
We/PRP develop/VBP a/DT regression/NN -/HYPH based/VBN algorithm/NN which/WDT only/RB needs/VBZ to/TO maintain/VB the/DT first/JJ and/CC second/JJ order/NN statistics/NNS of/IN training/NN data/NNS in/IN memory/NN ,/, resulting/VBG a/DT storage/NN requirement/NN independent/JJ from/IN the/DT size/NN of/IN training/NN data/NNS ./.
To/TO efficiently/RB handle/VB high/JJ dimensional/JJ data/NNS ,/, we/PRP develop/VBP a/DT randomized/JJ algorithm/NN that/WDT approximates/VBZ the/DT covariance/NN matrices/NNS by/IN low/JJ rank/NN matrices/NNS ./.
We/PRP verify/VBP ,/, both/CC theoretically/RB and/CC empirically/RB ,/, the/DT effectiveness/NN of/IN the/DT proposed/VBN algorithm/NN ./.
