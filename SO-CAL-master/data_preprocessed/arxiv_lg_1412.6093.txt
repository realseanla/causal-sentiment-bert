Since/IN the/DT advent/NN of/IN deep/JJ learning/NN ,/, it/PRP has/VBZ been/VBN used/VBN to/TO solve/VB various/JJ problems/NNS using/VBG many/JJ different/JJ architectures/NNS ./.
However/RB ,/, these/DT architectures/NNS do/VBP not/RB always/RB adequately/RB consider/VB the/DT temporal/JJ dependencies/NNS in/IN data/NNS ./.
We/PRP thus/RB propose/VBP a/DT new/JJ generic/JJ architecture/NN called/VBD the/DT Deep/NNP Belief/NNP Network/NNP -/HYPH Bidirectional/NNP Long/NNP Short/NNP -/HYPH Term/NNP Memory/NN (/-LRB- DBN/NN -/HYPH BLSTM/NN )/-RRB- network/NN that/WDT models/NNS sequences/NNS by/IN keeping/VBG track/NN of/IN the/DT temporal/JJ information/NN while/IN enabling/VBG deep/JJ representations/NNS in/IN the/DT data/NNS ./.
We/PRP demonstrate/VBP the/DT generality/NN of/IN this/DT new/JJ architecture/NN by/IN applying/VBG it/PRP to/IN the/DT generative/JJ task/NN of/IN music/NN generation/NN and/CC obtain/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS ./.
