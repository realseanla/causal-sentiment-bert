It/PRP has/VBZ recently/RB been/VBN shown/VBN that/IN supervised/JJ learning/NN with/IN the/DT popular/JJ logistic/JJ loss/NN is/VBZ equivalent/JJ to/IN optimizing/VBG the/DT exponential/JJ loss/NN over/IN sufficient/JJ statistics/NNS about/IN the/DT class/NN :/: Rademacher/NNP observations/NNS (/-LRB- rados/NNS )/-RRB- ./.
We/PRP first/RB show/VBP that/IN this/DT unexpected/JJ equivalence/NN can/MD actually/RB be/VB generalized/VBN to/IN other/JJ example/NN //HYPH rado/NN losses/NNS ,/, with/IN necessary/JJ and/CC sufficient/JJ conditions/NNS for/IN the/DT equivalence/NN ,/, exemplified/VBN on/IN four/CD losses/NNS that/WDT bear/VBP popular/JJ names/NNS in/IN various/JJ fields/NNS :/: exponential/JJ (/-LRB- boosting/VBG )/-RRB- ,/, mean/VB -/HYPH variance/NN (/-LRB- finance/NN )/-RRB- ,/, Linear/NNP Hinge/NNP (/-LRB- on/IN -/HYPH line/NN learning/NN )/-RRB- ,/, ReLU/NN (/-LRB- deep/JJ learning/NN )/-RRB- ,/, and/CC unhinged/VBN (/-LRB- statistics/NNS )/-RRB- ./.
Second/RB ,/, we/PRP show/VBP that/IN the/DT generalization/NN unveils/VBZ a/DT surprising/JJ new/JJ connection/NN to/IN regularized/VBN learning/NN ,/, and/CC in/IN particular/JJ a/DT sufficient/JJ condition/NN under/IN which/WDT regularizing/VBG the/DT loss/NN over/IN examples/NNS is/VBZ equivalent/JJ to/IN regularizing/VBG the/DT rados/NNS (/-LRB- with/IN Minkowski/NNP sums/NNS )/-RRB- in/IN the/DT equivalent/JJ rado/NN loss/NN ./.
This/DT brings/VBZ simple/JJ and/CC powerful/JJ rado/NN -/HYPH based/VBN learning/NN algorithms/NNS for/IN sparsity/NN -/HYPH controlling/VBG regularization/NN ,/, that/IN we/PRP exemplify/VBP on/IN a/DT boosting/VBG algorithm/NN for/IN the/DT regularized/VBN exponential/JJ rado/NN -/HYPH loss/NN ,/, which/WDT formally/RB boosts/VBZ over/IN four/CD types/NNS of/IN regularization/NN ,/, including/VBG the/DT popular/JJ ridge/NN and/CC lasso/NN ,/, and/CC the/DT recently/RB coined/VBN slope/NN ---/, we/PRP obtain/VBP the/DT first/JJ proven/VBN boosting/VBG algorithm/NN for/IN this/DT last/JJ regularization/NN ./.
Through/IN our/PRP$ first/JJ contribution/NN on/IN the/DT equivalence/NN of/IN rado/NN and/CC example/NN -/HYPH based/VBN losses/NNS ,/, Omega/NN -/HYPH R.AdaBoost/NN ~/SYM appears/VBZ to/TO be/VB an/DT efficient/JJ proxy/NN to/TO boost/VB the/DT regularized/VBN logistic/JJ loss/NN over/IN examples/NNS using/VBG whichever/WDT of/IN the/DT four/CD regularizers/NNS ./.
Experiments/NNS display/VBP that/IN regularization/NN consistently/RB improves/VBZ performances/NNS of/IN rado/NN -/HYPH based/VBN learning/NN ,/, and/CC may/MD challenge/VB or/CC beat/VB the/DT state/NN of/IN the/DT art/NN of/IN example/NN -/HYPH based/VBN learning/NN even/RB when/WRB learning/VBG over/IN small/JJ sets/NNS of/IN rados/NNS ./.
Finally/RB ,/, we/PRP connect/VBP regularization/NN to/IN differential/JJ privacy/NN ,/, and/CC display/VBP how/WRB tiny/JJ budgets/NNS can/MD be/VB afforded/VBN on/IN big/JJ domains/NNS while/IN beating/VBG (/-LRB- protected/VBN )/-RRB- example/NN -/HYPH based/VBN learning/NN ./.
