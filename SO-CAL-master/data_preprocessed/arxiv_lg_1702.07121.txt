The/DT problem/NN of/IN on/IN -/HYPH line/NN off/IN -/HYPH policy/NN evaluation/NN (/-LRB- OPE/NN )/-RRB- has/VBZ been/VBN actively/RB studied/VBN in/IN the/DT last/JJ decade/NN due/IN to/IN its/PRP$ importance/NN both/DT as/IN a/DT stand/NN -/HYPH alone/JJ problem/NN and/CC as/IN a/DT module/NN in/IN a/DT policy/NN improvement/NN scheme/NN ./.
However/RB ,/, most/JJS Temporal/JJ Difference/NN (/-LRB- TD/NN )/-RRB- based/VBN solutions/NNS ignore/VBP the/DT discrepancy/NN between/IN the/DT stationary/JJ distribution/NN of/IN the/DT behavior/NN and/CC target/NN policies/NNS and/CC its/PRP$ effect/NN on/IN the/DT convergence/NN limit/NN when/WRB function/NN approximation/NN is/VBZ applied/VBN ./.
In/IN this/DT paper/NN we/PRP propose/VBP the/DT Consistent/JJ Off/NN -/HYPH Policy/NN Temporal/JJ Difference/NN (/-LRB- COP/NN -/HYPH TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ ,/, $/$ \/SYM beta/NN $/$ )/-RRB- )/-RRB- algorithm/NN that/WDT addresses/VBZ this/DT issue/NN and/CC reduces/VBZ this/DT bias/NN at/IN some/DT computational/JJ expense/NN ./.
We/PRP show/VBP that/IN COP/NN -/HYPH TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ ,/, $/$ \/SYM beta/NN $/$ )/-RRB- can/MD be/VB designed/VBN to/TO converge/VB to/IN the/DT same/JJ value/NN that/WDT would/MD have/VB been/VBN obtained/VBN by/IN using/VBG on/IN -/HYPH policy/NN TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- with/IN the/DT target/NN policy/NN ./.
Subsequently/RB ,/, the/DT proposed/VBN scheme/NN leads/VBZ to/IN a/DT related/JJ and/CC promising/JJ heuristic/NN we/PRP call/VBP log/NN -/HYPH COP/NN -/HYPH TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ ,/, $/$ \/SYM beta/NN $/$ )/-RRB- ./.
Both/DT algorithms/NNS have/VBP favorable/JJ empirical/JJ results/NNS to/IN the/DT current/JJ state/NN of/IN the/DT art/NN on/IN -/HYPH line/NN OPE/NN algorithms/NNS ./.
Finally/RB ,/, our/PRP$ formulation/NN sheds/VBZ some/DT new/JJ light/NN on/IN the/DT recently/RB proposed/VBN Emphatic/JJ TD/NN learning/NN ./.
