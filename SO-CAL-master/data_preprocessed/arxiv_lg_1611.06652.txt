Adaptive/JJ stochastic/JJ gradient/NN methods/NNS such/JJ as/IN AdaGrad/NNP have/VBP gained/VBN popularity/NN in/IN particular/JJ for/IN training/VBG deep/JJ neural/JJ networks/NNS ./.
The/DT most/RBS commonly/RB used/VBN and/CC studied/VBN variant/JJ maintains/VBZ a/DT diagonal/JJ matrix/NN approximation/NN to/IN second/JJ order/NN information/NN by/IN accumulating/VBG past/JJ gradients/NNS which/WDT are/VBP used/VBN to/TO tune/VB the/DT step/NN size/NN adaptively/RB ./.
In/IN certain/JJ situations/NNS the/DT full/JJ -/HYPH matrix/NN variant/NN of/IN AdaGrad/NNP is/VBZ expected/VBN to/TO attain/VB better/JJR performance/NN ,/, however/RB in/IN high/JJ dimensions/NNS it/PRP is/VBZ computationally/RB impractical/JJ ./.
We/PRP present/VBP Ada/NNP -/: LR/NN and/CC RadaGrad/NNP two/CD computationally/RB efficient/JJ approximations/NNS to/IN full/JJ -/HYPH matrix/NN AdaGrad/NNP based/VBN on/IN randomized/JJ dimensionality/NN reduction/NN ./.
They/PRP are/VBP able/JJ to/TO capture/VB dependencies/NNS between/IN features/NNS and/CC achieve/VB similar/JJ performance/NN to/IN full/JJ -/HYPH matrix/NN AdaGrad/NNP but/CC at/IN a/DT much/RB smaller/JJR computational/JJ cost/NN ./.
We/PRP show/VBP that/IN the/DT regret/NN of/IN Ada/NNP -/: LR/NN is/VBZ close/JJ to/IN the/DT regret/NN of/IN full/JJ -/HYPH matrix/NN AdaGrad/NNP which/WDT can/MD have/VB an/DT up/RB -/HYPH to/TO exponentially/RB smaller/JJR dependence/NN on/IN the/DT dimension/NN than/IN the/DT diagonal/JJ variant/NN ./.
Empirically/RB ,/, we/PRP show/VBP that/IN Ada/NNP -/: LR/NN and/CC RadaGrad/NNP perform/VBP similarly/RB to/IN full/JJ -/HYPH matrix/NN AdaGrad/NNP ./.
On/IN the/DT task/NN of/IN training/NN convolutional/JJ neural/JJ networks/NNS as/RB well/RB as/IN recurrent/JJ neural/JJ networks/NNS ,/, RadaGrad/NNP achieves/VBZ faster/RBR convergence/NN than/IN diagonal/JJ AdaGrad/NNP ./.
