As/IN deep/JJ nets/NNS are/VBP increasingly/RB used/VBN in/IN applications/NNS suited/VBN for/IN mobile/JJ devices/NNS ,/, a/DT fundamental/JJ dilemma/NN becomes/VBZ apparent/JJ :/: the/DT trend/NN in/IN deep/JJ learning/NN is/VBZ to/TO grow/VB models/NNS to/TO absorb/VB ever/RB -/HYPH increasing/VBG data/NNS set/VBN sizes/NNS ;/: however/RB mobile/JJ devices/NNS are/VBP designed/VBN with/IN very/RB little/JJ memory/NN and/CC can/MD not/RB store/VB such/JJ large/JJ models/NNS ./.
We/PRP present/VBP a/DT novel/JJ network/NN architecture/NN ,/, HashedNets/NNPS ,/, that/IN exploits/NNS inherent/JJ redundancy/NN in/IN neural/JJ networks/NNS to/TO achieve/VB drastic/JJ reductions/NNS in/IN model/NN sizes/NNS ./.
HashedNets/NNP uses/VBZ a/DT low/JJ -/HYPH cost/NN hash/NN function/NN to/IN randomly/RB group/NN connection/NN weights/NNS into/IN hash/NN buckets/NNS ,/, and/CC all/DT connections/NNS within/IN the/DT same/JJ hash/NN bucket/NN share/NN a/DT single/JJ parameter/NN value/NN ./.
These/DT parameters/NNS are/VBP tuned/VBN to/TO adjust/VB to/IN the/DT HashedNets/NNPS weight/NN sharing/VBG architecture/NN with/IN standard/JJ backprop/NN during/IN training/NN ./.
Our/PRP$ hashing/VBG procedure/NN introduces/VBZ no/DT additional/JJ memory/NN overhead/RB ,/, and/CC we/PRP demonstrate/VBP on/IN several/JJ benchmark/NN data/NNS sets/VBZ that/IN HashedNets/NNP shrink/VB the/DT storage/NN requirements/NNS of/IN neural/JJ networks/NNS substantially/RB while/IN mostly/RB preserving/VBG generalization/NN performance/NN ./.
