We/PRP present/VBP the/DT first/JJ reinforcement/NN -/HYPH learning/NN model/NN to/IN self/NN -/HYPH improve/VB its/PRP$ reward/NN -/HYPH modulated/VBN training/NN implemented/VBN through/IN a/DT continuously/RB improving/VBG "/`` intuition/NN "/'' neural/JJ network/NN ./.
An/DT agent/NN was/VBD trained/VBN how/WRB to/TO play/VB the/DT arcade/NN video/NN game/NN Pong/NNP with/IN two/CD reward/NN -/HYPH based/VBN alternatives/NNS ,/, one/CD where/WRB the/DT paddle/NN was/VBD placed/VBN randomly/RB during/IN training/NN ,/, and/CC a/DT second/JJ where/WRB the/DT paddle/NN was/VBD simultaneously/RB trained/VBN on/IN three/CD additional/JJ neural/JJ networks/NNS such/JJ that/IN it/PRP could/MD develop/VB a/DT sense/NN of/IN "/`` certainty/NN "/'' as/IN to/IN how/WRB probable/JJ its/PRP$ own/JJ predicted/VBN paddle/NN position/NN will/MD be/VB to/TO return/VB the/DT ball/NN ./.
If/IN the/DT agent/NN was/VBD less/JJR than/IN 95/CD percent/NN certain/JJ to/TO return/VB the/DT ball/NN ,/, the/DT policy/NN used/VBD an/DT intuition/NN neural/JJ network/NN to/TO place/VB the/DT paddle/NN ./.
We/PRP trained/VBD both/DT architectures/NNS for/IN an/DT equivalent/JJ number/NN of/IN epochs/NNS and/CC tested/VBN learning/NN performance/NN by/IN letting/VBG the/DT trained/VBN programs/NNS play/VBP against/IN a/DT near/JJ -/HYPH perfect/JJ opponent/NN ./.
Through/IN this/DT ,/, we/PRP found/VBD that/IN the/DT reinforcement/NN learning/VBG model/NN that/WDT uses/VBZ an/DT intuition/NN neural/JJ network/NN for/IN placing/VBG the/DT paddle/NN during/IN reward/NN training/NN quickly/RB overtakes/VBZ the/DT simple/JJ architecture/NN in/IN its/PRP$ ability/NN to/TO outplay/VB the/DT near/JJ -/HYPH perfect/JJ opponent/NN ,/, additionally/RB outscoring/VBG that/IN opponent/NN by/IN an/DT increasingly/RB wide/JJ margin/NN after/IN additional/JJ epochs/NNS of/IN training/NN ./.
