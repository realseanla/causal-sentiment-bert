Distributed/VBN representations/NNS of/IN meaning/NN are/VBP a/DT natural/JJ way/NN to/TO encode/VB covariance/NN relationships/NNS between/IN words/NNS and/CC phrases/NNS in/IN NLP/NN ./.
By/IN overcoming/VBG data/NNS sparsity/NN problems/NNS ,/, as/RB well/RB as/IN providing/VBG information/NN about/IN semantic/JJ relatedness/NN which/WDT is/VBZ not/RB available/JJ in/IN discrete/JJ representations/NNS ,/, distributed/VBN representations/NNS have/VBP proven/VBN useful/JJ in/IN many/JJ NLP/NN tasks/NNS ./.
In/IN particular/JJ ,/, recent/JJ work/NN has/VBZ shown/VBN how/WRB compositional/JJ semantic/JJ representations/NNS can/MD successfully/RB be/VB applied/VBN to/IN a/DT number/NN of/IN monolingual/JJ applications/NNS such/JJ as/IN sentiment/NN analysis/NN ./.
At/IN the/DT same/JJ time/NN ,/, there/EX has/VBZ been/VBN some/DT initial/JJ success/NN in/IN work/NN on/IN learning/VBG shared/VBN word/NN -/HYPH level/NN representations/NNS across/IN languages/NNS ./.
We/PRP combine/VBP these/DT two/CD approaches/NNS by/IN proposing/VBG a/DT method/NN for/IN learning/VBG compositional/JJ representations/NNS in/IN a/DT multilingual/JJ setup/NN ./.
Our/PRP$ model/NN learns/VBZ to/TO assign/VB similar/JJ embeddings/NNS to/IN aligned/VBN sentences/NNS and/CC dissimilar/JJ ones/NNS to/IN sentence/NN which/WDT are/VBP not/RB aligned/VBN while/IN not/RB requiring/VBG word/NN alignments/NNS ./.
We/PRP show/VBP that/IN our/PRP$ representations/NNS are/VBP semantically/RB informative/JJ and/CC apply/VB them/PRP to/IN a/DT cross-lingual/JJ document/NN classification/NN task/NN where/WRB we/PRP outperform/VBP the/DT previous/JJ state/NN of/IN the/DT art/NN ./.
Further/RB ,/, by/IN employing/VBG parallel/JJ corpora/NNS of/IN multiple/JJ language/NN pairs/NNS we/PRP find/VBP that/IN our/PRP$ model/NN learns/VBZ representations/NNS that/WDT capture/VBP semantic/JJ relationships/NNS across/IN languages/NNS for/IN which/WDT no/DT parallel/JJ data/NNS was/VBD used/VBN ./.
