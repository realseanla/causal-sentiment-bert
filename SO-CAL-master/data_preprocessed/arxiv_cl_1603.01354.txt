State/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN sequence/NN labeling/NN systems/NNS traditionally/RB require/VBP large/JJ amounts/NNS of/IN task/NN -/HYPH specific/JJ knowledge/NN in/IN the/DT form/NN of/IN hand/NN -/HYPH crafted/VBN features/NNS and/CC data/NNS pre-processing/NN ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP a/DT novel/JJ neutral/JJ network/NN architecture/NN that/WDT benefits/VBZ from/IN both/DT word/NN -/HYPH and/CC character/NN -/HYPH level/NN representations/NNS automatically/RB ,/, by/IN using/VBG combination/NN of/IN bidirectional/JJ LSTM/NN ,/, CNN/NNP and/CC CRF/NNP ./.
Our/PRP$ system/NN is/VBZ truly/RB end/NN -/HYPH to/IN -/HYPH end/NN ,/, requiring/VBG no/DT feature/NN engineering/NN or/CC data/NNS pre-processing/JJ ,/, thus/RB making/VBG it/PRP applicable/JJ to/IN a/DT wide/JJ range/NN of/IN sequence/NN labeling/NN tasks/NNS on/IN different/JJ languages/NNS ./.
We/PRP evaluate/VBP our/PRP$ system/NN on/IN two/CD data/NNS sets/VBZ for/IN two/CD sequence/NN labeling/NN tasks/NNS ---/, Penn/NNP Treebank/NNP WSJ/NNP corpus/NN for/IN part/NN -/HYPH of/IN -/HYPH speech/NN (/-LRB- POS/NN )/-RRB- tagging/NN and/CC CoNLL/NN 2003/CD corpus/NN for/IN named/VBN entity/NN recognition/NN (/-LRB- NER/NN )/-RRB- ./.
We/PRP obtain/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN both/CC the/DT two/CD data/NNS ---/: 97.55/CD \/SYM percent/NN accuracy/NN for/IN POS/NN tagging/NN and/CC 91.21/CD \/SYM percent/NN F1/NN for/IN NER/NN ./.
