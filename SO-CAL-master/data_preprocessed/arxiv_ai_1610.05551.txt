Recent/JJ work/NN on/IN weighted/JJ model/NN counting/NN has/VBZ been/VBN very/RB successfully/RB applied/VBN to/IN the/DT problem/NN of/IN probabilistic/JJ inference/NN in/IN Bayesian/JJ networks/NNS ./.
The/DT probability/NN distribution/NN is/VBZ encoded/VBN into/IN a/DT Boolean/NNP normal/JJ form/NN and/CC compiled/VBN to/IN a/DT target/NN language/NN ,/, in/IN order/NN to/TO represent/VB local/JJ structure/NN expressed/VBN among/IN conditional/JJ probabilities/NNS more/RBR efficiently/RB ./.
We/PRP show/VBP that/IN further/JJ improvements/NNS are/VBP possible/JJ ,/, by/IN exploiting/VBG the/DT knowledge/NN that/WDT is/VBZ lost/VBN during/IN the/DT encoding/VBG phase/NN and/CC incorporating/VBG it/PRP into/IN a/DT compiler/NN inspired/VBN by/IN Satisfiability/NNP Modulo/NNP Theories/NNPS ./.
Constraints/NNS among/IN variables/NNS are/VBP used/VBN as/IN a/DT background/NN theory/NN ,/, which/WDT allows/VBZ us/PRP to/TO optimize/VB the/DT Shannon/NNP decomposition/NN ./.
We/PRP propose/VBP a/DT new/JJ language/NN ,/, called/VBN Weighted/NNP Positive/JJ Binary/JJ Decision/NN Diagrams/NNPS ,/, that/IN reduces/VBZ the/DT cost/NN of/IN probabilistic/JJ inference/NN by/IN using/VBG this/DT decomposition/NN variant/NN to/TO induce/VB an/DT arithmetic/NN circuit/NN of/IN reduced/VBN size/NN ./.
