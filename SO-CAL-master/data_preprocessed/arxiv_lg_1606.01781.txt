The/DT dominant/JJ approach/NN for/IN many/JJ NLP/NN tasks/NNS are/VBP recurrent/JJ neural/JJ networks/NNS ,/, in/IN particular/JJ LSTMs/NNPS ,/, and/CC convolutional/JJ neural/JJ networks/NNS ./.
However/RB ,/, these/DT architectures/NNS are/VBP rather/RB shallow/JJ in/IN comparison/NN to/IN the/DT deep/JJ convolutional/JJ networks/NNS which/WDT are/VBP very/RB successful/JJ in/IN computer/NN vision/NN ./.
We/PRP present/VBP a/DT new/JJ architecture/NN for/IN text/NN processing/NN which/WDT operates/VBZ directly/RB on/IN the/DT character/NN level/NN and/CC uses/VBZ only/RB small/JJ convolutions/NNS and/CC pooling/VBG operations/NNS ./.
We/PRP are/VBP able/JJ to/TO show/VB that/IN the/DT performance/NN of/IN this/DT model/NN increases/VBZ with/IN the/DT depth/NN :/: using/VBG up/RP to/IN 29/CD convolutional/JJ layers/NNS ,/, we/PRP report/VBP significant/JJ improvements/NNS over/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN on/IN several/JJ public/JJ text/NN classification/NN tasks/NNS ./.
To/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ time/NN that/WDT very/RB deep/RB convolutional/JJ nets/NNS have/VBP been/VBN applied/VBN to/IN NLP/NN ./.
