We/PRP present/VBP a/DT semi-supervised/JJ way/NN of/IN training/VBG a/DT character/NN -/HYPH based/VBN encoder/NN -/HYPH decoder/NN recurrent/JJ neural/JJ network/NN for/IN morphological/JJ reinflection/NN ,/, the/DT task/NN of/IN generating/VBG one/CD inflected/JJ word/NN form/NN from/IN another/DT ./.
This/DT is/VBZ achieved/VBN by/IN using/VBG unlabeled/JJ tokens/NNS or/CC random/JJ string/NN as/IN training/NN data/NNS for/IN an/DT autoencoding/NN task/NN ,/, adapting/VBG a/DT network/NN for/IN morphological/JJ reinflection/NN ,/, and/CC performing/VBG multi-task/VB training/NN ./.
We/PRP thus/RB use/VBP limited/JJ labeled/VBN data/NNS more/RBR effectively/RB ,/, obtaining/VBG up/RP to/IN 9.9/CD percent/NN improvement/NN over/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baselines/NNS for/IN 8/CD different/JJ languages/NNS ./.
