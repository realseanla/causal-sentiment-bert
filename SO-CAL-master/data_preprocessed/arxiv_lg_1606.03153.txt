Text/VB embeddings/NNS have/VBP played/VBN a/DT key/JJ role/NN in/IN obtaining/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN natural/JJ language/NN processing/NN ./.
Word2Vec/NN and/CC its/PRP$ variants/NNS have/VBP successfully/RB mapped/VBN words/NNS with/IN similar/JJ syntactic/JJ or/CC semantic/JJ meanings/NNS to/IN nearby/JJ vectors/NNS ./.
However/RB ,/, extracting/VBG universal/JJ embeddings/NNS of/IN longer/JJR word/NN -/HYPH sequences/NNS remains/VBZ a/DT challenging/JJ task/NN ./.
We/PRP employ/VBP the/DT convolutional/JJ dictionary/NN model/NN for/IN unsupervised/JJ learning/NN of/IN embeddings/NNS for/IN variable/JJ length/NN word/NN -/HYPH sequences/NNS ./.
We/PRP propose/VBP a/DT two/CD -/HYPH phase/NN ConvDic/NNP DeconvDec/NNP framework/NN that/WDT first/RB learns/VBZ dictionary/JJ elements/NNS (/-LRB- i.e./FW ,/, phrase/NN templates/NNS )/-RRB- ,/, and/CC then/RB employs/VBZ them/PRP for/IN decoding/VBG the/DT activations/NNS ./.
The/DT estimated/VBN activations/NNS are/VBP then/RB used/VBN as/IN embeddings/NNS for/IN downstream/JJ tasks/NNS such/JJ as/IN sentiment/NN analysis/NN ,/, paraphrase/NN detection/NN ,/, and/CC semantic/JJ textual/JJ similarity/NN estimation/NN ./.
We/PRP propose/VBP a/DT convolutional/JJ tensor/NN decomposition/NN algorithm/NN for/IN learning/VBG the/DT phrase/NN templates/NNS ./.
It/PRP is/VBZ shown/VBN to/TO be/VB more/RBR accurate/JJ ,/, and/CC much/RB more/RBR efficient/JJ than/IN the/DT popular/JJ alternating/VBG minimization/NN in/IN dictionary/NN learning/NN literature/NN ./.
Our/PRP$ word/NN -/HYPH sequence/NN embeddings/NNS achieve/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN in/IN sentiment/NN classification/NN ,/, semantic/JJ textual/JJ similarity/NN estimation/NN ,/, and/CC paraphrase/NN detection/NN over/IN eight/CD datasets/NNS from/IN various/JJ domains/NNS ,/, without/IN requiring/VBG pre-training/NN or/CC additional/JJ features/NNS ./.
