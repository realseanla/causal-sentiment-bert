Here/RB we/PRP study/VBP the/DT problem/NN of/IN predicting/VBG labels/NNS for/IN large/JJ text/NN corpora/NN where/WRB each/DT text/NN can/MD be/VB assigned/VBN multiple/JJ labels/NNS ./.
The/DT problem/NN might/MD seem/VB trivial/JJ when/WRB the/DT number/NN of/IN labels/NNS is/VBZ small/JJ ,/, and/CC can/MD be/VB easily/RB solved/VBN using/VBG a/DT series/NN of/IN one/CD -/HYPH vs/IN -/HYPH all/DT classifiers/NNS ./.
However/RB ,/, as/IN the/DT number/NN of/IN labels/NNS increases/VBZ to/IN several/JJ thousand/CD ,/, the/DT parameter/NN space/NN becomes/VBZ extremely/RB large/JJ ,/, and/CC it/PRP is/VBZ no/RB longer/RBR possible/JJ to/TO use/VB the/DT one/CD -/HYPH vs/IN -/HYPH all/DT technique/NN ./.
Here/RB we/PRP propose/VBP a/DT model/NN based/VBN on/IN the/DT factorization/NN of/IN higher/JJR order/NN word/NN vector/NN moments/NNS ,/, as/RB well/RB as/IN the/DT cross/NN moments/NNS between/IN the/DT labels/NNS and/CC the/DT words/NNS for/IN multi-label/JJ prediction/NN ./.
Our/PRP$ model/NN provides/VBZ guaranteed/VBN converge/VBP bounds/NNS on/IN the/DT extracted/VBN parameters/NNS ./.
Further/RB ,/, our/PRP$ model/NN takes/VBZ only/RB three/CD passes/NNS through/IN the/DT training/NN dataset/NN to/TO extract/VB the/DT parameters/NNS ,/, resulting/VBG in/IN a/DT highly/RB scalable/JJ algorithm/NN that/WDT can/MD train/VB on/IN GB/NNP 's/POS of/IN data/NNS consisting/VBG of/IN millions/NNS of/IN documents/NNS with/IN hundreds/NNS of/IN thousands/NNS of/IN labels/NNS using/VBG a/DT nominal/JJ resource/NN of/IN a/DT single/JJ processor/NN with/IN 16/CD GB/NNP RAM/NNP ./.
Our/PRP$ model/NN achieves/VBZ 10x/NN -/HYPH 15x/NN order/NN of/IN speed/NN -/HYPH up/NN on/IN large/JJ -/HYPH scale/NN datasets/NNS while/IN producing/VBG competitive/JJ performance/NN in/IN comparison/NN with/IN existing/VBG benchmark/NN algorithms/NNS ./.
