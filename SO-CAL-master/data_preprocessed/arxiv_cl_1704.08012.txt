Language/NNP models/NNS are/VBP typically/RB applied/VBN at/IN the/DT sentence/NN level/NN ,/, without/IN access/NN to/IN the/DT broader/JJR document/NN context/NN ./.
We/PRP present/VBP a/DT neural/JJ language/NN model/NN that/WDT incorporates/VBZ document/NN context/NN in/IN the/DT form/NN of/IN a/DT topic/NN model/NN -/HYPH like/JJ architecture/NN ,/, thus/RB providing/VBG a/DT succinct/JJ representation/NN of/IN the/DT broader/JJR document/NN context/NN outside/IN of/IN the/DT current/JJ sentence/NN ./.
Experiments/NNS over/IN a/DT range/NN of/IN datasets/NNS demonstrate/VBP that/IN our/PRP$ model/NN outperforms/VBZ a/DT pure/JJ sentence/NN -/HYPH based/VBN model/NN in/IN terms/NNS of/IN language/NN model/NN perplexity/NN ,/, and/CC leads/VBZ to/IN topics/NNS that/WDT are/VBP potentially/RB more/RBR coherent/JJ than/IN those/DT produced/VBN by/IN a/DT standard/JJ LDA/NNP topic/NN model/NN ./.
Our/PRP$ model/NN also/RB has/VBZ the/DT ability/NN to/TO generate/VB related/JJ sentences/NNS for/IN a/DT topic/NN ,/, providing/VBG another/DT way/NN to/TO interpret/VB topics/NNS ./.
