We/PRP describe/VBP an/DT LSTM/NN -/HYPH based/VBN model/NN which/WDT we/PRP call/VBP Byte/NN -/HYPH to/IN -/HYPH Span/NN (/-LRB- BTS/NN )/-RRB- that/WDT reads/VBZ text/NN as/IN bytes/NNS and/CC outputs/NNS span/VBP annotations/NNS of/IN the/DT form/NN [/-LRB- start/NN ,/, length/NN ,/, label/NN ]/-RRB- where/WRB start/VBP positions/NNS ,/, lengths/NNS ,/, and/CC labels/NNS are/VBP separate/JJ entries/NNS in/IN our/PRP$ vocabulary/NN ./.
Because/IN we/PRP operate/VBP on/IN unicode/JJ bytes/NNS rather/RB than/IN language/NN -/HYPH specific/JJ words/NNS or/CC characters/NNS ,/, we/PRP can/MD analyze/VB text/NN in/IN many/JJ languages/NNS with/IN a/DT single/JJ model/NN ./.
Due/IN to/IN the/DT small/JJ vocabulary/NN size/NN ,/, these/DT multilingual/JJ models/NNS are/VBP very/RB compact/JJ ,/, but/CC produce/VBP results/NNS similar/JJ to/IN or/CC better/JJR than/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN in/IN Part/NN -/HYPH of/IN -/HYPH Speech/NNP tagging/NN and/CC Named/VBN Entity/NN Recognition/NN that/WDT use/VBP only/RB the/DT provided/VBN training/NN datasets/NNS (/-LRB- no/DT external/JJ data/NNS sources/NNS )/-RRB- ./.
Our/PRP$ models/NNS are/VBP learning/VBG "/'' from/IN scratch/NN "/'' in/IN that/IN they/PRP do/VBP not/RB rely/VB on/IN any/DT elements/NNS of/IN the/DT standard/JJ pipeline/NN in/IN Natural/NNP Language/NNP Processing/NNP ./.
