We/PRP present/VBP a/DT learning/NN to/TO learn/VB approach/NN for/IN training/NN recurrent/JJ neural/JJ networks/NNS to/TO perform/VB black/JJ -/HYPH box/NN global/JJ optimization/NN ./.
In/IN the/DT meta/NN -/HYPH learning/NN phase/NN we/PRP use/VBP a/DT large/JJ set/NN of/IN smooth/JJ target/NN functions/VBZ to/TO learn/VB a/DT recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- optimizer/NN ,/, which/WDT is/VBZ either/CC a/DT long/JJ -/HYPH short/JJ term/NN memory/NN network/NN or/CC a/DT differentiable/JJ neural/JJ computer/NN ./.
After/IN learning/VBG ,/, the/DT RNN/NNP can/MD be/VB applied/VBN to/TO learn/VB policies/NNS in/IN reinforcement/NN learning/NN ,/, as/RB well/RB as/IN other/JJ black/JJ -/HYPH box/NN learning/NN tasks/NNS ,/, including/VBG continuous/JJ correlated/VBN bandits/NNS and/CC experimental/JJ design/NN ./.
We/PRP compare/VBP this/DT approach/NN to/IN Bayesian/JJ optimization/NN ,/, with/IN emphasis/NN on/IN the/DT issues/NNS of/IN computation/NN speed/NN ,/, horizon/NN length/NN ,/, and/CC exploration/NN -/HYPH exploitation/NN trade/NN -/HYPH offs/NNS ./.
