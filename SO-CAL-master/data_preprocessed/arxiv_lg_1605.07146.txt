Deep/JJ residual/JJ networks/NNS were/VBD shown/VBN to/TO be/VB able/JJ to/TO scale/VB up/RP to/IN thousands/NNS of/IN layers/NNS and/CC still/RB have/VBP improving/VBG performance/NN ./.
However/RB ,/, each/DT fraction/NN of/IN a/DT percent/NN of/IN improved/VBN accuracy/NN costs/NNS nearly/RB doubling/VBG the/DT number/NN of/IN layers/NNS ,/, and/CC so/RB training/VBG very/RB deep/JJ residual/JJ networks/NNS has/VBZ a/DT problem/NN of/IN diminishing/VBG feature/NN reuse/VB ,/, which/WDT makes/VBZ these/DT networks/NNS very/RB slow/RB to/TO train/VB ./.
To/TO tackle/VB these/DT problems/NNS ,/, in/IN this/DT paper/NN we/PRP conduct/VBP a/DT detailed/JJ experimental/JJ study/NN on/IN the/DT architecture/NN of/IN ResNet/NNP blocks/NNS ,/, based/VBN on/IN which/WDT we/PRP propose/VBP a/DT novel/JJ architecture/NN where/WRB we/PRP decrease/VBP depth/NN and/CC increase/NN width/NN of/IN residual/JJ networks/NNS ./.
We/PRP call/VBP the/DT resulting/VBG network/NN structures/NNS wide/JJ residual/JJ networks/NNS (/-LRB- WRNs/NNS )/-RRB- and/CC show/VBP that/IN these/DT are/VBP far/RB superior/JJ over/IN their/PRP$ commonly/RB used/VBN thin/JJ and/CC very/RB deep/JJ counterparts/NNS ./.
For/IN example/NN ,/, we/PRP demonstrate/VBP that/IN even/RB a/DT simple/JJ 16/CD -/HYPH layer/NN -/HYPH deep/JJ wide/JJ residual/JJ network/NN outperforms/VBZ in/IN accuracy/NN and/CC efficiency/NN all/DT previous/JJ deep/JJ residual/JJ networks/NNS ,/, including/VBG thousand/CD -/HYPH layer/NN -/HYPH deep/JJ networks/NNS ,/, achieving/VBG new/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN CIFAR/NN -/HYPH 10/CD ,/, CIFAR/NN -/HYPH 100/CD and/CC SVHN/NNP ./.
