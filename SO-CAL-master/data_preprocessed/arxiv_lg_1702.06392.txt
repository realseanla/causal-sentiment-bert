FPGA/NNP -/HYPH based/VBN hardware/NN accelerators/NNS for/IN convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- have/VBP obtained/VBN great/JJ attentions/NNS due/IN to/IN their/PRP$ higher/JJR energy/NN efficiency/NN than/IN GPUs/NNS ./.
However/RB ,/, it/PRP is/VBZ challenging/VBG for/IN FPGA/NNP -/HYPH based/VBN solutions/NNS to/TO achieve/VB a/DT higher/JJR throughput/NN than/IN GPU/NNP counterparts/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP demonstrate/VBP that/IN FPGA/NNP acceleration/NN can/MD be/VB a/DT superior/JJ solution/NN in/IN terms/NNS of/IN both/DT throughput/NN and/CC energy/NN efficiency/NN when/WRB a/DT CNN/NNP is/VBZ trained/VBN with/IN binary/JJ constraints/NNS on/IN weights/NNS and/CC activations/NNS ./.
Specifically/RB ,/, we/PRP propose/VBP an/DT optimized/VBN accelerator/NN architecture/NN tailored/VBN for/IN bitwise/NN convolution/NN and/CC normalization/NN that/WDT features/VBZ massive/JJ spatial/JJ parallelism/NN with/IN deep/JJ pipelines/NNS stages/NNS ./.
Experiment/NN results/NNS show/VBP that/IN the/DT proposed/VBN architecture/NN is/VBZ 8.3/CD x/SYM faster/RBR and/CC 75x/VB more/JJR energy/NN -/HYPH efficient/JJ than/IN a/DT Titan/NNP X/NNP GPU/NNP for/IN processing/VBG online/JJ individual/JJ requests/NNS (/-LRB- in/IN small/JJ batch/NN size/NN )/-RRB- ./.
For/IN processing/NN static/NN data/NNS (/-LRB- in/IN large/JJ batch/NN size/NN )/-RRB- ,/, the/DT proposed/VBN solution/NN is/VBZ on/IN a/DT par/NN with/IN a/DT Titan/NNP X/NNP GPU/NNP in/IN terms/NNS of/IN throughput/NN while/IN delivering/VBG 9.5/CD x/SYM higher/JJR energy/NN efficiency/NN ./.
