In/IN this/DT paper/NN we/PRP propose/VBP the/DT construction/NN of/IN linguistic/JJ descriptions/NNS of/IN images/NNS ./.
This/DT is/VBZ achieved/VBN through/IN the/DT extraction/NN of/IN scene/NN description/NN graphs/NNS (/-LRB- SDGs/NNS )/-RRB- from/IN visual/JJ scenes/NNS using/VBG an/DT automatically/RB constructed/VBN knowledge/NN base/NN ./.
SDGs/NNS are/VBP constructed/VBN using/VBG both/DT vision/NN and/CC reasoning/NN ./.
Specifically/RB ,/, commonsense/JJ reasoning/NN is/VBZ applied/VBN on/IN (/-LRB- a/DT )/-RRB- detections/NNS obtained/VBN from/IN existing/VBG perception/NN methods/NNS on/IN given/VBN images/NNS ,/, (/-LRB- b/LS )/-RRB- a/DT "/`` commonsense/JJ "/'' knowledge/NN base/NN constructed/VBN using/VBG natural/JJ language/NN processing/NN of/IN image/NN annotations/NNS and/CC (/-LRB- c/LS )/-RRB- lexical/JJ ontological/JJ knowledge/NN from/IN resources/NNS such/JJ as/IN WordNet/NNP ./.
Amazon/NNP Mechanical/NNP Turk/NNP (/-LRB- AMT/NNP )/-RRB- -/HYPH based/VBN evaluations/NNS on/IN Flickr8k/NN ,/, Flickr30k/NN and/CC MS/NN -/HYPH COCO/NN datasets/NNS show/VBP that/IN in/IN most/JJS cases/NNS ,/, sentences/NNS auto/NN -/HYPH constructed/VBN from/IN SDGs/NNS obtained/VBN by/IN our/PRP$ method/NN give/VB a/DT more/RBR relevant/JJ and/CC thorough/JJ description/NN of/IN an/DT image/NN than/IN a/DT recent/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN image/NN caption/NN based/VBN approach/NN ./.
Our/PRP$ Image/NN -/HYPH Sentence/NN Alignment/NN Evaluation/NN results/NNS are/VBP also/RB comparable/JJ to/IN that/DT of/IN the/DT recent/JJ state/NN -/HYPH of/IN -/HYPH the/DT art/NN approaches/NNS ./.
