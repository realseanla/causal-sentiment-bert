Recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- based/VBN character/NN -/HYPH level/NN language/NN models/NNS (/-LRB- CLMs/NNS )/-RRB- are/VBP extremely/RB useful/JJ for/IN modeling/VBG unseen/JJ words/NNS by/IN nature/NN ./.
However/RB ,/, their/PRP$ performance/NN is/VBZ generally/RB much/RB worse/JJR than/IN the/DT word/NN -/HYPH level/NN language/NN models/NNS (/-LRB- WLMs/NNS )/-RRB- ,/, since/IN CLMs/NNS need/VBP to/TO consider/VB longer/JJR history/NN of/IN tokens/NNS to/TO properly/RB predict/VB the/DT next/JJ one/NN ./.
We/PRP address/VBP this/DT problem/NN by/IN proposing/VBG hierarchical/JJ RNN/NN architectures/NNS ,/, which/WDT consist/VBP of/IN multiple/JJ modules/NNS with/IN different/JJ clock/NN rates/NNS ./.
Despite/IN the/DT multi-clock/JJ structures/NNS ,/, the/DT input/NN and/CC output/NN layers/NNS operate/VBP with/IN the/DT character/NN -/HYPH level/NN clock/NN ,/, which/WDT allows/VBZ the/DT existing/VBG RNN/NN CLM/NN training/NN approaches/VBZ to/TO be/VB directly/RB applicable/JJ without/IN any/DT modifications/NNS ./.
Our/PRP$ CLM/NN models/NNS show/VBP better/JJR perplexity/NN than/IN Kneser/NNP -/HYPH Ney/NNP (/-LRB- KN/NNP )/-RRB- 5/CD -/HYPH gram/NN WLMs/NNS on/IN the/DT One/CD Billion/CD Word/NNP Benchmark/NNP with/IN only/RB 2/CD percent/NN of/IN parameters/NNS ./.
Also/RB ,/, we/PRP present/VBP real/JJ -/HYPH time/NN character/NN -/HYPH level/NN end/NN -/HYPH to/IN -/HYPH end/NN speech/NN recognition/NN examples/NNS on/IN the/DT Wall/NNP Street/NNP Journal/NNP (/-LRB- WSJ/NNP )/-RRB- corpus/NN ,/, where/WRB replacing/VBG traditional/JJ mono/NN -/HYPH clock/NN RNN/NN CLMs/NNS with/IN the/DT proposed/VBN models/NNS results/VBZ in/IN better/JJR recognition/NN accuracies/NNS even/RB though/IN the/DT number/NN of/IN parameters/NNS are/VBP reduced/VBN to/IN 30/CD percent/NN ./.
