While/IN end/NN -/HYPH to/IN -/HYPH end/NN neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- has/VBZ made/VBN remarkable/JJ progress/NN recently/RB ,/, NMT/NN systems/NNS only/RB rely/VBP on/IN parallel/JJ corpora/NNS for/IN parameter/NN estimation/NN ./.
Since/IN parallel/JJ corpora/NNS are/VBP usually/RB limited/VBN in/IN quantity/NN ,/, quality/NN ,/, and/CC coverage/NN ,/, especially/RB for/IN low/JJ -/HYPH resource/NN languages/NNS ,/, it/PRP is/VBZ appealing/VBG to/TO exploit/VB monolingual/JJ corpora/NNS to/TO improve/VB NMT/NN ./.
We/PRP propose/VBP a/DT semi-supervised/JJ approach/NN for/IN training/NN NMT/NN models/NNS on/IN the/DT concatenation/NN of/IN labeled/VBN (/-LRB- parallel/JJ corpora/NN )/-RRB- and/CC unlabeled/JJ (/-LRB- monolingual/JJ corpora/NN )/-RRB- data/NNS ./.
The/DT central/JJ idea/NN is/VBZ to/TO reconstruct/VB the/DT monolingual/JJ corpora/NNS using/VBG an/DT autoencoder/NN ,/, in/IN which/WDT the/DT source/NN -/HYPH to/IN -/HYPH target/NN and/CC target/NN -/HYPH to/IN -/HYPH source/NN translation/NN models/NNS serve/VBP as/IN the/DT encoder/NN and/CC decoder/NN ,/, respectively/RB ./.
Our/PRP$ approach/NN can/MD not/RB only/RB exploit/VB the/DT monolingual/JJ corpora/NNS of/IN the/DT target/NN language/NN ,/, but/CC also/RB of/IN the/DT source/NN language/NN ./.
Experiments/NNS on/IN the/DT Chinese/JJ -/HYPH English/JJ dataset/NN show/VBP that/IN our/PRP$ approach/NN achieves/VBZ significant/JJ improvements/NNS over/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN SMT/NN and/CC NMT/NN systems/NNS ./.
