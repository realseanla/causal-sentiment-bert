Optimal/JJ probabilistic/JJ approach/NN in/IN reinforcement/NN learning/NN is/VBZ computationally/RB infeasible/JJ ./.
Its/PRP$ simplification/NN consisting/VBG in/IN neglecting/VBG difference/NN between/IN true/JJ environment/NN and/CC its/PRP$ model/NN estimated/VBN using/VBG limited/JJ number/NN of/IN observations/NNS causes/VBZ exploration/NN vs/IN exploitation/NN problem/NN ./.
Uncertainty/NN can/MD be/VB expressed/VBN in/IN terms/NNS of/IN a/DT probability/NN distribution/NN over/IN the/DT space/NN of/IN environment/NN models/NNS ,/, and/CC this/DT uncertainty/NN can/MD be/VB propagated/VBN to/IN the/DT action/NN -/HYPH value/NN function/NN via/IN Bellman/NNP iterations/NNS ,/, which/WDT are/VBP computationally/RB insufficiently/RB efficient/JJ though/RB ./.
We/PRP consider/VBP possibility/NN of/IN directly/RB measuring/VBG uncertainty/NN of/IN the/DT action/NN -/HYPH value/NN function/NN ,/, and/CC analyze/VB sufficiency/NN of/IN this/DT facilitated/VBD approach/NN ./.
