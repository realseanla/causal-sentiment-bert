We/PRP study/VBP the/DT problem/NN of/IN minimising/VBG regret/NN in/IN two/CD -/HYPH armed/JJ bandit/NN problems/NNS with/IN Gaussian/JJ rewards/NNS ./.
Our/PRP$ objective/NN is/VBZ to/TO use/VB this/DT simple/JJ setting/NN to/TO illustrate/VB that/DT strategies/NNS based/VBN on/IN an/DT exploration/NN phase/NN (/-LRB- up/RP to/IN a/DT stopping/VBG time/NN )/-RRB- followed/VBN by/IN exploitation/NN are/VBP necessarily/RB suboptimal/JJ ./.
The/DT results/NNS hold/VBP regardless/RB of/IN whether/IN or/CC not/RB the/DT difference/NN in/IN means/NNS between/IN the/DT two/CD arms/NNS is/VBZ known/VBN ./.
Besides/IN the/DT main/JJ message/NN ,/, we/PRP also/RB refine/VBP existing/VBG deviation/NN inequalities/NNS ,/, which/WDT allow/VBP us/PRP to/TO design/VB fully/RB sequential/JJ strategies/NNS with/IN finite/NN -/HYPH time/NN regret/NN guarantees/NNS that/WDT are/VBP (/-LRB- a/DT )/-RRB- asymptotically/RB optimal/JJ as/IN the/DT horizon/NN grows/VBZ and/CC (/-LRB- b/LS )/-RRB- order/NN -/HYPH optimal/JJ in/IN the/DT minimax/NN sense/NN ./.
Furthermore/RB we/PRP provide/VBP empirical/JJ evidence/NN that/IN the/DT theory/NN also/RB holds/VBZ in/IN practice/NN and/CC discuss/VB extensions/NNS to/IN non-gaussian/JJ and/CC multiple/JJ -/HYPH armed/JJ case/NN ./.
