This/DT work/NN focuses/VBZ on/IN dynamic/JJ regret/NN of/IN online/JJ convex/NN optimization/NN that/WDT compares/VBZ the/DT performance/NN of/IN online/JJ learning/NN to/IN a/DT clairvoyant/JJ who/WP knows/VBZ the/DT sequence/NN of/IN loss/NN functions/NNS in/IN advance/NN and/CC hence/RB selects/VBZ the/DT minimizer/NN of/IN the/DT loss/NN function/NN at/IN each/DT step/NN ./.
By/IN assuming/VBG that/IN the/DT clairvoyant/JJ moves/NNS slowly/RB (/-LRB- i.e./FW ,/, the/DT minimizers/NNS change/VBP slowly/RB )/-RRB- ,/, we/PRP present/VBP several/JJ improved/JJ variation/NN -/HYPH based/VBN upper/JJ bounds/NNS of/IN the/DT dynamic/JJ regret/NN under/IN the/DT true/JJ and/CC noisy/JJ gradient/NN feedback/NN ,/, which/WDT are/VBP {/-LRB- \/SYM it/PRP optimal/JJ }/-RRB- in/IN light/NN of/IN the/DT presented/VBN lower/JJR bounds/NNS ./.
The/DT key/NN to/IN our/PRP$ analysis/NN is/VBZ to/TO explore/VB a/DT regularity/NN metric/JJ that/WDT measures/VBZ the/DT temporal/JJ changes/NNS in/IN the/DT clairvoyant/NN 's/POS minimizers/NNS ,/, to/TO which/WDT we/PRP refer/VBP as/IN {/-LRB- \/SYM it/PRP path/NN variation/NN }/-RRB- ./.
Firstly/RB ,/, we/PRP present/VBP a/DT general/JJ lower/JJR bound/VBN in/IN terms/NNS of/IN the/DT path/NN variation/NN ,/, and/CC then/RB show/VBP that/IN under/IN full/JJ information/NN or/CC gradient/NN feedback/NN we/PRP are/VBP able/JJ to/TO achieve/VB an/DT optimal/JJ dynamic/JJ regret/NN ./.
Secondly/RB ,/, we/PRP present/VBP a/DT lower/JJR bound/VBN with/IN noisy/JJ gradient/NN feedback/NN and/CC then/RB show/VBP that/IN we/PRP can/MD achieve/VB optimal/JJ dynamic/JJ regrets/NNS under/IN a/DT stochastic/JJ gradient/NN feedback/NN and/CC two/CD -/HYPH point/NN bandit/NN feedback/NN ./.
Moreover/RB ,/, for/IN a/DT sequence/NN of/IN smooth/JJ loss/NN functions/NNS that/WDT admit/VBP a/DT small/JJ variation/NN in/IN the/DT gradients/NNS ,/, our/PRP$ dynamic/JJ regret/NN under/IN the/DT two/CD -/HYPH point/NN bandit/NN feedback/NN matches/VBZ what/WP is/VBZ achieved/VBN with/IN full/JJ information/NN ./.
