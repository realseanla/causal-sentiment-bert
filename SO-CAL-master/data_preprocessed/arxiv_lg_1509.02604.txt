The/DT alternating/VBG direction/NN method/NN of/IN multipliers/NNS (/-LRB- ADMM/NN )/-RRB- has/VBZ been/VBN recognized/VBN as/IN a/DT versatile/JJ approach/NN for/IN solving/VBG modern/JJ large/JJ -/HYPH scale/NN machine/NN learning/NN and/CC signal/NN processing/NN problems/NNS efficiently/RB ./.
When/WRB the/DT data/NNS size/NN and/CC //HYPH or/CC the/DT problem/NN dimension/NN is/VBZ large/JJ ,/, a/DT distributed/VBN version/NN of/IN ADMM/NNP can/MD be/VB used/VBN ,/, which/WDT is/VBZ capable/JJ of/IN distributing/VBG the/DT computation/NN load/NN and/CC the/DT data/NNS set/VBN to/IN a/DT network/NN of/IN computing/VBG nodes/NNS ./.
Unfortunately/RB ,/, a/DT direct/JJ synchronous/JJ implementation/NN of/IN such/JJ algorithm/NN does/VBZ not/RB scale/VB well/RB with/IN the/DT problem/NN size/NN ,/, as/IN the/DT algorithm/NN speed/NN is/VBZ limited/VBN by/IN the/DT slowest/JJS computing/NN nodes/NNS ./.
To/TO address/VB this/DT issue/NN ,/, in/IN a/DT companion/NN paper/NN ,/, we/PRP have/VBP proposed/VBN an/DT asynchronous/JJ distributed/VBN ADMM/NN (/-LRB- AD/NN -/HYPH ADMM/NN )/-RRB- and/CC studied/VBD its/PRP$ worst/JJS -/HYPH case/NN convergence/NN conditions/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP further/RB the/DT study/NN by/IN characterizing/VBG the/DT conditions/NNS under/IN which/WDT the/DT AD/NN -/HYPH ADMM/NN achieves/VBZ linear/JJ convergence/NN ./.
Our/PRP$ conditions/NNS as/RB well/RB as/IN the/DT resulting/VBG linear/JJ rates/NNS reveal/VBP the/DT impact/NN that/WDT various/JJ algorithm/NN parameters/NNS ,/, network/NN delay/NN and/CC network/NN size/NN have/VBP on/IN the/DT algorithm/NN performance/NN ./.
To/TO demonstrate/VB the/DT superior/JJ time/NN efficiency/NN of/IN the/DT proposed/VBN AD/NN -/HYPH ADMM/NN ,/, we/PRP test/VBP the/DT AD/NN -/HYPH ADMM/NN on/IN a/DT high/JJ -/HYPH performance/NN computer/NN cluster/NN by/IN solving/VBG a/DT large/JJ -/HYPH scale/NN logistic/JJ regression/NN problem/NN ./.
