We/PRP study/VBP nonconvex/JJ finite/NN -/HYPH sum/NN problems/NNS and/CC analyze/VB stochastic/JJ variance/NN reduced/VBN gradient/NN (/-LRB- SVRG/NN )/-RRB- methods/NNS for/IN them/PRP ./.
SVRG/NNP and/CC related/JJ methods/NNS have/VBP recently/RB surged/VBN into/IN prominence/NN for/IN convex/NN optimization/NN given/VBN their/PRP$ edge/NN over/IN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- ;/: but/CC their/PRP$ theoretical/JJ analysis/NN almost/RB exclusively/RB assumes/VBZ convexity/NN ./.
In/IN contrast/NN ,/, we/PRP prove/VBP non-asymptotic/JJ rates/NNS of/IN convergence/NN (/-LRB- to/IN stationary/JJ points/NNS )/-RRB- of/IN SVRG/NN for/IN nonconvex/JJ optimization/NN ,/, and/CC show/VBP that/IN it/PRP is/VBZ provably/RB faster/JJR than/IN SGD/NNP and/CC gradient/NN descent/NN ./.
We/PRP also/RB analyze/VB a/DT subclass/NN of/IN nonconvex/NN problems/NNS on/IN which/WDT SVRG/NNP attains/VBZ linear/JJ convergence/NN to/IN the/DT global/JJ optimum/JJ ./.
We/PRP extend/VBP our/PRP$ analysis/NN to/IN mini-batch/NN variants/NNS of/IN SVRG/NNP ,/, showing/VBG (/-LRB- theoretical/JJ )/-RRB- linear/JJ speedup/NN due/IN to/IN mini-batching/NN in/IN parallel/JJ settings/NNS ./.
