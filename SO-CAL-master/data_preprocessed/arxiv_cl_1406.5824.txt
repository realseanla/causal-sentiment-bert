In/IN this/DT paper/NN we/PRP present/VBP VideoSET/NNP ,/, a/DT method/NN for/IN Video/NNP Summary/NNP Evaluation/NN through/IN Text/VB that/DT can/MD evaluate/VB how/WRB well/RB a/DT video/NN summary/NN is/VBZ able/JJ to/TO retain/VB the/DT semantic/JJ information/NN contained/VBN in/IN its/PRP$ original/JJ video/NN ./.
We/PRP observe/VBP that/IN semantics/NNS is/VBZ most/RBS easily/RB expressed/VBN in/IN words/NNS ,/, and/CC develop/VB a/DT text/NN -/HYPH based/VBN approach/NN for/IN the/DT evaluation/NN ./.
Given/VBN a/DT video/NN summary/NN ,/, a/DT text/NN representation/NN of/IN the/DT video/NN summary/NN is/VBZ first/JJ generated/VBN ,/, and/CC an/DT NLP/NN -/HYPH based/VBN metric/JJ is/VBZ then/RB used/VBN to/TO measure/VB its/PRP$ semantic/JJ distance/NN to/IN ground/NN -/HYPH truth/NN text/NN summaries/NNS written/VBN by/IN humans/NNS ./.
We/PRP show/VBP that/IN our/PRP$ technique/NN has/VBZ higher/JJR agreement/NN with/IN human/JJ judgment/NN than/IN pixel/NN -/HYPH based/VBN distance/NN metrics/NNS ./.
We/PRP also/RB release/VB text/NN annotations/NNS and/CC ground/NN -/HYPH truth/NN text/NN summaries/NNS for/IN a/DT number/NN of/IN publicly/RB available/JJ video/NN datasets/NNS ,/, for/IN use/NN by/IN the/DT computer/NN vision/NN community/NN ./.
