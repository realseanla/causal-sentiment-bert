In/IN this/DT paper/NN ,/, we/PRP study/VBP a/DT variant/NN of/IN the/DT framework/NN of/IN online/JJ learning/NN using/VBG expert/NN advice/NN with/IN limited/JJ //HYPH bandit/NN feedback/NN ---/, we/PRP consider/VBP each/DT expert/NN a/DT learning/NN entity/NN and/CC thereby/RB capture/NN scenarios/NNS that/WDT are/VBP more/RBR realistic/JJ and/CC practical/JJ for/IN real/JJ -/HYPH world/NN applications/NNS ./.
In/IN our/PRP$ setting/NN ,/, the/DT feedback/NN at/IN any/DT time/NN $/$ t/CD $/$ is/VBZ limited/VBN in/IN a/DT sense/NN that/IN it/PRP is/VBZ only/RB available/JJ to/IN the/DT expert/NN $/$ i/CD ^/SYM t/NN $/$ that/WDT has/VBZ been/VBN selected/VBN by/IN the/DT central/JJ algorithm/NN (/-LRB- forecaster/NN )/-RRB- ,/, i.e./FW ,/, only/RB the/DT expert/NN $/$ i/CD ^/SYM t/NN $/$ receives/VBZ feedback/NN from/IN the/DT environment/NN and/CC gets/VBZ to/TO learn/VB at/IN time/NN $/$ t/CD $/$ ./.
We/PRP consider/VBP a/DT generic/JJ black/JJ -/HYPH box/NN approach/NN whereby/WRB the/DT forecaster/NN does/VBZ n't/RB control/VB or/CC know/VB the/DT learning/NN dynamics/NNS of/IN the/DT experts/NNS apart/RB from/IN knowing/VBG the/DT following/VBG no/DT -/HYPH regret/NN learning/NN property/NN :/: the/DT average/JJ regret/NN of/IN any/DT expert/JJ $/NN j/NN $/$ vanishes/VBZ at/IN a/DT rate/NN of/IN at/RB least/RBS $/$ O/UH (/-LRB- t_j/NN ^/SYM {/-LRB- \/SYM beta/NN -/HYPH 1/CD }/-RRB- )/-RRB- $/$ with/IN $/$ t_j/CD $/$ learning/VBG steps/NNS where/WRB $/$ \/SYM beta/NN \/SYM in/IN [/-LRB- 0/CD ,/, 1/CD ]/-RRB- $/$ is/VBZ a/DT parameter/NN ./.
We/PRP prove/VBP the/DT following/VBG hardness/NN result/NN :/: without/IN any/DT coordination/NN between/IN the/DT forecaster/NN and/CC the/DT experts/NNS ,/, it/PRP is/VBZ impossible/JJ to/TO design/VB a/DT forecaster/NN achieving/VBG no/DT -/HYPH regret/NN guarantees/NNS in/IN the/DT worst/JJS -/HYPH case/NN ./.
In/IN order/NN to/TO circumvent/VB this/DT hardness/NN result/NN ,/, we/PRP consider/VBP a/DT practical/JJ assumption/NN allowing/VBG the/DT forecaster/NN to/TO "/`` guide/VB "/'' the/DT learning/NN process/NN of/IN the/DT experts/NNS by/IN filtering/NN //HYPH blocking/VBG some/DT of/IN the/DT feedbacks/NNS observed/VBN by/IN them/PRP from/IN the/DT environment/NN ,/, i.e./FW ,/, not/RB allowing/VBG the/DT selected/VBN expert/NN $/$ i/CD ^/SYM t/NN $/$ to/TO learn/VB at/IN time/NN $/$ t/CD $/$ for/IN some/DT time/NN steps/NNS ./.
Then/RB ,/, we/PRP design/VBP a/DT novel/JJ no/NN -/HYPH regret/NN learning/NN algorithm/NN \/SYM algo/NN for/IN this/DT problem/NN setting/VBG by/IN carefully/RB guiding/VBG the/DT feedbacks/NNS observed/VBN by/IN experts/NNS ./.
We/PRP prove/VBP that/IN \/SYM algo/NN achieves/VBZ the/DT worst/JJS -/HYPH case/NN expected/VBN cumulative/JJ regret/NN of/IN $/$ O/UH (/-LRB- T/NN ^/SYM \/SYM frac/SYM {/-LRB- 1/CD }/-RRB- {/-LRB- 2/CD -/HYPH \/SYM beta/NN }/-RRB- )/-RRB- $/$ after/IN $/$ T$/CD time/NN steps/NNS and/CC matches/VBZ the/DT regret/NN bound/VBN of/IN $/$ \/SYM Theta/NNP (/-LRB- T/NNP ^/SYM \/SYM frac/SYM {/-LRB- 1/CD }/-RRB- {/-LRB- 2/CD }/-RRB- )/-RRB- $/$ for/IN the/DT special/JJ case/NN of/IN multi-armed/JJ bandits/NNS ./.
