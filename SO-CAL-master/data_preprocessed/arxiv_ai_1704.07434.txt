To/TO bridge/VB the/DT gap/NN between/IN humans/NNS and/CC machines/NNS in/IN image/NN understanding/NN and/CC describing/VBG ,/, we/PRP need/VBP further/JJ insight/NN into/IN how/WRB people/NNS describe/VBP a/DT perceived/VBN scene/NN ./.
In/IN this/DT paper/NN ,/, we/PRP study/VBP the/DT agreement/NN between/IN bottom/NN -/HYPH up/NN saliency/NN -/HYPH based/VBN visual/JJ attention/NN and/CC object/NN referrals/NNS in/IN scene/NN description/NN constructs/NNS ./.
We/PRP investigate/VBP the/DT properties/NNS of/IN human/JJ -/HYPH written/VBN descriptions/NNS and/CC machine/NN -/HYPH generated/VBN ones/NNS ./.
We/PRP then/RB propose/VB a/DT saliency/NN -/HYPH boosted/VBN image/NN captioning/NN model/NN in/IN order/NN to/TO investigate/VB benefits/NNS from/IN low/JJ -/HYPH level/NN cues/NNS in/IN language/NN models/NNS ./.
We/PRP learn/VBP that/IN (/-LRB- 1/CD )/-RRB- humans/NNS mention/VBP more/JJR salient/JJ objects/NNS earlier/RBR than/IN less/JJR salient/JJ ones/NNS in/IN their/PRP$ descriptions/NNS ,/, (/-LRB- 2/LS )/-RRB- the/DT better/JJR a/DT captioning/NN model/NN performs/VBZ ,/, the/DT better/JJR attention/NN agreement/NN it/PRP has/VBZ with/IN human/JJ descriptions/NNS ,/, (/-LRB- 3/LS )/-RRB- the/DT proposed/VBN saliency/NN -/HYPH boosted/VBN model/NN ,/, compared/VBN to/IN its/PRP$ baseline/NN form/NN ,/, does/VBZ not/RB improve/VB significantly/RB on/IN the/DT MS/NNP COCO/NNP database/NN ,/, indicating/VBG explicit/JJ bottom/NN -/HYPH up/NN boosting/VBG does/VBZ not/RB help/VB when/WRB the/DT task/NN is/VBZ well/RB learnt/VBN and/CC tuned/VBN on/IN a/DT data/NN ,/, (/-LRB- 4/LS )/-RRB- a/DT better/JJR generalization/NN ability/NN is/VBZ ,/, however/RB ,/, observed/VBN for/IN the/DT saliency/NN -/HYPH boosted/VBN model/NN on/IN unseen/JJ data/NNS ./.
