Traditional/JJ approaches/NNS to/IN Semantic/JJ Role/NN Labeling/NN (/-LRB- SRL/NN )/-RRB- depend/VBP heavily/RB on/IN manual/JJ feature/NN engineering/NN ./.
Recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- with/IN long/JJ -/HYPH short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- only/RB treats/VBZ sentence/NN as/IN sequence/NN data/NNS and/CC can/MD not/RB utilize/VB higher/JJR level/NN syntactic/JJ information/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP Syntax/NNP Aware/NNP LSTM/NNP (/-LRB- SA/NNP -/HYPH LSTM/NNP )/-RRB- which/WDT gives/VBZ RNN/NN -/HYPH LSTM/NN ability/NN to/TO utilize/VB higher/JJR level/NN syntactic/JJ information/NN gained/VBN from/IN dependency/NN relationship/NN information/NN ./.
SA/NNP -/HYPH LSTM/NNP also/RB assigns/VBZ different/JJ trainable/JJ weights/NNS to/IN different/JJ types/NNS of/IN dependency/NN relationship/NN automatically/RB ./.
Experiment/NN results/NNS on/IN Chinese/JJ Proposition/NNP Bank/NNP (/-LRB- CPB/NNP )/-RRB- show/VBP that/IN ,/, even/RB without/IN pre-training/VBG or/CC introducing/VBG any/DT other/JJ extra/JJ semantically/RB annotated/VBN resources/NNS ,/, our/PRP$ SA/NNP -/HYPH LSTM/NNP model/NN still/RB outperforms/VBZ the/DT state/NN of/IN the/DT art/NN significantly/RB base/NN on/IN Student/NNP 's/POS t/NN -/HYPH test/NN (/-LRB- $/$ p/NN &lt;/SYM 0.05/CD $/$ )/-RRB- ./.
Trained/VBN weights/NNS of/IN types/NNS of/IN dependency/NN relationship/NN form/VB a/DT stable/JJ and/CC self/NN -/HYPH explanatory/JJ pattern/NN ./.
