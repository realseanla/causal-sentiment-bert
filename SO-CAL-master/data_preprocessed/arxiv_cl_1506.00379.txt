Representation/NN learning/NN of/IN knowledge/NN bases/NNS (/-LRB- KBs/NNS )/-RRB- aims/VBZ to/TO embed/VB both/DT entities/NNS and/CC relations/NNS into/IN a/DT low/JJ -/HYPH dimensional/JJ space/NN ./.
Most/JJS existing/VBG methods/NNS only/RB consider/VBP direct/JJ relations/NNS in/IN representation/NN learning/NN ./.
We/PRP argue/VBP that/IN multiple/JJ -/HYPH step/NN relation/NN paths/NNS also/RB contain/VBP rich/JJ inference/NN patterns/NNS between/IN entities/NNS ,/, and/CC propose/VB a/DT path/NN -/HYPH based/VBN representation/NN learning/NN model/NN ./.
This/DT model/NN considers/VBZ relation/NN paths/NNS as/IN translations/NNS between/IN entities/NNS for/IN representation/NN learning/NN ,/, and/CC addresses/NNS two/CD key/JJ challenges/NNS :/: (/-LRB- 1/LS )/-RRB- Since/IN not/RB all/DT relation/NN paths/NNS are/VBP reliable/JJ ,/, we/PRP design/VBP a/DT path/NN -/HYPH constraint/NN resource/NN allocation/NN algorithm/NN to/TO measure/VB the/DT reliability/NN of/IN relation/NN paths/NNS ./.
(/-LRB- 2/LS )/-RRB- We/PRP represent/VBP relation/NN paths/NNS via/IN semantic/JJ composition/NN of/IN relation/NN embeddings/NNS ./.
Experimental/JJ results/NNS on/IN real/JJ -/HYPH world/NN datasets/NNS show/VBP that/IN ,/, as/IN compared/VBN with/IN baselines/NNS ,/, our/PRP$ model/NN achieves/VBZ significant/JJ and/CC consistent/JJ improvements/NNS on/IN knowledge/NN base/NN completion/NN and/CC relation/NN extraction/NN from/IN text/NN ./.
