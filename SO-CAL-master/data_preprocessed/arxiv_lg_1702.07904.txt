Variational/NNP autoencoders/NNS (/-LRB- VAE/NNP )/-RRB- often/RB use/VBP Gaussian/NNP or/CC category/NN distribution/NN to/IN model/NN the/DT inference/NN process/NN ./.
This/DT puts/VBZ a/DT limit/NN on/IN variational/JJ learning/NN because/IN this/DT simplified/VBN assumption/NN does/VBZ not/RB match/VB the/DT true/JJ posterior/JJ distribution/NN ,/, which/WDT is/VBZ usually/RB much/RB more/RBR sophisticated/JJ ./.
To/TO break/VB this/DT limitation/NN and/CC apply/VB arbitrary/JJ parametric/JJ distribution/NN during/IN inference/NN ,/, this/DT paper/NN derives/VBZ a/DT \/NN emph/NN {/-LRB- semi-continuous/JJ }/-RRB- latent/NN representation/NN ,/, which/WDT approximates/VBZ a/DT continuous/JJ density/NN up/IN to/IN a/DT prescribed/VBN precision/NN ,/, and/CC is/VBZ much/RB easier/JJR to/TO analyze/VB than/IN its/PRP$ continuous/JJ counterpart/NN because/IN it/PRP is/VBZ fundamentally/RB discrete/JJ ./.
We/PRP showcase/VBP the/DT proposition/NN by/IN applying/VBG polynomial/JJ exponential/JJ family/NN distributions/NNS as/IN the/DT posterior/JJ ,/, which/WDT are/VBP universal/JJ probability/NN density/NN function/NN generators/NNS ./.
Our/PRP$ experimental/JJ results/NNS show/VBP consistent/JJ improvements/NNS over/IN commonly/RB used/VBN VAE/NNP models/NNS ./.
