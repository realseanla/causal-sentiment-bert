The/DT optimization/NN of/IN deep/JJ neural/JJ networks/NNS can/MD be/VB more/RBR challenging/JJ than/IN traditional/JJ convex/NN optimization/NN problems/NNS due/IN to/IN the/DT highly/RB non-convex/JJ nature/NN of/IN the/DT loss/NN function/NN ,/, e.g./FW it/PRP can/MD involve/VB pathological/JJ landscapes/NNS such/JJ as/IN saddle/NN -/HYPH surfaces/NNS that/WDT can/MD be/VB difficult/JJ to/TO escape/VB for/IN algorithms/NNS based/VBN on/IN simple/JJ gradient/NN descent/NN ./.
In/IN this/DT paper/NN ,/, we/PRP attack/VBP the/DT problem/NN of/IN optimization/NN of/IN highly/RB non-convex/JJ neural/JJ networks/NNS by/IN starting/VBG with/IN a/DT smoothed/VBN --/: or/CC \/SYM textit/FW {/-LRB- mollified/VBN }/-RRB- --/: objective/JJ function/NN that/WDT gradually/RB has/VBZ a/DT more/RBR non-convex/JJ energy/NN landscape/NN during/IN the/DT training/NN ./.
Our/PRP$ proposition/NN is/VBZ inspired/VBN by/IN the/DT recent/JJ studies/NNS in/IN continuation/NN methods/NNS :/: similar/JJ to/IN curriculum/NN methods/NNS ,/, we/PRP begin/VBP learning/VBG an/DT easier/JJR (/-LRB- possibly/RB convex/NN )/-RRB- objective/JJ function/NN and/CC let/VB it/PRP evolve/VB during/IN the/DT training/NN ,/, until/IN it/PRP eventually/RB goes/VBZ back/RB to/IN being/VBG the/DT original/JJ ,/, difficult/JJ to/TO optimize/VB ,/, objective/JJ function/NN ./.
The/DT complexity/NN of/IN the/DT mollified/VBN networks/NNS is/VBZ controlled/VBN by/IN a/DT single/JJ hyperparameter/NN which/WDT is/VBZ annealed/VBN during/IN the/DT training/NN ./.
We/PRP show/VBP improvements/NNS on/IN various/JJ difficult/JJ optimization/NN tasks/NNS and/CC establish/VB a/DT relationship/NN with/IN recent/JJ works/NNS on/IN continuation/NN methods/NNS for/IN neural/JJ networks/NNS and/CC mollifiers/NNS ./.
