Recurrent/JJ Neural/JJ Network/NN (/-LRB- RNN/NN )/-RRB- are/VBP a/DT popular/JJ choice/NN for/IN modeling/VBG temporal/JJ and/CC sequential/JJ tasks/NNS and/CC achieve/VB many/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN various/JJ complex/JJ problems/NNS ./.
However/RB ,/, most/JJS of/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN RNNs/NNS have/VBP millions/NNS of/IN parameters/NNS and/CC require/VBP many/JJ computational/JJ resources/NNS for/IN training/NN and/CC predicting/VBG new/JJ data/NNS ./.
This/DT paper/NN proposes/VBZ an/DT alternative/JJ RNN/NN model/NN to/TO reduce/VB the/DT number/NN of/IN parameters/NNS significantly/RB by/IN representing/VBG the/DT weight/NN parameters/NNS based/VBN on/IN Tensor/NNP Train/NNP (/-LRB- TT/NN )/-RRB- format/NN ./.
In/IN this/DT paper/NN ,/, we/PRP implement/VBP the/DT TT/NN -/HYPH format/NN representation/NN for/IN several/JJ RNN/NN architectures/NNS such/JJ as/IN simple/JJ RNN/NN and/CC Gated/VBN Recurrent/JJ Unit/NN (/-LRB- GRU/NN )/-RRB- ./.
We/PRP compare/VBP and/CC evaluate/VBP our/PRP$ proposed/VBN RNN/NN model/NN with/IN uncompressed/JJ RNN/NN model/NN on/IN sequence/NN classification/NN and/CC sequence/NN prediction/NN tasks/NNS ./.
Our/PRP$ proposed/VBN RNNs/NNS with/IN TT/NN -/HYPH format/NN are/VBP able/JJ to/TO preserve/VB the/DT performance/NN while/IN reducing/VBG the/DT number/NN of/IN RNN/NN parameters/NNS significantly/RB up/RB to/IN 40/CD times/NNS smaller/JJR ./.
