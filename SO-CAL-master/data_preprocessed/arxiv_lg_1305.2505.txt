In/IN this/DT paper/NN ,/, we/PRP study/VBP the/DT generalization/NN properties/NNS of/IN online/JJ learning/NN based/VBN stochastic/JJ methods/NNS for/IN supervised/JJ learning/NN problems/NNS where/WRB the/DT loss/NN function/NN is/VBZ dependent/JJ on/IN more/JJR than/IN one/CD training/NN sample/NN (/-LRB- e.g./FW ,/, metric/JJ learning/NN ,/, ranking/VBG )/-RRB- ./.
We/PRP present/VBP a/DT generic/JJ decoupling/NN technique/NN that/WDT enables/VBZ us/PRP to/TO provide/VB Rademacher/NNP complexity/NN -/HYPH based/VBN generalization/NN error/NN bounds/NNS ./.
Our/PRP$ bounds/NNS are/VBP in/IN general/JJ tighter/JJR than/IN those/DT obtained/VBN by/IN Wang/NNP et/FW al/FW (/-LRB- COLT/NNP 2012/CD )/-RRB- for/IN the/DT same/JJ problem/NN ./.
Using/VBG our/PRP$ decoupling/VBG technique/NN ,/, we/PRP are/VBP further/RB able/JJ to/TO obtain/VB fast/JJ convergence/NN rates/NNS for/IN strongly/RB convex/JJ pairwise/JJ loss/NN functions/NNS ./.
We/PRP are/VBP also/RB able/JJ to/TO analyze/VB a/DT class/NN of/IN memory/NN efficient/JJ online/JJ learning/NN algorithms/NNS for/IN pairwise/JJ learning/NN problems/NNS that/WDT use/VBP only/RB a/DT bounded/VBN subset/NN of/IN past/JJ training/NN samples/NNS to/TO update/VB the/DT hypothesis/NN at/IN each/DT step/NN ./.
Finally/RB ,/, in/IN order/NN to/TO complement/VB our/PRP$ generalization/NN bounds/NNS ,/, we/PRP propose/VBP a/DT novel/JJ memory/NN efficient/JJ online/JJ learning/NN algorithm/NN for/IN higher/JJR order/NN learning/NN problems/NNS with/IN bounded/VBN regret/NN guarantees/NNS ./.
