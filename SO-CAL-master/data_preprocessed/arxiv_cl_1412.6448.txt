Neural/JJ language/NN models/NNS learn/VBP word/NN representations/NNS ,/, or/CC embeddings/NNS ,/, that/DT capture/NN rich/JJ linguistic/JJ and/CC conceptual/JJ information/NN ./.
Here/RB we/PRP investigate/VB the/DT embeddings/NNS learned/VBN by/IN neural/JJ machine/NN translation/NN models/NNS ,/, a/DT recently/RB -/HYPH developed/VBN class/NN of/IN neural/JJ language/NN model/NN ./.
We/PRP show/VBP that/IN embeddings/NNS from/IN translation/NN models/NNS outperform/VBP those/DT learned/VBN by/IN monolingual/JJ models/NNS at/IN tasks/NNS that/WDT require/VBP knowledge/NN of/IN both/DT conceptual/JJ similarity/NN and/CC lexical/JJ -/HYPH syntactic/JJ role/NN ./.
We/PRP further/RB show/VBP that/IN these/DT effects/NNS hold/VBP when/WRB translating/VBG from/IN both/DT English/NNP to/TO French/NNP and/CC English/NNP to/IN German/NNP ,/, and/CC argue/VBP that/IN the/DT desirable/JJ properties/NNS of/IN translation/NN embeddings/NNS should/MD emerge/VB largely/RB independently/RB of/IN the/DT source/NN and/CC target/NN languages/NNS ./.
Finally/RB ,/, we/PRP apply/VBP a/DT new/JJ method/NN for/IN training/NN neural/JJ translation/NN models/NNS with/IN very/RB large/JJ vocabularies/NNS ,/, and/CC show/VBP that/IN this/DT vocabulary/NN expansion/NN algorithm/NN results/VBZ in/IN minimal/JJ degradation/NN of/IN embedding/VBG quality/NN ./.
Our/PRP$ embedding/NN spaces/NNS can/MD be/VB queried/VBN in/IN an/DT online/JJ demo/NN and/CC downloaded/VBN from/IN our/PRP$ web/NN page/NN ./.
Overall/RB ,/, our/PRP$ analyses/NNS indicate/VBP that/IN translation/NN -/HYPH based/VBN embeddings/NNS should/MD be/VB used/VBN in/IN applications/NNS that/WDT require/VBP concepts/NNS to/TO be/VB organised/VBN according/VBG to/IN similarity/NN and/CC //HYPH or/CC lexical/JJ function/NN ,/, while/IN monolingual/JJ embeddings/NNS are/VBP better/RBR suited/JJ to/IN modelling/NN (/-LRB- nonspecific/JJ )/-RRB- inter-word/JJ relatedness/NN ./.
