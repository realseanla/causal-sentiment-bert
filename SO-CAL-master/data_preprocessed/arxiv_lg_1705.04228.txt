Given/VBN an/DT existing/VBG trained/VBN neural/JJ network/NN ,/, it/PRP is/VBZ often/RB desirable/JJ to/TO be/VB able/JJ to/TO add/VB new/JJ capabilities/NNS without/IN hindering/VBG performance/NN of/IN already/RB learned/VBN tasks/NNS ./.
Existing/VBG approaches/NNS either/CC learn/VBP sub-optimal/JJ solutions/NNS ,/, require/VBP joint/JJ training/NN ,/, or/CC incur/VB a/DT substantial/JJ increment/NN in/IN the/DT number/NN of/IN parameters/NNS for/IN each/DT added/VBN task/NN ,/, typically/RB as/RB many/JJ as/IN the/DT original/JJ network/NN ./.
We/PRP propose/VBP a/DT method/NN which/WDT fully/RB preserves/VBZ performance/NN on/IN the/DT original/JJ task/NN ,/, with/IN only/RB a/DT small/JJ increase/NN (/-LRB- around/IN 20/CD percent/NN )/-RRB- in/IN the/DT number/NN of/IN required/VBN parameters/NNS while/IN performing/VBG on/IN par/JJ with/IN more/RBR costly/JJ fine/JJ -/HYPH tuning/NN procedures/NNS ,/, which/WDT typically/RB double/VBP the/DT number/NN of/IN parameters/NNS ./.
The/DT learned/VBN architecture/NN can/MD be/VB controlled/VBN to/TO switch/VB between/IN various/JJ learned/VBN representations/NNS ,/, enabling/VBG a/DT single/JJ network/NN to/TO solve/VB a/DT task/NN from/IN multiple/JJ different/JJ domains/NNS ./.
We/PRP conduct/VBP extensive/JJ experiments/NNS showing/VBG the/DT effectiveness/NN of/IN our/PRP$ method/NN and/CC explore/VB different/JJ aspects/NNS of/IN its/PRP$ behavior/NN ./.
