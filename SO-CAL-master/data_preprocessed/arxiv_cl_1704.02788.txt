We/PRP present/VBP a/DT simple/JJ yet/CC effective/JJ approach/NN for/IN linking/VBG entities/NNS in/IN queries/NNS ./.
The/DT key/JJ idea/NN is/VBZ to/TO search/VB sentences/NNS similar/JJ to/IN a/DT query/NN from/IN Wikipedia/NNP articles/NNS and/CC directly/RB use/VB the/DT human/JJ -/HYPH annotated/JJ entities/NNS in/IN the/DT similar/JJ sentences/NNS as/IN candidate/NN entities/NNS for/IN the/DT query/NN ./.
Then/RB ,/, we/PRP employ/VBP a/DT rich/JJ set/NN of/IN features/NNS ,/, such/JJ as/IN link/NN -/HYPH probability/NN ,/, context/NN -/HYPH matching/NN ,/, word/NN embeddings/NNS ,/, and/CC relatedness/NN among/IN candidate/NN entities/NNS as/RB well/RB as/IN their/PRP$ related/JJ entities/NNS ,/, to/TO rank/VB the/DT candidates/NNS under/IN a/DT regression/NN based/VBN framework/NN ./.
The/DT advantages/NNS of/IN our/PRP$ approach/NN lie/NN in/IN two/CD aspects/NNS ,/, which/WDT contribute/VBP to/IN the/DT ranking/NN process/NN and/CC final/JJ linking/VBG result/NN ./.
First/RB ,/, it/PRP can/MD greatly/RB reduce/VB the/DT number/NN of/IN candidate/NN entities/NNS by/IN filtering/VBG out/RP irrelevant/JJ entities/NNS with/IN the/DT words/NNS in/IN the/DT query/NN ./.
Second/RB ,/, we/PRP can/MD obtain/VB the/DT query/NN sensitive/JJ prior/JJ probability/NN in/IN addition/NN to/IN the/DT static/NN link/NN -/HYPH probability/NN derived/VBN from/IN all/DT Wikipedia/NNP articles/NNS ./.
We/PRP conduct/VBP experiments/NNS on/IN two/CD benchmark/NN datasets/NNS on/IN entity/NN linking/VBG for/IN queries/NNS ,/, namely/RB the/DT ERD14/NN dataset/NN and/CC the/DT GERDAQ/NNP dataset/NN ./.
Experimental/JJ results/NNS show/VBP that/IN our/PRP$ method/NN outperforms/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS and/CC yields/NNS 75.0/CD percent/NN in/IN F1/NN on/IN the/DT ERD14/NN dataset/NN and/CC 56.9/CD percent/NN on/IN the/DT GERDAQ/NNP dataset/NN ./.
