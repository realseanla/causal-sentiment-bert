There/EX is/VBZ a/DT lot/NN of/IN research/NN interest/NN in/IN encoding/VBG variable/JJ length/NN sentences/NNS into/IN fixed/VBN length/NN vectors/NNS ,/, in/IN a/DT way/NN that/WDT preserves/VBZ the/DT sentence/NN meanings/NNS ./.
Two/CD common/JJ methods/NNS include/VBP representations/NNS based/VBN on/IN averaging/VBG word/NN vectors/NNS ,/, and/CC representations/NNS based/VBN on/IN the/DT hidden/JJ states/NNS of/IN recurrent/JJ neural/JJ networks/NNS such/JJ as/IN LSTMs/NNS ./.
The/DT sentence/NN vectors/NNS are/VBP used/VBN as/IN features/NNS for/IN subsequent/JJ machine/NN learning/NN tasks/NNS or/CC for/IN pre-training/VBG in/IN the/DT context/NN of/IN deep/JJ learning/NN ./.
However/RB ,/, not/RB much/RB is/VBZ known/VBN about/IN the/DT properties/NNS that/WDT are/VBP encoded/VBN in/IN these/DT sentence/NN representations/NNS and/CC about/IN the/DT language/NN information/NN they/PRP capture/VBP ./.
We/PRP propose/VBP a/DT framework/NN that/WDT facilitates/VBZ better/JJR understanding/NN of/IN the/DT encoded/VBN representations/NNS ./.
We/PRP define/VBP prediction/NN tasks/NNS around/IN isolated/VBN aspects/NNS of/IN sentence/NN structure/NN (/-LRB- namely/RB sentence/NN length/NN ,/, word/NN content/NN ,/, and/CC word/NN order/NN )/-RRB- ,/, and/CC score/NN representations/NNS by/IN the/DT ability/NN to/TO train/VB a/DT classifier/NN to/TO solve/VB each/DT prediction/NN task/NN when/WRB using/VBG the/DT representation/NN as/IN input/NN ./.
We/PRP demonstrate/VBP the/DT potential/JJ contribution/NN of/IN the/DT approach/NN by/IN analyzing/VBG different/JJ sentence/NN representation/NN mechanisms/NNS ./.
The/DT analysis/NN sheds/VBZ light/NN on/IN the/DT relative/JJ strengths/NNS of/IN different/JJ sentence/NN embedding/NN methods/NNS with/IN respect/NN to/IN these/DT low/JJ level/NN prediction/NN tasks/NNS ,/, and/CC on/IN the/DT effect/NN of/IN the/DT encoded/VBN vector/NN 's/POS dimensionality/NN on/IN the/DT resulting/VBG representations/NNS ./.
