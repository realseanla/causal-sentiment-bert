We/PRP study/VBP a/DT variant/NN of/IN the/DT contextual/JJ bandit/NN problem/NN ,/, where/WRB on/IN each/DT round/NN ,/, the/DT learner/NN plays/VBZ a/DT sequence/NN of/IN actions/NNS ,/, receives/VBZ a/DT feature/NN for/IN each/DT individual/JJ action/NN ,/, and/CC reward/VB that/DT is/VBZ linearly/RB related/VBN to/IN these/DT features/NNS ./.
This/DT setting/NN has/VBZ applications/NNS to/TO network/VB routing/NN ,/, crowd/NN -/HYPH sourcing/NN ,/, personalized/VBN search/NN ,/, and/CC many/JJ other/JJ domains/NNS ./.
If/IN the/DT linear/JJ transformation/NN is/VBZ known/VBN ,/, we/PRP analyze/VBP an/DT algorithm/NN that/WDT is/VBZ structurally/RB similar/JJ to/IN the/DT algorithm/NN of/IN Agarwal/NNP et/NNP a./NN [/-LRB- 2014/CD ]/-RRB- and/CC show/VBP that/IN it/PRP enjoys/VBZ a/DT regret/NN bound/VBN between/IN $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM sqrt/NN {/-LRB- KLT/NN \/SYM ln/NN N/NN }/-RRB- )/-RRB- $/$ and/CC $/$ \/CD tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- L/NN \/SYM sqrt/NN {/-LRB- KT/NNP \/SYM ln/NN N/NN }/-RRB- )/-RRB- $/$ ,/, where/WRB $/$ K$/CD is/VBZ the/DT number/NN of/IN actions/NNS ,/, $/$ L$/CD is/VBZ the/DT length/NN of/IN each/DT action/NN sequence/NN ,/, $/$ T$/CD is/VBZ the/DT number/NN of/IN rounds/NNS ,/, and/CC $/$ N$/CD is/VBZ the/DT number/NN of/IN policies/NNS ./.
If/IN the/DT linear/JJ transformation/NN is/VBZ unknown/JJ ,/, we/PRP show/VBP that/IN an/DT algorithm/NN that/WDT first/RB explores/VBZ to/TO learn/VB the/DT unknown/JJ weights/NNS via/IN linear/JJ regression/NN and/CC thereafter/RB uses/VBZ the/DT estimated/VBN weights/NNS can/MD achieve/VB $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/NFP |/NFP w/IN \/SYM |/NFP _/NFP 1/CD (/-LRB- KT/NNP )/-RRB- ^/SYM {/-LRB- 3/4/CD }/-RRB- \/SYM sqrt/NN {/-LRB- \/SYM ln/NN N/NN }/-RRB- )/-RRB- $/$ regret/NN ,/, where/WRB $/$ w/IN $/$ is/VBZ the/DT true/JJ (/-LRB- unknown/JJ )/-RRB- weight/NN vector/NN ./.
Both/DT algorithms/NNS use/VBP an/DT optimization/NN oracle/NN to/TO avoid/VB explicit/JJ enumeration/NN of/IN the/DT policies/NNS and/CC consequently/RB are/VBP computationally/RB efficient/JJ whenever/WRB an/DT efficient/JJ algorithm/NN for/IN the/DT fully/RB supervised/JJ setting/NN is/VBZ available/JJ ./.
