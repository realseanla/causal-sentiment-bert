We/PRP consider/VBP the/DT problem/NN of/IN optimizing/VBG the/DT sum/NN of/IN a/DT smooth/JJ convex/NN function/NN and/CC a/DT non-smooth/JJ convex/NN function/NN using/VBG proximal/JJ -/HYPH gradient/NN methods/NNS ,/, where/WRB an/DT error/NN is/VBZ present/JJ in/IN the/DT calculation/NN of/IN the/DT gradient/NN of/IN the/DT smooth/JJ term/NN or/CC in/IN the/DT proximity/NN operator/NN with/IN respect/NN to/IN the/DT non-smooth/JJ term/NN ./.
We/PRP show/VBP that/IN both/CC the/DT basic/JJ proximal/JJ -/HYPH gradient/NN method/NN and/CC the/DT accelerated/VBN proximal/JJ -/HYPH gradient/NN method/NN achieve/VB the/DT same/JJ convergence/NN rate/NN as/IN in/IN the/DT error/NN -/HYPH free/JJ case/NN ,/, provided/VBD that/IN the/DT errors/NNS decrease/VBP at/IN appropriate/JJ rates.Using/NN these/DT rates/NNS ,/, we/PRP perform/VBP as/RB well/RB as/IN or/CC better/JJR than/IN a/DT carefully/RB chosen/VBN fixed/VBN error/NN level/NN on/IN a/DT set/NN of/IN structured/JJ sparsity/NN problems/NNS ./.
