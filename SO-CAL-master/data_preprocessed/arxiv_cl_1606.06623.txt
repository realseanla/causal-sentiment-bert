We/PRP investigate/VBP the/DT integration/NN of/IN word/NN embeddings/NNS as/IN classification/NN features/NNS in/IN the/DT setting/NN of/IN large/JJ scale/NN text/NN classification/NN ./.
Such/JJ representations/NNS have/VBP been/VBN used/VBN in/IN a/DT plethora/NN of/IN tasks/NNS ,/, however/RB their/PRP$ application/NN in/IN classification/NN scenarios/NNS with/IN thousands/NNS of/IN classes/NNS has/VBZ not/RB been/VBN extensively/RB researched/VBN ,/, partially/RB due/IN to/IN hardware/NN limitations/NNS ./.
In/IN this/DT work/NN ,/, we/PRP examine/VBP efficient/JJ composition/NN functions/VBZ to/TO obtain/VB document/NN -/HYPH level/NN from/IN word/NN -/HYPH level/NN embeddings/NNS and/CC we/PRP subsequently/RB investigate/VB their/PRP$ combination/NN with/IN the/DT traditional/JJ one/CD -/HYPH hot/JJ -/HYPH encoding/VBG representations/NNS ./.
By/IN presenting/VBG empirical/JJ evidence/NN on/IN large/JJ ,/, multi-class/NN ,/, multi-label/JJ classification/NN problems/NNS ,/, we/PRP demonstrate/VBP the/DT efficiency/NN and/CC the/DT performance/NN benefits/NNS of/IN this/DT combination/NN ./.
