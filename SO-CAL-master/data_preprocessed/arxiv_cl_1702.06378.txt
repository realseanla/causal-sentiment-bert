Segmental/JJ conditional/JJ random/JJ fields/NNS (/-LRB- SCRFs/NNS )/-RRB- and/CC connectionist/NN temporal/JJ classification/NN (/-LRB- CTC/NN )/-RRB- are/VBP two/CD sequence/NN labeling/NN objectives/NNS used/VBN for/IN end/NN -/HYPH to/IN -/HYPH end/NN training/NN of/IN speech/NN recognition/NN models/NNS ./.
Both/DT models/NNS define/VBP the/DT transcription/NN probability/NN by/IN marginalizing/VBG decisions/NNS about/IN latent/NN segmentation/NN alternatives/NNS to/IN derive/VBP a/DT sequence/NN probability/NN :/: the/DT former/JJ uses/VBZ a/DT globally/RB normalized/VBN joint/JJ model/NN of/IN segment/NN labels/NNS and/CC durations/NNS ,/, and/CC the/DT latter/JJ classifies/VBZ each/DT frame/NN as/IN either/CC an/DT output/NN symbol/NN or/CC a/DT "/`` continuation/NN "/'' of/IN the/DT previous/JJ label/NN ./.
In/IN this/DT paper/NN ,/, we/PRP train/VBP a/DT recognition/NN model/NN by/IN optimizing/VBG an/DT interpolation/NN between/IN the/DT SCRF/NNP and/CC CTC/NNP losses/NNS ,/, where/WRB the/DT same/JJ recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- encoder/NN used/VBN for/IN feature/NN extraction/NN for/IN both/DT outputs/NNS ./.
We/PRP find/VBP that/IN this/DT multi-task/VB objective/NN improves/VBZ recognition/NN accuracy/NN when/WRB decoding/VBG with/IN either/CC the/DT SCRF/NN or/CC CTC/NN models/NNS ./.
Additionally/RB ,/, we/PRP show/VBP that/IN CTC/NN can/MD also/RB be/VB used/VBN to/TO pretrain/VB the/DT RNN/NNP encoder/NN ,/, which/WDT improves/VBZ the/DT convergence/NN rate/NN when/WRB learning/VBG the/DT joint/JJ model/NN ./.
