With/IN the/DT fast/JJ development/NN of/IN deep/JJ learning/NN ,/, people/NNS have/VBP started/VBN to/TO train/VB very/RB big/JJ neural/JJ networks/NNS using/VBG massive/JJ data/NNS ./.
Asynchronous/JJ Stochastic/JJ Gradient/NN Descent/NN (/-LRB- ASGD/NN )/-RRB- is/VBZ widely/RB used/VBN to/TO fulfill/VB this/DT task/NN ,/, which/WDT ,/, however/RB ,/, is/VBZ known/VBN to/TO suffer/VB from/IN the/DT problem/NN of/IN delayed/VBN gradient/NN ./.
That/DT is/VBZ ,/, when/WRB a/DT local/JJ worker/NN adds/VBZ the/DT gradient/NN it/PRP calculates/VBZ to/IN the/DT global/JJ model/NN ,/, the/DT global/JJ model/NN may/MD have/VB been/VBN updated/VBN by/IN other/JJ workers/NNS and/CC this/DT gradient/NN becomes/VBZ "/`` delayed/VBN "/'' ./.
We/PRP propose/VBP a/DT novel/JJ technology/NN to/TO compensate/VB this/DT delay/NN ,/, so/RB as/IN to/TO make/VB the/DT optimization/NN behavior/NN of/IN ASGD/NNP closer/RBR to/IN that/DT of/IN sequential/JJ SGD/NNP ./.
This/DT is/VBZ done/VBN by/IN leveraging/VBG Taylor/NNP expansion/NN of/IN the/DT gradient/NN function/NN and/CC efficient/JJ approximators/NNS to/IN the/DT Hessian/JJ matrix/NN of/IN the/DT loss/NN function/NN ./.
We/PRP call/VBP the/DT corresponding/VBG new/JJ algorithm/NN Delay/NNP Compensated/VBN ASGD/NN (/-LRB- DC/NNP -/HYPH ASGD/NNP )/-RRB- ./.
We/PRP evaluated/VBD the/DT proposed/VBN algorithm/NN on/IN CIFAR/NN -/HYPH 10/CD and/CC ImageNet/NNP datasets/NNS ,/, and/CC experimental/JJ results/NNS demonstrate/VBP that/IN DC/NNP -/HYPH ASGD/NNP can/MD outperform/VB both/DT synchronous/JJ SGD/NNP and/CC ASGD/NNP ,/, and/CC nearly/RB approaches/VBZ the/DT performance/NN of/IN sequential/JJ SGD/NNP ./.
