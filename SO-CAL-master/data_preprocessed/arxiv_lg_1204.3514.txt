We/PRP consider/VBP the/DT problem/NN of/IN PAC/NN -/HYPH learning/NN from/IN distributed/VBN data/NNS and/CC analyze/VB fundamental/JJ communication/NN complexity/NN questions/NNS involved/VBN ./.
In/IN addition/NN to/IN providing/VBG general/JJ upper/JJ and/CC lower/JJR bounds/NNS on/IN the/DT amount/NN of/IN communication/NN needed/VBN for/IN learning/NN ,/, we/PRP also/RB present/JJ tight/JJ results/NNS for/IN a/DT number/NN of/IN common/JJ concept/NN classes/NNS including/VBG conjunctions/NNS ,/, parity/NN functions/NNS ,/, and/CC decision/NN lists/NNS ./.
For/IN linear/JJ separators/NNS ,/, we/PRP show/VBP that/IN for/IN non-concentrated/JJ distributions/NNS ,/, we/PRP can/MD use/VB a/DT version/NN of/IN the/DT Perceptron/NNP algorithm/NN to/TO learn/VB with/IN much/RB less/JJR communication/NN than/IN the/DT number/NN of/IN updates/NNS given/VBN by/IN the/DT usual/JJ margin/NN bound/VBN ./.
Our/PRP$ general/JJ results/NNS show/VBP that/IN in/IN addition/NN to/IN VC/NNP -/HYPH dimension/NN and/CC covering/VBG number/NN ,/, quantities/NNS such/JJ as/IN the/DT teaching/NN -/HYPH dimension/NN and/CC mistake/NN -/HYPH bound/VBN of/IN a/DT class/NN play/VB an/DT important/JJ role/NN in/IN determining/VBG communication/NN requirements/NNS ./.
We/PRP also/RB show/VBP that/IN boosting/VBG can/MD be/VB performed/VBN in/IN a/DT generic/JJ manner/NN in/IN the/DT distributed/VBN setting/NN to/TO achieve/VB communication/NN with/IN only/RB logarithmic/JJ dependence/NN on/IN 1/CD //SYM epsilon/NN for/IN any/DT concept/NN class/NN ./.
We/PRP additionally/RB present/VBP an/DT analysis/NN of/IN privacy/NN ,/, considering/VBG both/CC differential/JJ privacy/NN and/CC a/DT notion/NN of/IN distributional/JJ privacy/NN that/WDT is/VBZ especially/RB appealing/VBG in/IN this/DT context/NN ./.
