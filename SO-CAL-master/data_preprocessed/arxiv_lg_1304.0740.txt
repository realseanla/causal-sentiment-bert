Traditional/JJ algorithms/NNS for/IN stochastic/JJ optimization/NN require/VBP projecting/VBG the/DT solution/NN at/IN each/DT iteration/NN into/IN a/DT given/VBN domain/NN to/TO ensure/VB its/PRP$ feasibility/NN ./.
When/WRB facing/VBG complex/JJ domains/NNS ,/, such/JJ as/IN positive/JJ semi-definite/JJ cones/NNS ,/, the/DT projection/NN operation/NN can/MD be/VB expensive/JJ ,/, leading/VBG to/IN a/DT high/JJ computational/JJ cost/NN per/IN iteration/NN ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT novel/JJ algorithm/NN that/WDT aims/VBZ to/TO reduce/VB the/DT number/NN of/IN projections/NNS for/IN stochastic/JJ optimization/NN ./.
The/DT proposed/VBN algorithm/NN combines/VBZ the/DT strength/NN of/IN several/JJ recent/JJ developments/NNS in/IN stochastic/JJ optimization/NN ,/, including/VBG mini-batch/NN ,/, extra-gradient/NN ,/, and/CC epoch/NN gradient/NN descent/NN ,/, in/IN order/NN to/TO effectively/RB explore/VB the/DT smoothness/NN and/CC strong/JJ convexity/NN ./.
We/PRP show/VBP ,/, both/CC in/IN expectation/NN and/CC with/IN a/DT high/JJ probability/NN ,/, that/IN when/WRB the/DT objective/JJ function/NN is/VBZ both/DT smooth/JJ and/CC strongly/RB convex/JJ ,/, the/DT proposed/VBN algorithm/NN achieves/VBZ the/DT optimal/JJ $/NN O/NN (/-LRB- 1/CD //SYM T/NN )/-RRB- $/$ rate/NN of/IN convergence/NN with/IN only/RB $/$ O/UH (/-LRB- \/SYM log/NN T/NN )/-RRB- $/$ projections/NNS ./.
Our/PRP$ empirical/JJ study/NN verifies/VBZ the/DT theoretical/JJ result/NN ./.
