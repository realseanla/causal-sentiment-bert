In/IN Imitation/NN Learning/NN ,/, a/DT supervisor/NN 's/POS policy/NN is/VBZ observed/VBN and/CC the/DT intended/JJ behavior/NN is/VBZ learned/VBN ./.
A/DT known/JJ problem/NN with/IN this/DT approach/NN is/VBZ covariate/NN shift/NN ,/, which/WDT occurs/VBZ because/IN the/DT agent/NN visits/VBZ different/JJ states/NNS than/IN the/DT supervisor/NN ./.
Rolling/JJ out/IN the/DT current/JJ agent/NN 's/POS policy/NN ,/, an/DT on/IN -/HYPH policy/NN method/NN ,/, allows/VBZ for/IN collecting/VBG data/NNS along/IN a/DT distribution/NN similar/JJ to/IN the/DT updated/VBN agent/NN 's/POS policy/NN ./.
However/RB this/DT approach/NN can/MD become/VB less/RBR effective/JJ as/IN the/DT demonstrations/NNS are/VBP collected/VBN in/IN very/RB large/JJ batch/NN sizes/NNS ,/, which/WDT reduces/VBZ the/DT relevance/NN of/IN data/NNS collected/VBN in/IN previous/JJ iterations/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO alleviate/VB the/DT covariate/NN shift/NN via/IN the/DT injection/NN of/IN artificial/JJ noise/NN into/IN the/DT supervisor/NN 's/POS policy/NN ./.
We/PRP prove/VBP an/DT improved/VBN bound/VBN on/IN the/DT loss/NN due/IN to/IN the/DT covariate/NN shift/NN ,/, and/CC introduce/VB an/DT algorithm/NN that/WDT leverages/VBZ our/PRP$ analysis/NN to/TO estimate/VB the/DT level/NN of/IN $/$ \/CD epsilon/CD $/$ -/HYPH greedy/JJ noise/NN to/TO inject/VB ./.
In/IN a/DT driving/VBG simulator/NN domain/NN where/WRB an/DT agent/NN learns/VBZ an/DT image/NN -/HYPH to/IN -/HYPH action/NN deep/JJ network/NN policy/NN ,/, our/PRP$ algorithm/NN Dart/NNP achieves/VBZ a/DT better/JJR performance/NN than/IN DAgger/NN with/IN 75/CD percent/NN fewer/JJR demonstrations/NNS ./.
