Text/VB simplification/NN (/-LRB- TS/NN )/-RRB- aims/VBZ to/TO reduce/VB the/DT lexical/JJ and/CC structural/JJ complexity/NN of/IN a/DT text/NN ,/, while/IN still/RB retaining/VBG the/DT semantic/JJ meaning/NN ./.
Current/JJ automatic/JJ TS/NN techniques/NNS are/VBP limited/VBN to/IN either/CC lexical/JJ -/HYPH level/NN applications/NNS or/CC manually/RB defining/VBG a/DT large/JJ amount/NN of/IN rules/NNS ./.
Since/IN deep/JJ neural/JJ networks/NNS are/VBP powerful/JJ models/NNS that/WDT have/VBP achieved/VBN excellent/JJ performance/NN over/IN many/JJ difficult/JJ tasks/NNS ,/, in/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO use/VB the/DT Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- Encoder/NN -/HYPH Decoder/NN model/NN for/IN sentence/NN level/NN TS/NNP ,/, which/WDT makes/VBZ minimal/JJ assumptions/NNS about/IN word/NN sequence/NN ./.
We/PRP conduct/VBP preliminary/JJ experiments/NNS to/TO find/VB that/IN the/DT model/NN is/VBZ able/JJ to/TO learn/VB operation/NN rules/NNS such/JJ as/IN reversing/VBG ,/, sorting/VBG and/CC replacing/VBG from/IN sequence/NN pairs/NNS ,/, which/WDT shows/VBZ that/IN the/DT model/NN may/MD potentially/RB discover/VB and/CC apply/VB rules/NNS such/JJ as/IN modifying/VBG sentence/NN structure/NN ,/, substituting/VBG words/NNS ,/, and/CC removing/VBG words/NNS for/IN TS/NNP ./.
