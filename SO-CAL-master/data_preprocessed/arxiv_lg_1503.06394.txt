Logarithms/NNS of/IN determinants/NNS of/IN large/JJ positive/JJ definite/JJ matrices/NNS appear/VBP ubiquitously/RB in/IN machine/NN learning/NN applications/NNS including/VBG Gaussian/JJ graphical/JJ and/CC Gaussian/JJ process/NN models/NNS ,/, partition/NN functions/NNS of/IN discrete/JJ graphical/JJ models/NNS ,/, minimum/JJ -/HYPH volume/NN ellipsoids/NNS ,/, metric/JJ learning/NN and/CC kernel/NN learning/NN ./.
Log/VB -/HYPH determinant/NN computation/NN involves/VBZ the/DT Cholesky/NNP decomposition/NN at/IN the/DT cost/NN cubic/JJ in/IN the/DT number/NN of/IN variables/NNS ,/, i.e./FW ,/, the/DT matrix/NN dimension/NN ,/, which/WDT makes/VBZ it/PRP prohibitive/JJ for/IN large/JJ -/HYPH scale/NN applications/NNS ./.
We/PRP propose/VBP a/DT linear/JJ -/HYPH time/NN randomized/VBD algorithm/NN to/IN approximate/JJ log/NN -/HYPH determinants/NNS for/IN very/RB large/JJ -/HYPH scale/NN positive/JJ definite/JJ and/CC general/JJ non-singular/JJ matrices/NNS using/VBG a/DT stochastic/JJ trace/NN approximation/NN ,/, called/VBD the/DT Hutchinson/NNP method/NN ,/, coupled/VBN with/IN Chebyshev/NNP polynomial/JJ expansions/NNS that/IN both/DT rely/VBP on/IN efficient/JJ matrix/NN -/HYPH vector/NN multiplications/NNS ./.
We/PRP establish/VBP rigorous/JJ additive/JJ and/CC multiplicative/JJ approximation/NN error/NN bounds/NNS depending/VBG on/IN the/DT condition/NN number/NN of/IN the/DT input/NN matrix/NN ./.
In/IN our/PRP$ experiments/NNS ,/, the/DT proposed/VBN algorithm/NN can/MD provide/VB very/RB high/JJ accuracy/NN solutions/NNS at/IN orders/NNS of/IN magnitude/NN faster/RBR time/NN than/IN the/DT Cholesky/NNP decomposition/NN and/CC Schur/NNP completion/NN ,/, and/CC enables/VBZ us/PRP to/TO compute/VB log/NN -/HYPH determinants/NNS of/IN matrices/NNS involving/VBG tens/NNS of/IN millions/NNS of/IN variables/NNS ./.
