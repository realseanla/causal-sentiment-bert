Recent/JJ papers/NNS have/VBP shown/VBN that/IN neural/JJ networks/NNS obtain/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN several/JJ different/JJ sequence/NN tagging/NN tasks/NNS ./.
One/CD appealing/JJ property/NN of/IN such/JJ systems/NNS is/VBZ their/PRP$ generality/NN ,/, as/IN excellent/JJ performance/NN can/MD be/VB achieved/VBN with/IN a/DT unified/JJ architecture/NN and/CC without/IN task/NN -/HYPH specific/JJ feature/NN engineering/NN ./.
However/RB ,/, it/PRP is/VBZ unclear/JJ if/IN such/JJ systems/NNS can/MD be/VB used/VBN for/IN tasks/NNS without/IN large/JJ amounts/NNS of/IN training/NN data/NNS ./.
In/IN this/DT paper/NN we/PRP explore/VBP the/DT problem/NN of/IN transfer/NN learning/NN for/IN neural/JJ sequence/NN taggers/NNS ,/, where/WRB a/DT source/NN task/NN with/IN plentiful/JJ annotations/NNS (/-LRB- e.g./FW ,/, POS/NN tagging/VBG on/IN Penn/NNP Treebank/NNP )/-RRB- is/VBZ used/VBN to/TO improve/VB performance/NN on/IN a/DT target/NN task/NN with/IN fewer/JJR available/JJ annotations/NNS (/-LRB- e.g./FW ,/, POS/NN tagging/VBG for/IN microblogs/NNS )/-RRB- ./.
We/PRP examine/VBP the/DT effects/NNS of/IN transfer/NN learning/NN for/IN deep/JJ hierarchical/JJ recurrent/JJ networks/NNS across/IN domains/NNS ,/, applications/NNS ,/, and/CC languages/NNS ,/, and/CC show/VBP that/IN significant/JJ improvement/NN can/MD often/RB be/VB obtained/VBN ./.
These/DT improvements/NNS lead/VBP to/IN improvements/NNS over/IN the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN on/IN several/JJ well/RB -/HYPH studied/VBN tasks/NNS ./.
