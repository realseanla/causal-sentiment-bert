We/PRP show/VBP that/IN the/DT exponential/JJ convergence/NN rate/NN of/IN stochastic/JJ gradient/NN descent/NN for/IN smooth/JJ strongly/RB convex/JJ objectives/NNS can/MD be/VB markedly/RB improved/VBN by/IN perturbing/VBG the/DT row/NN selection/NN rule/NN in/IN the/DT direction/NN of/IN sampling/NN estimates/NNS proportionally/RB to/IN the/DT Lipschitz/NNP constants/NNS of/IN their/PRP$ gradients/NNS ./.
That/DT is/VBZ ,/, we/PRP show/VBP that/IN partially/RB biased/JJ sampling/NN allows/VBZ a/DT convergence/NN rate/NN with/IN linear/JJ dependence/NN on/IN the/DT average/JJ condition/NN number/NN of/IN the/DT system/NN ,/, compared/VBN to/IN dependence/NN on/IN the/DT average/JJ squared/JJ condition/NN number/NN for/IN standard/JJ stochastic/JJ gradient/NN descent/NN ./.
We/PRP assume/VBP the/DT regime/NN where/WRB all/DT stochastic/JJ estimates/NNS share/VBP an/DT optimum/JJ and/CC so/RB such/JJ an/DT exponential/JJ rate/NN is/VBZ possible/JJ ./.
We/PRP then/RB recast/VBD the/DT randomized/JJ Kaczmarz/NNP algorithm/NN for/IN solving/VBG overdetermined/JJ linear/JJ systems/NNS as/IN an/DT instance/NN of/IN preconditioned/VBN stochastic/JJ gradient/NN descent/NN ,/, and/CC apply/VB our/PRP$ results/NNS to/TO prove/VB its/PRP$ exponential/JJ convergence/NN ,/, but/CC to/IN the/DT solution/NN of/IN a/DT weighted/JJ least/JJS squares/NNS problem/NN rather/RB than/IN the/DT original/JJ least/JJS squares/NNS problem/NN ./.
We/PRP present/VBP a/DT modified/VBN Kaczmarz/NNP algorithm/NN with/IN partially/RB biased/JJ sampling/NN which/WDT does/VBZ converge/VB to/IN the/DT original/JJ least/JJS squares/NNS solution/NN with/IN the/DT same/JJ exponential/JJ convergence/NN rate/NN ./.
