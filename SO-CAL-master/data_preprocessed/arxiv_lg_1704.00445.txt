We/PRP consider/VBP the/DT stochastic/JJ bandit/NN problem/NN with/IN a/DT continuous/JJ set/NN of/IN arms/NNS ,/, with/IN the/DT expected/VBN reward/NN function/NN over/IN the/DT arms/NNS assumed/VBD to/TO be/VB fixed/VBN but/CC unknown/JJ ./.
We/PRP provide/VBP two/CD new/JJ Gaussian/JJ process/NN -/HYPH based/VBN algorithms/NNS for/IN continuous/JJ bandit/NN optimization/NN -/HYPH Improved/VBN GP/NNP -/HYPH UCB/NNP (/-LRB- IGP/NNP -/HYPH UCB/NNP )/-RRB- and/CC GP/NNP -/HYPH Thomson/NNP sampling/NN (/-LRB- GP/NNP -/HYPH TS/NNP )/-RRB- ,/, and/CC derive/VBP corresponding/VBG regret/NN bounds/NNS ./.
Specifically/RB ,/, the/DT bounds/NNS hold/VBP when/WRB the/DT expected/VBN reward/NN function/NN belongs/VBZ to/IN the/DT reproducing/VBG kernel/NN Hilbert/NNP space/NN (/-LRB- RKHS/NN )/-RRB- that/WDT naturally/RB corresponds/VBZ to/IN a/DT Gaussian/JJ process/NN kernel/NN used/VBN as/IN input/NN by/IN the/DT algorithms/NNS ./.
Along/IN the/DT way/NN ,/, we/PRP derive/VBP a/DT new/JJ self/NN -/HYPH normalized/VBN concentration/NN inequality/NN for/IN vector/NN -/HYPH valued/VBN martingales/NNS of/IN arbitrary/JJ ,/, possibly/RB infinite/JJ ,/, dimension/NN ./.
Finally/RB ,/, experimental/JJ evaluation/NN and/CC comparisons/NNS to/IN existing/VBG algorithms/NNS on/IN synthetic/JJ and/CC real/JJ -/HYPH world/NN environments/NNS are/VBP carried/VBN out/RP that/IN highlight/NN the/DT favorable/JJ gains/NNS of/IN the/DT proposed/VBN strategies/NNS in/IN many/JJ cases/NNS ./.
