We/PRP introduce/VBP a/DT new/JJ surrogate/JJ loss/NN function/NN called/VBN orbit/NN loss/NN in/IN the/DT structured/JJ prediction/NN framework/NN ,/, which/WDT has/VBZ good/JJ theoretical/JJ and/CC practical/JJ advantages/NNS ./.
While/IN the/DT orbit/NN loss/NN is/VBZ not/RB convex/JJ ,/, it/PRP has/VBZ a/DT simple/JJ analytical/JJ gradient/NN and/CC a/DT simple/JJ perceptron/NN -/HYPH like/JJ learning/NN rule/NN ./.
We/PRP analyze/VBP the/DT new/JJ loss/NN theoretically/RB and/CC state/VB a/DT PAC/NN -/HYPH Bayesian/JJ generalization/NN bound/VBN ./.
We/PRP also/RB prove/VBP that/IN the/DT new/JJ loss/NN is/VBZ consistent/JJ in/IN the/DT strong/JJ sense/NN ;/: namely/RB ,/, the/DT risk/NN achieved/VBN by/IN the/DT set/NN of/IN the/DT trained/VBN parameters/NNS approaches/VBZ the/DT infimum/JJ risk/NN achievable/JJ by/IN any/DT linear/JJ decoder/NN over/IN the/DT given/VBN features/NNS ./.
Methods/NNS that/WDT are/VBP aimed/VBN at/IN risk/NN minimization/NN ,/, such/JJ as/IN the/DT structured/JJ ramp/NN loss/NN ,/, the/DT structured/JJ probit/NN loss/NN and/CC the/DT direct/JJ loss/NN minimization/NN require/VBP at/IN least/RBS two/CD inference/NN operations/NNS per/IN training/NN iteration/NN ./.
In/IN this/DT sense/NN ,/, the/DT orbit/NN loss/NN is/VBZ more/RBR efficient/JJ as/IN it/PRP requires/VBZ only/RB one/CD inference/NN operation/NN per/IN training/NN iteration/NN ,/, while/IN yields/NNS similar/JJ performance/NN ./.
We/PRP conclude/VBP the/DT paper/NN with/IN an/DT empirical/JJ comparison/NN of/IN the/DT proposed/JJ loss/NN function/NN to/IN the/DT structured/JJ hinge/NN loss/NN ,/, the/DT structured/JJ ramp/NN loss/NN ,/, the/DT structured/JJ probit/NN loss/NN and/CC the/DT direct/JJ loss/NN minimization/NN method/NN on/IN several/JJ benchmark/NN datasets/NNS and/CC tasks/NNS ./.
