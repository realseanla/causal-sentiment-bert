The/DT classification/NN of/IN high/JJ dimensional/JJ data/NNS with/IN kernel/NN methods/NNS is/VBZ considered/VBN in/IN this/DT article/NN ./.
Exploit/VB -/HYPH ing/VBG the/DT emptiness/NN property/NN of/IN high/JJ dimensional/JJ spaces/NNS ,/, a/DT kernel/NN based/VBN on/IN the/DT Mahalanobis/NNPS distance/NN is/VBZ proposed/VBN ./.
The/DT computation/NN of/IN the/DT Mahalanobis/NNPS distance/NN requires/VBZ the/DT inversion/NN of/IN a/DT covariance/NN matrix/NN ./.
In/IN high/JJ dimensional/JJ spaces/NNS ,/, the/DT estimated/VBN covariance/NN matrix/NN is/VBZ ill/RB -/HYPH conditioned/VBN and/CC its/PRP$ inversion/NN is/VBZ unstable/JJ or/CC impossible/JJ ./.
Using/VBG a/DT parsimonious/JJ statistical/JJ model/NN ,/, namely/RB the/DT High/JJ Dimensional/NNP Discriminant/NNP Analysis/NNP model/NN ,/, the/DT specific/JJ signal/NN and/CC noise/NN subspaces/NNS are/VBP estimated/VBN for/IN each/DT considered/VBN class/NN making/VBG the/DT inverse/NN of/IN the/DT class/NN specific/JJ covariance/NN matrix/NN explicit/JJ and/CC stable/JJ ,/, leading/VBG to/IN the/DT definition/NN of/IN a/DT parsimonious/JJ Mahalanobis/NNP kernel/NN ./.
A/DT SVM/NN based/VBN framework/NN is/VBZ used/VBN for/IN selecting/VBG the/DT hyperparameters/NNS of/IN the/DT parsimonious/JJ Mahalanobis/NNP kernel/NN by/IN optimizing/VBG the/DT so/RB -/HYPH called/VBN radius/NN -/HYPH margin/NN bound/VBN ./.
Experimental/JJ results/NNS on/IN three/CD high/JJ dimensional/JJ data/NNS sets/NNS show/VBP that/IN the/DT proposed/VBN kernel/NN is/VBZ suitable/JJ for/IN classifying/VBG high/JJ dimensional/JJ data/NNS ,/, providing/VBG better/JJR classification/NN accuracies/NNS than/IN the/DT conventional/JJ Gaussian/JJ kernel/NN ./.
