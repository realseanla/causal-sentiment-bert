The/DT goal/NN of/IN a/DT learner/NN in/IN standard/JJ online/JJ learning/NN is/VBZ to/TO maintain/VB an/DT average/JJ loss/NN close/NN to/IN the/DT loss/NN of/IN the/DT best/RBS -/HYPH performing/VBG single/JJ function/NN in/IN some/DT class/NN ./.
In/IN many/JJ real/JJ -/HYPH world/NN problems/NNS ,/, such/JJ as/IN rating/NN or/CC ranking/VBG items/NNS ,/, there/EX is/VBZ no/DT single/JJ best/JJS target/NN function/NN during/IN the/DT runtime/NN of/IN the/DT algorithm/NN ,/, instead/RB the/DT best/JJS (/-LRB- local/JJ )/-RRB- target/NN function/NN is/VBZ drifting/VBG over/IN time/NN ./.
We/PRP develop/VBP a/DT novel/NN last/JJ -/HYPH step/NN minmax/NN optimal/JJ algorithm/NN in/IN context/NN of/IN a/DT drift/NN ./.
We/PRP analyze/VBP the/DT algorithm/NN in/IN the/DT worst/JJS -/HYPH case/NN regret/NN framework/NN and/CC show/VBP that/IN it/PRP maintains/VBZ an/DT average/JJ loss/NN close/NN to/IN that/DT of/IN the/DT best/JJS slowly/RB changing/VBG sequence/NN of/IN linear/JJ functions/NNS ,/, as/RB long/RB as/IN the/DT total/NN of/IN drift/NN is/VBZ sublinear/NN ./.
In/IN some/DT situations/NNS ,/, our/PRP$ bound/JJ improves/VBZ over/IN existing/VBG bounds/NNS ,/, and/CC additionally/RB the/DT algorithm/NN suffers/VBZ logarithmic/JJ regret/NN when/WRB there/EX is/VBZ no/DT drift/NN ./.
We/PRP also/RB build/VBP on/IN the/DT H_infinity/NN filter/NN and/CC its/PRP$ bound/VBN ,/, and/CC develop/VB and/CC analyze/VB a/DT second/JJ algorithm/NN for/IN drifting/VBG setting/NN ./.
Synthetic/JJ simulations/NNS demonstrate/VBP the/DT advantages/NNS of/IN our/PRP$ algorithms/NNS in/IN a/DT worst/RBS -/HYPH case/NN constant/JJ drift/NN setting/NN ./.
