Low/JJ dimensional/JJ representations/NNS of/IN words/NNS allow/VBP accurate/JJ NLP/NN models/NNS to/TO be/VB trained/VBN on/IN limited/JJ annotated/JJ data/NNS ./.
While/IN most/JJS representations/NNS ignore/VBP words/NNS '/POS local/JJ context/NN ,/, a/DT natural/JJ way/NN to/TO induce/VB context/NN -/HYPH dependent/JJ representations/NNS is/VBZ to/TO perform/VB inference/NN in/IN a/DT probabilistic/JJ latent/NN -/HYPH variable/JJ sequence/NN model/NN ./.
Given/VBN the/DT recent/JJ success/NN of/IN continuous/JJ vector/NN space/NN word/NN representations/NNS ,/, we/PRP provide/VBP such/PDT an/DT inference/NN procedure/NN for/IN continuous/JJ states/NNS ,/, where/WRB words/NNS '/POS representations/NNS are/VBP given/VBN by/IN the/DT posterior/JJ mean/NN of/IN a/DT linear/JJ dynamical/JJ system/NN ./.
Here/RB ,/, efficient/JJ inference/NN can/MD be/VB performed/VBN using/VBG Kalman/NNP filtering/NN ./.
Our/PRP$ learning/NN algorithm/NN is/VBZ extremely/RB scalable/JJ ,/, operating/VBG on/IN simple/JJ cooccurrence/NN counts/NNS for/IN both/DT parameter/NN initialization/NN using/VBG the/DT method/NN of/IN moments/NNS and/CC subsequent/JJ iterations/NNS of/IN EM/NNP ./.
In/IN our/PRP$ experiments/NNS ,/, we/PRP employ/VBP our/PRP$ inferred/VBN word/NN embeddings/NNS as/IN features/NNS in/IN standard/JJ tagging/NN tasks/NNS ,/, obtaining/VBG significant/JJ accuracy/NN improvements/NNS ./.
Finally/RB ,/, the/DT Kalman/NNP filter/NN updates/NNS can/MD be/VB seen/VBN as/IN a/DT linear/JJ recurrent/JJ neural/JJ network/NN ./.
We/PRP demonstrate/VBP that/IN using/VBG the/DT parameters/NNS of/IN our/PRP$ model/NN to/IN initialize/VB a/DT non-linear/JJ recurrent/JJ neural/JJ network/NN language/NN model/NN reduces/VBZ its/PRP$ training/NN time/NN by/IN a/DT day/NN and/CC yields/NNS lower/JJR perplexity/NN ./.
