Currently/RB successful/JJ methods/NNS for/IN video/NN description/NN are/VBP based/VBN on/IN encoder/NN -/HYPH decoder/NN sentence/NN generation/NN using/VBG recur/NN -/HYPH rent/NN neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- ./.
Recent/JJ work/NN has/VBZ shown/VBN the/DT advantage/NN of/IN integrating/VBG temporal/JJ and/CC //HYPH or/CC spatial/JJ attention/NN mechanisms/NNS into/IN these/DT models/NNS ,/, in/IN which/WDT the/DT decoder/NN net/NN -/HYPH work/NN predicts/VBZ each/DT word/NN in/IN the/DT description/NN by/IN selectively/RB giving/VBG more/JJR weight/NN to/IN encoded/VBN features/NNS from/IN specific/JJ time/NN frames/NNS (/-LRB- temporal/JJ attention/NN )/-RRB- or/CC to/IN features/NNS from/IN specific/JJ spatial/JJ regions/NNS (/-LRB- spatial/JJ attention/NN )/-RRB- ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO expand/VB the/DT attention/NN model/NN to/TO selectively/RB attend/VB not/RB just/RB to/IN specific/JJ times/NNS or/CC spatial/JJ regions/NNS ,/, but/CC to/TO specific/JJ modalities/NNS of/IN input/NN such/JJ as/IN image/NN features/NNS ,/, motion/NN features/NNS ,/, and/CC audio/JJ features/NNS ./.
Our/PRP$ new/JJ modality/NN -/HYPH dependent/JJ attention/NN mechanism/NN ,/, which/WDT we/PRP call/VBP multimodal/JJ attention/NN ,/, provides/VBZ a/DT natural/JJ way/NN to/IN fuse/NN multimodal/JJ information/NN for/IN video/NN description/NN ./.
We/PRP evaluate/VBP our/PRP$ method/NN on/IN the/DT Youtube2Text/NN dataset/NN ,/, achieving/VBG results/NNS that/WDT are/VBP competitive/JJ with/IN current/JJ state/NN of/IN the/DT art/NN ./.
More/RBR importantly/RB ,/, we/PRP demonstrate/VBP that/IN our/PRP$ model/NN incorporating/VBG multimodal/JJ attention/NN as/RB well/RB as/IN temporal/JJ attention/NN significantly/RB outperforms/VBZ the/DT model/NN that/WDT uses/VBZ temporal/JJ attention/NN alone/RB ./.
