Hyperparameter/NN selection/NN generally/RB relies/VBZ on/IN running/VBG multiple/JJ full/JJ training/NN trials/NNS ,/, with/IN hyperparameter/NN selection/NN based/VBN on/IN validation/NN set/VBN performance/NN ./.
We/PRP propose/VBP a/DT gradient/NN -/HYPH based/VBN approach/NN for/IN locally/RB adjusting/VBG hyperparameters/NNS on/IN the/DT fly/NN in/IN which/WDT we/PRP adjust/VBP the/DT hyperparameters/NNS so/RB as/IN to/TO make/VB the/DT model/NN parameter/NN gradients/NNS ,/, and/CC hence/RB updates/NNS ,/, more/JJR advantageous/JJ for/IN the/DT validation/NN cost/NN ./.
We/PRP explore/VBP the/DT approach/NN for/IN tuning/NN regularization/NN hyperparameters/NNS and/CC find/VB that/IN in/IN experiments/NNS on/IN MNIST/NNP the/DT resulting/VBG regularization/NN levels/NNS are/VBP within/IN the/DT optimal/JJ regions/NNS ./.
The/DT method/NN is/VBZ less/JJR computationally/RB demanding/VBG compared/VBN to/IN similar/JJ gradient/NN -/HYPH based/VBN approaches/NNS to/IN hyperparameter/NN selection/NN ,/, only/RB requires/VBZ a/DT few/JJ trials/NNS ,/, and/CC consistently/RB finds/VBZ solid/JJ hyperparameter/NN values/NNS which/WDT makes/VBZ it/PRP a/DT useful/JJ tool/NN for/IN training/NN neural/JJ network/NN models/NNS ./.
