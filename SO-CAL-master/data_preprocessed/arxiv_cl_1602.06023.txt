In/IN this/DT work/NN ,/, we/PRP cast/VBD text/NN summarization/NN as/IN a/DT sequence/NN -/HYPH to/IN -/HYPH sequence/NN problem/NN and/CC apply/VB the/DT attentional/JJ encoder/NN -/HYPH decoder/NN RNN/NN that/WDT has/VBZ been/VBN shown/VBN to/TO be/VB successful/JJ for/IN Machine/NN Translation/NN (/-LRB- Bahdanau/NNP et/FW al./FW (/-LRB- 2014/CD )/-RRB- )/-RRB- ./.
Our/PRP$ experiments/NNS show/VBP that/IN the/DT proposed/VBN architecture/NN significantly/RB outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT art/NN model/NN of/IN Rush/NNP et/FW al./FW (/-LRB- 2015/CD )/-RRB- on/IN the/DT Gigaword/NNP dataset/NN without/IN any/DT additional/JJ tuning/NN ./.
We/PRP also/RB propose/VBP additional/JJ extensions/NNS to/IN the/DT standard/JJ architecture/NN ,/, which/WDT we/PRP show/VBP contribute/VB to/IN further/JJ improvement/NN in/IN performance/NN ./.
