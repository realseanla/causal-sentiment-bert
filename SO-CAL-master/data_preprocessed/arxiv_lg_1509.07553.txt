Many/JJ interesting/JJ machine/NN learning/NN problems/NNS are/VBP best/JJS posed/VBN by/IN considering/VBG instances/NNS that/WDT are/VBP distributions/NNS ,/, or/CC sample/NN sets/NNS drawn/VBN from/IN distributions/NNS ./.
Previous/JJ work/NN devoted/VBN to/IN machine/NN learning/NN tasks/NNS with/IN distributional/JJ inputs/NNS has/VBZ done/VBN so/RB through/IN pairwise/JJ kernel/NN evaluations/NNS between/IN pdfs/NNS (/-LRB- or/CC sample/NN sets/NNS )/-RRB- ./.
While/IN such/PDT an/DT approach/NN is/VBZ fine/JJ for/IN smaller/JJR datasets/NNS ,/, the/DT computation/NN of/IN an/DT $/$ N/CD \/NN times/NNS N$/NNP Gram/NNP matrix/NN is/VBZ prohibitive/JJ in/IN large/JJ datasets/NNS ./.
Recent/JJ scalable/JJ estimators/NNS that/WDT work/VBP over/IN pdfs/NNS have/VBP done/VBN so/RB only/RB with/IN kernels/NNS that/WDT use/VBP Euclidean/JJ metrics/NNS ,/, like/IN the/DT $/$ L_2/CD $/$ distance/NN ./.
However/RB ,/, there/EX are/VBP a/DT myriad/NN of/IN other/JJ useful/JJ metrics/NNS available/JJ ,/, such/JJ as/IN total/JJ variation/NN ,/, Hellinger/NNP distance/NN ,/, and/CC the/DT Jensen/NNP -/HYPH Shannon/NNP divergence/NN ./.
This/DT work/NN develops/VBZ the/DT first/JJ random/JJ features/NNS for/IN pdfs/NNS whose/WP$ dot/NN product/NN approximates/VBZ kernels/NNS using/VBG these/DT non-Euclidean/JJ metrics/NNS ,/, allowing/VBG estimators/NNS using/VBG such/JJ kernels/NNS to/TO scale/VB to/IN large/JJ datasets/NNS by/IN working/VBG in/IN a/DT primal/JJ space/NN ,/, without/IN computing/VBG large/JJ Gram/NNP matrices/NNS ./.
We/PRP provide/VBP an/DT analysis/NN of/IN the/DT approximation/NN error/NN in/IN using/VBG our/PRP$ proposed/VBN random/JJ features/NNS and/CC show/VBP empirically/RB the/DT quality/NN of/IN our/PRP$ approximation/NN both/CC in/IN estimating/VBG a/DT Gram/NNP matrix/NN and/CC in/IN solving/VBG learning/NN tasks/NNS in/IN real/JJ -/HYPH world/NN and/CC synthetic/JJ data/NNS ./.
