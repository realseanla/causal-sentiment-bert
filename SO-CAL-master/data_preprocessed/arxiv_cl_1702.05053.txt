Neural/JJ attention/NN models/NNS have/VBP achieved/VBN great/JJ success/NN in/IN different/JJ NLP/NN tasks/NNS ./.
How/WRB -/HYPH ever/RB ,/, they/PRP have/VBP not/RB fulfilled/VBN their/PRP$ promise/NN on/IN the/DT AMR/NNP parsing/VBG task/NN due/IN to/IN the/DT data/NNS sparsity/NN issue/NN ./.
In/IN this/DT paper/NN ,/, we/PRP de/IN -/HYPH scribe/NN a/DT sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN for/IN AMR/NNP parsing/VBG and/CC present/JJ different/JJ ways/NNS to/TO tackle/VB the/DT data/NNS sparsity/NN problem/NN ./.
We/PRP show/VBP that/IN our/PRP$ methods/NNS achieve/VBP significant/JJ improvement/NN over/IN a/DT baseline/NN neural/JJ atten/NN -/HYPH tion/NN model/NN and/CC our/PRP$ results/NNS are/VBP also/RB compet/NN -/HYPH itive/NN against/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS that/WDT do/VBP not/RB use/VB extra/JJ linguistic/JJ resources/NNS ./.
