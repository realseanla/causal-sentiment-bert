Robust/JJ classification/NN becomes/VBZ challenging/JJ when/WRB classes/NNS contain/VBP multiple/JJ subclasses/NNS ./.
Examples/NNS include/VBP multi-font/JJ optical/JJ character/NN recognition/NN and/CC automated/VBN protein/NN function/NN prediction/NN ./.
In/IN correlation/NN -/HYPH based/VBN nearest/JJS -/HYPH neighbor/NN classification/NN ,/, the/DT maximin/NN correlation/NN approach/NN (/-LRB- MCA/NNP )/-RRB- provides/VBZ the/DT worst/JJS -/HYPH case/NN optimal/JJ solution/NN by/IN minimizing/VBG the/DT maximum/JJ misclassification/NN risk/NN through/IN an/DT iterative/JJ procedure/NN ./.
Despite/IN the/DT optimality/NN ,/, the/DT original/JJ MCA/NNP has/VBZ drawbacks/NNS that/WDT have/VBP limited/VBN its/PRP$ wide/JJ applicability/NN in/IN practice/NN ./.
That/DT is/VBZ ,/, the/DT MCA/NNP tends/VBZ to/TO be/VB sensitive/JJ to/IN outliers/NNS ,/, can/MD not/RB effectively/RB handle/VB nonlinearities/NNS in/IN datasets/NNS ,/, and/CC suffers/VBZ from/IN having/VBG high/JJ computational/JJ complexity/NN ./.
To/TO address/VB these/DT limitations/NNS ,/, we/PRP propose/VBP an/DT improved/JJ solution/NN ,/, named/VBN regularized/VBN maximin/NN correlation/NN approach/NN (/-LRB- R/NN -/HYPH MCA/NNP )/-RRB- ./.
We/PRP first/RB reformulate/VB MCA/NNP as/IN a/DT quadratically/RB constrained/VBN linear/JJ programming/NN (/-LRB- QCLP/NN )/-RRB- problem/NN ,/, incorporate/VB regularization/NN by/IN introducing/VBG slack/NN variables/NNS into/IN the/DT primal/JJ problem/NN of/IN the/DT QCLP/NN ,/, and/CC derive/VBP the/DT corresponding/VBG Lagrangian/NNP dual/RB ./.
The/DT dual/JJ formulation/NN enables/VBZ us/PRP to/TO apply/VB the/DT kernel/NN trick/NN to/IN R/NNP -/HYPH MCA/NNP so/IN that/IN it/PRP can/MD better/RBR handle/VB nonlinearities/NNS ./.
Our/PRP$ experimental/JJ results/NNS demonstrate/VBP that/IN the/DT regularization/NN and/CC kernelization/NN make/VBP the/DT proposed/VBN R/NN -/HYPH MCA/NNP more/RBR robust/JJ and/CC accurate/JJ for/IN various/JJ classification/NN tasks/NNS than/IN the/DT original/JJ MCA/NNP ./.
Furthermore/RB ,/, when/WRB the/DT data/NNS size/NN or/CC dimensionality/NN grows/VBZ ,/, R/NNP -/HYPH MCA/NNP runs/VBZ substantially/RB faster/RBR by/IN solving/VBG either/CC the/DT primal/JJ or/CC dual/JJ (/-LRB- whichever/WDT has/VBZ a/DT smaller/JJR variable/JJ dimension/NN )/-RRB- of/IN the/DT QCLP/NN ./.
