Neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- heavily/RB relies/VBZ on/IN an/DT attention/NN network/NN to/TO produce/VB a/DT context/NN vector/NN for/IN each/DT target/NN word/NN prediction/NN ./.
In/IN practice/NN ,/, we/PRP find/VBP that/IN context/NN vectors/NNS for/IN different/JJ target/NN words/NNS are/VBP quite/RB similar/JJ to/IN one/CD another/DT and/CC therefore/RB are/VBP insufficient/JJ in/IN discriminatively/RB predicting/VBG target/NN words/NNS ./.
The/DT reason/NN for/IN this/DT might/MD be/VB that/IN context/NN vectors/NNS produced/VBN by/IN the/DT vanilla/NN attention/NN network/NN are/VBP just/RB a/DT weighted/JJ sum/NN of/IN source/NN representations/NNS that/WDT are/VBP invariant/JJ to/IN decoder/NN states/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ GRU-gated/JJ attention/NN model/NN (/-LRB- GAtt/NN )/-RRB- for/IN NMT/NN which/WDT enhances/VBZ the/DT degree/NN of/IN discrimination/NN of/IN context/NN vectors/NNS by/IN enabling/VBG source/NN representations/NNS to/TO be/VB sensitive/JJ to/IN the/DT partial/JJ translation/NN generated/VBN by/IN the/DT decoder/NN ./.
GAtt/NNP uses/VBZ a/DT gated/VBN recurrent/JJ unit/NN (/-LRB- GRU/NN )/-RRB- to/TO combine/VB two/CD types/NNS of/IN information/NN :/: treating/VBG a/DT source/NN annotation/NN vector/NN originally/RB produced/VBN by/IN the/DT bidirectional/JJ encoder/NN as/IN the/DT history/NN state/NN while/IN the/DT corresponding/VBG previous/JJ decoder/NN state/NN as/IN the/DT input/NN to/IN the/DT GRU/NNP ./.
The/DT GRU/NNP -/HYPH combined/VBN information/NN forms/VBZ a/DT new/JJ source/NN annotation/NN vector/NN ./.
In/IN this/DT way/NN ,/, we/PRP can/MD obtain/VB translation/NN -/HYPH sensitive/JJ source/NN representations/NNS which/WDT are/VBP then/RB feed/VB into/IN the/DT attention/NN network/NN to/TO generate/VB discriminative/JJ context/NN vectors/NNS ./.
We/PRP further/RB propose/VB a/DT variant/NN that/WDT regards/VBZ a/DT source/NN annotation/NN vector/NN as/IN the/DT current/JJ input/NN while/IN the/DT previous/JJ decoder/NN state/NN as/IN the/DT history/NN ./.
Experiments/NNS on/IN NIST/NNP Chinese/NNP -/HYPH English/NNP translation/NN tasks/NNS show/VBP that/IN both/DT GAtt/NN -/HYPH based/VBN models/NNS achieve/VBP significant/JJ improvements/NNS over/IN the/DT vanilla/NN attentionbased/VBN NMT/NNP ./.
Further/JJ analyses/NNS on/IN attention/NN weights/NNS and/CC context/NN vectors/NNS demonstrate/VBP the/DT effectiveness/NN of/IN GAtt/NNP in/IN improving/VBG the/DT discrimination/NN power/NN of/IN representations/NNS and/CC handling/VBG the/DT challenging/JJ issue/NN of/IN over-translation/NN ./.
