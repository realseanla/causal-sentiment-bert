Ensemble/NN methods/NNS are/VBP arguably/RB the/DT most/RBS trustworthy/JJ techniques/NNS for/IN boosting/VBG the/DT performance/NN of/IN machine/NN learning/NN models/NNS ./.
Popular/NNP independent/JJ ensembles/NNS (/-LRB- IE/NNP )/-RRB- relying/VBG on/IN naive/JJ averaging/NN //HYPH voting/NN scheme/NN have/VBP been/VBN of/IN typical/JJ choice/NN for/IN most/JJS applications/NNS involving/VBG deep/JJ neural/JJ networks/NNS ,/, but/CC they/PRP do/VBP not/RB consider/VB advanced/JJ collaboration/NN among/IN ensemble/NN models/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP new/JJ ensemble/NN methods/NNS specialized/VBN for/IN deep/JJ neural/JJ networks/NNS ,/, called/VBN confident/JJ multiple/JJ choice/NN learning/NN (/-LRB- CMCL/NN )/-RRB- :/: it/PRP is/VBZ a/DT variant/NN of/IN multiple/JJ choice/NN learning/NN (/-LRB- MCL/NN )/-RRB- via/IN addressing/VBG its/PRP$ overconfidence/NN issue.In/NNP particular/JJ ,/, the/DT proposed/VBN major/JJ components/NNS of/IN CMCL/NNP beyond/IN the/DT original/JJ MCL/NN scheme/NN are/VBP (/-LRB- i/LS )/-RRB- new/JJ loss/NN ,/, i.e./FW ,/, confident/JJ oracle/NN loss/NN ,/, (/-LRB- ii/LS )/-RRB- new/JJ architecture/NN ,/, i.e./FW ,/, feature/NN sharing/NN and/CC (/-LRB- iii/LS )/-RRB- new/JJ training/NN method/NN ,/, i.e./FW ,/, stochastic/JJ labeling/NN ./.
We/PRP demonstrate/VBP the/DT effect/NN of/IN CMCL/NNP via/IN experiments/NNS on/IN the/DT image/NN classification/NN on/IN CIFAR/NNP and/CC SVHN/NNP ,/, and/CC the/DT foreground/NN -/HYPH background/NN segmentation/NN on/IN the/DT iCoseg/NNP ./.
In/IN particular/JJ ,/, CMCL/NN using/VBG 5/CD residual/JJ networks/NNS provides/VBZ 14.05/CD percent/NN and/CC 6.60/CD percent/NN relative/JJ reductions/NNS in/IN the/DT top/JJ -/HYPH 1/CD error/NN rates/NNS from/IN the/DT corresponding/VBG IE/NNP scheme/NN for/IN the/DT classification/NN task/NN on/IN CIFAR/NNP and/CC SVHN/NNP ,/, respectively/RB ./.
