We/PRP present/VBP a/DT general/JJ theoretical/JJ analysis/NN of/IN structured/JJ prediction/NN ./.
By/IN introducing/VBG a/DT new/JJ complexity/NN measure/NN that/WDT explicitly/RB factors/NNS in/IN the/DT structure/NN of/IN the/DT output/NN space/NN and/CC the/DT loss/NN function/NN ,/, we/PRP are/VBP able/JJ to/TO derive/VB new/JJ data/NNS -/HYPH dependent/JJ learning/NN guarantees/NNS for/IN a/DT broad/JJ family/NN of/IN losses/NNS and/CC for/IN hypothesis/NN sets/NNS with/IN an/DT arbitrary/JJ factor/NN graph/NN decomposition/NN ./.
We/PRP extend/VBP this/DT theory/NN by/IN leveraging/VBG the/DT principle/NN of/IN Voted/VBN Risk/NN Minimization/NN (/-LRB- VRM/NN )/-RRB- and/CC showing/VBG that/IN learning/NN is/VBZ possible/JJ with/IN complex/JJ factor/NN graphs/NNS ./.
We/PRP both/DT present/JJ new/JJ learning/NN bounds/NNS in/IN this/DT advanced/JJ setting/NN as/RB well/RB as/IN derive/VBP two/CD new/JJ families/NNS of/IN algorithms/NNS ,/, \/SYM emph/NN {/-LRB- Voted/VBD Conditional/JJ Random/NNP Fields/NNPS }/-RRB- and/CC \/SYM emph/NN {/-LRB- Voted/VBD Structured/VBN Boosting/VBG }/-RRB- ,/, which/WDT can/MD make/VB use/NN of/IN very/RB complex/JJ features/NNS and/CC factor/NN graphs/NNS without/IN overfitting/VBG ./.
Finally/RB ,/, we/PRP also/RB validate/VBP our/PRP$ theory/NN through/IN experiments/NNS on/IN several/JJ datasets/NNS ./.
