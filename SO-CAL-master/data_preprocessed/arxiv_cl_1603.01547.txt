Several/JJ large/JJ cloze/NN -/HYPH style/NN context/NN -/HYPH question/NN -/HYPH answer/NN datasets/NNS have/VBP been/VBN introduced/VBN recently/RB :/: the/DT CNN/NNP and/CC Daily/NNP Mail/NNP news/NN data/NNS and/CC the/DT Children/NNPS 's/POS Book/NNP Test/NNP ./.
Thanks/NN to/IN the/DT size/NN of/IN these/DT datasets/NNS ,/, the/DT associated/VBN text/NN comprehension/NN task/NN is/VBZ well/RB suited/JJ for/IN deep/JJ -/HYPH learning/NN techniques/NNS that/WDT currently/RB seem/VBP to/TO outperform/VB all/DT alternative/JJ approaches/NNS ./.
We/PRP present/VBP a/DT new/JJ ,/, simple/JJ model/NN that/WDT uses/VBZ attention/NN to/TO directly/RB pick/VB the/DT answer/NN from/IN the/DT context/NN as/IN opposed/VBN to/IN computing/VBG the/DT answer/NN using/VBG a/DT blended/JJ representation/NN of/IN words/NNS in/IN the/DT document/NN as/RB is/VBZ usual/JJ in/IN similar/JJ models/NNS ./.
This/DT makes/VBZ the/DT model/NN particularly/RB suitable/JJ for/IN question/NN -/HYPH answering/VBG problems/NNS where/WRB the/DT answer/NN is/VBZ a/DT single/JJ word/NN from/IN the/DT document/NN ./.
Our/PRP$ model/NN outperforms/VBZ models/NNS previously/RB proposed/VBN for/IN these/DT tasks/NNS by/IN a/DT large/JJ margin/NN ./.
