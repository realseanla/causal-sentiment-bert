Reducing/VBG the/DT amount/NN of/IN human/JJ supervision/NN is/VBZ a/DT key/JJ problem/NN in/IN machine/NN learning/NN and/CC a/DT natural/JJ approach/NN is/VBZ that/IN of/IN exploiting/VBG the/DT relations/NNS (/-LRB- structure/NN )/-RRB- among/IN different/JJ tasks/NNS ./.
This/DT is/VBZ the/DT idea/NN at/IN the/DT core/NN of/IN multi-task/VB learning/NN ./.
In/IN this/DT context/NN a/DT fundamental/JJ question/NN is/VBZ how/WRB to/TO incorporate/VB the/DT tasks/NNS structure/NN in/IN the/DT learning/NN problem.We/NN tackle/VBP this/DT question/NN by/IN studying/VBG a/DT general/JJ computational/JJ framework/NN that/WDT allows/VBZ to/TO encode/VB a-priori/JJ knowledge/NN of/IN the/DT tasks/NNS structure/NN in/IN the/DT form/NN of/IN a/DT convex/NN penalty/NN ;/: in/IN this/DT setting/VBG a/DT variety/NN of/IN previously/RB proposed/VBN methods/NNS can/MD be/VB recovered/VBN as/IN special/JJ cases/NNS ,/, including/VBG linear/JJ and/CC non-linear/JJ approaches/NNS ./.
Within/IN this/DT framework/NN ,/, we/PRP show/VBP that/IN tasks/NNS and/CC their/PRP$ structure/NN can/MD be/VB efficiently/RB learned/VBN considering/VBG a/DT convex/NN optimization/NN problem/NN that/WDT can/MD be/VB approached/VBN by/IN means/NNS of/IN block/NN coordinate/NN methods/NNS such/JJ as/IN alternating/VBG minimization/NN and/CC for/IN which/WDT we/PRP prove/VBP convergence/NN to/IN the/DT global/JJ minimum/NN ./.
