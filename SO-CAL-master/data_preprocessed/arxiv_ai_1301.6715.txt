We/PRP consider/VBP the/DT problem/NN of/IN finding/VBG good/JJ finite/NN -/HYPH horizon/NN policies/NNS for/IN POMDPs/NNS under/IN the/DT expected/VBN reward/NN metric/JJ ./.
The/DT policies/NNS considered/VBN are/VBP {/-LRB- em/PRP free/JJ finite/NN -/HYPH memory/NN policies/NNS with/IN limited/JJ memory/NN }/-RRB- ;/: a/DT policy/NN is/VBZ a/DT mapping/NN from/IN the/DT space/NN of/IN observation/NN -/HYPH memory/NN pairs/NNS to/IN the/DT space/NN of/IN action/NN -/HYPH memeory/NN pairs/NNS (/-LRB- the/DT policy/NN updates/NNS the/DT memory/NN as/IN it/PRP goes/VBZ )/-RRB- ,/, and/CC the/DT number/NN of/IN possible/JJ memory/NN states/NNS is/VBZ a/DT parameter/NN of/IN the/DT input/NN to/IN the/DT policy/NN -/HYPH finding/VBG algorithms/NNS ./.
The/DT algorithms/NNS considered/VBN here/RB are/VBP preliminary/JJ implementations/NNS of/IN three/CD search/NN heuristics/NNS :/: local/JJ search/NN ,/, simulated/JJ annealing/NN ,/, and/CC genetic/JJ algorithms/NNS ./.
We/PRP compare/VBP their/PRP$ outcomes/NNS to/IN each/DT other/JJ and/CC to/IN the/DT optimal/JJ policies/NNS for/IN each/DT instance/NN ./.
We/PRP compare/VBP run/NN times/NNS of/IN each/DT policy/NN and/CC of/IN a/DT dynamic/JJ programming/NN algorithm/NN for/IN POMDPs/NNS developed/VBN by/IN Hansen/NNP that/WDT iteratively/RB improves/VBZ a/DT finite/NN -/HYPH state/NN controller/NN ---/, the/DT previous/JJ state/NN of/IN the/DT art/NN for/IN finite/JJ memory/NN policies/NNS ./.
The/DT value/NN of/IN the/DT best/JJS policy/NN can/MD only/RB improve/VB as/IN the/DT amount/NN of/IN memory/NN increases/NNS ,/, up/IN to/IN the/DT amount/NN needed/VBN for/IN an/DT optimal/JJ finite/NN -/HYPH memory/NN policy/NN ./.
Our/PRP$ most/RBS surprising/JJ finding/NN is/VBZ that/IN more/JJR memory/NN helps/VBZ in/IN another/DT way/NN :/: given/VBN more/JJR memory/NN than/IN is/VBZ needed/VBN for/IN an/DT optimal/JJ policy/NN ,/, the/DT algorithms/NNS are/VBP more/RBR likely/JJ to/TO converge/VB to/IN optimal/JJ -/HYPH valued/VBN policies/NNS ./.
