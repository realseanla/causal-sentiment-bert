In/IN this/DT paper/NN we/PRP argue/VBP for/IN the/DT fundamental/JJ importance/NN of/IN the/DT value/NN distribution/NN :/: the/DT distribution/NN of/IN the/DT random/JJ return/NN received/VBN by/IN a/DT reinforcement/NN learning/VBG agent/NN ./.
This/DT is/VBZ in/IN contrast/NN to/IN the/DT common/JJ approach/NN to/IN reinforcement/NN learning/NN which/WDT models/NNS the/DT expectation/NN of/IN this/DT return/NN ,/, or/CC value/NN ./.
Although/IN there/EX is/VBZ an/DT established/VBN body/NN of/IN literature/NN studying/VBG the/DT value/NN distribution/NN ,/, thus/RB far/RB it/PRP has/VBZ always/RB been/VBN used/VBN for/IN a/DT specific/JJ purpose/NN such/JJ as/IN implementing/VBG risk/NN -/HYPH aware/JJ behaviour/NN ./.
We/PRP begin/VBP with/IN theoretical/JJ results/NNS in/IN both/CC the/DT policy/NN evaluation/NN and/CC control/NN settings/NNS ,/, exposing/VBG a/DT significant/JJ distributional/JJ instability/NN in/IN the/DT latter/JJ ./.
We/PRP then/RB use/VBP the/DT distributional/JJ perspective/NN to/TO design/VB a/DT new/JJ algorithm/NN which/WDT applies/VBZ Bellman/NNP 's/POS equation/NN to/IN the/DT learning/NN of/IN approximate/JJ value/NN distributions/NNS ./.
We/PRP evaluate/VBP our/PRP$ algorithm/NN using/VBG the/DT suite/NN of/IN games/NNS from/IN the/DT Arcade/NNP Learning/NNP Environment/NNP ./.
We/PRP obtain/VBP both/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS and/CC anecdotal/JJ evidence/NN demonstrating/VBG the/DT importance/NN of/IN the/DT value/NN distribution/NN in/IN approximate/JJ reinforcement/NN learning/NN ./.
Finally/RB ,/, we/PRP combine/VBP theoretical/JJ and/CC empirical/JJ evidence/NN to/TO highlight/VB the/DT ways/NNS in/IN which/WDT the/DT value/NN distribution/NN impacts/NNS learning/VBG in/IN the/DT approximate/JJ setting/NN ./.
