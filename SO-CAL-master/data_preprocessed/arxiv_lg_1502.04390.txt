Parameter/NN -/HYPH specific/JJ adaptive/JJ learning/NN rate/NN methods/NNS are/VBP computationally/RB efficient/JJ ways/NNS to/TO reduce/VB the/DT ill/JJ -/HYPH conditioning/NN problems/NNS encountered/VBD when/WRB training/VBG large/JJ deep/JJ networks/NNS ./.
Following/VBG recent/JJ work/NN that/WDT strongly/RB suggests/VBZ that/IN most/JJS of/IN the/DT critical/JJ points/NNS encountered/VBD when/WRB training/VBG such/JJ networks/NNS are/VBP saddle/NN points/NNS ,/, we/PRP find/VBP how/WRB considering/VBG the/DT presence/NN of/IN negative/JJ eigenvalues/NNS of/IN the/DT Hessian/JJ could/MD help/VB us/PRP design/VB better/JJR suited/JJ adaptive/JJ learning/NN rate/NN schemes/NNS ,/, i.e./FW ,/, diagonal/JJ preconditioners/NNS ./.
We/PRP show/VBP that/IN the/DT optimal/JJ preconditioner/NN is/VBZ based/VBN on/IN taking/VBG the/DT absolute/JJ value/NN of/IN the/DT Hessian/NNP 's/POS eigenvalues/NNS ,/, which/WDT is/VBZ not/RB what/WP Newton/NNP and/CC classical/JJ preconditioners/NNS like/IN Jacobi/NNP 's/POS do/VBP ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ adaptive/JJ learning/NN rate/NN scheme/NN based/VBN on/IN the/DT equilibration/NN preconditioner/NN and/CC show/VBP that/IN RMSProp/NNP approximates/VBZ it/PRP ,/, which/WDT may/MD explain/VB some/DT of/IN its/PRP$ success/NN in/IN the/DT presence/NN of/IN saddle/NN points/NNS ./.
Whereas/IN RMSProp/NNP is/VBZ a/DT biased/JJ estimator/NN of/IN the/DT equilibration/NN preconditioner/NN ,/, the/DT proposed/VBN stochastic/JJ estimator/NN ,/, ESGD/NN ,/, is/VBZ unbiased/JJ and/CC only/RB adds/VBZ a/DT small/JJ percentage/NN to/IN computing/NN time/NN ./.
We/PRP find/VBP that/IN both/DT schemes/NNS yield/VBP very/RB similar/JJ step/NN directions/NNS but/CC that/IN ESGD/NNP sometimes/RB surpasses/VBZ RMSProp/NNP in/IN terms/NNS of/IN convergence/NN speed/NN ,/, always/RB clearly/RB improving/VBG over/IN plain/JJ stochastic/JJ gradient/NN descent/NN ./.
