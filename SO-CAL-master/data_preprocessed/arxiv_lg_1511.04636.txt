In/IN this/DT paper/NN ,/, we/PRP propose/VBP the/DT deep/JJ reinforcement/NN relevance/NN network/NN (/-LRB- DRRN/NN )/-RRB- ,/, a/DT novel/JJ deep/JJ architecture/NN ,/, for/IN handling/VBG an/DT unbounded/JJ action/NN space/NN with/IN applications/NNS to/IN language/NN understanding/NN for/IN text/NN -/HYPH based/VBN games/NNS ./.
For/IN a/DT particular/JJ class/NN of/IN games/NNS ,/, a/DT user/NN must/MD choose/VB among/IN a/DT variable/JJ number/NN of/IN actions/NNS described/VBN by/IN text/NN ,/, with/IN the/DT goal/NN of/IN maximizing/VBG long/RB -/HYPH term/NN reward/NN ./.
In/IN these/DT games/NNS ,/, the/DT best/JJS action/NN is/VBZ typically/RB that/DT which/WDT fits/VBZ the/DT best/JJS to/IN the/DT current/JJ situation/NN (/-LRB- modeled/VBN as/IN a/DT state/NN in/IN the/DT DRRN/NNP )/-RRB- ,/, also/RB described/VBN by/IN text/NN ./.
Because/IN of/IN the/DT exponential/JJ complexity/NN of/IN natural/JJ language/NN with/IN respect/NN to/IN sentence/NN length/NN ,/, there/EX is/VBZ typically/RB an/DT unbounded/JJ set/NN of/IN unique/JJ actions/NNS ./.
Therefore/RB ,/, it/PRP is/VBZ very/RB difficult/JJ to/TO pre-define/VB the/DT action/NN set/VBN as/IN in/IN the/DT deep/JJ Q/NN -/HYPH network/NN (/-LRB- DQN/NN )/-RRB- ./.
To/TO address/VB this/DT challenge/NN ,/, the/DT DRRN/NNP extracts/NNS high/JJ -/HYPH level/NN embedding/NN vectors/NNS from/IN the/DT texts/NNS that/WDT describe/VBP states/NNS and/CC actions/NNS ,/, respectively/RB ,/, and/CC computes/VBZ the/DT inner/JJ products/NNS between/IN the/DT state/NN and/CC action/NN embedding/NN vectors/NNS to/TO approximate/VB the/DT Q/NN -/HYPH function/NN ./.
We/PRP evaluate/VBP the/DT DRRN/NNP on/IN two/CD popular/JJ text/NN games/NNS ,/, showing/VBG superior/JJ performance/NN over/IN the/DT DQN/NNP ./.
