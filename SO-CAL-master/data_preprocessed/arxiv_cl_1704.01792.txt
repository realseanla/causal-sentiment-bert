Automatic/JJ question/NN generation/NN aims/VBZ to/TO generate/VB questions/NNS from/IN a/DT text/NN passage/NN where/WRB the/DT generated/VBN questions/NNS can/MD be/VB answered/VBN by/IN certain/JJ sub-spans/NNS of/IN the/DT given/VBN passage/NN ./.
Traditional/JJ methods/NNS mainly/RB use/VBP rigid/JJ heuristic/NN rules/NNS to/TO transform/VB a/DT sentence/NN into/IN related/JJ questions/NNS ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP to/TO apply/VB the/DT neural/JJ encoder/NN -/HYPH decoder/NN model/NN to/TO generate/VB meaningful/JJ and/CC diverse/JJ questions/NNS from/IN natural/JJ language/NN sentences/NNS ./.
The/DT encoder/NN reads/VBZ the/DT input/NN text/NN and/CC the/DT answer/NN position/NN ,/, to/TO produce/VB an/DT answer/NN -/HYPH aware/JJ input/NN representation/NN ,/, which/WDT is/VBZ fed/VBN to/IN the/DT decoder/NN to/TO generate/VB an/DT answer/NN focused/VBD question/NN ./.
We/PRP conduct/VBP a/DT preliminary/JJ study/NN on/IN neural/JJ question/NN generation/NN from/IN text/NN with/IN the/DT SQuAD/NN dataset/NN ,/, and/CC the/DT experiment/NN results/NNS show/VBP that/IN our/PRP$ method/NN can/MD produce/VB fluent/JJ and/CC diverse/JJ questions/NNS ./.
