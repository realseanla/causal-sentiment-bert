We/PRP describe/VBP an/DT attentive/JJ encoder/NN that/WDT combines/VBZ tree/NN -/HYPH structured/VBN recursive/JJ neural/JJ networks/NNS and/CC sequential/JJ recurrent/JJ neural/JJ networks/NNS for/IN modelling/VBG sentence/NN pairs/NNS ./.
Since/IN existing/VBG attentive/JJ models/NNS exert/VBP attention/NN on/IN the/DT sequential/JJ structure/NN ,/, we/PRP propose/VBP a/DT way/NN to/TO incorporate/VB attention/NN into/IN the/DT tree/NN topology/NN ./.
Specially/RB ,/, given/VBN a/DT pair/NN of/IN sentences/NNS ,/, our/PRP$ attentive/JJ encoder/NN uses/VBZ the/DT representation/NN of/IN one/CD sentence/NN ,/, which/WDT generated/VBD via/IN an/DT RNN/NN ,/, to/TO guide/VB the/DT structural/JJ encoding/NN of/IN the/DT other/JJ sentence/NN on/IN the/DT dependency/NN parse/VB tree/NN ./.
We/PRP evaluate/VBP the/DT proposed/VBN attentive/JJ encoder/NN on/IN three/CD tasks/NNS :/: semantic/JJ similarity/NN ,/, paraphrase/NN identification/NN and/CC true/JJ -/HYPH false/JJ question/NN selection/NN ./.
Experimental/JJ results/NNS show/VBP that/IN our/PRP$ encoder/NN outperforms/VBZ all/DT baselines/NNS and/CC achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN two/CD tasks/NNS ./.
