The/DT past/JJ several/JJ years/NNS have/VBP seen/VBN remarkable/JJ progress/NN in/IN generative/JJ models/NNS which/WDT produce/VBP convincing/JJ samples/NNS of/IN images/NNS and/CC other/JJ modalities/NNS ./.
A/DT shared/VBN component/NN of/IN many/JJ powerful/JJ generative/JJ models/NNS is/VBZ a/DT decoder/NN network/NN ,/, a/DT parametric/JJ deep/JJ neural/JJ net/NN that/WDT defines/VBZ a/DT generative/JJ distribution/NN ./.
Examples/NNS include/VBP variational/JJ autoencoders/NNS ,/, generative/JJ adversarial/JJ networks/NNS ,/, and/CC generative/JJ moment/NN matching/VBG networks/NNS ./.
Unfortunately/RB ,/, it/PRP can/MD be/VB difficult/JJ to/TO quantify/VB the/DT performance/NN of/IN these/DT models/NNS because/IN of/IN the/DT intractability/NN of/IN log/NN -/HYPH likelihood/NN estimation/NN ,/, and/CC inspecting/VBG samples/NNS can/MD be/VB misleading/JJ ./.
We/PRP propose/VBP to/TO use/VB Annealed/NNP Importance/NNP Sampling/NNP for/IN evaluating/VBG log/NN -/HYPH likelihoods/NNS for/IN decoder/NN -/HYPH based/VBN models/NNS and/CC validate/VB its/PRP$ accuracy/NN using/VBG bidirectional/JJ Monte/NNP Carlo/NNP ./.
Using/VBG this/DT technique/NN ,/, we/PRP analyze/VBP the/DT performance/NN of/IN decoder/NN -/HYPH based/VBN models/NNS ,/, the/DT effectiveness/NN of/IN existing/JJ log/NN -/HYPH likelihood/NN estimators/NNS ,/, the/DT degree/NN of/IN overfitting/VBG ,/, and/CC the/DT degree/NN to/TO which/WDT these/DT models/NNS miss/VBP important/JJ modes/NNS of/IN the/DT data/NNS distribution/NN ./.
