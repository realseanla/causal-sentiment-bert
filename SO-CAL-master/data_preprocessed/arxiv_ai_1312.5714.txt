Reinforcement/NN learning/NN treats/VBZ each/DT input/NN ,/, feature/NN ,/, or/CC stimulus/NN as/IN having/VBG a/DT positive/JJ or/CC negative/JJ reward/NN value/NN ./.
Some/DT stimuli/NNS ,/, however/RB ,/, negate/VB or/CC inhibit/VB the/DT values/NNS of/IN certain/JJ other/JJ predictors/NNS (/-LRB- excitors/NNS )/-RRB- when/WRB presented/VBN with/IN them/PRP ,/, but/CC are/VBP otherwise/RB neutral/JJ ./.
We/PRP show/VBP that/IN both/DT linear/JJ and/CC non-linear/JJ value/NN -/HYPH function/NN approximators/NNS assign/VBP inhibitory/JJ features/NNS a/DT strong/JJ value/NN with/IN the/DT opposite/JJ valence/NN of/IN the/DT predictor/NN it/PRP inhibits/VBZ (/-LRB- i.e./FW ,/, inhibitor/NN =/SYM -/HYPH excitor/NN )/-RRB- ./.
In/IN one/CD circumstance/NN ,/, this/DT gives/VBZ a/DT correct/JJ prediction/NN (/-LRB- i.e./FW ,/, excitor/NN inhibitor/NN =/SYM neutral/JJ outcome/NN )/-RRB- ./.
Importantly/RB ,/, however/RB ,/, value/NN -/HYPH function/NN approximators/NNS incorrectly/RB predict/VBP that/IN when/WRB the/DT inhibitor/NN is/VBZ presented/VBN alone/RB ,/, a/DT negative/JJ or/CC oppositely/RB valenced/VBN outcome/NN will/MD follow/VB whereas/IN the/DT inhibitor/NN alone/RB is/VBZ actually/RB followed/VBN by/IN a/DT neutral/JJ outcome/NN ./.
Essentially/RB ,/, we/PRP show/VBP that/IN having/VBG reward/NN value/NN as/IN a/DT direct/JJ predictive/JJ target/NN can/MD make/VB inhibitors/NNS indistinguishable/JJ from/IN excitors/NNS that/WDT predict/VBP the/DT oppositely/RB valenced/VBN outcome/NN ./.
We/PRP show/VBP that/IN this/DT problem/NN can/MD be/VB easily/RB avoided/VBN if/IN the/DT reinforcement/NN learning/VBG problem/NN is/VBZ broken/VBN into/IN 1/CD )/-RRB- a/DT supervised/JJ learning/NN module/NN that/WDT predicts/VBZ the/DT positive/JJ appearance/NN of/IN primary/JJ reinforcements/NNS and/CC 2/CD )/-RRB- a/DT reinforcement/NN learning/VBG module/NN which/WDT sums/VBZ their/PRP$ agent/NN -/HYPH defined/VBN values/NNS ./.
