In/IN typical/JJ neural/JJ machine/NN translation/NN ~/SYM (/-LRB- NMT/NN )/-RRB- ,/, the/DT decoder/NN generates/VBZ a/DT sentence/NN word/NN by/IN word/NN ,/, packing/VBG all/DT linguistic/JJ granularities/NNS in/IN the/DT same/JJ time/NN -/HYPH scale/NN of/IN RNN/NNP ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ type/NN of/IN decoder/NN for/IN NMT/NNP ,/, which/WDT splits/VBZ the/DT decode/NN state/NN into/IN two/CD parts/NNS and/CC updates/NNS them/PRP in/IN two/CD different/JJ time/NN -/HYPH scales/NNS ./.
Specifically/RB ,/, we/PRP first/RB predict/VBP a/DT chunk/NN time/NN -/HYPH scale/NN state/NN for/IN phrasal/JJ modeling/NN ,/, on/IN top/NN of/IN which/WDT multiple/JJ word/NN time/NN -/HYPH scale/NN states/NNS are/VBP generated/VBN ./.
In/IN this/DT way/NN ,/, the/DT target/NN sentence/NN is/VBZ translated/VBN hierarchically/RB from/IN chunks/NNS to/IN words/NNS ,/, with/IN information/NN in/IN different/JJ granularities/NNS being/VBG leveraged/VBN ./.
Experiments/NNS show/VBP that/IN our/PRP$ proposed/VBN model/NN significantly/RB improves/VBZ the/DT translation/NN performance/NN over/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN NMT/NN model/NN ./.
