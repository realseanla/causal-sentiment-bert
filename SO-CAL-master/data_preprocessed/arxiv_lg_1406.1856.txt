We/PRP provide/VBP a/DT general/JJ mechanism/NN to/TO design/VB online/JJ learning/NN algorithms/NNS based/VBN on/IN a/DT minimax/NN analysis/NN within/IN a/DT drifting/VBG -/HYPH games/NNS framework/NN ./.
Different/JJ online/JJ learning/NN settings/NNS (/-LRB- Hedge/NNP ,/, multi-armed/JJ bandit/NN problems/NNS and/CC online/JJ convex/NN optimization/NN )/-RRB- are/VBP studied/VBN by/IN converting/VBG into/IN various/JJ kinds/NNS of/IN drifting/VBG games/NNS ./.
The/DT original/JJ minimax/NN analysis/NN for/IN drifting/VBG games/NNS is/VBZ then/RB used/VBN and/CC generalized/VBN by/IN applying/VBG a/DT series/NN of/IN relaxations/NNS ,/, starting/VBG from/IN choosing/VBG a/DT convex/NN surrogate/NN of/IN the/DT 0/CD -/HYPH 1/CD loss/NN function/NN ./.
With/IN different/JJ choices/NNS of/IN surrogates/NNS ,/, we/PRP not/RB only/RB recover/VB existing/VBG algorithms/NNS ,/, but/CC also/RB propose/VB new/JJ algorithms/NNS that/WDT are/VBP totally/RB parameter/NN -/HYPH free/JJ and/CC enjoy/VB other/JJ useful/JJ properties/NNS ./.
Moreover/RB ,/, our/PRP$ drifting/VBG -/HYPH games/NNS framework/NN naturally/RB allows/VBZ us/PRP to/TO study/VB high/JJ probability/NN bounds/NNS without/IN resorting/VBG to/IN any/DT concentration/NN results/NNS ,/, and/CC also/RB a/DT generalized/VBN notion/NN of/IN regret/NN that/WDT measures/VBZ how/WRB good/JJ the/DT algorithm/NN is/VBZ compared/VBN to/IN all/DT but/IN the/DT top/JJ small/JJ fraction/NN of/IN candidates/NNS ./.
Finally/RB ,/, we/PRP translate/VBP our/PRP$ new/JJ Hedge/NNP algorithm/NN into/IN a/DT new/JJ adaptive/JJ boosting/VBG algorithm/NN that/WDT is/VBZ computationally/RB faster/RBR as/IN shown/VBN in/IN experiments/NNS ,/, since/IN it/PRP ignores/VBZ a/DT large/JJ number/NN of/IN examples/NNS on/IN each/DT round/NN ./.
