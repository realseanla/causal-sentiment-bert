Reading/VBG an/DT article/NN and/CC answering/VBG questions/NNS about/IN its/PRP$ content/NN is/VBZ a/DT fundamental/JJ task/NN for/IN natural/JJ language/NN understanding/NN ./.
While/IN most/JJS successful/JJ neural/JJ approaches/NNS to/IN this/DT problem/NN rely/VBP on/IN recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- ,/, training/NN RNNs/NNS over/RB long/RB documents/NNS can/MD be/VB prohibitively/RB slow/JJ ./.
We/PRP present/VBP a/DT novel/JJ framework/NN for/IN question/NN answering/VBG that/DT can/MD efficiently/RB scale/VB to/IN longer/JJR documents/NNS while/IN maintaining/VBG or/CC even/RB improving/VBG performance/NN ./.
Our/PRP$ approach/NN combines/VBZ a/DT coarse/JJ ,/, inexpensive/JJ model/NN for/IN selecting/VBG one/CD or/CC more/JJR relevant/JJ sentences/NNS and/CC a/DT more/RBR expensive/JJ RNN/NN that/WDT produces/VBZ the/DT answer/NN from/IN those/DT sentences/NNS ./.
A/DT central/JJ challenge/NN is/VBZ the/DT lack/NN of/IN intermediate/JJ supervision/NN for/IN the/DT coarse/JJ model/NN ,/, which/WDT we/PRP address/VBP using/VBG reinforcement/NN learning/NN ./.
Experiments/NNS demonstrate/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN a/DT challenging/JJ subset/NN of/IN the/DT WIKIREADING/NN dataset/NN (/-LRB- Hewlett/NNP et/FW al./FW ,/, 2016/CD )/-RRB- and/CC on/IN a/DT newly/RB -/HYPH gathered/VBN dataset/NN ,/, while/IN reducing/VBG the/DT number/NN of/IN sequential/JJ RNN/NN steps/NNS by/IN 88/CD percent/NN against/IN a/DT standard/JJ sequence/NN to/IN sequence/NN model/NN ./.
