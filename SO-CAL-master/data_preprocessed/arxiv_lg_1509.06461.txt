The/DT popular/JJ Q/NN -/HYPH learning/VBG algorithm/NN is/VBZ known/VBN to/TO overestimate/VB action/NN values/NNS under/IN certain/JJ conditions/NNS ./.
It/PRP was/VBD not/RB previously/RB known/VBN whether/IN ,/, in/IN practice/NN ,/, such/JJ overestimations/NNS are/VBP common/JJ ,/, whether/IN this/DT harms/VBZ performance/NN ,/, and/CC whether/IN they/PRP can/MD generally/RB be/VB prevented/VBN ./.
In/IN this/DT paper/NN ,/, we/PRP answer/VBP all/PDT these/DT questions/NNS affirmatively/RB ./.
In/IN particular/JJ ,/, we/PRP first/RB show/VBP that/IN the/DT recent/JJ DQN/NNP algorithm/NN ,/, which/WDT combines/VBZ Q/NN -/HYPH learning/NN with/IN a/DT deep/JJ neural/JJ network/NN ,/, suffers/VBZ from/IN substantial/JJ overestimations/NNS in/IN some/DT games/NNS in/IN the/DT Atari/NNP 2600/CD domain/NN ./.
We/PRP then/RB show/VBP that/IN the/DT idea/NN behind/IN the/DT Double/JJ Q/NN -/HYPH learning/VBG algorithm/NN ,/, which/WDT was/VBD introduced/VBN in/IN a/DT tabular/JJ setting/NN ,/, can/MD be/VB generalized/VBN to/TO work/VB with/IN large/JJ -/HYPH scale/NN function/NN approximation/NN ./.
We/PRP propose/VBP a/DT specific/JJ adaptation/NN to/IN the/DT DQN/NNP algorithm/NN and/CC show/VBP that/IN the/DT resulting/VBG algorithm/NN not/RB only/RB reduces/VBZ the/DT observed/VBN overestimations/NNS ,/, as/IN hypothesized/VBN ,/, but/CC that/IN this/DT also/RB leads/VBZ to/IN much/RB better/JJR performance/NN on/IN several/JJ games/NNS ./.
