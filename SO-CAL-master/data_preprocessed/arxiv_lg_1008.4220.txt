Sparse/JJ methods/NNS for/IN supervised/JJ learning/NN aim/NN at/IN finding/VBG good/JJ linear/JJ predictors/NNS from/IN as/IN few/JJ variables/NNS as/IN possible/JJ ,/, i.e./FW ,/, with/IN small/JJ cardinality/NN of/IN their/PRP$ supports/NNS ./.
This/DT combinatorial/JJ selection/NN problem/NN is/VBZ often/RB turned/VBN into/IN a/DT convex/NN optimization/NN problem/NN by/IN replacing/VBG the/DT cardinality/NN function/NN by/IN its/PRP$ convex/JJ envelope/NN (/-LRB- tightest/JJS convex/NN lower/JJR bound/JJ )/-RRB- ,/, in/IN this/DT case/NN the/DT L1/NN -/HYPH norm/NN ./.
In/IN this/DT paper/NN ,/, we/PRP investigate/VBP more/RBR general/JJ set/NN -/HYPH functions/NNS than/IN the/DT cardinality/NN ,/, that/DT may/MD incorporate/VB prior/JJ knowledge/NN or/CC structural/JJ constraints/NNS which/WDT are/VBP common/JJ in/IN many/JJ applications/NNS :/: namely/RB ,/, we/PRP show/VBP that/IN for/IN nonincreasing/JJ submodular/JJ set/NN -/HYPH functions/NNS ,/, the/DT corresponding/VBG convex/NN envelope/NN can/MD be/VB obtained/VBN from/IN its/PRP$ Lovasz/NNP extension/NN ,/, a/DT common/JJ tool/NN in/IN submodular/JJ analysis/NN ./.
This/DT defines/VBZ a/DT family/NN of/IN polyhedral/JJ norms/NNS ,/, for/IN which/WDT we/PRP provide/VBP generic/JJ algorithmic/JJ tools/NNS (/-LRB- subgradients/NNS and/CC proximal/JJ operators/NNS )/-RRB- and/CC theoretical/JJ results/NNS (/-LRB- conditions/NNS for/IN support/NN recovery/NN or/CC high/JJ -/HYPH dimensional/JJ inference/NN )/-RRB- ./.
By/IN selecting/VBG specific/JJ submodular/JJ functions/NNS ,/, we/PRP can/MD give/VB a/DT new/JJ interpretation/NN to/IN known/VBN norms/NNS ,/, such/JJ as/IN those/DT based/VBN on/IN rank/NN -/HYPH statistics/NNS or/CC grouped/VBN norms/NNS with/IN potentially/RB overlapping/VBG groups/NNS ;/: we/PRP also/RB define/VBP new/JJ norms/NNS ,/, in/IN particular/JJ ones/NNS that/WDT can/MD be/VB used/VBN as/IN non-factorial/JJ priors/NNS for/IN supervised/JJ learning/NN ./.
