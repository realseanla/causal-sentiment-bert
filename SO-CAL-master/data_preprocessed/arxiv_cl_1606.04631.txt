Video/NN captioning/NN has/VBZ been/VBN attracting/VBG broad/JJ research/NN attention/NN in/IN multimedia/NNS community/NN ./.
However/RB ,/, most/JJS existing/VBG approaches/NNS either/CC ignore/VB temporal/JJ information/NN among/IN video/NN frames/NNS or/CC just/RB employ/VB local/JJ contextual/JJ temporal/JJ knowledge/NN ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT novel/JJ video/NN captioning/VBG framework/NN ,/, termed/VBN as/IN \/SYM emph/NN {/-LRB- Bidirectional/JJ Long/JJ -/HYPH Short/JJ Term/NN Memory/NN }/-RRB- (/-LRB- BiLSTM/NNP )/-RRB- ,/, which/WDT deeply/RB captures/VBZ bidirectional/JJ global/JJ temporal/JJ structure/NN in/IN video/NN ./.
Specifically/RB ,/, we/PRP first/RB devise/VB a/DT joint/JJ visual/JJ modelling/NN approach/NN to/TO encode/VB video/NN data/NNS by/IN combining/VBG a/DT forward/JJ LSTM/NN pass/NN ,/, a/DT backward/JJ LSTM/NN pass/NN ,/, together/RB with/IN visual/JJ features/NNS from/IN Convolutional/JJ Neural/JJ Networks/NNS (/-LRB- CNNs/NNS )/-RRB- ./.
Then/RB ,/, we/PRP inject/VBP the/DT derived/VBN video/NN representation/NN into/IN the/DT subsequent/JJ language/NN model/NN for/IN initialization/NN ./.
The/DT benefits/NNS are/VBP in/IN two/CD folds/NNS :/: 1/LS )/-RRB- comprehensively/RB preserving/VBG sequential/JJ and/CC visual/JJ information/NN ;/: and/CC 2/LS )/-RRB- adaptively/RB learning/VBG dense/JJ visual/JJ features/NNS and/CC sparse/JJ semantic/JJ representations/NNS for/IN videos/NNS and/CC sentences/NNS ,/, respectively/RB ./.
We/PRP verify/VBP the/DT effectiveness/NN of/IN our/PRP$ proposed/VBN video/NN captioning/NN framework/NN on/IN a/DT commonly/RB -/HYPH used/VBN benchmark/NN ,/, i.e./FW ,/, Microsoft/NNP Video/NNP Description/NN (/-LRB- MSVD/NN )/-RRB- corpus/NN ,/, and/CC the/DT experimental/JJ results/NNS demonstrate/VBP that/IN the/DT superiority/NN of/IN the/DT proposed/VBN approach/NN as/IN compared/VBN to/IN several/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS ./.
