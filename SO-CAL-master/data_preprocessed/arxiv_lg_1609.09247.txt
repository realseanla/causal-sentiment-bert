Recently/RB ,/, these/DT has/VBZ been/VBN a/DT surge/NN on/IN studying/VBG how/WRB to/TO obtain/VB partially/RB annotated/VBN data/NNS for/IN model/NN supervision/NN ./.
However/RB ,/, there/EX still/RB lacks/VBZ a/DT systematic/JJ study/NN on/IN how/WRB to/TO train/VB statistical/JJ models/NNS with/IN partial/JJ annotation/NN (/-LRB- PA/NNP )/-RRB- ./.
Taking/VBG dependency/NN parsing/VBG as/IN our/PRP$ case/NN study/NN ,/, this/DT paper/NN describes/VBZ and/CC compares/VBZ two/CD straightforward/JJ approaches/NNS for/IN three/CD mainstream/NN dependency/NN parsers/NNS ./.
The/DT first/JJ approach/NN is/VBZ previously/RB proposed/VBN to/TO directly/RB train/VB a/DT log/NN -/HYPH linear/JJ graph/NN -/HYPH based/VBN parser/NN (/-LRB- LLGPar/NN )/-RRB- with/IN PA/NNP based/VBN on/IN a/DT forest/NN -/HYPH based/VBN objective/NN ./.
This/DT work/NN for/IN the/DT first/JJ time/NN proposes/VBZ the/DT second/JJ approach/NN to/IN directly/RB training/VBG a/DT linear/JJ graph/NN -/HYPH based/VBN parse/VBP (/-LRB- LGPar/NNP )/-RRB- and/CC a/DT linear/JJ transition/NN -/HYPH based/VBN parser/NN (/-LRB- LTPar/NN )/-RRB- with/IN PA/NNP based/VBN on/IN the/DT idea/NN of/IN constrained/VBN decoding/NN ./.
We/PRP conduct/VBP extensive/JJ experiments/NNS on/IN Penn/NNP Treebank/NNP under/IN three/CD different/JJ settings/NNS for/IN simulating/VBG PA/NNP ,/, i.e./FW ,/, random/JJ dependencies/NNS ,/, most/JJS uncertain/JJ dependencies/NNS ,/, and/CC dependencies/NNS with/IN divergent/JJ outputs/NNS from/IN the/DT three/CD parsers/NNS ./.
The/DT results/NNS show/VBP that/IN LLGPar/NNP is/VBZ most/RBS effective/JJ in/IN learning/VBG from/IN PA/NNP and/CC LTPar/NNP lags/VBZ behind/IN the/DT graph/NN -/HYPH based/VBN counterparts/NNS by/IN large/JJ margin/NN ./.
Moreover/RB ,/, LGPar/NN and/CC LTPar/NN can/MD achieve/VB best/JJS performance/NN by/IN using/VBG LLGPar/NN to/TO complete/VB PA/NNP into/IN full/JJ annotation/NN (/-LRB- FA/NN )/-RRB- ./.
