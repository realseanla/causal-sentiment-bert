We/PRP introduce/VBP the/DT multiresolution/NN recurrent/JJ neural/JJ network/NN ,/, which/WDT extends/VBZ the/DT sequence/NN -/HYPH to/IN -/HYPH sequence/NN framework/NN to/IN model/NN natural/JJ language/NN generation/NN as/IN two/CD parallel/JJ discrete/JJ stochastic/JJ processes/NNS :/: a/DT sequence/NN of/IN high/JJ -/HYPH level/NN coarse/JJ tokens/NNS ,/, and/CC a/DT sequence/NN of/IN natural/JJ language/NN tokens/NNS ./.
There/EX are/VBP many/JJ ways/NNS to/TO estimate/VB or/CC learn/VB the/DT high/JJ -/HYPH level/NN coarse/JJ tokens/NNS ,/, but/CC we/PRP argue/VBP that/IN a/DT simple/JJ extraction/NN procedure/NN is/VBZ sufficient/JJ to/TO capture/VB a/DT wealth/NN of/IN high/JJ -/HYPH level/NN discourse/NN semantics/NNS ./.
Such/JJ procedure/NN allows/VBZ training/VBG the/DT multiresolution/NN recurrent/JJ neural/JJ network/NN by/IN maximizing/VBG the/DT exact/JJ joint/JJ log/NN -/HYPH likelihood/NN over/IN both/DT sequences/NNS ./.
In/IN contrast/NN to/IN the/DT standard/JJ log/NN -/HYPH likelihood/NN objective/JJ w.r.t./JJ natural/JJ language/NN tokens/NNS (/-LRB- word/NN perplexity/NN )/-RRB- ,/, optimizing/VBG the/DT joint/JJ log/NN -/HYPH likelihood/NN biases/NNS the/DT model/NN towards/IN modeling/VBG high/JJ -/HYPH level/NN abstractions/NNS ./.
We/PRP apply/VBP the/DT proposed/VBN model/NN to/IN the/DT task/NN of/IN dialogue/NN response/NN generation/NN in/IN two/CD challenging/JJ domains/NNS :/: the/DT Ubuntu/NNP technical/JJ support/NN domain/NN ,/, and/CC Twitter/NNP conversations/NNS ./.
On/IN Ubuntu/NNP ,/, the/DT model/NN outperforms/VBZ competing/VBG approaches/NNS by/IN a/DT substantial/JJ margin/NN ,/, achieving/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS according/VBG to/IN both/DT automatic/JJ evaluation/NN metrics/NNS and/CC a/DT human/JJ evaluation/NN study/NN ./.
On/IN Twitter/NNP ,/, the/DT model/NN appears/VBZ to/TO generate/VB more/RBR relevant/JJ and/CC on/IN -/HYPH topic/NN responses/NNS according/VBG to/IN automatic/JJ evaluation/NN metrics/NNS ./.
Finally/RB ,/, our/PRP$ experiments/NNS demonstrate/VBP that/IN the/DT proposed/VBN model/NN is/VBZ more/RBR adept/JJ at/IN overcoming/VBG the/DT sparsity/NN of/IN natural/JJ language/NN and/CC is/VBZ better/JJR able/JJ to/TO capture/VB long/RB -/HYPH term/NN structure/NN ./.
