Noisy/JJ data/NNS ,/, non-convex/JJ objectives/NNS ,/, model/NN misspecification/NN ,/, and/CC numerical/JJ instability/NN can/MD all/DT cause/VB undesired/JJ behaviors/NNS in/IN machine/NN learning/NN systems/NNS ./.
As/IN a/DT result/NN ,/, detecting/VBG actual/JJ implementation/NN errors/NNS can/MD be/VB extremely/RB difficult/JJ ./.
We/PRP demonstrate/VBP a/DT methodology/NN in/IN which/WDT developers/NNS use/VBP an/DT interactive/JJ proof/NN assistant/NN to/IN both/DT implement/VB their/PRP$ system/NN and/CC to/IN state/NN a/DT formal/JJ theorem/NN defining/VBG what/WP it/PRP means/VBZ for/IN their/PRP$ system/NN to/TO be/VB correct/JJ ./.
The/DT process/NN of/IN proving/VBG this/DT theorem/NN interactively/RB in/IN the/DT proof/NN assistant/NN exposes/VBZ all/DT implementation/NN errors/NNS since/IN any/DT error/NN in/IN the/DT program/NN would/MD cause/VB the/DT proof/NN to/TO fail/VB ./.
As/IN a/DT case/NN study/NN ,/, we/PRP implement/VBP a/DT new/JJ system/NN ,/, Certigrad/NNP ,/, for/IN optimizing/VBG over/IN stochastic/JJ computation/NN graphs/NNS ,/, and/CC we/PRP generate/VBP a/DT formal/JJ (/-LRB- i.e./FW machine/NN -/HYPH checkable/JJ )/-RRB- proof/NN that/IN the/DT gradients/NNS sampled/VBN by/IN the/DT system/NN are/VBP unbiased/JJ estimates/NNS of/IN the/DT true/JJ mathematical/JJ gradients/NNS ./.
We/PRP train/VBP a/DT variational/JJ autoencoder/NN using/VBG Certigrad/NNP and/CC find/VB the/DT performance/NN comparable/JJ to/IN training/VBG the/DT same/JJ model/NN in/IN TensorFlow/NNP ./.
