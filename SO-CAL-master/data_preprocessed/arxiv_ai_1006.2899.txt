In/IN this/DT paper/NN we/PRP propose/VBP an/DT approximated/VBN structured/JJ prediction/NN framework/NN for/IN large/JJ scale/NN graphical/JJ models/NNS and/CC derive/VBP message/NN -/HYPH passing/VBG algorithms/NNS for/IN learning/VBG their/PRP$ parameters/NNS efficiently/RB ./.
We/PRP first/RB relate/VBP CRFs/NNS and/CC structured/JJ SVMs/NNS and/CC show/VBP that/IN in/IN CRFs/NNS a/DT variant/NN of/IN the/DT log/NN -/HYPH partition/NN function/NN ,/, known/VBN as/IN soft/JJ -/HYPH max/NN ,/, smoothly/RB approximates/VBZ the/DT hinge/NN loss/NN function/NN of/IN structured/JJ SVMs/NNS ./.
We/PRP then/RB propose/VB an/DT intuitive/JJ approximation/NN for/IN the/DT structured/JJ prediction/NN problem/NN ,/, using/VBG duality/NN ,/, based/VBN on/IN local/JJ entropy/NN approximations/NNS and/CC derive/VBP an/DT efficient/JJ message/NN -/HYPH passing/VBG algorithm/NN that/WDT is/VBZ guaranteed/VBN to/TO converge/VB to/IN the/DT optimum/JJ for/IN concave/JJ entropy/NN approximations/NNS ./.
Unlike/IN existing/VBG approaches/NNS ,/, this/DT allows/VBZ us/PRP to/TO learn/VB efficiently/RB graphical/JJ models/NNS with/IN cycles/NNS and/CC very/RB large/JJ number/NN of/IN parameters/NNS ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ approach/NN in/IN an/DT image/NN denoising/NN task/NN ./.
This/DT task/NN was/VBD previously/RB solved/VBN by/IN sharing/VBG parameters/NNS across/IN cliques/NNS ./.
In/IN contrast/NN ,/, our/PRP$ algorithm/NN is/VBZ able/JJ to/TO efficiently/RB learn/VB large/JJ number/NN of/IN parameters/NNS resulting/VBG in/IN orders/NNS of/IN magnitude/NN better/RBR prediction/NN ./.
