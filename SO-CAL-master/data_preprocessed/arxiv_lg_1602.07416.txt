Memory/NN units/NNS have/VBP been/VBN widely/RB used/VBN to/TO enrich/VB the/DT capabilities/NNS of/IN deep/JJ networks/NNS on/IN capturing/VBG long/RB -/HYPH term/NN dependencies/NNS in/IN reasoning/NN and/CC prediction/NN tasks/NNS ,/, but/CC little/JJ investigation/NN exists/VBZ on/IN deep/JJ generative/NN models/NNS (/-LRB- DGMs/NNS )/-RRB- which/WDT are/VBP good/JJ at/IN inferring/VBG high/JJ -/HYPH level/NN invariant/JJ representations/NNS from/IN unlabeled/JJ data/NNS ./.
This/DT paper/NN presents/VBZ a/DT deep/JJ generative/JJ model/NN with/IN a/DT possibly/RB large/JJ external/JJ memory/NN and/CC an/DT attention/NN mechanism/NN to/TO capture/VB the/DT local/JJ detail/NN information/NN that/WDT is/VBZ often/RB lost/VBN in/IN the/DT bottom/NN -/HYPH up/NN abstraction/NN process/NN in/IN representation/NN learning/NN ./.
By/IN adopting/VBG a/DT smooth/JJ attention/NN model/NN ,/, the/DT whole/JJ network/NN is/VBZ trained/VBN end/NN -/HYPH to/IN -/HYPH end/NN by/IN optimizing/VBG a/DT variational/NN bound/VBN of/IN data/NNS likelihood/NN via/IN auto/NN -/HYPH encoding/VBG variational/JJ Bayesian/JJ methods/NNS ,/, where/WRB an/DT asymmetric/JJ recognition/NN network/NN is/VBZ learnt/VBN jointly/RB to/TO infer/VB high/JJ -/HYPH level/NN invariant/JJ representations/NNS ./.
The/DT asymmetric/JJ architecture/NN can/MD reduce/VB the/DT competition/NN between/IN bottom/NN -/HYPH up/NN invariant/JJ feature/NN extraction/NN and/CC top/JJ -/HYPH down/JJ generation/NN of/IN instance/NN details/NNS ./.
Our/PRP$ experiments/NNS on/IN several/JJ datasets/NNS demonstrate/VBP that/IN memory/NN can/MD significantly/RB boost/VB the/DT performance/NN of/IN DGMs/NNS and/CC even/RB achieve/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN various/JJ tasks/NNS ,/, including/VBG density/NN estimation/NN ,/, image/NN generation/NN ,/, and/CC missing/JJ value/NN imputation/NN ./.
