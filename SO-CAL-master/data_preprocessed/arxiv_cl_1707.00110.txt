The/DT standard/JJ content/NN -/HYPH based/VBN attention/NN mechanism/NN typically/RB used/VBN in/IN sequence/NN -/HYPH to/IN -/HYPH sequence/NN models/NNS is/VBZ computationally/RB expensive/JJ as/IN it/PRP requires/VBZ the/DT comparison/NN of/IN large/JJ encoder/NN and/CC decoder/NN states/NNS at/IN each/DT time/NN step/NN ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP an/DT alternative/JJ attention/NN mechanism/NN based/VBN on/IN a/DT fixed/VBN size/NN memory/NN representation/NN that/WDT is/VBZ more/RBR efficient/JJ ./.
Our/PRP$ technique/NN predicts/VBZ a/DT compact/JJ set/NN of/IN K/NNP attention/NN contexts/NNS during/IN encoding/VBG and/CC lets/VBZ the/DT decoder/NN compute/VB an/DT efficient/JJ lookup/NN that/WDT does/VBZ not/RB need/VB to/TO consult/VB the/DT memory/NN ./.
We/PRP show/VBP that/IN our/PRP$ approach/NN performs/VBZ on/IN -/HYPH par/NN with/IN the/DT standard/JJ attention/NN mechanism/NN while/IN yielding/VBG inference/NN speedups/NNS of/IN 20/CD percent/NN for/IN real/JJ -/HYPH world/NN translation/NN tasks/NNS and/CC more/JJR for/IN tasks/NNS with/IN longer/JJR sequences/NNS ./.
By/IN visualizing/VBG attention/NN scores/NNS we/PRP demonstrate/VBP that/IN our/PRP$ models/NNS learn/VBP distinct/JJ ,/, meaningful/JJ alignments/NNS ./.
