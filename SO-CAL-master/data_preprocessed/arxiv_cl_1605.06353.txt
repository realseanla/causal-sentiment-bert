In/IN this/DT work/NN ,/, we/PRP study/VBP parameter/NN tuning/NN towards/IN the/DT M$/$ ^/SYM 2/CD $/$ metric/JJ ,/, the/DT standard/JJ metric/JJ for/IN automatic/JJ grammar/NN error/NN correction/NN (/-LRB- GEC/NNP )/-RRB- tasks/NNS ./.
After/IN implementing/VBG M$/$ ^/SYM 2/CD $/$ as/IN a/DT scorer/NN in/IN the/DT Moses/NNP tuning/NN framework/NN ,/, we/PRP investigate/VBP interactions/NNS of/IN dense/JJ and/CC sparse/JJ features/NNS ,/, different/JJ optimizers/NNS ,/, and/CC tuning/VB strategies/NNS for/IN the/DT CoNLL/NN -/HYPH 2014/CD shared/VBD task/NN ./.
We/PRP notice/VBP erratic/JJ behavior/NN when/WRB optimizing/VBG sparse/JJ feature/NN weights/NNS with/IN M$/$ ^/SYM 2/CD $/$ and/CC offer/VBP partial/JJ solutions/NNS ./.
To/IN our/PRP$ surprise/NN ,/, we/PRP find/VBP that/IN a/DT bare/JJ -/HYPH bones/NNS phrase/NN -/HYPH based/VBN SMT/NNP setup/NN with/IN task/NN -/HYPH specific/JJ parameter/NN -/HYPH tuning/NN outperforms/VBZ all/DT previously/RB published/VBN results/NNS for/IN the/DT CoNLL/NN -/HYPH 2014/CD test/NN set/VBN by/IN a/DT large/JJ margin/NN (/-LRB- 46.37/CD percent/NN M$/$ ^/SYM 2/CD $/$ over/IN previously/RB 40.56/CD percent/NN ,/, by/IN a/DT neural/JJ encoder/NN -/HYPH decoder/NN model/NN )/-RRB- while/IN being/VBG trained/VBN on/IN the/DT same/JJ data/NNS ./.
Our/PRP$ newly/RB introduced/VBN dense/JJ and/CC sparse/JJ features/NNS widen/VBP that/IN gap/NN ,/, and/CC we/PRP improve/VBP the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN to/IN 49.49/CD percent/NN M$/$ ^/SYM 2/CD $/$ ./.
