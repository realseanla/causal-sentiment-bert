With/IN a/DT weighting/NN scheme/NN proportional/JJ to/IN t/NN ,/, a/DT traditional/JJ stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- algorithm/NN achieves/VBZ a/DT high/JJ probability/NN convergence/NN rate/NN of/IN O/NN (/-LRB- {/-LRB- \/SYM kappa/NN }/-RRB- //HYPH T/NN )/-RRB- for/IN strongly/RB convex/JJ functions/NNS ,/, instead/RB of/IN O/NN (/-LRB- {/-LRB- \/SYM kappa/NN }/-RRB- ln/NN (/-LRB- T/NN )/-RRB- //HYPH T/NN )/-RRB- ./.
We/PRP also/RB prove/VBP that/IN an/DT accelerated/VBN SGD/NNP algorithm/NN also/RB achieves/VBZ a/DT rate/NN of/IN O/NN (/-LRB- {/-LRB- \/SYM kappa/NN }/-RRB- //HYPH T/NN )/-RRB- ./.
