We/PRP reduce/VBP a/DT broad/JJ class/NN of/IN machine/NN learning/NN problems/NNS ,/, usually/RB addressed/VBN by/IN EM/NNP or/CC sampling/NN ,/, to/IN the/DT problem/NN of/IN finding/VBG the/DT $/$ k/CD $/$ extremal/CD rays/NNS spanning/VBG the/DT conical/JJ hull/NN of/IN a/DT data/NN point/NN set/NN ./.
These/DT $/$ k/CD $/$ "/`` anchors/NNS "/'' lead/VBP to/IN a/DT global/JJ solution/NN and/CC a/DT more/RBR interpretable/JJ model/NN that/WDT can/MD even/RB outperform/VB EM/NNP and/CC sampling/NN on/IN generalization/NN error/NN ./.
To/TO find/VB the/DT $/$ k/CD $/$ anchors/NNS ,/, we/PRP propose/VBP a/DT novel/JJ divide/NN -/HYPH and/CC -/HYPH conquer/VB learning/NN scheme/NN "/'' DCA/NNP "/'' that/WDT distributes/VBZ the/DT problem/NN to/IN $/$ \/CD mathcal/NN O/NN (/-LRB- k/CD \/SYM log/NN k/NN )/-RRB- $/$ same/JJ -/HYPH type/NN sub-problems/NNS on/IN different/JJ low/JJ -/HYPH D/NN random/JJ hyperplanes/NNS ,/, each/DT can/MD be/VB solved/VBN by/IN any/DT solver/NN ./.
For/IN the/DT 2D/NN sub-problem/NN ,/, we/PRP present/VBP a/DT non-iterative/JJ solver/NN that/WDT only/RB needs/VBZ to/TO compute/VB an/DT array/NN of/IN cosine/NN values/NNS and/CC its/PRP$ max/NN //HYPH min/NN entries/NNS ./.
DCA/NNP also/RB provides/VBZ a/DT faster/RBR subroutine/NN for/IN other/JJ methods/NNS to/TO check/VB whether/IN a/DT point/NN is/VBZ covered/VBN in/IN a/DT conical/JJ hull/NN ,/, which/WDT improves/VBZ algorithm/NN design/NN in/IN multiple/JJ dimensions/NNS and/CC brings/VBZ significant/JJ speedup/NN to/IN learning/NN ./.
We/PRP apply/VBP our/PRP$ method/NN to/TO GMM/NNP ,/, HMM/NNP ,/, LDA/NNP ,/, NMF/NNP and/CC subspace/NN clustering/NN ,/, then/RB show/VB its/PRP$ competitive/JJ performance/NN and/CC scalability/NN over/IN other/JJ methods/NNS on/IN rich/JJ datasets/NNS ./.
