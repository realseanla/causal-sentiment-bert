Active/JJ learning/NN aims/VBZ to/TO select/VB a/DT small/JJ subset/NN of/IN data/NNS for/IN annotation/NN such/JJ that/IN a/DT classifier/NN learned/VBD on/IN the/DT data/NNS is/VBZ highly/RB accurate/JJ ./.
This/DT is/VBZ usually/RB done/VBN using/VBG heuristic/NN selection/NN methods/NNS ,/, however/RB the/DT effectiveness/NN of/IN such/JJ methods/NNS is/VBZ limited/JJ and/CC moreover/RB ,/, the/DT performance/NN of/IN heuristics/NNS varies/VBZ between/IN datasets/NNS ./.
To/TO address/VB these/DT shortcomings/NNS ,/, we/PRP introduce/VBP a/DT novel/JJ formulation/NN by/IN reframing/VBG the/DT active/JJ learning/NN as/IN a/DT reinforcement/NN learning/VBG problem/NN and/CC explicitly/RB learning/VBG a/DT data/NN selection/NN policy/NN ,/, where/WRB the/DT policy/NN takes/VBZ the/DT role/NN of/IN the/DT active/JJ learning/NN heuristic/NN ./.
Importantly/RB ,/, our/PRP$ method/NN allows/VBZ the/DT selection/NN policy/NN learned/VBN using/VBG simulation/NN on/IN one/CD language/NN to/TO be/VB transferred/VBN to/IN other/JJ languages/NNS ./.
We/PRP demonstrate/VBP our/PRP$ method/NN using/VBG cross-lingual/JJ named/VBN entity/NN recognition/NN ,/, observing/VBG uniform/JJ improvements/NNS over/IN traditional/JJ active/JJ learning/NN ./.
