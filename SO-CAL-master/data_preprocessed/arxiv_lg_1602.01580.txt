We/PRP consider/VBP planning/VBG problems/NNS ,/, that/DT often/RB arise/VBP in/IN autonomous/JJ driving/NN applications/NNS ,/, in/IN which/WDT an/DT agent/NN should/MD decide/VB on/IN immediate/JJ actions/NNS so/IN as/IN to/TO optimize/VB a/DT long/JJ term/NN objective/NN ./.
For/IN example/NN ,/, when/WRB a/DT car/NN tries/VBZ to/TO merge/VB in/IN a/DT roundabout/JJ it/PRP should/MD decide/VB on/IN an/DT immediate/JJ acceleration/NN //HYPH braking/NN command/NN ,/, while/IN the/DT long/JJ term/NN effect/NN of/IN the/DT command/NN is/VBZ the/DT success/NN //HYPH failure/NN of/IN the/DT merge/NN ./.
Such/JJ problems/NNS are/VBP characterized/VBN by/IN continuous/JJ state/NN and/CC action/NN spaces/NNS ,/, and/CC by/IN interaction/NN with/IN multiple/JJ agents/NNS ,/, whose/WP$ behavior/NN can/MD be/VB adversarial/JJ ./.
We/PRP argue/VBP that/IN dual/JJ versions/NNS of/IN the/DT MDP/NNP framework/NN (/-LRB- that/IN depend/VB on/IN the/DT value/NN function/NN and/CC the/DT $/$ Q$/CD function/NN )/-RRB- are/VBP problematic/JJ for/IN autonomous/JJ driving/NN applications/NNS due/IN to/IN the/DT non/FW Markovian/FW of/IN the/DT natural/JJ state/NN space/NN representation/NN ,/, and/CC due/IN to/IN the/DT continuous/JJ state/NN and/CC action/NN spaces/NNS ./.
We/PRP propose/VBP to/TO tackle/VB the/DT planning/NN task/NN by/IN decomposing/VBG the/DT problem/NN into/IN two/CD phases/NNS :/: First/RB ,/, we/PRP apply/VBP supervised/JJ learning/NN for/IN predicting/VBG the/DT near/JJ future/NN based/VBN on/IN the/DT present/JJ ./.
We/PRP require/VBP that/IN the/DT predictor/NN will/MD be/VB differentiable/JJ with/IN respect/NN to/IN the/DT representation/NN of/IN the/DT present/JJ ./.
Second/RB ,/, we/PRP model/VBP a/DT full/JJ trajectory/NN of/IN the/DT agent/NN using/VBG a/DT recurrent/JJ neural/JJ network/NN ,/, where/WRB unexplained/JJ factors/NNS are/VBP modeled/VBN as/IN (/-LRB- additive/JJ )/-RRB- input/NN nodes/NNS ./.
This/DT allows/VBZ us/PRP to/TO solve/VB the/DT long/JJ -/HYPH term/NN planning/NN problem/NN using/VBG supervised/JJ learning/NN techniques/NNS and/CC direct/JJ optimization/NN over/IN the/DT recurrent/JJ neural/JJ network/NN ./.
Our/PRP$ approach/NN enables/VBZ us/PRP to/TO learn/VB robust/JJ policies/NNS by/IN incorporating/VBG adversarial/JJ elements/NNS to/IN the/DT environment/NN ./.
