In/IN the/DT Bayesian/JJ Reinforcement/NN Learning/NN (/-LRB- BRL/NN )/-RRB- setting/NN ,/, agents/NNS try/VBP to/TO maximise/VB the/DT collected/VBN rewards/NNS while/IN interacting/VBG with/IN their/PRP$ environment/NN while/IN using/VBG some/DT prior/JJ knowledge/NN that/WDT is/VBZ accessed/VBN beforehand/RB ./.
Many/JJ BRL/NN algorithms/NNS have/VBP already/RB been/VBN proposed/VBN ,/, but/CC even/RB though/IN a/DT few/JJ toy/NN examples/NNS exist/VBP in/IN the/DT literature/NN ,/, there/EX are/VBP still/RB no/DT extensive/JJ or/CC rigorous/JJ benchmarks/NNS to/TO compare/VB them/PRP ./.
The/DT paper/NN addresses/VBZ this/DT problem/NN ,/, and/CC provides/VBZ a/DT new/JJ BRL/NN comparison/NN methodology/NN along/IN with/IN the/DT corresponding/VBG open/JJ source/NN library/NN ./.
In/IN this/DT methodology/NN ,/, a/DT comparison/NN criterion/NN that/WDT measures/VBZ the/DT performance/NN of/IN algorithms/NNS on/IN large/JJ sets/NNS of/IN Markov/NNP Decision/NN Processes/NNS (/-LRB- MDPs/NNS )/-RRB- drawn/VBN from/IN some/DT probability/NN distributions/NNS is/VBZ defined/VBN ./.
In/IN order/NN to/TO enable/VB the/DT comparison/NN of/IN non-anytime/JJ algorithms/NNS ,/, our/PRP$ methodology/NN also/RB includes/VBZ a/DT detailed/JJ analysis/NN of/IN the/DT computation/NN time/NN requirement/NN of/IN each/DT algorithm/NN ./.
Our/PRP$ library/NN is/VBZ released/VBN with/IN all/DT source/NN code/NN and/CC documentation/NN :/: it/PRP includes/VBZ three/CD test/NN problems/NNS ,/, each/DT of/IN which/WDT has/VBZ two/CD different/JJ prior/JJ distributions/NNS ,/, and/CC seven/CD state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN RL/NN algorithms/NNS ./.
Finally/RB ,/, our/PRP$ library/NN is/VBZ illustrated/VBN by/IN comparing/VBG all/PDT the/DT available/JJ algorithms/NNS and/CC the/DT results/NNS are/VBP discussed/VBN ./.
