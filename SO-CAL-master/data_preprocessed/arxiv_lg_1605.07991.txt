We/PRP propose/VBP a/DT novel/NN ,/, efficient/JJ approach/NN for/IN distributed/VBN sparse/JJ learning/NN in/IN high/JJ -/HYPH dimensions/NNS ,/, where/WRB observations/NNS are/VBP randomly/RB partitioned/VBN across/IN machines/NNS ./.
Computationally/RB ,/, at/IN each/DT round/NN our/PRP$ method/NN only/RB requires/VBZ the/DT master/NN machine/NN to/TO solve/VB a/DT shifted/VBN ell_1/NN regularized/VBN M/NN -/HYPH estimation/NN problem/NN ,/, and/CC other/JJ workers/NNS to/IN compute/VB the/DT gradient/NN ./.
In/IN respect/NN of/IN communication/NN ,/, the/DT proposed/VBN approach/NN provably/RB matches/VBZ the/DT estimation/NN error/NN bound/VBN of/IN centralized/JJ methods/NNS within/IN constant/JJ rounds/NNS of/IN communications/NNS (/-LRB- ignoring/VBG logarithmic/JJ factors/NNS )/-RRB- ./.
We/PRP conduct/VBP extensive/JJ experiments/NNS on/IN both/DT simulated/JJ and/CC real/JJ world/NN datasets/NNS ,/, and/CC demonstrate/VBP encouraging/VBG performances/NNS on/IN high/JJ -/HYPH dimensional/JJ regression/NN and/CC classification/NN tasks/NNS ./.
