The/DT skip/VB -/HYPH thought/VBN model/NN has/VBZ been/VBN proven/VBN to/TO be/VB effective/JJ at/IN learning/VBG sentence/NN representations/NNS and/CC capturing/VBG sentence/NN semantics/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT suite/NN of/IN techniques/NNS to/TO trim/VB and/CC improve/VB it/PRP ./.
First/RB ,/, we/PRP validate/VBP a/DT hypothesis/NN that/IN ,/, given/VBN a/DT current/JJ sentence/NN ,/, inferring/VBG the/DT previous/JJ and/CC inferring/VBG the/DT next/JJ sentence/NN provide/VBP similar/JJ supervision/NN power/NN ,/, therefore/RB only/RB one/CD decoder/NN for/IN predicting/VBG the/DT next/JJ sentence/NN is/VBZ preserved/VBN in/IN our/PRP$ trimmed/RBR skip/VB -/HYPH thought/VBN model/NN ./.
Second/RB ,/, we/PRP present/VBP a/DT connection/NN layer/NN between/IN encoder/NN and/CC decoder/NN to/TO help/VB the/DT model/NN to/IN generalize/VB better/JJR on/IN semantic/JJ relatedness/NN tasks/NNS ./.
Third/JJ ,/, we/PRP found/VBD that/IN a/DT good/JJ word/NN embedding/NN initialization/NN is/VBZ also/RB essential/JJ for/IN learning/VBG better/JJR sentence/NN representations/NNS ./.
We/PRP train/VBP our/PRP$ model/NN unsupervised/JJ on/IN a/DT large/JJ corpus/NN with/IN contiguous/JJ sentences/NNS ,/, and/CC then/RB evaluate/VB the/DT trained/VBN model/NN on/IN 7/CD supervised/VBD tasks/NNS ,/, which/WDT includes/VBZ semantic/JJ relatedness/NN ,/, paraphrase/NN detection/NN ,/, and/CC text/NN classification/NN benchmarks/NNS ./.
We/PRP empirically/RB show/VBP that/IN ,/, our/PRP$ proposed/VBN model/NN is/VBZ a/DT faster/RBR ,/, lighter/JJR -/HYPH weight/NN and/CC equally/RB powerful/JJ alternative/NN to/IN the/DT original/JJ skip/VB -/HYPH thought/VBN model/NN ./.
