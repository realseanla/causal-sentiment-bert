We/PRP study/VBP a/DT general/JJ version/NN of/IN the/DT adversarial/JJ online/JJ learning/NN problem/NN ./.
We/PRP are/VBP given/VBN a/DT decision/NN set/VBD $/$ \/CD mathcal/NN {/-LRB- X/NN }/-RRB- $/$ in/IN a/DT reflexive/JJ Banach/NNP space/NN $/$ X$/CD and/CC a/DT sequence/NN of/IN reward/NN vectors/NNS in/IN the/DT dual/JJ space/NN of/IN $/$ X$/CD ./.
At/IN each/DT iteration/NN ,/, we/PRP choose/VBP an/DT action/NN from/IN $/$ \/CD mathcal/NN {/-LRB- X/NN }/-RRB- $/$ ,/, based/VBN on/IN the/DT observed/VBN sequence/NN of/IN previous/JJ rewards/NNS ./.
Our/PRP$ goal/NN is/VBZ to/TO minimize/VB regret/NN ,/, defined/VBN as/IN the/DT gap/NN between/IN the/DT realized/VBN reward/NN and/CC the/DT reward/NN of/IN the/DT best/JJS fixed/VBN action/NN in/IN hindsight/NN ./.
Using/VBG results/NNS from/IN infinite/JJ dimensional/JJ convex/NN analysis/NN ,/, we/PRP generalize/VBP the/DT method/NN of/IN Dual/JJ Averaging/NN (/-LRB- or/CC Follow/VB the/DT Regularized/NNP Leader/NNP )/-RRB- to/IN our/PRP$ setting/NN and/CC obtain/VB general/JJ upper/JJ bounds/NNS on/IN the/DT worst/JJS -/HYPH case/NN regret/NN that/WDT subsume/VBP a/DT wide/JJ range/NN of/IN results/NNS from/IN the/DT literature/NN ./.
Under/IN the/DT assumption/NN of/IN uniformly/RB continuous/JJ rewards/NNS ,/, we/PRP obtain/VBP explicit/JJ anytime/NN regret/NN bounds/NNS in/IN a/DT setting/NN where/WRB the/DT decision/NN set/NN is/VBZ the/DT set/NN of/IN probability/NN distributions/NNS on/IN a/DT compact/JJ metric/JJ space/NN $/$ S$/CD whose/WP$ Radon/NNP -/HYPH Nikodym/NNP derivatives/NNS are/VBP elements/NNS of/IN $/$ L/NN ^/SYM p/NN (/-LRB- S/NN )/-RRB- $/$ for/IN some/DT $/$ p/NN &gt;/SYM 1/CD $/$ ./.
Importantly/RB ,/, we/PRP make/VBP no/DT convexity/NN assumptions/NNS on/IN either/CC the/DT set/VBN $/$ S$/CD or/CC the/DT reward/NN functions/VBZ ./.
We/PRP also/RB prove/VBP a/DT general/JJ lower/JJR bound/VBN on/IN the/DT worst/JJS -/HYPH case/NN regret/NN for/IN any/DT online/JJ algorithm/NN ./.
We/PRP then/RB apply/VBP these/DT results/NNS to/IN the/DT problem/NN of/IN learning/NN in/IN repeated/VBN continuous/JJ two/CD -/HYPH player/NN zero/CD -/HYPH sum/NN games/NNS ,/, in/IN which/WDT players/NNS '/POS strategy/NN sets/NNS are/VBP compact/JJ metric/JJ spaces/NNS ./.
In/IN doing/VBG so/RB ,/, we/PRP first/RB prove/VB that/IN if/IN both/DT players/NNS play/VBP a/DT Hannan/NNP -/HYPH consistent/JJ strategy/NN ,/, then/RB with/IN probability/NN 1/CD the/DT empirical/JJ distributions/NNS of/IN play/NN weakly/RB converge/VBP to/IN the/DT set/NN of/IN Nash/NNP equilibria/NNS of/IN the/DT game/NN ./.
We/PRP then/RB show/VBP that/IN ,/, under/IN mild/JJ assumptions/NNS ,/, Dual/JJ Averaging/NN on/IN the/DT (/-LRB- infinite/JJ -/HYPH dimensional/JJ )/-RRB- space/NN of/IN probability/NN distributions/NNS indeed/RB achieves/VBZ Hannan/NNP -/HYPH consistency/NN ./.
Finally/RB ,/, we/PRP illustrate/VBP our/PRP$ results/NNS through/IN numerical/JJ examples/NNS ./.
