We/PRP describe/VBP and/CC analyze/VBP a/DT simple/JJ algorithm/NN for/IN principal/JJ component/NN analysis/NN ,/, VR/NN -/HYPH PCA/NN ,/, which/WDT uses/VBZ computationally/RB cheap/JJ stochastic/JJ iterations/NNS ,/, yet/CC converges/VBZ exponentially/RB fast/JJ to/IN the/DT optimal/JJ solution/NN ./.
In/IN contrast/NN ,/, existing/VBG algorithms/NNS suffer/VBP either/CC from/IN slow/JJ convergence/NN ,/, or/CC computationally/RB intensive/JJ iterations/NNS whose/WP$ runtime/NN scales/NNS with/IN the/DT data/NNS size/NN ./.
The/DT algorithm/NN builds/VBZ on/IN a/DT recent/JJ variance/NN -/HYPH reduced/VBN stochastic/JJ gradient/NN technique/NN ,/, which/WDT was/VBD previously/RB analyzed/VBN for/IN strongly/RB convex/JJ optimization/NN ,/, whereas/IN here/RB we/PRP apply/VBP it/PRP to/IN the/DT non-convex/JJ PCA/NN problem/NN ,/, using/VBG a/DT very/RB different/JJ analysis/NN ./.
