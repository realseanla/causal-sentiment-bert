Recently/RB ,/, several/JJ algorithms/NNS for/IN symbolic/JJ regression/NN (/-LRB- SR/NNP )/-RRB- emerged/VBD which/WDT employ/VBP a/DT form/NN of/IN multiple/JJ linear/JJ regression/NN (/-LRB- LR/NN )/-RRB- to/TO produce/VB generalized/VBN linear/JJ models/NNS ./.
The/DT use/NN of/IN LR/NN allows/VBZ the/DT algorithms/NNS to/TO create/VB models/NNS with/IN relatively/RB small/JJ error/NN right/NN from/IN the/DT beginning/NN of/IN the/DT search/NN ;/: such/JJ algorithms/NNS are/VBP thus/RB claimed/VBN to/TO be/VB (/-LRB- sometimes/RB by/IN orders/NNS of/IN magnitude/NN )/-RRB- faster/JJR than/IN SR/NNP algorithms/NNS based/VBN on/IN vanilla/NN genetic/JJ programming/NN ./.
However/RB ,/, a/DT systematic/JJ comparison/NN of/IN these/DT algorithms/NNS on/IN a/DT common/JJ set/NN of/IN problems/NNS is/VBZ still/RB missing/VBG ./.
In/IN this/DT paper/NN we/PRP conceptually/RB and/CC experimentally/RB compare/VB several/JJ representatives/NNS of/IN such/JJ algorithms/NNS (/-LRB- GPTIPS/NNP ,/, FFX/NNP ,/, and/CC EFS/NNP )/-RRB- ./.
They/PRP are/VBP applied/VBN as/IN off/IN -/HYPH the/DT -/HYPH shelf/NN ,/, ready/JJ -/HYPH to/IN -/HYPH use/NN techniques/NNS ,/, mostly/RB using/VBG their/PRP$ default/NN settings/NNS ./.
The/DT methods/NNS are/VBP compared/VBN on/IN several/JJ synthetic/JJ and/CC real/JJ -/HYPH world/NN SR/NNP benchmark/NN problems/NNS ./.
Their/PRP$ performance/NN is/VBZ also/RB related/VBN to/IN the/DT performance/NN of/IN three/CD conventional/JJ machine/NN learning/VBG algorithms/NNS ---/, multiple/JJ regression/NN ,/, random/JJ forests/NNS and/CC support/NN vector/NN regression/NN ./.
