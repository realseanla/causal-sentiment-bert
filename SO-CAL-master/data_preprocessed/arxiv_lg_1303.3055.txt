We/PRP study/VBP the/DT problem/NN of/IN learning/VBG Markov/NNP decision/NN processes/NNS with/IN finite/JJ state/NN and/CC action/NN spaces/NNS when/WRB the/DT transition/NN probability/NN distributions/NNS and/CC loss/NN functions/NNS are/VBP chosen/VBN adversarially/RB and/CC are/VBP allowed/VBN to/TO change/VB with/IN time/NN ./.
We/PRP introduce/VBP an/DT algorithm/NN whose/WP$ regret/NN with/IN respect/NN to/IN any/DT policy/NN in/IN a/DT comparison/NN class/NN grows/VBZ as/IN the/DT square/JJ root/NN of/IN the/DT number/NN of/IN rounds/NNS of/IN the/DT game/NN ,/, provided/VBD the/DT transition/NN probabilities/NNS satisfy/VBP a/DT uniform/NN mixing/VBG condition/NN ./.
Our/PRP$ approach/NN is/VBZ efficient/JJ as/RB long/RB as/IN the/DT comparison/NN class/NN is/VBZ polynomial/JJ and/CC we/PRP can/MD compute/VB expectations/NNS over/IN sample/NN paths/NNS for/IN each/DT policy/NN ./.
Designing/NNP an/DT efficient/JJ algorithm/NN with/IN small/JJ regret/NN for/IN the/DT general/JJ case/NN remains/VBZ an/DT open/JJ problem/NN ./.
