We/PRP consider/VBP a/DT setting/NN for/IN Inverse/JJ Reinforcement/NN Learning/NN (/-LRB- IRL/NN )/-RRB- where/WRB the/DT learner/NN is/VBZ extended/VBN with/IN the/DT ability/NN to/TO actively/RB select/VB multiple/JJ environments/NNS ,/, observing/VBG an/DT agent/NN 's/POS behavior/NN on/IN each/DT environment/NN ./.
We/PRP first/RB demonstrate/VBP that/IN if/IN the/DT learner/NN can/MD experiment/VB with/IN any/DT transition/NN dynamics/NNS on/IN some/DT fixed/VBN set/NN of/IN states/NNS and/CC actions/NNS ,/, then/RB there/RB exists/VBZ an/DT algorithm/NN that/WDT reconstructs/VBZ the/DT agent/NN 's/POS reward/NN function/NN to/IN the/DT fullest/JJS extent/NN theoretically/RB possible/JJ ,/, and/CC that/IN requires/VBZ only/RB a/DT small/JJ (/-LRB- logarithmic/JJ )/-RRB- number/NN of/IN experiments/NNS ./.
We/PRP contrast/VBP this/DT result/NN to/IN what/WP is/VBZ known/VBN about/IN IRL/NN in/IN single/JJ fixed/VBN environments/NNS ,/, namely/RB that/IN the/DT true/JJ reward/NN function/NN is/VBZ fundamentally/RB unidentifiable/JJ ./.
We/PRP then/RB extend/VBP this/DT setting/NN to/IN the/DT more/RBR realistic/JJ case/NN where/WRB the/DT learner/NN may/MD not/RB select/VB any/DT transition/NN dynamic/NN ,/, but/CC rather/RB is/VBZ restricted/VBN to/IN some/DT fixed/VBN set/NN of/IN environments/NNS that/IN it/PRP may/MD try/VB ./.
We/PRP connect/VBP the/DT problem/NN of/IN maximizing/VBG the/DT information/NN derived/VBN from/IN experiments/NNS to/IN submodular/JJ function/NN maximization/NN and/CC demonstrate/VBP that/IN a/DT greedy/JJ algorithm/NN is/VBZ near/IN optimal/JJ (/-LRB- up/RP to/IN logarithmic/JJ factors/NNS )/-RRB- ./.
Finally/RB ,/, we/PRP empirically/RB validate/VBP our/PRP$ algorithm/NN on/IN an/DT environment/NN inspired/VBN by/IN behavioral/JJ psychology/NN ./.
