In/IN recent/JJ years/NNS ,/, neural/JJ networks/NNS have/VBP enjoyed/VBN a/DT renaissance/NN as/IN function/NN approximators/NNS in/IN reinforcement/NN learning/NN ./.
Two/CD decades/NNS after/IN Teasauro/NNP 's/POS TD/NN -/HYPH Gammon/NNP achieved/VBD near/IN top/JJ -/HYPH level/NN human/JJ performance/NN in/IN backgammon/NN ,/, the/DT deep/JJ reinforcement/NN learning/VBG algorithm/NN DQN/NNP (/-LRB- combining/VBG Q/NN -/HYPH learning/NN with/IN a/DT deep/JJ neural/JJ network/NN ,/, experience/NN replay/NN ,/, and/CC a/DT separate/JJ target/NN network/NN )/-RRB- achieved/VBN human/JJ -/HYPH level/NN performance/NN in/IN many/JJ Atari/NNP 2600/CD games/NNS ./.
The/DT purpose/NN of/IN this/DT study/NN is/VBZ twofold/JJ ./.
First/RB ,/, based/VBN on/IN the/DT expected/VBN energy/NN restricted/VBN Boltzmann/NNP machine/NN (/-LRB- EE/NN -/HYPH RBM/NN )/-RRB- ,/, we/PRP propose/VBP two/CD activation/NN functions/NNS for/IN neural/JJ network/NN function/NN approximation/NN in/IN reinforcement/NN learning/NN :/: the/DT sigmoid/NN -/HYPH weighted/VBN linear/JJ (/-LRB- SiL/NN )/-RRB- unit/NN and/CC its/PRP$ derivative/JJ function/NN (/-LRB- SiLd1/NN )/-RRB- ./.
The/DT activation/NN of/IN the/DT SiL/NN unit/NN is/VBZ computed/VBN by/IN the/DT sigmoid/NN function/NN multiplied/VBN by/IN its/PRP$ input/NN ,/, which/WDT is/VBZ equal/JJ to/IN the/DT contribution/NN to/IN the/DT output/NN from/IN one/CD hidden/VBN unit/NN in/IN an/DT EE/NN -/HYPH RBM/NN ./.
Second/RB ,/, we/PRP suggest/VBP that/IN the/DT more/RBR traditional/JJ approach/NN of/IN using/VBG on/IN -/HYPH policy/NN learning/NN with/IN eligibility/NN traces/NNS ,/, instead/RB of/IN experience/NN replay/NN ,/, and/CC softmax/JJ action/NN selection/NN can/MD be/VB competitive/JJ with/IN DQN/NN ,/, without/IN the/DT need/NN for/IN a/DT separate/JJ target/NN network/NN ./.
We/PRP validate/VBP our/PRP$ proposed/VBN approach/NN by/IN ,/, first/RB ,/, achieving/VBG new/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN both/DT stochastic/JJ SZ/NNP -/HYPH Tetris/NNP and/CC Tetris/NNP with/IN a/DT small/JJ 10x10/NN board/NN ,/, using/VBG TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- learning/NN and/CC shallow/JJ SiLd1/NN network/NN agents/NNS ,/, and/CC ,/, then/RB ,/, outperforming/VBG DQN/NNP in/IN the/DT Atari/NNP 2600/CD domain/NN by/IN using/VBG a/DT deep/JJ Sarsa/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- agent/NN with/IN SiL/NN and/CC SiLd1/NN hidden/VBN units/NNS ./.
