Distance/NNP metric/JJ learning/NN aims/VBZ to/TO learn/VB from/IN the/DT given/VBN training/NN data/NNS a/DT valid/JJ distance/NN metric/JJ ,/, with/IN which/WDT the/DT similarity/NN between/IN data/NNS samples/NNS can/MD be/VB more/RBR effectively/RB evaluated/VBN for/IN classification/NN ./.
Metric/NNP learning/NN is/VBZ often/RB formulated/VBN as/IN a/DT convex/NN or/CC nonconvex/JJ optimization/NN problem/NN ,/, while/IN many/JJ existing/VBG metric/JJ learning/NN algorithms/NNS become/VBP inefficient/JJ for/IN large/JJ scale/NN problems/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP formulate/VBP metric/JJ learning/NN as/IN a/DT kernel/NN classification/NN problem/NN ,/, and/CC solve/VB it/PRP by/IN iterated/VBN training/NN of/IN support/NN vector/NN machines/NNS (/-LRB- SVM/NN )/-RRB- ./.
The/DT new/JJ formulation/NN is/VBZ easy/JJ to/TO implement/VB ,/, efficient/JJ in/IN training/NN ,/, and/CC tractable/JJ for/IN large/JJ -/HYPH scale/NN problems/NNS ./.
Two/CD novel/JJ metric/JJ learning/NN models/NNS ,/, namely/RB Positive/JJ -/HYPH semidefinite/NN Constrained/VBN Metric/NNP Learning/NNP (/-LRB- PCML/NNP )/-RRB- and/CC Nonnegative/JJ -/HYPH coefficient/NN Constrained/VBN Metric/NNP Learning/NNP (/-LRB- NCML/NNP )/-RRB- ,/, are/VBP developed/VBN ./.
Both/DT PCML/NN and/CC NCML/NN can/MD guarantee/VB the/DT global/JJ optimality/NN of/IN their/PRP$ solutions/NNS ./.
Experimental/JJ results/NNS on/IN UCI/NNP dataset/NN classification/NN ,/, handwritten/JJ digit/NN recognition/NN ,/, face/NN verification/NN and/CC person/NN re-identification/NN demonstrate/VBP that/IN the/DT proposed/VBN metric/JJ learning/NN methods/NNS achieve/VBP higher/JJR classification/NN accuracy/NN than/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS and/CC they/PRP are/VBP significantly/RB more/RBR efficient/JJ in/IN training/NN ./.
