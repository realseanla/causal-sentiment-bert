Most/JJS existing/VBG sequence/NN labelling/NN models/NNS rely/VBP on/IN a/DT fixed/VBN decomposition/NN of/IN a/DT target/NN sequence/NN into/IN a/DT sequence/NN of/IN basic/JJ units/NNS ./.
These/DT methods/NNS suffer/VBP from/IN two/CD major/JJ drawbacks/NNS :/: 1/LS )/-RRB- the/DT set/NN of/IN basic/JJ units/NNS is/VBZ fixed/VBN ,/, such/JJ as/IN the/DT set/NN of/IN words/NNS ,/, characters/NNS or/CC phonemes/NNS in/IN speech/NN recognition/NN ,/, and/CC 2/LS )/-RRB- the/DT decomposition/NN of/IN target/NN sequences/NNS is/VBZ fixed/VBN ./.
These/DT drawbacks/NNS usually/RB result/VBP in/IN sub-optimal/JJ performance/NN of/IN modeling/NN sequences/NNS ./.
In/IN this/DT pa/NN -/HYPH per/NN ,/, we/PRP extend/VBP the/DT popular/JJ CTC/NN loss/NN criterion/NN to/TO alleviate/VB these/DT limitations/NNS ,/, and/CC propose/VB a/DT new/JJ loss/NN function/NN called/VBN Gram/NNP -/HYPH CTC/NNP ./.
While/IN preserving/VBG the/DT advantages/NNS of/IN CTC/NN ,/, Gram/NNP -/HYPH CTC/NNP automatically/RB learns/VBZ the/DT best/JJS set/NN of/IN basic/JJ units/NNS (/-LRB- grams/NNS )/-RRB- ,/, as/RB well/RB as/IN the/DT most/RBS suitable/JJ decomposition/NN of/IN tar/NN -/HYPH get/NN sequences/NNS ./.
Unlike/IN CTC/NN ,/, Gram/NNP -/HYPH CTC/NNP allows/VBZ the/DT model/NN to/IN output/NN variable/JJ number/NN of/IN characters/NNS at/IN each/DT time/NN step/NN ,/, which/WDT enables/VBZ the/DT model/NN to/TO capture/VB longer/JJR term/NN dependency/NN and/CC improves/VBZ the/DT computational/JJ efficiency/NN ./.
We/PRP demonstrate/VBP that/IN the/DT proposed/VBN Gram/NNP -/HYPH CTC/NNP improves/VBZ CTC/NN in/IN terms/NNS of/IN both/DT performance/NN and/CC efficiency/NN on/IN the/DT large/JJ vocabulary/NN speech/NN recognition/NN task/NN at/IN multiple/JJ scales/NNS of/IN data/NNS ,/, and/CC that/DT with/IN Gram/NNP -/HYPH CTC/NNP we/PRP can/MD outperform/VB the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN on/IN a/DT standard/JJ speech/NN benchmark/NN ./.
