We/PRP develop/VBP an/DT efficient/JJ alternating/VBG framework/NN for/IN learning/VBG Factorization/NNP Machine/NNP (/-LRB- FM/NNP )/-RRB- on/IN steaming/VBG data/NNS with/IN provable/JJ guarantees/NNS ./.
When/WRB the/DT feature/NN is/VBZ $/$ d/LS $/$ -/HYPH dimension/NN and/CC the/DT target/NN second/JJ order/NN coefficient/NN matrix/NN in/IN FM/NNP is/VBZ of/IN rank/NN $/$ k/CD $/$ ,/, our/PRP$ algorithm/NN converges/VBZ linearly/RB ,/, achieves/VBZ $/$ O/UH (/-LRB- \/SYM epsilon/NN )/-RRB- $/$ recovery/NN error/NN after/IN retrieving/VBG $/$ O/UH (/-LRB- k/CD ^/SYM {/-LRB- 3/CD }/-RRB- d/NN \/SYM log/NN (/-LRB- 1/CD //SYM \/SYM epsilon/SYM )/-RRB- )/-RRB- $/$ training/VBG instances/NNS ,/, consumes/VBZ $/$ O/UH (/-LRB- kd/NN )/-RRB- $/$ memory/NN in/IN one/CD -/HYPH pass/NN of/IN dataset/NN and/CC only/RB requires/VBZ matrix/NN -/HYPH vector/NN product/NN operations/NNS in/IN each/DT iteration/NN ./.
The/DT key/JJ ingredient/NN of/IN our/PRP$ framework/NN is/VBZ a/DT construction/NN of/IN an/DT estimation/NN sequence/NN endowed/VBN with/IN a/DT so/RB -/HYPH called/VBN Conditionally/RB Independent/JJ RIP/UH condition/NN ./.
As/IN special/JJ cases/NNS of/IN FM/NNP ,/, our/PRP$ framework/NN can/MD be/VB applied/VBN to/IN symmetric/JJ or/CC asymmetric/JJ rank/NN -/HYPH one/CD matrix/NN sensing/VBG problems/NNS ,/, such/JJ as/IN inductive/JJ matrix/NN completion/NN and/CC phase/NN retrieval/NN ./.
