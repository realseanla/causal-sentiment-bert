Traditional/JJ speaker/NN change/NN detection/NN in/IN dialogues/NNS is/VBZ typically/RB based/VBN on/IN audio/JJ input/NN ./.
In/IN some/DT scenarios/NNS ,/, however/RB ,/, researchers/NNS can/MD only/RB obtain/VB text/NN ,/, and/CC do/VBP not/RB have/VB access/NN to/IN raw/JJ audio/NN signals/NNS ./.
Moreover/RB ,/, with/IN the/DT increasing/VBG need/NN of/IN deep/JJ semantic/JJ processing/NN ,/, text/NN -/HYPH based/VBN dialogue/NN understanding/NN is/VBZ attracting/VBG more/JJR attention/NN in/IN the/DT community/NN ./.
These/DT raise/VBP the/DT problem/NN of/IN text/NN -/HYPH based/VBN speaker/NN change/NN detection/NN ./.
In/IN this/DT paper/NN ,/, we/PRP formulate/VBP the/DT task/NN as/IN a/DT matching/NN problem/NN of/IN utterances/NNS before/IN and/CC after/IN a/DT certain/JJ decision/NN point/NN ;/: we/PRP propose/VBP a/DT hierarchical/JJ recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- with/IN static/NN sentence/NN -/HYPH level/NN attention/NN ./.
Our/PRP$ model/NN comprises/VBZ three/CD main/JJ components/NNS :/: a/DT sentence/NN encoder/NN with/IN a/DT long/JJ short/JJ term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- -/HYPH based/VBN RNN/NNP ,/, a/DT context/NN encoder/NN with/IN another/DT LSTM/NNP -/HYPH RNN/NNP ,/, and/CC a/DT static/NN sentence/NN -/HYPH level/NN attention/NN mechanism/NN ,/, which/WDT allows/VBZ rich/JJ information/NN interaction/NN ./.
Experimental/JJ results/NNS show/VBP that/IN neural/JJ networks/NNS consistently/RB achieve/VB better/JJR performance/NN than/IN feature/NN -/HYPH based/VBN approaches/NNS ,/, and/CC that/IN our/PRP$ attention/NN -/HYPH based/VBN model/NN significantly/RB outperforms/VBZ non-attention/JJ neural/JJ networks/NNS ./.
