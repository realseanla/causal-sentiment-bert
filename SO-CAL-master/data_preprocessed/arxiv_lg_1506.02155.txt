Kernel/NNP methods/NNS represent/VBP one/CD of/IN the/DT most/RBS powerful/JJ tools/NNS in/IN machine/NN learning/NN to/TO tackle/VB problems/NNS expressed/VBN in/IN terms/NNS of/IN function/NN values/NNS and/CC derivatives/NNS due/IN to/IN their/PRP$ capability/NN to/TO represent/VB and/CC model/VB complex/JJ relations/NNS ./.
While/IN these/DT methods/NNS show/VBP good/JJ versatility/NN ,/, they/PRP are/VBP computationally/RB intensive/JJ and/CC have/VBP poor/JJ scalability/NN to/IN large/JJ data/NNS as/IN they/PRP require/VBP operations/NNS on/IN Gram/NNP matrices/NNS ./.
In/IN order/NN to/TO mitigate/VB this/DT serious/JJ computational/JJ limitation/NN ,/, recently/RB randomized/VBD constructions/NNS have/VBP been/VBN proposed/VBN in/IN the/DT literature/NN ,/, which/WDT allow/VBP the/DT application/NN of/IN fast/JJ linear/JJ algorithms/NNS ./.
Random/NNP Fourier/NNP features/VBZ (/-LRB- RFF/NNP )/-RRB- are/VBP among/IN the/DT most/RBS popular/JJ and/CC widely/RB applied/VBN constructions/NNS :/: they/PRP provide/VBP an/DT easily/RB computable/JJ ,/, low/JJ -/HYPH dimensional/JJ feature/NN representation/NN for/IN shift/NN -/HYPH invariant/JJ kernels/NNS ./.
Despite/IN the/DT popularity/NN of/IN RFFs/NNS ,/, very/RB little/RB is/VBZ understood/VBN theoretically/RB about/IN their/PRP$ approximation/NN quality/NN ./.
In/IN this/DT paper/NN ,/, we/PRP provide/VBP the/DT first/JJ detailed/JJ theoretical/JJ analysis/NN about/IN the/DT approximation/NN quality/NN of/IN RFFs/NNS by/IN establishing/VBG optimal/JJ (/-LRB- in/IN terms/NNS of/IN the/DT RFF/NNP dimension/NN )/-RRB- performance/NN guarantees/NNS in/IN uniform/NN and/CC $/$ L/NN ^/SYM r/VBP $/$ (/-LRB- $/$ 1/CD \/SYM le/FW r/VBP &lt;/-LRB- \/SYM infty/JJ $/$ )/-RRB- norms/NNS ./.
We/PRP also/RB propose/VBP a/DT RFF/NN approximation/NN to/IN derivatives/NNS of/IN a/DT kernel/NN with/IN a/DT theoretical/JJ study/NN on/IN its/PRP$ approximation/NN quality/NN ./.
