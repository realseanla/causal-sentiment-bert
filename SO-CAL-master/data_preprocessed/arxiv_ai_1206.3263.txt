Bounded/VBN policy/NN iteration/NN is/VBZ an/DT approach/NN to/IN solving/VBG infinite/JJ -/HYPH horizon/NN POMDPs/NNS that/WDT represents/VBZ policies/NNS as/IN stochastic/JJ finite/NN -/HYPH state/NN controllers/NNS and/CC iteratively/RB improves/VBZ a/DT controller/NN by/IN adjusting/VBG the/DT parameters/NNS of/IN each/DT node/NN using/VBG linear/JJ programming/NN ./.
In/IN the/DT original/JJ algorithm/NN ,/, the/DT size/NN of/IN the/DT linear/JJ programs/NNS ,/, and/CC thus/RB the/DT complexity/NN of/IN policy/NN improvement/NN ,/, depends/VBZ on/IN the/DT number/NN of/IN parameters/NNS of/IN each/DT node/NN ,/, which/WDT grows/VBZ with/IN the/DT size/NN of/IN the/DT controller/NN ./.
But/CC in/IN practice/NN ,/, the/DT number/NN of/IN parameters/NNS of/IN a/DT node/NN with/IN non-zero/JJ values/NNS is/VBZ often/RB very/RB small/JJ ,/, and/CC does/VBZ not/RB grow/VB with/IN the/DT size/NN of/IN the/DT controller/NN ./.
Based/VBN on/IN this/DT observation/NN ,/, we/PRP develop/VBP a/DT version/NN of/IN bounded/VBN policy/NN iteration/NN that/WDT leverages/VBZ the/DT sparse/JJ structure/NN of/IN a/DT stochastic/JJ finite/NN -/HYPH state/NN controller/NN ./.
In/IN each/DT iteration/NN ,/, it/PRP improves/VBZ a/DT policy/NN by/IN the/DT same/JJ amount/NN as/IN the/DT original/JJ algorithm/NN ,/, but/CC with/IN much/RB better/JJR scalability/NN ./.
