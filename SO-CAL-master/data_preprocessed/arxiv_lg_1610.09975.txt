We/PRP present/VBP results/NNS that/WDT show/VBP it/PRP is/VBZ possible/JJ to/TO build/VB a/DT competitive/JJ ,/, greatly/RB simplified/VBN ,/, large/JJ vocabulary/NN continuous/JJ speech/NN recognition/NN system/NN with/IN whole/JJ words/NNS as/IN acoustic/JJ units/NNS ./.
We/PRP model/VBP the/DT output/NN vocabulary/NN of/IN about/RB 100,000/CD words/NNS directly/RB using/VBG deep/JJ bi-directional/JJ LSTM/NN RNNs/NNS with/IN CTC/NN loss/NN ./.
The/DT model/NN is/VBZ trained/VBN on/IN 125,000/CD hours/NNS of/IN semi-supervised/VBN acoustic/JJ training/NN data/NNS ,/, which/WDT enables/VBZ us/PRP to/TO alleviate/VB the/DT data/NNS sparsity/NN problem/NN for/IN word/NN models/NNS ./.
We/PRP show/VBP that/IN the/DT CTC/NN word/NN models/NNS work/VBP very/RB well/RB as/IN an/DT end/NN -/HYPH to/IN -/HYPH end/NN all/RB -/HYPH neural/JJ speech/NN recognition/NN model/NN without/IN the/DT use/NN of/IN traditional/JJ context/NN -/HYPH dependent/JJ sub-word/JJ phone/NN units/NNS that/WDT require/VBP a/DT pronunciation/NN lexicon/NN ,/, and/CC without/IN any/DT language/NN model/NN removing/VBG the/DT need/NN to/TO decode/VB ./.
We/PRP demonstrate/VBP that/IN the/DT CTC/NN word/NN models/NNS perform/VBP better/JJR than/IN a/DT strong/JJ ,/, more/RBR complex/JJ ,/, state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baseline/NN with/IN sub-word/JJ units/NNS ./.
