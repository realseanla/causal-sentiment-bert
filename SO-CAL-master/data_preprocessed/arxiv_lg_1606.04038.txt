In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT framework/NN for/IN training/NN multiple/JJ neural/JJ networks/NNS simultaneously/RB ./.
The/DT parameters/NNS from/IN all/DT models/NNS are/VBP regularised/VBN by/IN the/DT tensor/NN trace/NN norm/NN ,/, so/IN that/IN one/CD neural/JJ network/NN is/VBZ encouraged/VBN to/TO reuse/VB others/NNS '/POS parameters/NNS if/IN possible/JJ --/: this/DT is/VBZ the/DT main/JJ motivation/NN behind/IN multi-task/VB learning/NN ./.
In/IN contrast/NN to/IN many/JJ deep/JJ multi-task/VB learning/VBG work/NN ,/, we/PRP do/VBP not/RB predefine/VB a/DT parameter/NN sharing/NN strategy/NN by/IN tying/VBG some/DT (/-LRB- usually/RB bottom/JJ )/-RRB- layers/NNS '/POS parameters/NNS ,/, instead/RB ,/, our/PRP$ framework/NN allows/VBZ the/DT sharing/NN for/IN all/DT shareable/JJ layers/NNS thus/RB the/DT sharing/NN strategy/NN is/VBZ learned/VBN from/IN a/DT pure/JJ data/NN -/HYPH driven/VBN way/NN ./.
