The/DT activation/NN function/NN of/IN Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNNs/NNS )/-RRB- has/VBZ undergone/VBN many/JJ changes/NNS during/IN the/DT last/JJ decades/NNS ./.
Since/IN the/DT advent/NN of/IN the/DT well/NN -/HYPH known/VBN non-saturated/JJ Rectified/VBN Linear/NNP Unit/NNP (/-LRB- ReLU/NNP )/-RRB- ,/, many/JJ have/VBP tried/VBN to/TO further/RB improve/VB the/DT performance/NN of/IN the/DT networks/NNS with/IN more/JJR elaborate/JJ functions/NNS ./.
Examples/NNS are/VBP the/DT Leaky/NNP ReLU/NNP (/-LRB- LReLU/NNP )/-RRB- to/TO remove/VB zero/CD gradients/NNS and/CC Exponential/JJ Linear/NNP Unit/NNP (/-LRB- ELU/NNP )/-RRB- to/TO reduce/VB bias/NN shift/NN ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP the/DT Parametric/NNP ELU/NNP (/-LRB- PELU/NNP )/-RRB- ,/, an/DT adaptive/JJ activation/NN function/NN that/WDT allows/VBZ the/DT DNNs/NNS to/TO adopt/VB different/JJ non-linear/JJ behaviors/NNS throughout/IN the/DT training/NN phase/NN ./.
We/PRP contribute/VBP in/IN three/CD ways/NNS :/: (/-LRB- 1/LS )/-RRB- we/PRP show/VBP that/IN PELU/NNP increases/VBZ the/DT network/NN flexibility/NN to/TO counter/VB vanishing/VBG gradient/NN ,/, (/-LRB- 2/LS )/-RRB- we/PRP provide/VBP a/DT gradient/NN -/HYPH based/VBN optimization/NN framework/NN to/TO learn/VB the/DT parameters/NNS of/IN the/DT function/NN ,/, and/CC (/-LRB- 3/LS )/-RRB- we/PRP conduct/VBP several/JJ experiments/NNS on/IN MNIST/NNP ,/, CIFAR/NNP -/HYPH 10/100/CD and/CC ImageNet/NNP with/IN different/JJ network/NN architectures/NNS ,/, such/JJ as/IN NiN/NNP ,/, Overfeat/NNP ,/, All/NNP -/HYPH CNN/NNP ,/, ResNet/NNP and/CC Vgg/NNP ,/, to/TO demonstrate/VB the/DT general/JJ applicability/NN of/IN the/DT approach/NN ./.
Our/PRP$ proposed/VBN PELU/NN has/VBZ shown/VBN relative/JJ error/NN improvements/NNS of/IN 4.45/CD percent/NN and/CC 5.68/CD percent/NN on/IN CIFAR/NN -/HYPH 10/CD and/CC 100/CD ,/, and/CC as/RB much/RB as/IN 7.28/CD percent/NN with/IN only/RB 0.0003/CD percent/NN parameter/NN increase/NN on/IN ImageNet/NNP ,/, along/IN with/IN faster/JJR convergence/NN rate/NN in/IN almost/RB all/DT test/NN scenarios/NNS ./.
We/PRP also/RB observed/VBD that/IN Vgg/NN using/VBG PELU/NN tended/VBD to/TO prefer/VB activations/NNS saturating/VBG close/JJ to/IN zero/CD ,/, as/IN in/IN ReLU/NNP ,/, except/IN at/IN last/JJ layer/NN ,/, which/WDT saturated/VBN near/IN -2/NN ./.
These/DT results/NNS suggest/VBP that/IN varying/VBG the/DT shape/NN of/IN the/DT activations/NNS during/IN training/NN along/IN with/IN the/DT other/JJ parameters/NNS helps/VBZ to/TO control/VB vanishing/VBG gradients/NNS and/CC bias/NN shift/NN ,/, thus/RB facilitating/VBG learning/NN ./.
