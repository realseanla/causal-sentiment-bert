In/IN this/DT paper/NN we/PRP propose/VBP a/DT multi-convex/JJ framework/NN for/IN multi-task/VB learning/NN that/WDT improves/VBZ predictions/NNS by/IN learning/VBG relationships/NNS both/CC between/IN tasks/NNS and/CC between/IN features/NNS ./.
Our/PRP$ framework/NN is/VBZ a/DT generalization/NN of/IN related/JJ methods/NNS in/IN multi-task/VB learning/NN ,/, that/IN either/CC learn/VB task/NN relationships/NNS ,/, or/CC feature/NN relationships/NNS ,/, but/CC not/RB both/DT ./.
We/PRP start/VBP with/IN a/DT hierarchical/JJ Bayesian/JJ model/NN ,/, and/CC use/VB the/DT empirical/JJ Bayes/NNP method/NN to/TO transform/VB the/DT underlying/VBG inference/NN problem/NN into/IN a/DT multi-convex/JJ optimization/NN problem/NN ./.
We/PRP propose/VBP a/DT coordinate-wise/JJ minimization/NN algorithm/NN that/WDT has/VBZ a/DT closed/JJ form/NN solution/NN for/IN each/DT block/NN subproblem/NN ./.
Naively/RB these/DT solutions/NNS would/MD be/VB expensive/JJ to/TO compute/VB ,/, but/CC by/IN using/VBG the/DT theory/NN of/IN doubly/RB stochastic/JJ matrices/NNS ,/, we/PRP are/VBP able/JJ to/TO reduce/VB the/DT underlying/VBG matrix/NN optimization/NN subproblem/NN into/IN a/DT minimum/JJ weight/NN perfect/JJ matching/NN problem/NN on/IN a/DT complete/JJ bipartite/JJ graph/NN ,/, and/CC solve/VB it/PRP analytically/RB and/CC efficiently/RB ./.
To/TO solve/VB the/DT weight/NN learning/NN subproblem/NN ,/, we/PRP propose/VBP three/CD different/JJ strategies/NNS ,/, including/VBG a/DT gradient/NN descent/NN method/NN with/IN linear/JJ convergence/NN guarantee/NN when/WRB the/DT instances/NNS are/VBP not/RB shared/VBN by/IN multiple/JJ tasks/NNS ,/, and/CC a/DT numerical/JJ solution/NN based/VBN on/IN Sylvester/NNP equation/NN when/WRB instances/NNS are/VBP shared/VBN ./.
We/PRP demonstrate/VBP the/DT efficiency/NN of/IN our/PRP$ method/NN on/IN both/DT synthetic/JJ datasets/NNS and/CC real/JJ -/HYPH world/NN datasets/NNS ./.
Experiments/NNS show/VBP that/IN the/DT proposed/VBN optimization/NN method/NN is/VBZ orders/NNS of/IN magnitude/NN faster/RBR than/IN an/DT off/RB -/HYPH the/DT -/HYPH shelf/NN projected/VBN gradient/NN method/NN ,/, and/CC our/PRP$ model/NN is/VBZ able/JJ to/TO exploit/VB the/DT correlation/NN structures/NNS among/IN multiple/JJ tasks/NNS and/CC features/NNS ./.
