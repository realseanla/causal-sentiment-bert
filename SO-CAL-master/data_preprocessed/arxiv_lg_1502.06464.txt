We/PRP propose/VBP rectified/VBN factor/NN networks/NNS (/-LRB- RFNs/NNS )/-RRB- as/IN generative/JJ unsupervised/JJ models/NNS ,/, which/WDT learn/VBP robust/JJ ,/, very/RB sparse/JJ ,/, and/CC non-linear/JJ codes/NNS with/IN many/JJ code/NN units/NNS ./.
RFN/NN learning/NN is/VBZ a/DT variational/JJ expectation/NN maximization/NN (/-LRB- EM/NN )/-RRB- algorithm/NN with/IN unknown/JJ prior/JJ which/WDT includes/VBZ (/-LRB- i/LS )/-RRB- rectified/VBN posterior/JJ means/NNS ,/, (/-LRB- ii/LS )/-RRB- normalized/VBN signals/NNS of/IN hidden/VBN units/NNS ,/, and/CC (/-LRB- iii/LS )/-RRB- dropout/NN ./.
Like/IN factor/NN analysis/NN ,/, RFNs/NNS explain/VBP the/DT data/NNS variance/NN by/IN their/PRP$ parameters/NNS ./.
For/IN pretraining/NN of/IN deep/JJ networks/NNS on/IN MNIST/NNP ,/, rectangle/NN data/NNS ,/, convex/NN shapes/NNS ,/, NORB/NNP ,/, and/CC CIFAR/NNP ,/, RFNs/NNS were/VBD superior/JJ to/IN restricted/VBN Boltzmann/NNP machines/NNS (/-LRB- RBMs/NNS )/-RRB- and/CC denoising/NN autoencoders/NNS ./.
On/IN CIFAR/NN -/HYPH 10/CD and/CC CIFAR/NN -/HYPH 100/CD ,/, RFN/NN pretraining/NN always/RB improved/VBD the/DT results/NNS of/IN deep/JJ networks/NNS for/IN different/JJ architectures/NNS like/IN AlexNet/NNP ,/, deep/JJ supervised/VBD net/NN (/-LRB- DSN/NN )/-RRB- ,/, and/CC a/DT simple/JJ "/`` Network/NNP In/NNP Network/NNP "/'' architecture/NN ./.
With/IN RFNs/NNS success/NN is/VBZ guaranteed/VBN ./.
