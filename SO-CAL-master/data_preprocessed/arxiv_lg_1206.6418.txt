Learning/VBG invariant/JJ representations/NNS is/VBZ an/DT important/JJ problem/NN in/IN machine/NN learning/NN and/CC pattern/NN recognition/NN ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT novel/JJ framework/NN of/IN transformation/NN -/HYPH invariant/JJ feature/NN learning/NN by/IN incorporating/VBG linear/JJ transformations/NNS into/IN the/DT feature/NN learning/VBG algorithms/NNS ./.
For/IN example/NN ,/, we/PRP present/VBP the/DT transformation/NN -/HYPH invariant/JJ restricted/JJ Boltzmann/JJ machine/NN that/WDT compactly/RB represents/VBZ data/NNS by/IN its/PRP$ weights/NNS and/CC their/PRP$ transformations/NNS ,/, which/WDT achieves/VBZ invariance/NN of/IN the/DT feature/NN representation/NN via/IN probabilistic/JJ max/NN pooling/VBG ./.
In/IN addition/NN ,/, we/PRP show/VBP that/IN our/PRP$ transformation/NN -/HYPH invariant/JJ feature/NN learning/VBG framework/NN can/MD also/RB be/VB extended/VBN to/IN other/JJ unsupervised/JJ learning/NN methods/NNS ,/, such/JJ as/IN autoencoders/NNS or/CC sparse/JJ coding/NN ./.
We/PRP evaluate/VBP our/PRP$ method/NN on/IN several/JJ image/NN classification/NN benchmark/NN datasets/NNS ,/, such/JJ as/IN MNIST/NN variations/NNS ,/, CIFAR/NN -/HYPH 10/CD ,/, and/CC STL/NNP -/HYPH 10/CD ,/, and/CC show/VBP competitive/JJ or/CC superior/JJ classification/NN performance/NN when/WRB compared/VBN to/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
Furthermore/RB ,/, our/PRP$ method/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN phone/NN classification/NN tasks/NNS with/IN the/DT TIMIT/NNP dataset/NN ,/, which/WDT demonstrates/VBZ wide/JJ applicability/NN of/IN our/PRP$ proposed/VBN algorithms/NNS to/IN other/JJ domains/NNS ./.
