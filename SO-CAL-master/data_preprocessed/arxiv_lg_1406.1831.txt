Autoencoders/NNS have/VBP emerged/VBN as/IN a/DT useful/JJ framework/NN for/IN unsupervised/JJ learning/NN of/IN internal/JJ representations/NNS ,/, and/CC a/DT wide/JJ variety/NN of/IN apparently/RB conceptually/RB disparate/JJ regularization/NN techniques/NNS have/VBP been/VBN proposed/VBN to/TO generate/VB useful/JJ features/NNS ./.
Here/RB we/PRP extend/VBP existing/VBG denoising/VBG autoencoders/NNS to/TO additionally/RB inject/VB noise/NN before/IN the/DT nonlinearity/NN ,/, and/CC at/IN the/DT hidden/JJ unit/NN activations/NNS ./.
We/PRP show/VBP that/IN a/DT wide/JJ variety/NN of/IN previous/JJ methods/NNS ,/, including/VBG denoising/NN ,/, contractive/JJ ,/, and/CC sparse/JJ autoencoders/NNS ,/, as/RB well/RB as/IN dropout/NN can/MD be/VB interpreted/VBN using/VBG this/DT framework/NN ./.
This/DT noise/NN injection/NN framework/NN reaps/VBZ practical/JJ benefits/NNS by/IN providing/VBG a/DT unified/VBN strategy/NN to/TO develop/VB new/JJ internal/JJ representations/NNS by/IN designing/VBG the/DT nature/NN of/IN the/DT injected/VBN noise/NN ./.
We/PRP show/VBP that/IN noisy/JJ autoencoders/NNS outperform/VBP denoising/VBG autoencoders/NNS at/IN the/DT very/RB task/NN of/IN denoising/NN ,/, and/CC are/VBP competitive/JJ with/IN other/JJ single/JJ -/HYPH layer/NN techniques/NNS on/IN MNIST/NNP ,/, and/CC CIFAR/NN -/HYPH 10/CD ./.
We/PRP also/RB show/VBP that/IN types/NNS of/IN noise/NN other/JJ than/IN dropout/NN improve/VB performance/NN in/IN a/DT deep/JJ network/NN through/IN sparsifying/NN ,/, decorrelating/NN ,/, and/CC spreading/VBG information/NN across/IN representations/NNS ./.
