In/IN this/DT paper/NN we/PRP present/VBP a/DT new/JJ way/NN of/IN predicting/VBG the/DT performance/NN of/IN a/DT reinforcement/NN learning/VBG policy/NN given/VBN historical/JJ data/NNS that/WDT may/MD have/VB been/VBN generated/VBN by/IN a/DT different/JJ policy/NN ./.
The/DT ability/NN to/TO evaluate/VB a/DT policy/NN from/IN historical/JJ data/NNS is/VBZ important/JJ for/IN applications/NNS where/WRB the/DT deployment/NN of/IN a/DT bad/JJ policy/NN can/MD be/VB dangerous/JJ or/CC costly/JJ ./.
We/PRP show/VBP empirically/RB that/IN our/PRP$ algorithm/NN produces/VBZ estimates/NNS that/WDT often/RB have/VBP orders/NNS of/IN magnitude/NN lower/JJR mean/JJ squared/VBD error/NN than/IN existing/VBG methods/NNS ---/, it/PRP makes/VBZ more/RBR efficient/JJ use/NN of/IN the/DT available/JJ data/NNS ./.
Our/PRP$ new/JJ estimator/NN is/VBZ based/VBN on/IN two/CD advances/NNS :/: an/DT extension/NN of/IN the/DT doubly/RB robust/JJ estimator/NN (/-LRB- Jiang/NNP and/CC Li/NNP ,/, 2015/CD )/-RRB- ,/, and/CC a/DT new/JJ way/NN to/IN mix/NN between/IN model/NN based/VBN estimates/NNS and/CC importance/NN sampling/NN based/VBN estimates/NNS ./.
