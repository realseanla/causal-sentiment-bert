We/PRP investigate/VBP two/CD new/JJ optimization/NN problems/NNS --/: minimizing/VBG a/DT submodular/JJ function/NN subject/NN to/IN a/DT submodular/JJ lower/JJR bound/JJ constraint/NN (/-LRB- submodular/JJ cover/NN )/-RRB- and/CC maximizing/VBG a/DT submodular/JJ function/NN subject/NN to/IN a/DT submodular/JJ upper/JJ bound/JJ constraint/NN (/-LRB- submodular/JJ knapsack/NN )/-RRB- ./.
We/PRP are/VBP motivated/VBN by/IN a/DT number/NN of/IN real/JJ -/HYPH world/NN applications/NNS in/IN machine/NN learning/NN including/VBG sensor/NN placement/NN and/CC data/NN subset/NN selection/NN ,/, which/WDT require/VBP maximizing/VBG a/DT certain/JJ submodular/JJ function/NN (/-LRB- like/IN coverage/NN or/CC diversity/NN )/-RRB- while/IN simultaneously/RB minimizing/VBG another/DT (/-LRB- like/IN cooperative/JJ cost/NN )/-RRB- ./.
These/DT problems/NNS are/VBP often/RB posed/VBN as/IN minimizing/VBG the/DT difference/NN between/IN submodular/JJ functions/NNS [/-LRB- 14/CD ,/, 35/CD ]/-RRB- which/WDT is/VBZ in/IN the/DT worst/JJS case/NN inapproximable/JJ ./.
We/PRP show/VBP ,/, however/RB ,/, that/IN by/IN phrasing/NN these/DT problems/NNS as/IN constrained/VBN optimization/NN ,/, which/WDT is/VBZ more/RBR natural/JJ for/IN many/JJ applications/NNS ,/, we/PRP achieve/VBP a/DT number/NN of/IN bounded/VBN approximation/NN guarantees/NNS ./.
We/PRP also/RB show/VBP that/IN both/CC these/DT problems/NNS are/VBP closely/RB related/JJ and/CC an/DT approximation/NN algorithm/NN solving/VBG one/PRP can/MD be/VB used/VBN to/TO obtain/VB an/DT approximation/NN guarantee/NN for/IN the/DT other/JJ ./.
We/PRP provide/VBP hardness/NN results/NNS for/IN both/DT problems/NNS thus/RB showing/VBG that/IN our/PRP$ approximation/NN factors/NNS are/VBP tight/JJ up/IN to/IN log/NN -/HYPH factors/NNS ./.
Finally/RB ,/, we/PRP empirically/RB demonstrate/VBP the/DT performance/NN and/CC good/JJ scalability/NN properties/NNS of/IN our/PRP$ algorithms/NNS ./.
