We/PRP present/VBP a/DT novel/JJ deep/JJ recurrent/JJ neural/JJ network/NN architecture/NN that/WDT learns/VBZ to/TO build/VB implicit/JJ plans/NNS in/IN an/DT end/NN -/HYPH to/IN -/HYPH end/NN manner/NN by/IN purely/RB interacting/VBG with/IN an/DT environment/NN in/IN reinforcement/NN learning/VBG setting/NN ./.
The/DT network/NN builds/VBZ an/DT internal/JJ plan/NN ,/, which/WDT is/VBZ continuously/RB updated/VBN upon/IN observation/NN of/IN the/DT next/JJ input/NN from/IN the/DT environment/NN ./.
It/PRP can/MD also/RB partition/VB this/DT internal/JJ representation/NN into/IN contiguous/JJ sub/NN -/HYPH sequences/NNS by/IN learning/VBG for/IN how/WRB long/RB the/DT plan/NN can/MD be/VB committed/VBN to/IN -/HYPH i.e./FW followed/VBD without/IN re-planing/VBG ./.
Combining/VBG these/DT properties/NNS ,/, the/DT proposed/VBN model/NN ,/, dubbed/VBN STRategic/JJ Attentive/JJ Writer/NN (/-LRB- STRAW/NN )/-RRB- can/MD learn/VB high/JJ -/HYPH level/NN ,/, temporally/RB abstracted/VBN macro/JJ -/HYPH actions/NNS of/IN varying/VBG lengths/NNS that/WDT are/VBP solely/RB learnt/VBN from/IN data/NNS without/IN any/DT prior/JJ information/NN ./.
These/DT macro-actions/NNS enable/VBP both/DT structured/JJ exploration/NN and/CC economic/JJ computation/NN ./.
We/PRP experimentally/RB demonstrate/VBP that/IN STRAW/NNP delivers/VBZ strong/JJ improvements/NNS on/IN several/JJ ATARI/NNP games/NNS by/IN employing/VBG temporally/RB extended/VBN planning/NN strategies/NNS (/-LRB- e.g/FW ./.
Ms./NNP Pacman/NNP and/CC Frostbite/NNP )/-RRB- ./.
It/PRP is/VBZ at/IN the/DT same/JJ time/NN a/DT general/JJ algorithm/NN that/WDT can/MD be/VB applied/VBN on/IN any/DT sequence/NN data/NNS ./.
To/IN that/DT end/NN ,/, we/PRP also/RB show/VBP that/IN when/WRB trained/VBN on/IN text/NN prediction/NN task/NN ,/, STRAW/NNP naturally/RB predicts/VBZ frequent/JJ n/NN -/HYPH grams/NNS (/-LRB- instead/RB of/IN macro-actions/NNS )/-RRB- ,/, demonstrating/VBG the/DT generality/NN of/IN the/DT approach/NN ./.
