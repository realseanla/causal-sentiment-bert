Most/JJS state/NN of/IN the/DT art/NN approaches/NNS for/IN Named/VBN Entity/NN Recognition/NN rely/VBP on/IN hand/NN crafted/VBN features/NNS and/CC annotated/VBN corpora/NNS ./.
Recently/RB Neural/JJ network/NN based/VBN models/NNS have/VBP been/VBN proposed/VBN which/WDT do/VBP not/RB require/VB handcrafted/JJ features/NNS but/CC still/RB require/VBP annotated/JJ corpora/NNS ./.
However/RB ,/, such/JJ annotated/JJ corpora/NNS may/MD not/RB be/VB available/JJ for/IN many/JJ languages/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT neural/JJ network/NN based/VBN model/NN which/WDT allows/VBZ sharing/VBG the/DT decoder/NN as/RB well/RB as/IN word/NN and/CC character/NN level/NN parameters/NNS between/IN two/CD languages/NNS thereby/RB allowing/VBG a/DT resource/NN fortunate/JJ language/NN to/TO aid/VB a/DT resource/NN deprived/VBN language/NN ./.
Specifically/RB ,/, we/PRP focus/VBP on/IN the/DT case/NN when/WRB limited/JJ annotated/VBN corpora/NN is/VBZ available/JJ in/IN one/CD language/NN (/-LRB- $/$ L_1/CD $/$ )/-RRB- and/CC abundant/JJ annotated/JJ corpora/NN is/VBZ available/JJ in/IN another/DT language/NN (/-LRB- $/$ L_2/CD $/$ )/-RRB- ./.
Sharing/VBG the/DT network/NN architecture/NN and/CC parameters/NNS between/IN $/$ L_1/CD $/$ and/CC $/$ L_2/CD $/$ leads/VBZ to/IN improved/VBN performance/NN in/IN $/$ L_1/CD $/$ ./.
Further/RB ,/, our/PRP$ approach/NN does/VBZ not/RB require/VB any/DT hand/NN crafted/VBN features/NNS but/CC instead/RB directly/RB learns/VBZ meaningful/JJ feature/NN representations/NNS from/IN the/DT training/NN data/NNS itself/PRP ./.
We/PRP experiment/VBP with/IN 4/CD language/NN pairs/NNS and/CC show/VBP that/IN indeed/RB in/IN a/DT resource/NN constrained/VBN setup/NN (/-LRB- lesser/JJR annotated/JJ corpora/NN )/-RRB- ,/, a/DT model/NN jointly/RB trained/VBN with/IN data/NNS from/IN another/DT language/NN performs/VBZ better/JJR than/IN a/DT model/NN trained/VBN only/RB on/IN the/DT limited/JJ corpora/NN in/IN one/CD language/NN ./.
