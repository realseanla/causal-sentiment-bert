The/DT Alternating/NNP Direction/NNP Method/NNP of/IN Multipliers/NNPS (/-LRB- ADMM/NNP )/-RRB- has/VBZ been/VBN studied/VBN for/IN years/NNS ,/, since/IN it/PRP can/MD be/VB applied/VBN to/IN many/JJ large/JJ -/HYPH scale/NN and/CC data/NN -/HYPH distributed/VBN machine/NN learning/NN tasks/NNS ./.
The/DT traditional/JJ ADMM/NN algorithm/NN needs/VBZ to/TO compute/VB an/DT (/-LRB- empirical/JJ )/-RRB- expected/VBN loss/NN function/NN on/IN all/PDT the/DT training/NN examples/NNS for/IN each/DT iteration/NN ,/, which/WDT results/VBZ in/IN a/DT computational/JJ complexity/NN propositional/JJ to/IN the/DT number/NN of/IN training/NN examples/NNS ./.
To/TO reduce/VB the/DT time/NN complexity/NN ,/, stochastic/JJ ADMM/NN algorithm/NN is/VBZ proposed/VBN to/TO replace/VB the/DT expected/JJ loss/NN function/NN by/IN a/DT random/JJ loss/NN function/NN associated/VBN with/IN one/CD single/JJ uniformly/RB drawn/VBN example/NN and/CC Bregman/NNP divergence/NN for/IN a/DT second/JJ order/NN proximal/JJ function/NN ./.
The/DT Bregman/NNP divergence/NN in/IN the/DT original/JJ stochastic/JJ ADMM/NN algorithm/NN is/VBZ derived/VBN from/IN half/NN squared/VBD norm/NN ,/, which/WDT could/MD be/VB a/DT suboptimal/JJ choice/NN ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT new/JJ stochastic/JJ ADMM/NN algorithm/NN ,/, using/VBG Bregman/NNP divergence/NN derived/VBN from/IN second/JJ order/NN proximal/JJ functions/NNS associated/VBN with/IN iteratively/RB updated/VBN matrices/NNS ./.
Our/PRP$ new/JJ stochastic/JJ ADMM/NN produces/VBZ a/DT new/JJ family/NN of/IN adaptive/JJ subgradient/NN methods/NNS ./.
We/PRP theoretically/RB prove/VBP that/IN their/PRP$ regret/NN bounds/NNS are/VBP as/RB good/JJ as/IN the/DT bounds/NNS achieved/VBN by/IN the/DT best/JJS proximal/JJ functions/NNS that/WDT can/MD be/VB chosen/VBN in/IN hindsight/NN ./.
Encouraging/VBG results/NNS confirm/VBP the/DT effectiveness/NN and/CC efficiency/NN of/IN the/DT proposed/VBN algorithms/NNS ./.
