Many/JJ machine/NN learning/NN tasks/NNS can/MD be/VB formulated/VBN as/IN Regularized/VBN Empirical/JJ Risk/NN Minimization/NN (/-LRB- R/NN -/HYPH ERM/NN )/-RRB- ,/, and/CC solved/VBN by/IN optimization/NN algorithms/NNS such/JJ as/IN gradient/NN descent/NN (/-LRB- GD/NN )/-RRB- ,/, stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- ,/, and/CC stochastic/JJ variance/NN reduction/NN (/-LRB- SVRG/NN )/-RRB- ./.
Conventional/JJ analysis/NN on/IN these/DT optimization/NN algorithms/NNS focuses/VBZ on/IN their/PRP$ convergence/NN rates/NNS during/IN the/DT training/NN process/NN ,/, however/RB ,/, people/NNS in/IN the/DT machine/NN learning/VBG community/NN may/MD care/VB more/JJR about/IN the/DT generalization/NN performance/NN of/IN the/DT learned/VBN model/NN on/IN unseen/JJ test/NN data/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP investigate/VBP on/IN this/DT issue/NN ,/, by/IN using/VBG stability/NN as/IN a/DT tool/NN ./.
In/IN particular/JJ ,/, we/PRP decompose/VBP the/DT generalization/NN error/NN for/IN R/NN -/HYPH ERM/NN ,/, and/CC derive/VBP its/PRP$ upper/JJ bound/VBN for/IN both/DT convex/NN and/CC non-convex/NN cases/NNS ./.
In/IN convex/NN cases/NNS ,/, we/PRP prove/VBP that/IN the/DT generalization/NN error/NN can/MD be/VB bounded/VBN by/IN the/DT convergence/NN rate/NN of/IN the/DT optimization/NN algorithm/NN and/CC the/DT stability/NN of/IN the/DT R/NN -/HYPH ERM/NN process/NN ,/, both/CC in/IN expectation/NN (/-LRB- in/IN the/DT order/NN of/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- (/-LRB- 1/CD //SYM n/NN )/-RRB- \/SYM mathbb/NN {/-LRB- E/NN }/-RRB- \/SYM rho/NN (/-LRB- T/NN )/-RRB- )/-RRB- $/$ ,/, where/WRB $/$ \/SYM rho/NN (/-LRB- T/NN )/-RRB- $/$ is/VBZ the/DT convergence/NN error/NN and/CC $/$ T$/CD is/VBZ the/DT number/NN of/IN iterations/NNS )/-RRB- and/CC in/IN high/JJ probability/NN (/-LRB- in/IN the/DT order/NN of/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- \/SYM left/JJ (/-LRB- \/SYM frac/NN {/-LRB- \/SYM log/NN {/-LRB- 1/CD //SYM \/SYM delta/NN }/-RRB- }/-RRB- {/-LRB- \/SYM sqrt/NN {/-LRB- n/NN }/-RRB- }/-RRB- \/SYM rho/NN (/-LRB- T/NN )/-RRB- \/SYM right/NN )/-RRB- $/$ with/IN probability/NN $/$ 1/CD -/HYPH \/SYM delta/NN $/$ )/-RRB- ./.
For/IN non-convex/JJ cases/NNS ,/, we/PRP can/MD also/RB obtain/VB a/DT similar/JJ expected/VBN generalization/NN error/NN bound/VBN ./.
Our/PRP$ theorems/NNS indicate/VBP that/IN 1/CD )/-RRB- along/IN with/IN the/DT training/NN process/NN ,/, the/DT generalization/NN error/NN will/MD decrease/VB for/IN all/PDT the/DT optimization/NN algorithms/NNS under/IN our/PRP$ investigation/NN ;/: 2/LS )/-RRB- Comparatively/RB speaking/VBG ,/, SVRG/NNP has/VBZ better/JJR generalization/NN ability/NN than/IN GD/NNP and/CC SGD/NNP ./.
We/PRP have/VBP conducted/VBN experiments/NNS on/IN both/DT convex/NN and/CC non-convex/NN problems/NNS ,/, and/CC the/DT experimental/JJ results/NNS verify/VBP our/PRP$ theoretical/JJ findings/NNS ./.
