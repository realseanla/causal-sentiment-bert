Recent/JJ results/NNS show/VBP that/IN deep/JJ neural/JJ networks/NNS achieve/VBP excellent/JJ performance/NN even/RB when/WRB ,/, during/IN training/NN ,/, weights/NNS are/VBP quantized/VBN and/CC projected/VBN to/IN a/DT binary/JJ representation/NN ./.
Here/RB ,/, we/PRP show/VBP that/IN this/DT is/VBZ just/RB the/DT tip/NN of/IN the/DT iceberg/NN :/: these/DT same/JJ networks/NNS ,/, during/IN testing/NN ,/, also/RB exhibit/VBP a/DT remarkable/JJ robustness/NN to/IN distortions/NNS beyond/IN quantization/NN ,/, including/VBG additive/JJ and/CC multiplicative/JJ noise/NN ,/, and/CC a/DT class/NN of/IN non-linear/JJ projections/NNS where/WRB binarization/NN is/VBZ just/RB a/DT special/JJ case/NN ./.
To/TO quantify/VB this/DT robustness/NN ,/, we/PRP show/VBP that/IN one/CD such/JJ network/NN achieves/VBZ 11/CD percent/NN test/NN error/NN on/IN CIFAR/NN -/HYPH 10/CD even/RB with/IN 0.68/CD effective/JJ bits/NNS per/IN weight/NN ./.
Furthermore/RB ,/, we/PRP find/VBP that/IN a/DT common/JJ training/NN heuristic/NN --/: namely/RB ,/, projecting/VBG quantized/VBN weights/NNS during/IN backpropagation/NN --/: can/MD be/VB altered/VBN (/-LRB- or/CC even/RB removed/VBN )/-RRB- and/CC networks/NNS still/RB achieve/VBP a/DT base/NN level/NN of/IN robustness/NN during/IN testing/NN ./.
Specifically/RB ,/, training/NN with/IN weight/NN projections/NNS other/JJ than/IN quantization/NN also/RB works/VBZ ,/, as/IN does/VBZ simply/RB clipping/VBG the/DT weights/NNS ,/, both/DT of/IN which/WDT have/VBP never/RB been/VBN reported/VBN before/IN ./.
We/PRP confirm/VBP our/PRP$ results/NNS for/IN CIFAR/NN -/HYPH 10/CD and/CC ImageNet/NNP datasets/NNS ./.
Finally/RB ,/, drawing/VBG from/IN these/DT ideas/NNS ,/, we/PRP propose/VBP a/DT stochastic/JJ projection/NN rule/NN that/WDT leads/VBZ to/IN a/DT new/JJ state/NN of/IN the/DT art/NN network/NN with/IN 7.64/CD percent/NN test/NN error/NN on/IN CIFAR/NN -/HYPH 10/CD using/VBG no/DT data/NNS augmentation/NN ./.
