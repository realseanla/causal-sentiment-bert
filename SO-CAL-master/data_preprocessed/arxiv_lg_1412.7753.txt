Recurrent/JJ neural/JJ network/NN is/VBZ a/DT powerful/JJ model/NN that/WDT learns/VBZ temporal/JJ patterns/NNS in/IN sequential/JJ data/NNS ./.
For/IN a/DT long/JJ time/NN ,/, it/PRP was/VBD believed/VBN that/IN recurrent/JJ networks/NNS are/VBP difficult/JJ to/TO train/VB using/VBG simple/JJ optimizers/NNS ,/, such/JJ as/IN stochastic/JJ gradient/NN descent/NN ,/, due/IN to/IN the/DT so/RB -/HYPH called/VBN vanishing/VBG gradient/NN problem/NN ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN learning/NN longer/JJR term/NN patterns/NNS in/IN real/JJ data/NNS ,/, such/JJ as/IN in/IN natural/JJ language/NN ,/, is/VBZ perfectly/RB possible/JJ using/VBG gradient/NN descent/NN ./.
This/DT is/VBZ achieved/VBN by/IN using/VBG a/DT slight/JJ structural/JJ modification/NN of/IN the/DT simple/JJ recurrent/JJ neural/JJ network/NN architecture/NN ./.
We/PRP encourage/VBP some/DT of/IN the/DT hidden/VBN units/NNS to/TO change/VB their/PRP$ state/NN slowly/RB by/IN making/VBG part/NN of/IN the/DT recurrent/JJ weight/NN matrix/NN close/NN to/IN identity/NN ,/, thus/RB forming/VBG kind/NN of/IN a/DT longer/JJR term/NN memory/NN ./.
We/PRP evaluate/VBP our/PRP$ model/NN in/IN language/NN modeling/NN experiments/NNS ,/, where/WRB we/PRP obtain/VBP similar/JJ performance/NN to/IN the/DT much/JJ more/RBR complex/JJ Long/JJ Short/JJ Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- networks/NNS (/-LRB- Hochreiter/NNP &amp;/CC Schmidhuber/NNP ,/, 1997/CD )/-RRB- ./.
