We/PRP present/VBP a/DT novel/JJ neural/JJ network/NN algorithm/NN ,/, the/DT Tensor/NNP Switching/NN (/-LRB- TS/NN )/-RRB- network/NN ,/, which/WDT generalizes/VBZ the/DT Rectified/VBN Linear/NNP Unit/NNP (/-LRB- ReLU/NNP )/-RRB- nonlinearity/NN to/IN tensor/NN -/HYPH valued/VBN hidden/JJ units/NNS ./.
The/DT TS/NNP network/NN copies/NNS its/PRP$ entire/JJ input/NN vector/NN to/IN different/JJ locations/NNS in/IN an/DT expanded/VBN representation/NN ,/, with/IN the/DT location/NN determined/VBN by/IN its/PRP$ hidden/JJ unit/NN activity/NN ./.
In/IN this/DT way/NN ,/, even/RB a/DT simple/JJ linear/JJ readout/NN from/IN the/DT TS/NN representation/NN can/MD implement/VB a/DT highly/RB expressive/JJ deep/JJ -/HYPH network/NN -/HYPH like/JJ function/NN ./.
The/DT TS/NNP network/NN hence/RB avoids/VBZ the/DT vanishing/VBG gradient/NN problem/NN by/IN construction/NN ,/, at/IN the/DT cost/NN of/IN larger/JJR representation/NN size/NN ./.
We/PRP develop/VBP several/JJ methods/NNS to/TO train/VB the/DT TS/NNP network/NN ,/, including/VBG equivalent/JJ kernels/NNS for/IN infinitely/RB wide/JJ and/CC deep/JJ TS/NN networks/NNS ,/, a/DT one/CD -/HYPH pass/NN linear/JJ learning/NN algorithm/NN ,/, and/CC two/CD backpropagation/NN -/HYPH inspired/VBN representation/NN learning/NN algorithms/NNS ./.
Our/PRP$ experimental/JJ results/NNS demonstrate/VBP that/IN the/DT TS/NNP network/NN is/VBZ indeed/RB more/RBR expressive/JJ and/CC consistently/RB learns/VBZ faster/JJR than/IN standard/JJ ReLU/NNP networks/NNS ./.
