We/PRP are/VBP developing/VBG a/DT general/JJ framework/NN for/IN using/VBG learned/VBN Bayesian/JJ models/NNS for/IN decision/NN -/HYPH theoretic/JJ control/NN of/IN search/NN and/CC reasoningalgorithms/NNS ./.
We/PRP illustrate/VBP the/DT approach/NN on/IN the/DT specific/JJ task/NN of/IN controlling/VBG both/DT general/JJ and/CC domain/NN -/HYPH specific/JJ solvers/NNS on/IN a/DT hard/JJ class/NN of/IN structured/JJ constraint/NN satisfaction/NN problems/NNS ./.
A/DT successful/JJ strategyfor/NN reducing/VBG the/DT high/JJ (/-LRB- and/CC even/RB infinite/JJ )/-RRB- variance/NN in/IN running/VBG time/NN typically/RB exhibited/VBN by/IN backtracking/VBG search/NN algorithms/NNS is/VBZ to/TO cut/VB off/RP and/CC restart/VB the/DT search/NN if/IN a/DT solution/NN is/VBZ not/RB found/VBN within/IN a/DT certainamount/NN of/IN time/NN ./.
Previous/JJ work/NN on/IN restart/VB strategies/NNS have/VBP employed/VBN fixed/VBN cut/VBN off/RP values/NNS ./.
We/PRP show/VBP how/WRB to/TO create/VB a/DT dynamic/JJ cut/NN off/RP strategy/NN by/IN learning/VBG a/DT Bayesian/JJ model/NN that/WDT predicts/VBZ the/DT ultimate/JJ length/NN of/IN a/DT trial/NN based/VBN on/IN observing/VBG the/DT early/JJ behavior/NN of/IN the/DT search/NN algorithm/NN ./.
Furthermore/RB ,/, we/PRP describe/VBP the/DT general/JJ conditions/NNS under/IN which/WDT a/DT dynamic/JJ restart/VB strategy/NN can/MD outperform/VB the/DT theoretically/RB optimal/JJ fixed/VBN strategy/NN ./.
