We/PRP study/VBP online/RB learning/VBG when/WRB individual/JJ instances/NNS are/VBP corrupted/VBN by/IN random/JJ noise/NN ./.
We/PRP assume/VBP the/DT noise/NN distribution/NN is/VBZ unknown/JJ ,/, and/CC may/MD change/VB over/IN time/NN with/IN no/DT restriction/NN other/JJ than/IN having/VBG zero/CD mean/NN and/CC bounded/VBD variance/NN ./.
Our/PRP$ technique/NN relies/VBZ on/IN a/DT family/NN of/IN unbiased/JJ estimators/NNS for/IN non-linear/JJ functions/NNS ,/, which/WDT may/MD be/VB of/IN independent/JJ interest/NN ./.
We/PRP show/VBP that/IN a/DT variant/NN of/IN online/JJ gradient/NN descent/NN can/MD learn/VB functions/NNS in/IN any/DT dot/NN -/HYPH product/NN (/-LRB- e.g./FW ,/, polynomial/JJ )/-RRB- or/CC Gaussian/JJ kernel/NN space/NN with/IN any/DT analytic/JJ convex/NN loss/NN function/NN ./.
Our/PRP$ variant/JJ uses/NNS randomized/VBD estimates/NNS that/WDT need/VBP to/TO query/VB a/DT random/JJ number/NN of/IN noisy/JJ copies/NNS of/IN each/DT instance/NN ,/, where/WRB with/IN high/JJ probability/NN this/DT number/NN is/VBZ upper/JJ bounded/VBN by/IN a/DT constant/JJ ./.
Allowing/VBG such/JJ multiple/JJ queries/NNS can/MD not/RB be/VB avoided/VBN :/: Indeed/RB ,/, we/PRP show/VBP that/IN online/JJ learning/NN is/VBZ in/IN general/JJ impossible/JJ when/WRB only/RB one/CD noisy/JJ copy/NN of/IN each/DT instance/NN can/MD be/VB accessed/VBN ./.
