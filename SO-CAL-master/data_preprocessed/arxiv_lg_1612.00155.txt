We/PRP investigate/VBP adversarial/JJ attacks/NNS for/IN autoencoders/NNS ./.
We/PRP propose/VBP a/DT procedure/NN that/WDT distorts/VBZ the/DT input/NN image/NN to/IN mislead/VB the/DT autoencoder/NN in/IN reconstructing/VBG a/DT completely/RB different/JJ target/NN image/NN ./.
We/PRP attack/VBP the/DT internal/JJ latent/NN representations/NNS ,/, attempting/VBG to/TO make/VB the/DT adversarial/JJ input/NN produce/VBP an/DT internal/JJ representation/NN as/IN similar/JJ as/IN possible/JJ as/IN the/DT target/NN 's/POS ./.
We/PRP find/VBP that/IN autoencoders/NNS are/VBP much/RB more/RBR robust/JJ to/IN the/DT attack/NN than/IN classifiers/NNS :/: while/IN some/DT examples/NNS have/VBP tolerably/RB small/JJ input/NN distortion/NN ,/, and/CC reasonable/JJ similarity/NN to/IN the/DT target/NN image/NN ,/, there/EX is/VBZ a/DT quasi-linear/NN trade/NN -/HYPH off/NN between/IN those/DT aims/NNS ./.
We/PRP report/VBP results/NNS on/IN MNIST/NNP and/CC SVHN/NNP datasets/NNS ,/, and/CC also/RB test/NN regular/JJ deterministic/JJ autoencoders/NNS ,/, reaching/VBG similar/JJ conclusions/NNS in/IN all/DT cases/NNS ./.
Finally/RB ,/, we/PRP show/VBP that/IN the/DT usual/JJ adversarial/JJ attack/NN for/IN classifiers/NNS ,/, while/IN being/VBG much/RB easier/JJR ,/, also/RB presents/VBZ a/DT direct/JJ proportion/NN between/IN distortion/NN on/IN the/DT input/NN ,/, and/CC misdirection/NN on/IN the/DT output/NN ./.
That/DT proportionality/NN however/RB is/VBZ hidden/VBN by/IN the/DT normalization/NN of/IN the/DT output/NN ,/, which/WDT maps/VBZ a/DT linear/JJ layer/NN into/IN non-linear/JJ probabilities/NNS ./.
