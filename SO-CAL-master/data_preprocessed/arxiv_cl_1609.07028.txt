Entity/NN images/NNS provide/VBP significant/JJ visual/JJ information/NN that/WDT helps/VBZ the/DT construction/NN of/IN knowledge/NN representations/NNS ./.
Most/JJS conventional/JJ methods/NNS learn/VBP knowledge/NN representations/NNS solely/RB from/IN structured/JJ triples/NNS ,/, ignoring/VBG rich/JJ visual/JJ information/NN extracted/VBN from/IN entity/NN images/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ Image/NN -/HYPH embodied/VBN Knowledge/NN Representation/NN Learning/VBG model/NN ,/, where/WRB knowledge/NN representations/NNS are/VBP learned/VBN with/IN both/DT triples/NNS and/CC images/NNS ./.
More/RBR specifically/RB ,/, for/IN each/DT image/NN of/IN an/DT entity/NN ,/, we/PRP construct/VBP image/NN -/HYPH based/VBN representations/NNS via/IN a/DT neural/JJ image/NN encoder/NN ,/, and/CC these/DT representations/NNS with/IN respect/NN to/IN multiple/JJ image/NN instances/NNS are/VBP then/RB integrated/VBN via/IN an/DT attention/NN -/HYPH based/VBN method/NN ./.
We/PRP evaluate/VBP our/PRP$ models/NNS on/IN knowledge/NN graph/NN completion/NN and/CC triple/JJ classification/NN ./.
Experimental/JJ results/NNS demonstrate/VBP that/IN our/PRP$ models/NNS outperform/VBP all/DT baselines/NNS on/IN both/DT tasks/NNS ,/, which/WDT indicates/VBZ the/DT significance/NN of/IN visual/JJ information/NN for/IN knowledge/NN representations/NNS and/CC the/DT capability/NN of/IN our/PRP$ models/NNS in/IN learning/VBG knowledge/NN representations/NNS with/IN images/NNS ./.
