Gaussian/JJ graphical/JJ models/NNS are/VBP of/IN great/JJ interest/NN in/IN statistical/JJ learning/NN ./.
Because/IN the/DT conditional/JJ independencies/NNS between/IN different/JJ nodes/NNS correspond/VBP to/IN zero/CD entries/NNS in/IN the/DT inverse/JJ covariance/NN matrix/NN of/IN the/DT Gaussian/JJ distribution/NN ,/, one/PRP can/MD learn/VB the/DT structure/NN of/IN the/DT graph/NN by/IN estimating/VBG a/DT sparse/JJ inverse/JJ covariance/NN matrix/NN from/IN sample/NN data/NNS ,/, by/IN solving/VBG a/DT convex/NN maximum/JJ likelihood/NN problem/NN with/IN an/DT $/$ \/CD ell_1/CD $/$ -/HYPH regularization/NN term/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT first/JJ -/HYPH order/NN method/NN based/VBN on/IN an/DT alternating/VBG linearization/NN technique/NN that/WDT exploits/VBZ the/DT problem/NN 's/POS special/JJ structure/NN ;/: in/IN particular/JJ ,/, the/DT subproblems/NNS solved/VBN in/IN each/DT iteration/NN have/VBP closed/VBN -/HYPH form/NN solutions/NNS ./.
Moreover/RB ,/, our/PRP$ algorithm/NN obtains/VBZ an/DT $/$ \/CD epsilon/CD $/$ -/HYPH optimal/JJ solution/NN in/IN $/$ O/UH (/-LRB- 1/CD //SYM \/SYM epsilon/SYM )/-RRB- $/$ iterations/NNS ./.
Numerical/NNP experiments/NNS on/IN both/DT synthetic/JJ and/CC real/JJ data/NNS from/IN gene/NN association/NN networks/NNS show/VBP that/IN a/DT practical/JJ version/NN of/IN this/DT algorithm/NN outperforms/VBZ other/JJ competitive/JJ algorithms/NNS ./.
