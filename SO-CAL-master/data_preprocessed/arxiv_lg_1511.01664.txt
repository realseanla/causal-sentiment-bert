In/IN this/DT paper/NN ,/, we/PRP utilize/VBP stochastic/JJ optimization/NN to/TO reduce/VB the/DT space/NN complexity/NN of/IN convex/NN composite/JJ optimization/NN with/IN a/DT nuclear/JJ norm/NN regularizer/NN ,/, where/WRB the/DT variable/NN is/VBZ a/DT matrix/NN of/IN size/NN $/$ m/CD \/SYM times/NNS n/NN $/$ ./.
By/IN constructing/VBG a/DT low/JJ -/HYPH rank/NN estimate/NN of/IN the/DT gradient/NN ,/, we/PRP propose/VBP an/DT iterative/JJ algorithm/NN based/VBN on/IN stochastic/JJ proximal/JJ gradient/NN descent/NN (/-LRB- SPGD/NN )/-RRB- ,/, and/CC take/VB the/DT last/JJ iterate/NN of/IN SPGD/NNP as/IN the/DT final/JJ solution/NN ./.
The/DT main/JJ advantage/NN of/IN the/DT proposed/VBN algorithm/NN is/VBZ that/IN its/PRP$ space/NN complexity/NN is/VBZ $/$ O/UH (/-LRB- m/NN n/NN )/-RRB- $/$ ,/, in/IN contrast/NN ,/, most/JJS of/IN previous/JJ algorithms/NNS have/VBP a/DT $/$ O/UH (/-LRB- mn/NN )/-RRB- $/$ space/NN complexity/NN ./.
Theoretical/JJ analysis/NN shows/VBZ that/IN it/PRP achieves/VBZ $/$ O/UH (/-LRB- \/SYM log/NN T/NN //HYPH \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ and/CC $/$ O/UH (/-LRB- \/SYM log/NN T/NN //HYPH T/NN )/-RRB- $/$ convergence/NN rates/NNS for/IN general/JJ convex/NN functions/NNS and/CC strongly/RB convex/JJ functions/NNS ,/, respectively/RB ./.
