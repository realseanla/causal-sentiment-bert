Conditional/JJ gradient/NN algorithms/NNS (/-LRB- also/RB often/RB called/VBN Frank/NNP -/HYPH Wolfe/NNP algorithms/NNS )/-RRB- are/VBP popular/JJ due/IN to/IN their/PRP$ simplicity/NN of/IN only/RB requiring/VBG a/DT linear/JJ optimization/NN oracle/NN and/CC more/RBR recently/RB they/PRP also/RB gained/VBD significant/JJ traction/NN for/IN online/JJ learning/NN ./.
While/IN simple/JJ in/IN principle/NN ,/, in/IN many/JJ cases/NNS the/DT actual/JJ implementation/NN of/IN the/DT linear/JJ optimization/NN oracle/NN is/VBZ costly/JJ ./.
We/PRP show/VBP a/DT general/JJ method/NN to/TO lazify/VB various/JJ conditional/JJ gradient/NN algorithms/NNS ,/, which/WDT in/IN actual/JJ computations/NNS leads/VBZ to/IN several/JJ orders/NNS of/IN magnitude/NN of/IN speedup/NN in/IN wall/NN -/HYPH clock/NN time/NN ./.
This/DT is/VBZ achieved/VBN by/IN using/VBG a/DT faster/RBR separation/NN oracle/NN instead/RB of/IN a/DT linear/JJ optimization/NN oracle/NN ,/, relying/VBG only/RB on/IN few/JJ linear/JJ optimization/NN oracle/NN calls/NNS ./.
