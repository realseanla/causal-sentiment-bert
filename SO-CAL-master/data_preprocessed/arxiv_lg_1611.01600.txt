Deep/JJ neural/JJ network/NN models/NNS ,/, though/IN very/RB powerful/JJ and/CC highly/RB successful/JJ ,/, are/VBP computationally/RB expensive/JJ in/IN terms/NNS of/IN space/NN and/CC time/NN ./.
Recently/RB ,/, there/EX have/VBP been/VBN a/DT number/NN of/IN attempts/NNS on/IN binarizing/VBG the/DT network/NN weights/NNS and/CC activations/NNS ./.
This/DT greatly/RB reduces/VBZ the/DT network/NN size/NN ,/, and/CC replaces/VBZ the/DT underlying/VBG multiplications/NNS to/IN additions/NNS or/CC even/RB XNOR/NNP bit/NN operations/NNS ./.
However/RB ,/, existing/VBG binarization/NN schemes/NNS are/VBP based/VBN on/IN simple/JJ matrix/NN approximation/NN and/CC ignore/VB the/DT effect/NN of/IN binarization/NN on/IN the/DT loss/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT proximal/JJ Newton/NNP algorithm/NN with/IN diagonal/JJ Hessian/JJ approximation/NN that/WDT directly/RB minimizes/VBZ the/DT loss/NN w.r.t./IN the/DT binarized/JJ weights/NNS ./.
The/DT underlying/VBG proximal/JJ step/NN has/VBZ an/DT efficient/JJ closed/JJ -/HYPH form/NN solution/NN ,/, and/CC the/DT second/JJ -/HYPH order/NN information/NN can/MD be/VB efficiently/RB obtained/VBN from/IN the/DT second/JJ moments/NNS already/RB computed/VBN by/IN the/DT Adam/NNP optimizer/NN ./.
Experiments/NNS on/IN both/DT feedforward/NN and/CC recurrent/JJ networks/NNS show/VBP that/IN the/DT proposed/JJ loss/NN -/HYPH aware/JJ binarization/NN algorithm/NN outperforms/VBZ existing/VBG binarization/NN schemes/NNS ,/, and/CC is/VBZ also/RB more/RBR robust/JJ for/IN wide/JJ and/CC deep/JJ networks/NNS ./.
