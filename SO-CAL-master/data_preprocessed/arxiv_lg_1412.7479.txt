Deep/JJ neural/JJ networks/NNS have/VBP been/VBN extremely/RB successful/JJ at/IN various/JJ image/NN ,/, speech/NN ,/, video/NN recognition/NN tasks/NNS because/IN of/IN their/PRP$ ability/NN to/TO model/VB deep/JJ structures/NNS within/IN the/DT data/NNS ./.
However/RB ,/, they/PRP are/VBP still/RB prohibitively/RB expensive/JJ to/TO train/VB and/CC apply/VB for/IN problems/NNS containing/VBG millions/NNS of/IN classes/NNS in/IN the/DT output/NN layer/NN ./.
Based/VBN on/IN the/DT observation/NN that/IN the/DT key/JJ computation/NN common/JJ to/IN most/JJS neural/JJ network/NN layers/NNS is/VBZ a/DT vector/NN //HYPH matrix/NN product/NN ,/, we/PRP propose/VBP a/DT fast/JJ locality/NN -/HYPH sensitive/JJ hashing/VBG technique/NN to/TO approximate/VB the/DT actual/JJ dot/NN product/NN enabling/VBG us/PRP to/TO scale/VB up/RP the/DT training/NN and/CC inference/NN to/IN millions/NNS of/IN output/NN classes/NNS ./.
We/PRP evaluate/VBP our/PRP$ technique/NN on/IN three/CD diverse/JJ large/JJ -/HYPH scale/NN recognition/NN tasks/NNS and/CC show/VBP that/IN our/PRP$ approach/NN can/MD train/VB large/JJ -/HYPH scale/NN models/NNS at/IN a/DT faster/JJR rate/NN (/-LRB- in/IN terms/NNS of/IN steps/NNS //, total/JJ time/NN )/-RRB- compared/VBN to/IN baseline/NN methods/NNS ./.
