In/IN this/DT work/NN ,/, we/PRP explore/VBP multiple/JJ neural/JJ architectures/NNS adapted/VBN for/IN the/DT task/NN of/IN automatic/JJ post-editing/NN of/IN machine/NN translation/NN output/NN ./.
We/PRP focus/VBP on/IN neural/JJ end/NN -/HYPH to/IN -/HYPH end/NN models/NNS that/WDT combine/VBP both/DT inputs/NNS $/$ mt/CD $/$ and/CC $/$ src/CD $/$ in/IN a/DT single/JJ neural/JJ architecture/NN ,/, modeling/VBG $/$ \/CD {/-LRB- mt/NN ,/, src/NN \/SYM }/-RRB- \/SYM rightarrow/NN pe/NN $/$ directly/RB ./.
Apart/RB from/IN that/DT ,/, we/PRP investigate/VBP the/DT influence/NN of/IN hard/JJ -/HYPH attention/NN models/NNS which/WDT seem/VBP to/TO be/VB well/RB -/HYPH suited/VBN for/IN monolingual/JJ tasks/NNS ,/, as/RB well/RB as/IN combinations/NNS of/IN both/DT ideas/NNS ./.
We/PRP report/VBP results/NNS on/IN data/NNS sets/VBZ provided/VBN during/IN the/DT WMT/NNP 2016/CD shared/VBD task/NN on/IN automatic/JJ post-editing/NN and/CC can/MD demonstrate/VB that/IN double/JJ -/HYPH attention/NN models/NNS that/WDT incorporate/VBP all/DT available/JJ data/NNS in/IN the/DT APE/NN scenario/NN in/IN a/DT single/JJ model/NN improve/VB on/IN the/DT best/JJS shared/VBN task/NN system/NN and/CC on/IN all/DT other/JJ published/VBN results/NNS after/IN the/DT shared/VBN task/NN ./.
Double/JJ -/HYPH attention/NN models/NNS that/WDT are/VBP combined/VBN with/IN hard/JJ attention/NN remain/VBP competitive/JJ despite/IN applying/VBG fewer/JJR changes/NNS to/IN the/DT input/NN ./.
