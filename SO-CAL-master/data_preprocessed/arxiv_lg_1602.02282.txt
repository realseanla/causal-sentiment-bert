Variational/NNP autoencoders/NNS are/VBP a/DT powerful/JJ framework/NN for/IN unsupervised/JJ learning/NN ./.
However/RB ,/, previous/JJ work/NN has/VBZ been/VBN restricted/VBN to/IN shallow/JJ models/NNS with/IN one/CD or/CC two/CD layers/NNS of/IN fully/RB factorized/VBN stochastic/JJ latent/NN variables/NNS ,/, limiting/VBG the/DT flexibility/NN of/IN the/DT latent/JJ representation/NN ./.
We/PRP propose/VBP three/CD advances/NNS in/IN training/NN algorithms/NNS of/IN variational/JJ autoencoders/NNS ,/, for/IN the/DT first/JJ time/NN allowing/VBG to/TO train/VB deep/JJ models/NNS of/IN up/RB to/IN five/CD stochastic/JJ layers/NNS ,/, (/-LRB- 1/LS )/-RRB- using/VBG a/DT structure/NN similar/JJ to/IN the/DT Ladder/NNP network/NN as/IN the/DT inference/NN model/NN ,/, (/-LRB- 2/LS )/-RRB- warm/JJ -/HYPH up/JJ period/NN to/TO support/VB stochastic/JJ units/NNS staying/VBG active/JJ in/IN early/JJ training/NN ,/, and/CC (/-LRB- 3/LS )/-RRB- use/NN of/IN batch/NN normalization/NN ./.
Using/VBG these/DT improvements/NNS we/PRP show/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN log/NN -/HYPH likelihood/NN results/NNS for/IN generative/JJ modeling/NN on/IN several/JJ benchmark/NN datasets/NNS ./.
