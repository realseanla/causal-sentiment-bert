Recurrent/JJ neural/JJ networks/NNS (/-LRB- RNN/NN )/-RRB- are/VBP capable/JJ of/IN learning/VBG to/TO encode/VB and/CC exploit/VB activation/NN history/NN over/IN an/DT arbitrary/JJ timescale/NN ./.
However/RB ,/, in/IN practice/NN ,/, state/NN of/IN the/DT art/NN gradient/NN descent/NN based/VBN training/NN methods/NNS are/VBP known/VBN to/TO suffer/VB from/IN difficulties/NNS in/IN learning/VBG long/JJ term/NN dependencies/NNS ./.
Here/RB ,/, we/PRP describe/VBP a/DT novel/JJ training/NN method/NN that/WDT involves/VBZ concurrent/JJ parallel/JJ cloned/VBN networks/NNS ,/, each/DT sharing/VBG the/DT same/JJ weights/NNS ,/, each/DT trained/VBN at/IN different/JJ stimulus/NN phase/NN and/CC each/DT maintaining/VBG independent/JJ activation/NN histories/NNS ./.
Training/NN proceeds/NNS by/IN recursively/RB performing/VBG batch/NN -/HYPH updates/NNS over/IN the/DT parallel/JJ clones/NNS as/IN activation/NN history/NN is/VBZ progressively/RB increased/VBN ./.
This/DT allows/VBZ conflicts/NNS to/TO propagate/VB hierarchically/RB from/IN short/JJ -/HYPH term/NN contexts/NNS towards/IN longer/RBR -/HYPH term/NN contexts/NNS until/IN they/PRP are/VBP resolved/VBN ./.
We/PRP illustrate/VBP the/DT parallel/JJ clones/NNS method/NN and/CC hierarchical/JJ conflict/NN propagation/NN with/IN a/DT character/NN -/HYPH level/NN deep/JJ RNN/NN tasked/VBN with/IN memorizing/VBG a/DT paragraph/NN of/IN Moby/NNP Dick/NNP (/-LRB- by/IN Herman/NNP Melville/NNP )/-RRB- ./.
