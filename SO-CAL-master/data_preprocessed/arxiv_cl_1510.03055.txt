Sequence/NN -/HYPH to/IN -/HYPH sequence/NN neural/JJ network/NN models/NNS for/IN generation/NN of/IN conversational/JJ responses/NNS tend/VBP to/TO generate/VB safe/JJ ,/, commonplace/JJ responses/NNS (/-LRB- e.g./FW ,/, \/SYM textit/FW {/-LRB- I/PRP do/VBP n't/RB know/VB }/-RRB- )/-RRB- regardless/RB of/IN the/DT input/NN ./.
We/PRP suggest/VBP that/IN the/DT traditional/JJ objective/JJ function/NN ,/, i.e./FW ,/, the/DT likelihood/NN of/IN output/NN (/-LRB- responses/NNS )/-RRB- given/VBN input/NN (/-LRB- messages/NNS )/-RRB- is/VBZ unsuited/JJ to/IN response/NN generation/NN tasks/NNS ./.
Instead/RB we/PRP propose/VBP using/VBG Maximum/NNP Mutual/NNP Information/NNP (/-LRB- MMI/NNP )/-RRB- as/IN objective/JJ function/NN in/IN neural/JJ models/NNS ./.
Experimental/JJ results/NNS demonstrate/VBP that/IN the/DT proposed/VBN objective/NN function/NN produces/VBZ more/JJR diverse/JJ ,/, interesting/JJ ,/, and/CC appropriate/JJ responses/NNS ,/, yielding/VBG substantive/JJ gains/NNS in/IN \/SYM bleu/JJ scores/NNS on/IN two/CD conversational/JJ datasets/NNS ./.
