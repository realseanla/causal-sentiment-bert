In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ method/NN for/IN calculating/VBG the/DT output/NN layer/NN in/IN neural/JJ machine/NN translation/NN systems/NNS ./.
The/DT method/NN is/VBZ based/VBN on/IN predicting/VBG a/DT binary/JJ code/NN for/IN each/DT word/NN and/CC can/MD reduce/VB computation/NN time/NN //, memory/NN requirements/NNS of/IN the/DT output/NN layer/NN to/TO be/VB logarithmic/JJ in/IN vocabulary/NN size/NN in/IN the/DT best/JJS case/NN ./.
In/IN addition/NN ,/, we/PRP also/RB introduce/VBP two/CD advanced/JJ approaches/NNS to/TO improve/VB the/DT robustness/NN of/IN the/DT proposed/VBN model/NN :/: using/VBG error/NN -/HYPH correcting/VBG codes/NNS and/CC combining/VBG softmax/NN and/CC binary/JJ codes/NNS ./.
Experiments/NNS on/IN two/CD English/JJ -/HYPH Japanese/JJ bidirectional/JJ translation/NN tasks/NNS show/VBP proposed/VBN models/NNS achieve/VBP BLEU/NN scores/NNS that/WDT approach/VBP the/DT softmax/NN ,/, while/IN reducing/VBG memory/NN usage/NN to/IN the/DT order/NN of/IN less/JJR than/IN 1/10/CD and/CC improving/VBG decoding/NN speed/NN on/IN CPUs/NNS by/IN x5/NN to/IN x10/NN ./.
