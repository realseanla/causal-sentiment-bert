We/PRP consider/VBP the/DT problem/NN of/IN efficient/JJ exploration/NN in/IN finite/JJ horizon/NN MDPs.We/NN show/VBP that/IN an/DT optimistic/JJ modification/NN to/IN model/NN -/HYPH based/VBN value/NN iteration/NN ,/, can/MD achieve/VB a/DT regret/NN bound/VBD $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM sqrt/NN {/-LRB- HSAT/NN }/-RRB- H/NN ^/SYM 2S/NN ^/SYM 2A/NN H/NN \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ where/WRB $/$ H$/CD is/VBZ the/DT time/NN horizon/NN ,/, $/$ S$/CD the/DT number/NN of/IN states/NNS ,/, $/$ A$/$ the/DT number/NN of/IN actions/NNS and/CC $/$ T$/CD the/DT time/NN elapsed/VBN ./.
This/DT result/NN improves/VBZ over/IN the/DT best/JJS previous/JJ known/VBN bound/JJ $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- HS/NNP \/SYM sqrt/NNP {/-LRB- AT/NN }/-RRB- )/-RRB- $/$ achieved/VBN by/IN the/DT UCRL2/NN algorithm.The/NN key/JJ significance/NN of/IN our/PRP$ new/JJ results/NNS is/VBZ that/IN when/WRB $/$ T/NN \/SYM geq/NN H/NN ^/SYM 3S/NN ^/SYM 3A/NN $/$ and/CC $/$ SA/NNP \/SYM geq/NNP H$/NNP ,/, it/PRP leads/VBZ to/IN a/DT regret/NN of/IN $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM sqrt/NN {/-LRB- HSAT/NNP }/-RRB- )/-RRB- $/$ that/WDT matches/VBZ the/DT established/VBN lower/JJR bounds/NNS of/IN $/$ \/SYM Omega/NN (/-LRB- \/SYM sqrt/NN {/-LRB- HSAT/NNP }/-RRB- )/-RRB- $/$ up/RP to/IN a/DT logarithmic/JJ factor/NN ./.
Our/PRP$ analysis/NN contain/VBP two/CD key/JJ insights/NNS ./.
We/PRP use/VBP careful/JJ application/NN of/IN concentration/NN inequalities/NNS to/IN the/DT optimal/JJ value/NN function/NN as/IN a/DT whole/JJ ,/, rather/RB than/IN to/IN the/DT transitions/NNS probabilities/NNS (/-LRB- to/TO improve/VB scaling/NN in/IN $/$ S$/CD )/-RRB- ,/, and/CC we/PRP use/VBP "/`` exploration/NN bonuses/NNS "/'' based/VBN on/IN Bernstein/NNP 's/POS inequality/NN ,/, together/RB with/IN using/VBG a/DT recursive/JJ -/HYPH Bellman/NN -/HYPH type/NN -/HYPH Law/NN of/IN Total/JJ Variance/NN (/-LRB- to/TO improve/VB scaling/NN in/IN $/$ H$/CD )/-RRB- ./.
