To/TO enhance/VB the/DT performance/NN of/IN affective/JJ models/NNS and/CC reduce/VB the/DT cost/NN of/IN acquiring/VBG physiological/JJ signals/NNS for/IN real/JJ -/HYPH world/NN applications/NNS ,/, we/PRP adopt/VBP multimodal/JJ deep/JJ learning/NN approach/NN to/TO construct/VB affective/JJ models/NNS from/IN multiple/JJ physiological/JJ signals/NNS ./.
For/IN unimodal/JJ enhancement/NN task/NN ,/, we/PRP indicate/VBP that/IN the/DT best/JJS recognition/NN accuracy/NN of/IN 82.11/CD percent/NN on/IN SEED/NN dataset/NN is/VBZ achieved/VBN with/IN shared/VBN representations/NNS generated/VBN by/IN Deep/NNP AutoEncoder/NNP (/-LRB- DAE/NNP )/-RRB- model/NN ./.
For/IN multimodal/JJ facilitation/NN tasks/NNS ,/, we/PRP demonstrate/VBP that/IN the/DT Bimodal/NNP Deep/NNP AutoEncoder/NNP (/-LRB- BDAE/NNP )/-RRB- achieves/VBZ the/DT mean/JJ accuracies/NNS of/IN 91.01/CD percent/NN and/CC 83.25/CD percent/NN on/IN SEED/NN and/CC DEAP/NN datasets/NNS ,/, respectively/RB ,/, which/WDT are/VBP much/JJ superior/JJ to/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN approaches/NNS ./.
For/IN cross-modal/JJ learning/NN task/NN ,/, our/PRP$ experimental/JJ results/NNS demonstrate/VBP that/IN the/DT mean/JJ accuracy/NN of/IN 66.34/CD percent/NN is/VBZ achieved/VBN on/IN SEED/NN dataset/NN through/IN shared/VBN representations/NNS generated/VBN by/IN EEG/NN -/HYPH based/VBN DAE/NNP as/IN training/NN samples/NNS and/CC shared/VBD representations/NNS generated/VBN by/IN eye/NN -/HYPH based/VBN DAE/NNP as/IN testing/VBG sample/NN ,/, and/CC vice/RB versa/RB ./.
