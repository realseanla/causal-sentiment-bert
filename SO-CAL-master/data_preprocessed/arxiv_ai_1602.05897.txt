We/PRP develop/VBP a/DT general/JJ duality/NN between/IN neural/JJ networks/NNS and/CC compositional/JJ kernels/NNS ,/, striving/VBG towards/IN a/DT better/JJR understanding/NN of/IN deep/JJ learning/NN ./.
We/PRP show/VBP that/IN initial/JJ representations/NNS generated/VBN by/IN common/JJ random/JJ initializations/NNS are/VBP sufficiently/RB rich/JJ to/TO express/VB all/DT functions/NNS in/IN the/DT dual/JJ kernel/NN space/NN ./.
Hence/RB ,/, though/IN the/DT training/NN objective/NN is/VBZ hard/JJ to/TO optimize/VB in/IN the/DT worst/JJS case/NN ,/, the/DT initial/JJ weights/NNS form/VBP a/DT good/JJ starting/NN point/NN for/IN optimization/NN ./.
Our/PRP$ dual/JJ view/NN also/RB reveals/VBZ a/DT pragmatic/JJ and/CC aesthetic/JJ perspective/NN of/IN neural/JJ networks/NNS and/CC underscores/VBZ their/PRP$ expressive/JJ power/NN ./.
