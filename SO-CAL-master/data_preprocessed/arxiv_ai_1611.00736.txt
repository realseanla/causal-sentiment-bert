The/DT Neural/JJ GPU/NN is/VBZ a/DT recent/JJ model/NN that/WDT can/MD learn/VB algorithms/NNS such/JJ as/IN multi-digit/JJ binary/JJ addition/NN and/CC binary/JJ multiplication/NN in/IN a/DT way/NN that/WDT generalizes/VBZ to/IN inputs/NNS of/IN arbitrary/JJ length/NN ./.
We/PRP show/VBP that/IN there/EX are/VBP two/CD simple/JJ ways/NNS of/IN improving/VBG the/DT performance/NN of/IN the/DT Neural/JJ GPU/NN :/: by/IN carefully/RB designing/VBG a/DT curriculum/NN ,/, and/CC by/IN increasing/VBG model/NN size/NN ./.
The/DT latter/JJ requires/VBZ careful/JJ memory/NN management/NN ,/, as/IN a/DT naive/JJ implementation/NN of/IN the/DT Neural/JJ GPU/NN is/VBZ memory/NN intensive/JJ ./.
We/PRP find/VBP that/IN these/DT techniques/NNS to/TO increase/VB the/DT set/NN of/IN algorithmic/JJ problems/NNS that/WDT can/MD be/VB solved/VBN by/IN the/DT Neural/JJ GPU/NN :/: we/PRP have/VBP been/VBN able/JJ to/TO learn/VB to/TO perform/VB all/PDT the/DT arithmetic/NN operations/NNS (/-LRB- and/CC generalize/VB to/IN arbitrarily/RB long/JJ numbers/NNS )/-RRB- when/WRB the/DT arguments/NNS are/VBP given/VBN in/IN the/DT decimal/JJ representation/NN (/-LRB- which/WDT ,/, surprisingly/RB ,/, has/VBZ not/RB been/VBN possible/JJ before/IN )/-RRB- ./.
We/PRP have/VBP also/RB been/VBN able/JJ to/TO train/VB the/DT Neural/JJ GPU/NN to/TO evaluate/VB long/JJ arithmetic/NN expressions/NNS with/IN multiple/JJ operands/NNS that/WDT require/VBP respecting/VBG the/DT precedence/NN order/NN of/IN the/DT operands/NNS ,/, although/IN these/DT have/VBP succeeded/VBN only/RB in/IN their/PRP$ binary/JJ representation/NN ,/, and/CC not/RB with/IN 100/CD \/SYM percent/NN accuracy/NN ./.
