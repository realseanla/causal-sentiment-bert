Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNNs/NNS )/-RRB- are/VBP analyzed/VBN via/IN the/DT theoretical/JJ framework/NN of/IN the/DT information/NN bottleneck/NN (/-LRB- IB/NN )/-RRB- principle/NN ./.
We/PRP first/RB show/VBP that/IN any/DT DNN/NN can/MD be/VB quantified/VBN by/IN the/DT mutual/JJ information/NN between/IN the/DT layers/NNS and/CC the/DT input/NN and/CC output/NN variables/NNS ./.
Using/VBG this/DT representation/NN we/PRP can/MD calculate/VB the/DT optimal/JJ information/NN theoretic/JJ limits/NNS of/IN the/DT DNN/NNP and/CC obtain/VB finite/JJ sample/NN generalization/NN bounds/NNS ./.
The/DT advantage/NN of/IN getting/VBG closer/JJR to/IN the/DT theoretical/JJ limit/NN is/VBZ quantifiable/JJ both/CC by/IN the/DT generalization/NN bound/VBN and/CC by/IN the/DT network/NN 's/POS simplicity/NN ./.
We/PRP argue/VBP that/IN both/CC the/DT optimal/JJ architecture/NN ,/, number/NN of/IN layers/NNS and/CC features/NNS //, connections/NNS at/IN each/DT layer/NN ,/, are/VBP related/VBN to/IN the/DT bifurcation/NN points/NNS of/IN the/DT information/NN bottleneck/NN tradeoff/NN ,/, namely/RB ,/, relevant/JJ compression/NN of/IN the/DT input/NN layer/NN with/IN respect/NN to/IN the/DT output/NN layer/NN ./.
The/DT hierarchical/JJ representations/NNS at/IN the/DT layered/JJ network/NN naturally/RB correspond/VBP to/IN the/DT structural/JJ phase/NN transitions/NNS along/IN the/DT information/NN curve/NN ./.
We/PRP believe/VBP that/IN this/DT new/JJ insight/NN can/MD lead/VB to/IN new/JJ optimality/NN bounds/NNS and/CC deep/JJ learning/NN algorithms/NNS ./.
