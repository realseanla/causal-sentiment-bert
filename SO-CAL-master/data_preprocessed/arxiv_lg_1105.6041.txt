The/DT classical/JJ perceptron/NN rule/NN provides/VBZ a/DT varying/VBG upper/JJ bound/VBN on/IN the/DT maximum/JJ margin/NN ,/, namely/RB the/DT length/NN of/IN the/DT current/JJ weight/NN vector/NN divided/VBN by/IN the/DT total/JJ number/NN of/IN updates/NNS up/IN to/IN that/DT time/NN ./.
Requiring/VBG that/IN the/DT perceptron/NN updates/NNS its/PRP$ internal/JJ state/NN whenever/WRB the/DT normalized/VBN margin/NN of/IN a/DT pattern/NN is/VBZ found/VBN not/RB to/TO exceed/VB a/DT certain/JJ fraction/NN of/IN this/DT dynamic/JJ upper/JJ bound/VBN we/PRP construct/VB a/DT new/JJ approximate/JJ maximum/JJ margin/NN classifier/NN called/VBD the/DT perceptron/NN with/IN dynamic/JJ margin/NN (/-LRB- PDM/NN )/-RRB- ./.
We/PRP demonstrate/VBP that/IN PDM/NNP converges/VBZ in/IN a/DT finite/JJ number/NN of/IN steps/NNS and/CC derive/VBP an/DT upper/JJ bound/VBN on/IN them/PRP ./.
We/PRP also/RB compare/VBP experimentally/RB PDM/NNP with/IN other/JJ perceptron/NN -/HYPH like/JJ algorithms/NNS and/CC support/NN vector/NN machines/NNS on/IN hard/JJ margin/NN tasks/NNS involving/VBG linear/JJ kernels/NNS which/WDT are/VBP equivalent/JJ to/IN 2/CD -/HYPH norm/NN soft/JJ margin/NN ./.
