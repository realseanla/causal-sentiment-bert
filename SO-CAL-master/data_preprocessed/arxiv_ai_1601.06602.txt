We/PRP present/VBP a/DT novel/JJ algorithm/NN for/IN anomaly/NN detection/NN on/IN very/RB large/JJ datasets/NNS and/CC data/NNS streams/NNS ./.
The/DT method/NN ,/, named/VBN EXPected/VBN Similarity/NN Estimation/NN (/-LRB- EXPoSE/VB )/-RRB- ,/, is/VBZ kernel/NN -/HYPH based/VBN and/CC able/JJ to/TO efficiently/RB compute/VB the/DT similarity/NN between/IN new/JJ data/NNS points/NNS and/CC the/DT distribution/NN of/IN regular/JJ data/NNS ./.
The/DT estimator/NN is/VBZ formulated/VBN as/IN an/DT inner/JJ product/NN with/IN a/DT reproducing/VBG kernel/NN Hilbert/NNP space/NN embedding/NN and/CC makes/VBZ no/DT assumption/NN about/IN the/DT type/NN or/CC shape/NN of/IN the/DT underlying/VBG data/NNS distribution/NN ./.
We/PRP show/VBP that/IN offline/RB (/-LRB- batch/NN )/-RRB- learning/NN with/IN EXPoSE/NN can/MD be/VB done/VBN in/IN linear/JJ time/NN and/CC online/NN (/-LRB- incremental/JJ )/-RRB- learning/NN takes/VBZ constant/JJ time/NN per/IN instance/NN and/CC model/NN update/NN ./.
Furthermore/RB ,/, EXPoSE/VB can/MD make/VB predictions/NNS in/IN constant/JJ time/NN ,/, while/IN it/PRP requires/VBZ only/RB constant/JJ memory/NN ./.
In/IN addition/NN we/PRP propose/VBP different/JJ methodologies/NNS for/IN concept/NN drift/NN adaptation/NN on/IN evolving/VBG data/NNS streams/NNS ./.
On/IN several/JJ real/JJ datasets/NNS we/PRP demonstrate/VBP that/IN our/PRP$ approach/NN can/MD compete/VB with/IN state/NN of/IN the/DT art/NN algorithms/NNS for/IN anomaly/NN detection/NN while/IN being/VBG significant/JJ faster/JJR than/IN techniques/NNS with/IN the/DT same/JJ discriminant/JJ power/NN ./.
