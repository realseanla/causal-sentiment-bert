We/PRP develop/VBP a/DT framework/NN for/IN post/NN model/NN selection/NN inference/NN ,/, via/IN marginal/JJ screening/NN ,/, in/IN linear/JJ regression/NN ./.
At/IN the/DT core/NN of/IN this/DT framework/NN is/VBZ a/DT result/NN that/IN characterizes/VBZ the/DT exact/JJ distribution/NN of/IN linear/JJ functions/NNS of/IN the/DT response/NN $/$ y/WRB $/$ ,/, conditional/JJ on/IN the/DT model/NN being/VBG selected/VBN (/-LRB- ``/`` condition/NN on/IN selection/NN "/'' framework/NN )/-RRB- ./.
This/DT allows/VBZ us/PRP to/TO construct/VB valid/JJ confidence/NN intervals/NNS and/CC hypothesis/NN tests/NNS for/IN regression/NN coefficients/NNS that/WDT account/VBP for/IN the/DT selection/NN procedure/NN ./.
In/IN contrast/NN to/IN recent/JJ work/NN in/IN high/JJ -/HYPH dimensional/JJ statistics/NNS ,/, our/PRP$ results/NNS are/VBP exact/JJ (/-LRB- non-asymptotic/JJ )/-RRB- and/CC require/VBP no/DT eigenvalue/NN -/HYPH like/JJ assumptions/NNS on/IN the/DT design/NN matrix/NN $/$ X$/CD ./.
Furthermore/RB ,/, the/DT computational/JJ cost/NN of/IN marginal/JJ regression/NN ,/, constructing/VBG confidence/NN intervals/NNS and/CC hypothesis/NN testing/NN is/VBZ negligible/JJ compared/VBN to/IN the/DT cost/NN of/IN linear/JJ regression/NN ,/, thus/RB making/VBG our/PRP$ methods/NNS particularly/RB suitable/JJ for/IN extremely/RB large/JJ datasets/NNS ./.
Although/IN we/PRP focus/VBP on/IN marginal/JJ screening/NN to/TO illustrate/VB the/DT applicability/NN of/IN the/DT condition/NN on/IN selection/NN framework/NN ,/, this/DT framework/NN is/VBZ much/RB more/RBR broadly/RB applicable/JJ ./.
We/PRP show/VBP how/WRB to/TO apply/VB the/DT proposed/VBN framework/NN to/IN several/JJ other/JJ selection/NN procedures/NNS including/VBG orthogonal/JJ matching/NN pursuit/NN ,/, non-negative/JJ least/JJS squares/NNS ,/, and/CC marginal/JJ screening/NN Lasso/NNP ./.
