Embedding/VBG learning/NN ,/, a.k.a./RB representation/NN learning/NN ,/, has/VBZ been/VBN shown/VBN to/TO be/VB able/JJ to/TO model/VB large/JJ -/HYPH scale/NN semantic/JJ knowledge/NN graphs/NNS ./.
A/DT key/JJ concept/NN is/VBZ a/DT mapping/NN of/IN the/DT knowledge/NN graph/NN to/IN a/DT tensor/NN representation/NN whose/WP$ entries/NNS are/VBP predicted/VBN by/IN models/NNS using/VBG latent/NN representations/NNS of/IN generalized/VBN entities/NNS ./.
Knowledge/NN graphs/NNS are/VBP typically/RB treated/VBN as/IN static/NN :/: A/DT knowledge/NN graph/NN grows/VBZ more/JJR links/NNS when/WRB more/JJR facts/NNS become/VBP available/JJ but/CC the/DT ground/NN truth/NN values/NNS associated/VBN with/IN links/NNS is/VBZ considered/VBN time/NN invariant/JJ ./.
In/IN this/DT paper/NN we/PRP address/VBP the/DT issue/NN of/IN knowledge/NN graphs/NNS where/WRB triple/JJ states/NNS depend/VBP on/IN time/NN ./.
We/PRP assume/VBP that/IN changes/NNS in/IN the/DT knowledge/NN graph/NN always/RB arrive/VBP in/IN form/NN of/IN events/NNS ,/, in/IN the/DT sense/NN that/IN the/DT events/NNS are/VBP the/DT gateway/NN to/IN the/DT knowledge/NN graph/NN ./.
We/PRP train/VBP an/DT event/NN prediction/NN model/NN which/WDT uses/VBZ both/DT knowledge/NN graph/NN background/NN information/NN and/CC information/NN on/IN recent/JJ events/NNS ./.
By/IN predicting/VBG future/JJ events/NNS ,/, we/PRP also/RB predict/VBP likely/JJ changes/NNS in/IN the/DT knowledge/NN graph/NN and/CC thus/RB obtain/VB a/DT model/NN for/IN the/DT evolution/NN of/IN the/DT knowledge/NN graph/NN as/RB well/RB ./.
Our/PRP$ experiments/NNS demonstrate/VBP that/IN our/PRP$ approach/NN performs/VBZ well/RB in/IN a/DT clinical/JJ application/NN ,/, a/DT recommendation/NN engine/NN and/CC a/DT sensor/NN network/NN application/NN ./.
