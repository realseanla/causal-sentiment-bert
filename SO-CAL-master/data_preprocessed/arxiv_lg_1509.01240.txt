We/PRP show/VBP that/IN any/DT model/NN trained/VBN by/IN a/DT stochastic/JJ gradient/NN method/NN with/IN few/JJ iterations/NNS has/VBZ vanishing/VBG generalization/NN error/NN ./.
We/PRP prove/VBP this/DT by/IN showing/VBG the/DT method/NN is/VBZ algorithmically/RB stable/JJ in/IN the/DT sense/NN of/IN Bousquet/NNP and/CC Elisseeff/NNP ./.
Our/PRP$ analysis/NN only/RB employs/VBZ elementary/JJ tools/NNS from/IN convex/NN and/CC continuous/JJ optimization/NN ./.
Our/PRP$ results/NNS apply/VBP to/IN both/DT convex/NN and/CC non-convex/JJ optimization/NN under/IN standard/JJ Lipschitz/NNP and/CC smoothness/NN assumptions/NNS ./.
