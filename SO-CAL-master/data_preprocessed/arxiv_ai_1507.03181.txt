In/IN this/DT paper/NN ,/, we/PRP focus/VBP on/IN a/DT novel/JJ knowledge/NN reuse/VBP scenario/NN where/WRB the/DT knowledge/NN in/IN the/DT source/NN schema/NN needs/VBZ to/TO be/VB translated/VBN to/IN a/DT semantically/RB heterogeneous/JJ target/NN schema/NN ./.
We/PRP refer/VBP to/IN this/DT task/NN as/IN "/`` knowledge/NN translation/NN "/'' (/-LRB- KT/NNP )/-RRB- ./.
Unlike/IN data/NNS translation/NN and/CC transfer/NN learning/NN ,/, KT/NNP does/VBZ not/RB require/VB any/DT data/NNS from/IN the/DT source/NN or/CC target/NN schema/NN ./.
We/PRP adopt/VBP a/DT probabilistic/JJ approach/NN to/IN KT/NNP by/IN representing/VBG the/DT knowledge/NN in/IN the/DT source/NN schema/NN ,/, the/DT mapping/NN between/IN the/DT source/NN and/CC target/NN schemas/NNS ,/, and/CC the/DT resulting/VBG knowledge/NN in/IN the/DT target/NN schema/NN all/DT as/IN probability/NN distributions/NNS ,/, specially/RB using/VBG Markov/NNP random/JJ fields/NNS and/CC Markov/NNP logic/NN networks/NNS ./.
Given/VBN the/DT source/NN knowledge/NN and/CC mappings/NNS ,/, we/PRP use/VBP standard/JJ learning/NN and/CC inference/NN algorithms/NNS for/IN probabilistic/JJ graphical/JJ models/NNS to/TO find/VB an/DT explicit/JJ probability/NN distribution/NN in/IN the/DT target/NN schema/NN that/WDT minimizes/VBZ the/DT Kullback/NNP -/HYPH Leibler/NNP divergence/NN from/IN the/DT implicit/JJ distribution/NN ./.
This/DT gives/VBZ us/PRP a/DT compact/JJ probabilistic/JJ model/NN that/WDT represents/VBZ knowledge/NN from/IN the/DT source/NN schema/NN as/RB well/RB as/IN possible/JJ ,/, respecting/VBG the/DT uncertainty/NN in/IN both/CC the/DT source/NN knowledge/NN and/CC the/DT mapping/NN ./.
In/IN experiments/NNS on/IN both/DT propositional/JJ and/CC relational/JJ domains/NNS ,/, we/PRP find/VBP that/IN the/DT knowledge/NN obtained/VBN by/IN KT/NNP is/VBZ comparable/JJ to/IN other/JJ approaches/NNS that/WDT require/VBP data/NNS ,/, demonstrating/VBG that/IN knowledge/NN can/MD be/VB reused/VBN without/IN data/NNS ./.
