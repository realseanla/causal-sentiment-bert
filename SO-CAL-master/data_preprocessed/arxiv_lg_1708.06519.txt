The/DT deployment/NN of/IN deep/JJ convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- in/IN many/JJ real/JJ world/NN applications/NNS is/VBZ largely/RB hindered/VBN by/IN their/PRP$ high/JJ computational/JJ cost/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ learning/NN scheme/NN for/IN CNNs/NNS to/IN simultaneously/RB 1/CD )/-RRB- reduce/VB the/DT model/NN size/NN ;/: 2/LS )/-RRB- decrease/VB the/DT run/NN -/HYPH time/NN memory/NN footprint/NN ;/: and/CC 3/LS )/-RRB- lower/VB the/DT number/NN of/IN computing/VBG operations/NNS ,/, without/IN compromising/VBG accuracy/NN ./.
This/DT is/VBZ achieved/VBN by/IN enforcing/VBG channel/NN -/HYPH level/NN sparsity/NN in/IN the/DT network/NN in/IN a/DT simple/JJ but/CC effective/JJ way/NN ./.
Different/JJ from/IN many/JJ existing/JJ approaches/NNS ,/, the/DT proposed/JJ method/NN directly/RB applies/VBZ to/IN modern/JJ CNN/NNP architectures/NNS ,/, introduces/VBZ minimum/JJ overhead/NN to/IN the/DT training/NN process/NN ,/, and/CC requires/VBZ no/DT special/JJ software/NN //HYPH hardware/NN accelerators/NNS for/IN the/DT resulting/VBG models/NNS ./.
We/PRP call/VBP our/PRP$ approach/NN network/NN slimming/VBG ,/, which/WDT takes/VBZ wide/RB and/CC large/JJ networks/NNS as/IN input/NN models/NNS ,/, but/CC during/IN training/NN insignificant/JJ channels/NNS are/VBP automatically/RB identified/VBN and/CC pruned/VBN afterwards/RB ,/, yielding/VBG thin/JJ and/CC compact/JJ models/NNS with/IN comparable/JJ accuracy/NN ./.
We/PRP empirically/RB demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ approach/NN with/IN several/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN CNN/NNP models/NNS ,/, including/VBG VGGNet/NNP ,/, ResNet/NNP and/CC DenseNet/NNP ,/, on/IN various/JJ image/NN classification/NN datasets/NNS ./.
For/IN VGGNet/NNP ,/, a/DT multi-pass/NN version/NN of/IN network/NN slimming/VBG gives/VBZ a/DT 20x/NN reduction/NN in/IN model/NN size/NN and/CC a/DT 5x/NN reduction/NN in/IN computing/VBG operations/NNS ./.
