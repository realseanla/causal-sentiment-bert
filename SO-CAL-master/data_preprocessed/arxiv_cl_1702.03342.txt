Explicit/JJ concept/NN space/NN models/NNS have/VBP proven/VBN efficacy/NN for/IN text/NN representation/NN in/IN many/JJ natural/JJ language/NN and/CC text/NN mining/NN applications/NNS ./.
The/DT idea/NN is/VBZ to/TO embed/VB textual/JJ structures/NNS into/IN a/DT semantic/JJ space/NN of/IN concepts/NNS which/WDT captures/VBZ the/DT main/JJ topics/NNS of/IN these/DT structures/NNS ./.
That/DT so/RB called/VBN bag/NN -/HYPH of/IN -/HYPH concepts/NNS representation/NN suffers/VBZ from/IN data/NNS sparsity/NN causing/VBG low/JJ similarity/NN scores/NNS between/IN similar/JJ texts/NNS due/IN to/IN low/JJ concept/NN overlap/NN ./.
In/IN this/DT paper/NN we/PRP propose/VBP two/CD neural/JJ embedding/NN models/NNS in/IN order/NN to/TO learn/VB continuous/JJ concept/NN vectors/NNS ./.
Once/IN learned/VBN ,/, we/PRP propose/VBP an/DT efficient/JJ vector/NN aggregation/NN method/NN to/TO generate/VB fully/RB dense/JJ bag/NN -/HYPH of/IN -/HYPH concepts/NNS representations/NNS ./.
Empirical/JJ results/NNS on/IN a/DT benchmark/NN dataset/NN for/IN measuring/VBG entity/NN semantic/JJ relatedness/NN show/NN superior/JJ performance/NN over/IN other/JJ concept/NN embedding/NN models/NNS ./.
In/IN addition/NN ,/, by/IN utilizing/VBG our/PRP$ efficient/JJ aggregation/NN method/NN ,/, we/PRP demonstrate/VBP the/DT effectiveness/NN of/IN the/DT densified/VBN vector/NN representation/NN over/IN the/DT typical/JJ sparse/JJ representations/NNS for/IN dataless/JJ classification/NN where/WRB we/PRP can/MD achieve/VB at/IN least/RBS same/JJ or/CC better/JJR accuracy/NN with/IN much/RB less/JJR dimensions/NNS ./.
