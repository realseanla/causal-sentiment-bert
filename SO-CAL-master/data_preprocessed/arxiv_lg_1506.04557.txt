Performing/VBG inference/NN and/CC learning/NN of/IN deep/JJ generative/JJ networks/NNS in/IN a/DT Bayesian/JJ setting/NN is/VBZ desirable/JJ ,/, where/WRB a/DT sparsity/NN -/HYPH inducing/VBG prior/JJ can/MD be/VB adopted/VBN on/IN model/NN parameters/NNS or/CC a/DT nonparametric/JJ Bayesian/JJ process/NN can/MD be/VB used/VBN to/TO infer/VB the/DT network/NN structure/NN ./.
However/RB ,/, posterior/JJ inference/NN for/IN such/JJ deep/JJ models/NNS is/VBZ an/DT extremely/RB challenging/JJ task/NN ,/, which/WDT has/VBZ largely/RB not/RB been/VBN well/RB -/HYPH addressed/VBN ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP doubly/RB stochastic/JJ gradient/NN -/HYPH based/VBN MCMC/NNP ,/, a/DT simple/JJ and/CC effective/JJ method/NN that/WDT can/MD be/VB widely/RB applied/VBN for/IN Bayesian/JJ inference/NN of/IN deep/JJ generative/NN models/NNS in/IN continuous/JJ parameter/NN spaces/NNS ./.
The/DT algorithm/NN is/VBZ doubly/RB stochastic/JJ in/IN the/DT sense/NN that/IN at/IN each/DT MCMC/NN sampling/NN step/NN a/DT mini-batch/NN of/IN data/NNS samples/NNS are/VBP randomly/RB drawn/VBN to/TO estimate/VB the/DT gradient/NN of/IN log/NN -/HYPH posterior/JJ and/CC the/DT intractable/JJ expectation/NN over/IN latent/JJ variables/NNS is/VBZ further/JJ estimated/VBN via/IN a/DT Monte/NNP Carlo/NNP sampler/NN ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN on/IN learning/VBG deep/JJ sigmoid/NN belief/NN networks/NNS (/-LRB- DSBNs/NNS )/-RRB- ./.
Compared/VBN to/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS using/VBG Gibbs/NNP sampling/NN with/IN data/NNS augmentation/NN ,/, our/PRP$ algorithm/NN is/VBZ much/RB more/RBR efficient/JJ and/CC manages/VBZ to/TO learn/VB DSBNs/NNS on/IN large/JJ datasets/NNS ./.
