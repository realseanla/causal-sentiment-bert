In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT novel/JJ recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- architecture/NN ./.
The/DT proposed/VBN RNN/NN ,/, gated/VBN -/HYPH feedback/NN RNN/NN (/-LRB- GF/NN -/HYPH RNN/NN )/-RRB- ,/, extends/VBZ the/DT existing/VBG approach/NN of/IN stacking/VBG multiple/JJ recurrent/JJ layers/NNS by/IN allowing/VBG and/CC controlling/VBG signals/NNS flowing/VBG from/IN upper/JJ recurrent/JJ layers/NNS to/TO lower/VB layers/NNS using/VBG a/DT global/JJ gating/NN unit/NN for/IN each/DT pair/NN of/IN layers/NNS ./.
The/DT recurrent/JJ signals/NNS exchanged/VBD between/IN layers/NNS are/VBP gated/VBN adaptively/RB based/VBN on/IN the/DT previous/JJ hidden/JJ state/NN and/CC the/DT current/JJ input/NN ./.
We/PRP evaluated/VBD the/DT proposed/VBN GF/NN -/HYPH RNN/NN with/IN different/JJ types/NNS of/IN recurrent/JJ units/NNS ,/, such/JJ as/IN tanh/NN ,/, long/JJ short/JJ -/HYPH term/NN memory/NN and/CC gated/VBN recurrent/JJ units/NNS ,/, on/IN the/DT tasks/NNS of/IN character/NN -/HYPH level/NN language/NN modeling/NN and/CC Python/NN program/NN evaluation/NN ./.
Our/PRP$ empirical/JJ evaluation/NN of/IN different/JJ RNN/NN units/NNS ,/, revealed/VBD that/IN in/IN both/DT tasks/NNS ,/, the/DT GF/NN -/HYPH RNN/NN outperforms/VBZ the/DT conventional/JJ approaches/NNS to/TO build/VB deep/RB stacked/VBN RNNs/NNS ./.
We/PRP suggest/VBP that/IN the/DT improvement/NN arises/VBZ because/IN the/DT GF/NN -/HYPH RNN/NN can/MD adaptively/RB assign/VB different/JJ layers/NNS to/IN different/JJ timescales/NNS and/CC layer/NN -/HYPH to/IN -/HYPH layer/NN interactions/NNS (/-LRB- including/VBG the/DT top/JJ -/HYPH down/JJ ones/NNS which/WDT are/VBP not/RB usually/RB present/JJ in/IN a/DT stacked/VBN RNN/NN )/-RRB- by/IN learning/VBG to/IN gate/NN these/DT interactions/NNS ./.
