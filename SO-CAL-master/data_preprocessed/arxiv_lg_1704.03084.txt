In/IN a/DT composite/JJ -/HYPH domain/NN task/NN -/HYPH completion/NN dialogue/NN system/NN ,/, a/DT conversation/NN agent/NN often/RB switches/VBZ among/IN multiple/JJ sub-domains/NNS before/IN it/PRP successfully/RB completes/VBZ the/DT task/NN ./.
Given/VBN such/PDT a/DT scenario/NN ,/, a/DT standard/JJ deep/JJ reinforcement/NN learning/VBG based/VBN dialogue/NN agent/NN may/MD suffer/VB to/TO find/VB a/DT good/JJ policy/NN due/IN to/IN the/DT issues/NNS such/JJ as/IN :/: increased/VBN state/NN and/CC action/NN spaces/NNS ,/, high/JJ sample/NN complexity/NN demands/NNS ,/, sparse/JJ reward/NN and/CC long/JJ horizon/NN ,/, etc/FW ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO use/VB hierarchical/JJ deep/JJ reinforcement/NN learning/VBG approach/NN which/WDT can/MD operate/VB at/IN different/JJ temporal/JJ scales/NNS and/CC is/VBZ intrinsically/RB motivated/VBN to/TO attack/VB these/DT problems/NNS ./.
Our/PRP$ hierarchical/JJ network/NN consists/VBZ of/IN two/CD levels/NNS :/: the/DT top/JJ -/HYPH level/NN meta/NN -/HYPH controller/NN for/IN subgoal/JJ selection/NN and/CC the/DT low/JJ -/HYPH level/NN controller/NN for/IN dialogue/NN policy/NN learning/NN ./.
Subgoals/NNS selected/VBN by/IN meta/NN -/HYPH controller/NN and/CC intrinsic/JJ rewards/NNS can/MD guide/VB the/DT controller/NN to/TO effectively/RB explore/VB in/IN the/DT state/NN -/HYPH action/NN space/NN and/CC mitigate/VB the/DT spare/JJ reward/NN and/CC long/JJ horizon/NN problems/NNS ./.
Experiments/NNS on/IN both/DT simulations/NNS and/CC human/JJ evaluation/NN show/VBP that/IN our/PRP$ model/NN significantly/RB outperforms/VBZ flat/JJ deep/JJ reinforcement/NN learning/VBG agents/NNS in/IN terms/NNS of/IN success/NN rate/NN ,/, rewards/NNS and/CC user/NN rating/NN ./.
