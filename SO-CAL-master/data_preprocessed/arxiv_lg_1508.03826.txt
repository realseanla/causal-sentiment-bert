Most/JJS existing/VBG word/NN embedding/NN methods/NNS can/MD be/VB categorized/VBN into/IN Neural/JJ Embedding/NN Models/NNS and/CC Matrix/NNP Factorization/NNP (/-LRB- MF/NNP )/-RRB- -/HYPH based/VBN methods/NNS ./.
However/RB some/DT models/NNS are/VBP opaque/JJ to/IN probabilistic/JJ interpretation/NN ,/, and/CC MF/NN -/HYPH based/VBN methods/NNS ,/, typically/RB solved/VBN using/VBG Singular/JJ Value/NN Decomposition/NN (/-LRB- SVD/NN )/-RRB- ,/, may/MD incur/VB loss/NN of/IN corpus/NN information/NN ./.
In/IN addition/NN ,/, it/PRP is/VBZ desirable/JJ to/TO incorporate/VB global/JJ latent/JJ factors/NNS ,/, such/JJ as/IN topics/NNS ,/, sentiments/NNS or/CC writing/VBG styles/NNS ,/, into/IN the/DT word/NN embedding/NN model/NN ./.
Since/IN generative/JJ models/NNS provide/VBP a/DT principled/JJ way/NN to/TO incorporate/VB latent/JJ factors/NNS ,/, we/PRP propose/VBP a/DT generative/JJ word/NN embedding/NN model/NN ,/, which/WDT is/VBZ easy/JJ to/TO interpret/VB ,/, and/CC can/MD serve/VB as/IN a/DT basis/NN of/IN more/JJR sophisticated/JJ latent/JJ factor/NN models/NNS ./.
The/DT model/NN inference/NN reduces/VBZ to/IN a/DT low/JJ rank/NN weighted/VBN positive/JJ semidefinite/NN approximation/NN problem/NN ./.
Its/PRP$ optimization/NN is/VBZ approached/VBN by/IN eigendecomposition/NN on/IN a/DT submatrix/NN ,/, followed/VBN by/IN online/JJ blockwise/JJ regression/NN ,/, which/WDT is/VBZ scalable/JJ and/CC avoids/VBZ the/DT information/NN loss/NN in/IN SVD/NNP ./.
In/IN experiments/NNS on/IN 7/CD common/JJ benchmark/NN datasets/NNS ,/, our/PRP$ vectors/NNS are/VBP competitive/JJ to/IN word2vec/NN ,/, and/CC better/JJR than/IN other/JJ MF/NNP -/HYPH based/VBN methods/NNS ./.
