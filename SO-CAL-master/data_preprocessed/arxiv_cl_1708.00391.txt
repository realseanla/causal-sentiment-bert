A/DT major/JJ challenge/NN in/IN paraphrase/NN research/NN is/VBZ the/DT lack/NN of/IN parallel/JJ corpora/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT new/JJ method/NN to/TO collect/VB large/JJ -/HYPH scale/NN sentential/JJ paraphrases/NNS from/IN Twitter/NNP by/IN linking/VBG tweets/NNS through/IN shared/VBN URLs/NNS ./.
The/DT main/JJ advantage/NN of/IN our/PRP$ method/NN is/VBZ its/PRP$ simplicity/NN ,/, as/IN it/PRP gets/VBZ rid/VBN of/IN the/DT classifier/NN or/CC human/JJ in/IN the/DT loop/NN needed/VBN to/TO select/VB data/NNS before/IN annotation/NN and/CC subsequent/JJ application/NN of/IN paraphrase/NN identification/NN algorithms/NNS in/IN the/DT previous/JJ work/NN ./.
We/PRP present/VBP the/DT largest/JJS human/JJ -/HYPH labeled/VBN paraphrase/NN corpus/NN to/IN date/NN of/IN 51,524/CD sentence/NN pairs/NNS and/CC the/DT first/JJ cross-domain/JJ benchmarking/NN for/IN automatic/JJ paraphrase/NN identification/NN ./.
In/IN addition/NN ,/, we/PRP show/VBP that/IN more/JJR than/IN 30,000/CD new/JJ sentential/JJ paraphrases/NNS can/MD be/VB easily/RB and/CC continuously/RB captured/VBN every/DT month/NN at/IN ~/SYM 70/CD percent/NN precision/NN ,/, and/CC demonstrate/VBP their/PRP$ utility/NN for/IN downstream/JJ NLP/NN tasks/NNS through/IN phrasal/JJ paraphrase/NN extraction/NN ./.
We/PRP make/VBP our/PRP$ code/NN and/CC data/NNS freely/RB available/JJ ./.
