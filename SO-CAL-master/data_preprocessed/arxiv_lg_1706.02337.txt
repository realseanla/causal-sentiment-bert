We/PRP present/VBP an/DT end/NN -/HYPH to/IN -/HYPH end/NN ,/, multimodal/JJ ,/, fully/RB convolutional/JJ network/NN for/IN extracting/VBG semantic/JJ structures/NNS from/IN document/NN images/NNS ./.
We/PRP consider/VBP document/NN semantic/JJ structure/NN extraction/NN as/IN a/DT pixel-wise/JJ segmentation/NN task/NN ,/, and/CC propose/VB a/DT unified/JJ model/NN that/WDT classifies/VBZ pixels/NNS based/VBN not/RB only/RB on/IN their/PRP$ visual/JJ appearance/NN ,/, as/IN in/IN the/DT traditional/JJ page/NN segmentation/NN task/NN ,/, but/CC also/RB on/IN the/DT content/NN of/IN underlying/VBG text/NN ./.
Moreover/RB ,/, we/PRP propose/VBP an/DT efficient/JJ synthetic/JJ document/NN generation/NN process/NN that/WDT we/PRP use/VBP to/TO generate/VB pretraining/NN data/NNS for/IN our/PRP$ network/NN ./.
Once/IN the/DT network/NN is/VBZ trained/VBN on/IN a/DT large/JJ set/NN of/IN synthetic/JJ documents/NNS ,/, we/PRP fine/RB -/HYPH tune/VB the/DT network/NN on/IN unlabeled/JJ real/JJ documents/NNS using/VBG a/DT semi-supervised/JJ approach/NN ./.
We/PRP systematically/RB study/VB the/DT optimum/JJ network/NN architecture/NN and/CC show/VBP that/IN both/CC our/PRP$ multimodal/JJ approach/NN and/CC the/DT synthetic/JJ data/NN pretraining/NN significantly/RB boost/VB the/DT performance/NN ./.
