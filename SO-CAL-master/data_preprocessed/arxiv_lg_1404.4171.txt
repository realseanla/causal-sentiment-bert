Dropout/NN and/CC other/JJ feature/NN noising/JJ schemes/NNS have/VBP shown/VBN promising/JJ results/NNS in/IN controlling/VBG over-fitting/NN by/IN artificially/RB corrupting/VBG the/DT training/NN data/NNS ./.
Though/IN extensive/JJ theoretical/JJ and/CC empirical/JJ studies/NNS have/VBP been/VBN performed/VBN for/IN generalized/VBN linear/JJ models/NNS ,/, little/JJ work/NN has/VBZ been/VBN done/VBN for/IN support/NN vector/NN machines/NNS (/-LRB- SVMs/NNS )/-RRB- ,/, one/CD of/IN the/DT most/RBS successful/JJ approaches/NNS for/IN supervised/JJ learning/NN ./.
This/DT paper/NN presents/VBZ dropout/NN training/NN for/IN linear/JJ SVMs/NNS ./.
To/TO deal/VB with/IN the/DT intractable/JJ expectation/NN of/IN the/DT non-smooth/JJ hinge/NN loss/NN under/IN corrupting/VBG distributions/NNS ,/, we/PRP develop/VBP an/DT iteratively/RB re-weighted/JJ least/JJS square/JJ (/-LRB- IRLS/NN )/-RRB- algorithm/NN by/IN exploring/VBG data/NNS augmentation/NN techniques/NNS ./.
Our/PRP$ algorithm/NN iteratively/RB minimizes/VBZ the/DT expectation/NN of/IN a/DT re-weighted/JJ least/JJS square/JJ problem/NN ,/, where/WRB the/DT re-weights/NNS have/VBP closed/VBN -/HYPH form/NN solutions/NNS ./.
The/DT similar/JJ ideas/NNS are/VBP applied/VBN to/TO develop/VB a/DT new/JJ IRLS/NN algorithm/NN for/IN the/DT expected/VBN logistic/JJ loss/NN under/IN corrupting/VBG distributions/NNS ./.
Our/PRP$ algorithms/NNS offer/VBP insights/NNS on/IN the/DT connection/NN and/CC difference/NN between/IN the/DT hinge/NN loss/NN and/CC logistic/JJ loss/NN in/IN dropout/NN training/NN ./.
Empirical/JJ results/NNS on/IN several/JJ real/JJ datasets/NNS demonstrate/VBP the/DT effectiveness/NN of/IN dropout/NN training/NN on/IN significantly/RB boosting/VBG the/DT classification/NN accuracy/NN of/IN linear/JJ SVMs/NNS ./.
