Fully/RB connected/JJ network/NN has/VBZ been/VBN widely/RB used/VBN in/IN deep/JJ learning/NN ,/, and/CC its/PRP$ computation/NN efficiency/NN is/VBZ highly/RB benefited/VBN from/IN the/DT matrix/NN multiplication/NN algorithm/NN with/IN cuBLAS/NN on/IN GPU/NNP ./.
However/RB ,/, We/PRP found/VBD that/IN ,/, there/EX exist/VBP some/DT drawbacks/NNS of/IN cuBLAS/NN in/IN calculating/VBG matrix/NN $/$ \/CD textbf/NN {/-LRB- A/DT }/-RRB- $/$ multiplies/VBZ the/DT transpose/NN of/IN matrix/NN $/$ \/CD textbf/NN {/-LRB- B/NN }/-RRB- $/$ (/-LRB- i.e./FW ,/, NT/NN operation/NN )/-RRB- ./.
To/TO reduce/VB the/DT impact/NN of/IN NT/NN operation/NN by/IN cuBLAS/NN ,/, we/PRP exploit/VBP the/DT out/NN -/HYPH of/IN -/HYPH place/NN transpose/NN of/IN matrix/NN $/$ \/CD textbf/NN {/-LRB- B/NN }/-RRB- $/$ to/TO avoid/VB using/VBG NT/NN operation/NN ,/, and/CC then/RB we/PRP apply/VBP our/PRP$ method/NN to/TO Caffe/NNP ,/, which/WDT is/VBZ a/DT popular/JJ deep/JJ learning/NN tool/NN ./.
Our/PRP$ contribution/NN is/VBZ two-fold/JJ ./.
First/RB ,/, we/PRP propose/VBP a/DT naive/JJ method/NN (/-LRB- TNN/NNP )/-RRB- and/CC model/NN -/HYPH based/VBN method/NN (/-LRB- MTNN/NN )/-RRB- to/TO increase/VB the/DT performance/NN in/IN calculating/VBG $/$ \/CD textbf/NN {/-LRB- A/DT }/-RRB- \/SYM times/NNS \/SYM textbf/NN {/-LRB- B/NN }/-RRB- ^/SYM T$/NN ,/, and/CC it/PRP achieves/VBZ about/IN 4.7/CD times/NNS performance/NN enhancement/NN in/IN our/PRP$ tested/VBN cases/NNS on/IN GTX1080/NN card/NN ./.
Second/RB ,/, we/PRP integrate/VBP MTNN/NN method/NN into/IN Caffe/NNP to/TO enhance/VB the/DT efficiency/NN in/IN training/NN fully/RB connected/VBN networks/NNS ,/, which/WDT achieves/VBZ about/RB 70/CD percent/NN speedup/NN compared/VBN to/IN the/DT original/JJ Caffe/NNP in/IN our/PRP$ configured/VBN fully/RB connected/VBN networks/NNS on/IN GTX1080/NN card/NN ./.
