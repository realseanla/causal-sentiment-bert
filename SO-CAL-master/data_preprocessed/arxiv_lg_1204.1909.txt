In/IN budget/NN -/HYPH limited/VBN multi-armed/JJ bandit/NN (/-LRB- MAB/NN )/-RRB- problems/NNS ,/, the/DT learner/NN 's/POS actions/NNS are/VBP costly/JJ and/CC constrained/VBN by/IN a/DT fixed/VBN budget/NN ./.
Consequently/RB ,/, an/DT optimal/JJ exploitation/NN policy/NN may/MD not/RB be/VB to/TO pull/VB the/DT optimal/JJ arm/NN repeatedly/RB ,/, as/RB is/VBZ the/DT case/NN in/IN other/JJ variants/NNS of/IN MAB/NNP ,/, but/CC rather/RB to/TO pull/VB the/DT sequence/NN of/IN different/JJ arms/NNS that/WDT maximises/VBZ the/DT agent/NN 's/POS total/JJ reward/NN within/IN the/DT budget/NN ./.
This/DT difference/NN from/IN existing/VBG MABs/NNS means/VBZ that/IN new/JJ approaches/NNS to/IN maximising/VBG the/DT total/JJ reward/NN are/VBP required/VBN ./.
Given/VBN this/DT ,/, we/PRP develop/VBP two/CD pulling/VBG policies/NNS ,/, namely/RB :/: (/-LRB- i/LS )/-RRB- KUBE/NN ;/: and/CC (/-LRB- ii/LS )/-RRB- fractional/JJ KUBE/NNP ./.
Whereas/IN the/DT former/JJ provides/VBZ better/JJR performance/NN up/IN to/IN 40/CD percent/NN in/IN our/PRP$ experimental/JJ settings/NNS ,/, the/DT latter/JJ is/VBZ computationally/RB less/RBR expensive/JJ ./.
We/PRP also/RB prove/VBP logarithmic/JJ upper/JJ bounds/NNS for/IN the/DT regret/NN of/IN both/DT policies/NNS ,/, and/CC show/VBP that/IN these/DT bounds/NNS are/VBP asymptotically/RB optimal/JJ (/-LRB- i.e./FW they/PRP only/RB differ/VBP from/IN the/DT best/JJS possible/JJ regret/NN by/IN a/DT constant/JJ factor/NN )/-RRB- ./.
