We/PRP present/VBP a/DT novel/JJ family/NN of/IN language/NN model/NN (/-LRB- LM/NN )/-RRB- estimation/NN techniques/NNS named/VBN Sparse/JJ Non-negative/JJ Matrix/NN (/-LRB- SNM/NN )/-RRB- estimation/NN ./.
A/DT first/JJ set/NN of/IN experiments/NNS empirically/RB evaluating/VBG it/PRP on/IN the/DT One/CD Billion/CD Word/NNP Benchmark/NNP shows/VBZ that/IN SNM/NNP $/NN n/NN $/$ -/HYPH gram/NN LMs/NNS perform/VBP almost/RB as/RB well/RB as/IN the/DT well/NN -/HYPH established/VBN Kneser/NN -/HYPH Ney/NN (/-LRB- KN/NN )/-RRB- models/NNS ./.
When/WRB using/VBG skip/VB -/HYPH gram/NN features/VBZ the/DT models/NNS are/VBP able/JJ to/TO match/VB the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- LMs/NNS ;/: combining/VBG the/DT two/CD modeling/NN techniques/NNS yields/VBZ the/DT best/JJS known/JJ result/NN on/IN the/DT benchmark/NN ./.
The/DT computational/JJ advantages/NNS of/IN SNM/NNP over/IN both/DT maximum/JJ entropy/NN and/CC RNN/NN LM/NN estimation/NN are/VBP probably/RB its/PRP$ main/JJ strength/NN ,/, promising/VBG an/DT approach/NN that/WDT has/VBZ the/DT same/JJ flexibility/NN in/IN combining/VBG arbitrary/JJ features/NNS effectively/RB and/CC yet/RB should/MD scale/VB to/IN very/RB large/JJ amounts/NNS of/IN data/NNS as/IN gracefully/RB as/RB $/$ n/NN $/$ -/HYPH gram/NN LMs/NNS do/VBP ./.
