Distributed/VBN dense/JJ word/NN vectors/NNS have/VBP been/VBN shown/VBN to/TO be/VB effective/JJ at/IN capturing/VBG token/JJ -/HYPH level/NN semantic/JJ and/CC syntactic/JJ regularities/NNS in/IN language/NN ,/, while/IN topic/NN models/NNS can/MD form/VB interpretable/JJ representations/NNS over/IN documents/NNS ./.
In/IN this/DT work/NN ,/, we/PRP describe/VBP lda2vec/NN ,/, a/DT model/NN that/WDT learns/VBZ dense/JJ word/NN vectors/NNS jointly/RB with/IN Dirichlet/NNP -/HYPH distributed/VBN latent/JJ document/NN -/HYPH level/NN mixtures/NNS of/IN topic/NN vectors/NNS ./.
In/IN contrast/NN to/IN continuous/JJ dense/JJ document/NN representations/NNS ,/, this/DT formulation/NN produces/VBZ sparse/JJ ,/, interpretable/JJ document/NN mixtures/NNS through/IN a/DT non-negative/JJ simplex/NN constraint/NN ./.
Our/PRP$ method/NN is/VBZ simple/JJ to/TO incorporate/VB into/IN existing/VBG automatic/JJ differentiation/NN frameworks/NNS and/CC allows/VBZ for/IN unsupervised/JJ document/NN representations/NNS geared/VBN for/IN use/NN by/IN scientists/NNS while/IN simultaneously/RB learning/VBG word/NN vectors/NNS and/CC the/DT linear/JJ relationships/NNS between/IN them/PRP ./.
