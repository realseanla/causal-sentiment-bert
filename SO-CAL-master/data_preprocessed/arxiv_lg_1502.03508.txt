Distributed/VBN optimization/NN algorithms/NNS for/IN large/JJ -/HYPH scale/NN machine/NN learning/NN suffer/VBP from/IN a/DT communication/NN bottleneck/NN ./.
Reducing/VBG communication/NN makes/VBZ the/DT efficient/JJ aggregation/NN of/IN partial/JJ work/NN from/IN different/JJ machines/NNS more/RBR challenging/JJ ./.
In/IN this/DT paper/NN we/PRP present/VBP a/DT novel/JJ generalization/NN of/IN the/DT recent/JJ communication/NN efficient/JJ primal/JJ -/HYPH dual/JJ coordinate/NN ascent/NN framework/NN (/-LRB- CoCoA/NN )/-RRB- ./.
Our/PRP$ framework/NN ,/, CoCoA/NN ,/, allows/VBZ for/IN additive/JJ combination/NN of/IN local/JJ updates/NNS to/IN the/DT global/JJ parameters/NNS at/IN each/DT iteration/NN ,/, whereas/IN previous/JJ schemes/NNS only/RB allowed/VBD conservative/JJ averaging/NN ./.
We/PRP give/VBP stronger/JJR (/-LRB- primal/JJ -/HYPH dual/JJ )/-RRB- convergence/NN rate/NN guarantees/NNS for/IN both/DT CoCoA/NN as/RB well/RB as/IN our/PRP$ new/JJ variants/NNS ,/, and/CC generalize/VB the/DT theory/NN for/IN both/DT methods/NNS to/TO also/RB cover/VB non-smooth/JJ convex/NN loss/NN functions/NNS ./.
We/PRP provide/VBP an/DT extensive/JJ experimental/JJ comparison/NN on/IN several/JJ real/JJ -/HYPH world/NN distributed/VBN datasets/NNS ,/, showing/VBG markedly/RB improved/VBN performance/NN ,/, especially/RB when/WRB scaling/VBG up/RP the/DT number/NN of/IN machines/NNS ./.
