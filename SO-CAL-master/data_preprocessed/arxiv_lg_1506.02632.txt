Cumulative/JJ prospect/NN theory/NN (/-LRB- CPT/NN )/-RRB- is/VBZ known/VBN to/TO model/VB human/JJ decisions/NNS well/RB ,/, with/IN substantial/JJ empirical/JJ evidence/NN supporting/VBG this/DT claim/NN ./.
CPT/NN works/NNS by/IN distorting/VBG probabilities/NNS and/CC is/VBZ more/RBR general/JJ than/IN the/DT classic/JJ expected/VBN utility/NN and/CC coherent/JJ risk/NN measures/NNS ./.
We/PRP bring/VBP this/DT idea/NN to/IN a/DT risk/NN -/HYPH sensitive/JJ reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- setting/NN and/CC design/NN algorithms/NNS for/IN both/DT estimation/NN and/CC control/NN ./.
The/DT estimation/NN scheme/NN that/WDT we/PRP propose/VBP uses/VBZ the/DT empirical/JJ distribution/NN in/IN order/NN to/TO estimate/VB the/DT CPT/NN -/HYPH value/NN of/IN a/DT random/JJ variable/NN ./.
We/PRP then/RB use/VB this/DT scheme/NN in/IN the/DT inner/JJ loop/NN of/IN policy/NN optimization/NN procedures/NNS for/IN a/DT Markov/NNP decision/NN process/NN (/-LRB- MDP/NN )/-RRB- ./.
We/PRP propose/VBP both/DT gradient/NN -/HYPH based/VBN as/RB well/RB as/IN gradient/NN -/HYPH free/JJ policy/NN optimization/NN algorithms/NNS ./.
The/DT former/JJ includes/VBZ both/DT first/JJ -/HYPH order/NN and/CC second/JJ -/HYPH order/NN methods/NNS that/WDT are/VBP based/VBN on/IN the/DT well/NN -/HYPH known/VBN simulation/NN optimization/NN idea/NN of/IN simultaneous/JJ perturbation/NN stochastic/JJ approximation/NN (/-LRB- SPSA/NNP )/-RRB- ,/, while/IN the/DT latter/JJ is/VBZ based/VBN on/IN a/DT reference/NN distribution/NN that/WDT concentrates/VBZ on/IN the/DT global/JJ optima/NN ./.
Using/VBG an/DT empirical/JJ distribution/NN over/IN the/DT policy/NN space/NN in/IN conjunction/NN with/IN Kullback/NN -/HYPH Leibler/NN (/-LRB- KL/NN )/-RRB- divergence/NN to/IN the/DT reference/NN distribution/NN ,/, we/PRP get/VBP a/DT global/JJ policy/NN optimization/NN scheme/NN ./.
We/PRP provide/VBP theoretical/JJ convergence/NN guarantees/NNS for/IN all/PDT the/DT proposed/VBN algorithms/NNS ./.
