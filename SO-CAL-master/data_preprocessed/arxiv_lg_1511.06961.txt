In/IN this/DT work/NN ,/, we/PRP leverage/VBP the/DT linear/JJ algebraic/JJ structure/NN of/IN distributed/VBN word/NN representations/NNS to/IN automatically/RB extend/VB knowledge/NN bases/NNS and/CC allow/VB a/DT machine/NN to/TO learn/VB new/JJ facts/NNS about/IN the/DT world/NN ./.
Our/PRP$ goal/NN is/VBZ to/TO extract/VB structured/JJ facts/NNS from/IN corpora/NNS in/IN a/DT simpler/JJR manner/NN ,/, without/IN applying/VBG classifiers/NNS or/CC patterns/NNS ,/, and/CC using/VBG only/RB the/DT co-occurrence/NN statistics/NNS of/IN words/NNS ./.
We/PRP demonstrate/VBP that/IN the/DT linear/JJ algebraic/JJ structure/NN of/IN word/NN embeddings/NNS can/MD be/VB used/VBN to/TO reduce/VB data/NN requirements/NNS for/IN methods/NNS of/IN learning/VBG facts/NNS ./.
In/IN particular/JJ ,/, we/PRP demonstrate/VBP that/IN words/NNS belonging/VBG to/IN a/DT common/JJ category/NN ,/, or/CC pairs/NNS of/IN words/NNS satisfying/VBG a/DT certain/JJ relation/NN ,/, form/VBP a/DT low/JJ -/HYPH rank/NN subspace/NN in/IN the/DT projected/VBN space/NN ./.
We/PRP compute/VBP a/DT basis/NN for/IN this/DT low/JJ -/HYPH rank/NN subspace/NN using/VBG singular/JJ value/NN decomposition/NN (/-LRB- SVD/NN )/-RRB- ,/, then/RB use/VB this/DT basis/NN to/TO discover/VB new/JJ facts/NNS and/CC to/TO fit/VB vectors/NNS for/IN less/JJR frequent/JJ words/NNS which/WDT we/PRP do/VBP not/RB yet/RB have/VB vectors/NNS for/IN ./.
