Word/NNP embeddings/NNS are/VBP now/RB a/DT standard/JJ technique/NN for/IN inducing/VBG meaning/VBG representations/NNS for/IN words/NNS ./.
For/IN getting/VBG good/JJ representations/NNS ,/, it/PRP is/VBZ important/JJ to/TO take/VB into/IN account/NN different/JJ senses/NNS of/IN a/DT word/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT mixture/NN model/NN for/IN learning/VBG multi-sense/JJ word/NN embeddings/NNS ./.
Our/PRP$ model/NN generalizes/VBZ the/DT previous/JJ works/NNS in/IN that/IN it/PRP allows/VBZ to/TO induce/VB different/JJ weights/NNS of/IN different/JJ senses/NNS of/IN a/DT word/NN ./.
The/DT experimental/JJ results/NNS show/VBP that/IN our/PRP$ model/NN outperforms/VBZ previous/JJ models/NNS on/IN standard/JJ evaluation/NN tasks/NNS ./.
