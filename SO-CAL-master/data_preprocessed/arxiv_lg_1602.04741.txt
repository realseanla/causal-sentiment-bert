We/PRP study/VBP networks/NNS of/IN communicating/VBG learning/VBG agents/NNS that/WDT cooperate/VBP to/TO solve/VB a/DT common/JJ nonstochastic/JJ bandit/NN problem/NN ./.
Agents/NNS use/VBP an/DT underlying/JJ communication/NN network/NN to/TO get/VB messages/NNS about/IN actions/NNS selected/VBN by/IN other/JJ agents/NNS ,/, and/CC drop/VB messages/NNS that/WDT took/VBD more/JJR than/IN $/$ d/LS $/$ hops/VBZ to/TO arrive/VB ,/, where/WRB $/$ d/LS $/$ is/VBZ a/DT delay/NN parameter/NN ./.
We/PRP introduce/VBP \/SYM textsc/NN {/-LRB- Exp3/NN -/HYPH Coop/NN }/-RRB- ,/, a/DT cooperative/JJ version/NN of/IN the/DT {/-LRB- \/SYM sc/NN Exp3/NN }/-RRB- algorithm/NN and/CC prove/VB that/IN with/IN $/$ K$/CD actions/NNS and/CC $/$ N$/CD agents/NNS the/DT average/JJ per/IN -/HYPH agent/NN regret/NN after/IN $/$ T$/CD rounds/NNS is/VBZ at/IN most/JJS of/IN order/NN $/$ \/CD sqrt/NN {/-LRB- \/SYM bigl/NN (/-LRB- d/NN 1/CD \/SYM tfrac/NN {/-LRB- K/NN }/-RRB- {/-LRB- N/NN }/-RRB- \/SYM alpha/NN _/NFP {/-LRB- \/SYM le/FW d/NNS }/-RRB- \/SYM bigr/NN )/-RRB- (/-LRB- T/NN \/SYM ln/NN K/NN )/-RRB- }/-RRB- $/$ ,/, where/WRB $/$ \/CD alpha/NN _/NFP {/-LRB- \/SYM le/FW d/NNS }/-RRB- $/$ is/VBZ the/DT independence/NN number/NN of/IN the/DT $/$ d/LS $/$ -/HYPH th/IN power/NN of/IN the/DT connected/JJ communication/NN graph/NN $/$ G$/CD ./.
We/PRP then/RB show/VBP that/IN for/IN any/DT connected/JJ graph/NN ,/, for/IN $/$ d/NN =/SYM \/SYM sqrt/SYM {/-LRB- K/NN }/-RRB- $/$ the/DT regret/NN bound/VBN is/VBZ $/$ K/CD ^/SYM {/-LRB- 1/4/CD }/-RRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- $/$ ,/, strictly/RB better/JJR than/IN the/DT minimax/NN regret/NN $/$ \/CD sqrt/NN {/-LRB- KT/NNP }/-RRB- $/$ for/IN noncooperating/VBG agents/NNS ./.
More/RBR informed/JJ choices/NNS of/IN $/$ d/LS $/$ lead/VB to/IN bounds/NNS which/WDT are/VBP arbitrarily/RB close/JJ to/IN the/DT full/JJ information/NN minimax/NN regret/NN $/$ \/CD sqrt/NN {/-LRB- T/NN \/SYM ln/NN K/NN }/-RRB- $/$ when/WRB $/$ G$/CD is/VBZ dense/JJ ./.
When/WRB $/$ G$/CD has/VBZ sparse/JJ components/NNS ,/, we/PRP show/VBP that/IN a/DT variant/NN of/IN \/SYM textsc/NN {/-LRB- Exp3/NN -/HYPH Coop/NN }/-RRB- ,/, allowing/VBG agents/NNS to/TO choose/VB their/PRP$ parameters/NNS according/VBG to/IN their/PRP$ centrality/NN in/IN $/$ G$/CD ,/, strictly/RB improves/VBZ the/DT regret/NN ./.
Finally/RB ,/, as/IN a/DT by/IN -/HYPH product/NN of/IN our/PRP$ analysis/NN ,/, we/PRP provide/VBP the/DT first/JJ characterization/NN of/IN the/DT minimax/NN regret/NN for/IN bandit/NN learning/NN with/IN delay/NN ./.
