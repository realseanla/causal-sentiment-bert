Parallel/JJ coordinate/NN descent/NN algorithms/NNS emerge/VBP with/IN the/DT growing/VBG demand/NN for/IN large/JJ -/HYPH scale/NN optimization/NN ./.
These/DT algorithms/NNS are/VBP usually/RB limited/VBN by/IN their/PRP$ divergence/NN under/IN high/JJ parallelism/NN or/CC need/VBP data/NNS preprocessing/VBG to/TO avoid/VB divergence/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT parallelized/JJ algorithm/NN ,/, termed/VBN as/IN Parallel/JJ Coordinate/NNP Descent/NNP Newton/NNP (/-LRB- PCDN/NNP )/-RRB- ,/, to/TO pursue/VB more/JJR parallelism/NN ./.
It/PRP randomly/RB partitions/VBZ the/DT feature/NN set/VBN into/IN $/$ b/CD $/$ subsets/NNS //, bundles/NNS with/IN size/NN of/IN $/$ P$/CD ,/, then/RB it/PRP sequentially/RB processes/VBZ each/DT bundle/NN by/IN first/JJ computing/VBG the/DT descent/NN directions/NNS for/IN each/DT feature/NN in/IN the/DT bundle/NN in/IN parallel/JJ and/CC then/RB conducting/VBG $/$ P$/CD -/HYPH dimensional/JJ line/NN search/NN to/TO obtain/VB the/DT stepsize/NN of/IN the/DT bundle/NN ./.
We/PRP will/MD show/VB that/IN :/: (/-LRB- 1/LS )/-RRB- PCDN/NN is/VBZ guaranteed/VBN to/TO converge/VB globally/RB ;/: (/-LRB- 2/LS )/-RRB- PCDN/NN can/MD converge/VB to/IN the/DT specified/VBN accuracy/NN $/$ \/CD epsilon/CD $/$ within/IN the/DT limited/JJ iteration/NN number/NN of/IN $/$ T/NN _/NFP \/SYM epsilon/NN $/$ ,/, and/CC the/DT iteration/NN number/NN $/$ T/NN _/NFP \/SYM epsilon/NN $/$ decreases/VBZ along/IN with/IN the/DT increasing/VBG of/IN parallelism/NN (/-LRB- bundle/NN size/NN $/$ P$/CD )/-RRB- ./.
PCDN/NNP is/VBZ applied/VBN to/IN large/JJ -/HYPH scale/NN $/$ L_1/CD $/$ -/HYPH regularized/VBN logistic/JJ regression/NN and/CC $/$ L_2/CD $/$ -/HYPH loss/NN SVM/NN ./.
Experimental/JJ evaluations/NNS over/IN five/CD public/JJ datasets/NNS indicate/VBP that/IN PCDN/NNP can/MD better/RBR exploit/VB parallelism/NN and/CC outperforms/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN algorithms/NNS in/IN speed/NN ,/, without/IN losing/VBG test/NN accuracy/NN ./.
