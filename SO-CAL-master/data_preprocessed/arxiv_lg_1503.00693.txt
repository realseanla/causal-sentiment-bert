When/WRB applying/VBG machine/NN learning/NN to/IN problems/NNS in/IN NLP/NN ,/, there/EX are/VBP many/JJ choices/NNS to/TO make/VB about/IN how/WRB to/TO represent/VB input/NN texts/NNS ./.
These/DT choices/NNS can/MD have/VB a/DT big/JJ effect/NN on/IN performance/NN ,/, but/CC they/PRP are/VBP often/RB uninteresting/JJ to/IN researchers/NNS or/CC practitioners/NNS who/WP simply/RB need/VBP a/DT module/NN that/WDT performs/VBZ well/RB ./.
We/PRP propose/VBP an/DT approach/NN to/IN optimizing/VBG over/IN this/DT space/NN of/IN choices/NNS ,/, formulating/VBG the/DT problem/NN as/IN global/JJ optimization/NN ./.
We/PRP apply/VBP a/DT sequential/JJ model/NN -/HYPH based/VBN optimization/NN technique/NN and/CC show/VBP that/IN our/PRP$ method/NN makes/VBZ standard/JJ linear/JJ models/NNS competitive/JJ with/IN more/JJR sophisticated/JJ ,/, expensive/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS based/VBN on/IN latent/JJ variable/JJ models/NNS or/CC neural/JJ networks/NNS on/IN various/JJ topic/NN classification/NN and/CC sentiment/NN analysis/NN problems/NNS ./.
Our/PRP$ approach/NN is/VBZ a/DT first/JJ step/NN towards/IN black/JJ -/HYPH box/NN NLP/NN systems/NNS that/WDT work/VBP with/IN raw/JJ text/NN and/CC do/VBP not/RB require/VB manual/JJ tuning/NN ./.
