We/PRP capitalize/VBP on/IN large/JJ amounts/NNS of/IN unlabeled/JJ video/NN in/IN order/NN to/TO learn/VB a/DT model/NN of/IN scene/NN dynamics/NNS for/IN both/DT video/NN recognition/NN tasks/NNS (/-LRB- e.g./FW action/NN classification/NN )/-RRB- and/CC video/NN generation/NN tasks/NNS (/-LRB- e.g./FW future/NN prediction/NN )/-RRB- ./.
We/PRP propose/VBP a/DT generative/JJ adversarial/JJ network/NN for/IN video/NN with/IN a/DT spatio/JJ -/HYPH temporal/JJ convolutional/JJ architecture/NN that/WDT untangles/VBZ the/DT scene/NN 's/POS foreground/NN from/IN the/DT background/NN ./.
Experiments/NNS suggest/VBP this/DT model/NN can/MD generate/VB tiny/JJ videos/NNS up/IN to/IN a/DT second/JJ at/IN full/JJ frame/NN rate/NN better/JJR than/IN simple/JJ baselines/NNS ,/, and/CC we/PRP show/VBP its/PRP$ utility/NN at/IN predicting/VBG plausible/JJ futures/NNS of/IN static/JJ images/NNS ./.
Moreover/RB ,/, experiments/NNS and/CC visualizations/NNS show/VBP the/DT model/NN internally/RB learns/VBZ useful/JJ features/NNS for/IN recognizing/VBG actions/NNS with/IN minimal/JJ supervision/NN ,/, suggesting/VBG scene/NN dynamics/NNS are/VBP a/DT promising/JJ signal/NN for/IN representation/NN learning/NN ./.
We/PRP believe/VBP generative/JJ video/NN models/NNS can/MD impact/VB many/JJ applications/NNS in/IN video/NN understanding/NN and/CC simulation/NN ./.
