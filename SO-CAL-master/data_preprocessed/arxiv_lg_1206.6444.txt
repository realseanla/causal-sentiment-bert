Motivated/VBN by/IN value/NN function/NN estimation/NN in/IN reinforcement/NN learning/NN ,/, we/PRP study/VBP statistical/JJ linear/JJ inverse/NN problems/NNS ,/, i.e./FW ,/, problems/NNS where/WRB the/DT coefficients/NNS of/IN a/DT linear/JJ system/NN to/TO be/VB solved/VBN are/VBP observed/VBN in/IN noise/NN ./.
We/PRP consider/VBP penalized/VBN estimators/NNS ,/, where/WRB performance/NN is/VBZ evaluated/VBN using/VBG a/DT matrix/NN -/HYPH weighted/JJ two/CD -/HYPH norm/NN of/IN the/DT defect/NN of/IN the/DT estimator/NN measured/VBN with/IN respect/NN to/IN the/DT true/JJ ,/, unknown/JJ coefficients/NNS ./.
Two/CD objective/JJ functions/NNS are/VBP considered/VBN depending/VBG whether/IN the/DT error/NN of/IN the/DT defect/NN measured/VBN with/IN respect/NN to/IN the/DT noisy/JJ coefficients/NNS is/VBZ squared/VBN or/CC unsquared/VBN ./.
We/PRP propose/VBP simple/JJ ,/, yet/CC novel/JJ and/CC theoretically/RB well/RB -/HYPH founded/VBN data/NN -/HYPH dependent/JJ choices/NNS for/IN the/DT regularization/NN parameters/NNS for/IN both/DT cases/NNS that/WDT avoid/VBP data/NN -/HYPH splitting/NN ./.
A/DT distinguishing/JJ feature/NN of/IN our/PRP$ analysis/NN is/VBZ that/IN we/PRP derive/VBP deterministic/JJ error/NN bounds/NNS in/IN terms/NNS of/IN the/DT error/NN of/IN the/DT coefficients/NNS ,/, thus/RB allowing/VBG the/DT complete/JJ separation/NN of/IN the/DT analysis/NN of/IN the/DT stochastic/JJ properties/NNS of/IN these/DT errors/NNS ./.
We/PRP show/VBP that/IN our/PRP$ results/NNS lead/VBP to/IN new/JJ insights/NNS and/CC bounds/NNS for/IN linear/JJ value/NN function/NN estimation/NN in/IN reinforcement/NN learning/NN ./.
