We/PRP consider/VBP the/DT problem/NN of/IN rank/NN loss/NN minimization/NN in/IN the/DT setting/NN of/IN multilabel/NN classification/NN ,/, which/WDT is/VBZ usually/RB tackled/VBN by/IN means/NNS of/IN convex/NN surrogate/JJ losses/NNS defined/VBN on/IN pairs/NNS of/IN labels/NNS ./.
Very/RB recently/RB ,/, this/DT approach/NN was/VBD put/VBN into/IN question/NN by/IN a/DT negative/JJ result/NN showing/VBG that/IN commonly/RB used/VBN pairwise/JJ surrogate/JJ losses/NNS ,/, such/JJ as/IN exponential/JJ and/CC logistic/JJ losses/NNS ,/, are/VBP inconsistent/JJ ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP a/DT positive/JJ result/NN which/WDT is/VBZ arguably/RB surprising/JJ in/IN light/NN of/IN the/DT previous/JJ one/CD :/: the/DT simpler/JJR univariate/JJ variants/NNS of/IN exponential/JJ and/CC logistic/JJ surrogates/NNS (/-LRB- i.e./FW ,/, defined/VBN on/IN single/JJ labels/NNS )/-RRB- are/VBP consistent/JJ for/IN rank/NN loss/NN minimization/NN ./.
Instead/RB of/IN directly/RB proving/VBG convergence/NN ,/, we/PRP give/VBP a/DT much/RB stronger/JJR result/NN by/IN deriving/VBG regret/NN bounds/NNS and/CC convergence/NN rates/NNS ./.
The/DT proposed/JJ losses/NNS suggest/VBP efficient/JJ and/CC scalable/JJ algorithms/NNS ,/, which/WDT are/VBP tested/VBN experimentally/RB ./.
