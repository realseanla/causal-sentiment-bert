We/PRP consider/VBP online/JJ learning/NN when/WRB the/DT time/NN horizon/NN is/VBZ unknown/JJ ./.
We/PRP apply/VBP a/DT minimax/NN analysis/NN ,/, beginning/VBG with/IN the/DT fixed/VBN horizon/NN case/NN ,/, and/CC then/RB moving/VBG on/IN to/IN two/CD unknown/JJ -/HYPH horizon/NN settings/NNS ,/, one/CD that/WDT assumes/VBZ the/DT horizon/NN is/VBZ chosen/VBN randomly/RB according/VBG to/IN some/DT known/JJ distribution/NN ,/, and/CC the/DT other/JJ which/WDT allows/VBZ the/DT adversary/NN full/JJ control/NN over/IN the/DT horizon/NN ./.
For/IN the/DT random/JJ horizon/NN setting/VBG with/IN restricted/JJ losses/NNS ,/, we/PRP derive/VBP a/DT fully/RB optimal/JJ minimax/NN algorithm/NN ./.
And/CC for/IN the/DT adversarial/JJ horizon/NN setting/NN ,/, we/PRP prove/VBP a/DT nontrivial/JJ lower/JJR bound/JJ which/WDT shows/VBZ that/IN the/DT adversary/NN obtains/VBZ strictly/RB more/JJR power/NN than/IN when/WRB the/DT horizon/NN is/VBZ fixed/VBN and/CC known/VBN ./.
Based/VBN on/IN the/DT minimax/NN solution/NN of/IN the/DT random/JJ horizon/NN setting/NN ,/, we/PRP then/RB propose/VB a/DT new/JJ adaptive/JJ algorithm/NN which/WDT "/`` pretends/VBZ "/'' that/IN the/DT horizon/NN is/VBZ drawn/VBN from/IN a/DT distribution/NN from/IN a/DT special/JJ family/NN ,/, but/CC no/RB matter/RB how/WRB the/DT actual/JJ horizon/NN is/VBZ chosen/VBN ,/, the/DT worst/JJS -/HYPH case/NN regret/NN is/VBZ of/IN the/DT optimal/JJ rate/NN ./.
Furthermore/RB ,/, our/PRP$ algorithm/NN can/MD be/VB generalized/VBN in/IN many/JJ ways/NNS ,/, including/VBG handling/VBG other/RB unknown/JJ information/NN and/CC other/JJ online/JJ learning/NN settings/NNS ./.
Experiments/NNS show/VBP that/IN our/PRP$ algorithm/NN outperforms/VBZ many/JJ other/JJ existing/VBG algorithms/NNS in/IN an/DT online/JJ linear/JJ optimization/NN setting/NN ./.
