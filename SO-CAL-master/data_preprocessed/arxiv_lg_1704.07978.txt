Deep/JJ Reinforcement/NN Learning/NN (/-LRB- RL/NN )/-RRB- recently/RB emerged/VBD as/IN one/CD of/IN the/DT most/RBS competitive/JJ approaches/NNS for/IN learning/VBG in/IN sequential/JJ decision/NN making/VBG problems/NNS with/IN fully/RB observable/JJ environments/NNS ,/, e.g./FW ,/, computer/NN Go/VB ./.
However/RB ,/, very/RB little/JJ work/NN has/VBZ been/VBN done/VBN in/IN deep/JJ RL/NN to/TO handle/VB partially/RB observable/JJ environments/NNS ./.
We/PRP propose/VBP a/DT new/JJ architecture/NN called/VBN Action/NNP -/HYPH specific/JJ Deep/JJ Recurrent/JJ Q/NN -/HYPH Network/NN (/-LRB- ADRQN/NN )/-RRB- to/TO enhance/VB learning/NN performance/NN in/IN partially/RB observable/JJ domains/NNS ./.
Actions/NNS are/VBP encoded/VBN by/IN a/DT fully/RB connected/VBN layer/NN and/CC coupled/VBN with/IN a/DT convolutional/JJ observation/NN to/IN form/VB an/DT action/NN -/HYPH observation/NN pair/NN ./.
The/DT time/NN series/NN of/IN action/NN -/HYPH observation/NN pairs/NNS are/VBP then/RB integrated/VBN by/IN an/DT LSTM/NN layer/NN that/WDT learns/VBZ latent/JJ states/NNS based/VBN on/IN which/WDT a/DT fully/RB connected/VBN layer/NN computes/VBZ Q/NN -/HYPH values/NNS as/IN in/IN conventional/JJ Deep/JJ Q/NN -/HYPH Networks/NNS (/-LRB- DQNs/NNS )/-RRB- ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ new/JJ architecture/NN in/IN several/JJ partially/RB observable/JJ domains/NNS ,/, including/VBG flickering/VBG Atari/NNP games/NNS ./.
