Policies/NNS for/IN complex/JJ visual/JJ tasks/NNS have/VBP been/VBN successfully/RB learned/VBN with/IN deep/JJ reinforcement/NN learning/NN ,/, using/VBG an/DT approach/NN called/VBD deep/JJ Q/NN -/HYPH networks/NNS (/-LRB- DQN/NN )/-RRB- ,/, but/CC relatively/RB large/JJ (/-LRB- task/NN -/HYPH specific/JJ )/-RRB- networks/NNS and/CC extensive/JJ training/NN are/VBP needed/VBN to/TO achieve/VB good/JJ performance/NN ./.
In/IN this/DT work/NN ,/, we/PRP present/VBP a/DT novel/JJ method/NN called/VBN policy/NN distillation/NN that/WDT can/MD be/VB used/VBN to/TO extract/VB the/DT policy/NN of/IN a/DT reinforcement/NN learning/VBG agent/NN and/CC train/VB a/DT new/JJ network/NN that/WDT performs/VBZ at/IN the/DT expert/NN level/NN while/IN being/VBG dramatically/RB smaller/JJR and/CC more/RBR efficient/JJ ./.
Furthermore/RB ,/, the/DT same/JJ method/NN can/MD be/VB used/VBN to/TO consolidate/VB multiple/JJ task/NN -/HYPH specific/JJ policies/NNS into/IN a/DT single/JJ policy/NN ./.
We/PRP demonstrate/VBP these/DT claims/NNS using/VBG the/DT Atari/NNP domain/NN and/CC show/VBP that/IN the/DT multi-task/VB distilled/JJ agent/NN outperforms/VBZ the/DT single/JJ -/HYPH task/NN teachers/NNS as/RB well/RB as/IN a/DT jointly/RB -/HYPH trained/VBN DQN/NNP agent/NN ./.
