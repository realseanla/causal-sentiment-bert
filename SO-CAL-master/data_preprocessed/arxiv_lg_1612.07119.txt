Research/NNP has/VBZ shown/VBN that/IN convolutional/JJ neural/JJ networks/NNS contain/VBP significant/JJ redundancy/NN ,/, and/CC high/JJ classification/NN accuracy/NN can/MD be/VB obtained/VBN even/RB when/WRB weights/NNS and/CC activations/NNS are/VBP reduced/VBN from/IN floating/VBG point/NN to/IN binary/JJ values/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP FINN/NNP ,/, a/DT framework/NN for/IN building/VBG fast/RB and/CC flexible/JJ FPGA/NNP accelerators/NNS using/VBG a/DT flexible/JJ heterogeneous/JJ streaming/NN architecture/NN ./.
By/IN utilizing/VBG a/DT novel/JJ set/NN of/IN optimizations/NNS that/WDT enable/VBP efficient/JJ mapping/NN of/IN binarized/VBN neural/JJ networks/NNS to/IN hardware/NN ,/, we/PRP implement/VBP fully/RB connected/VBN ,/, convolutional/JJ and/CC pooling/VBG layers/NNS ,/, with/IN per/IN -/HYPH layer/NN compute/VB resources/NNS being/VBG tailored/VBN to/IN user/NN -/HYPH provided/VBN throughput/NN requirements/NNS ./.
On/IN a/DT ZC706/NN embedded/VBN FPGA/NNP platform/NN drawing/VBG less/JJR than/IN 25/CD W/NNP total/JJ system/NN power/NN ,/, we/PRP demonstrate/VBP up/IN to/IN 12.3/CD million/CD image/NN classifications/NNS per/IN second/NN with/IN 0.31/CD {/-LRB- \/SYM mu/NNS }/-RRB- s/POS latency/NN on/IN the/DT MNIST/NNP dataset/NN with/IN 95.8/CD percent/NN accuracy/NN ,/, and/CC 21906/CD image/NN classifications/NNS per/IN second/NN with/IN 283/CD {/-LRB- \/SYM mu/NNS }/-RRB- s/POS latency/NN on/IN the/DT CIFAR/NN -/HYPH 10/CD and/CC SVHN/NN datasets/NNS with/IN respectively/RB 80.1/CD percent/NN and/CC 94.9/CD percent/NN accuracy/NN ./.
To/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, ours/PRP are/VBP the/DT fastest/JJS classification/NN rates/NNS reported/VBD to/IN date/NN on/IN these/DT benchmarks/NNS ./.
