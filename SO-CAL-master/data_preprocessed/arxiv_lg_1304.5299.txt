Can/MD we/PRP make/VB Bayesian/JJ posterior/JJ MCMC/NN sampling/NN more/RBR efficient/JJ when/WRB faced/VBN with/IN very/RB large/JJ datasets/NNS ?/.
We/PRP argue/VBP that/IN computing/VBG the/DT likelihood/NN for/IN N/NN datapoints/NNS twice/RB in/IN order/NN to/TO reach/VB a/DT single/JJ binary/JJ decision/NN is/VBZ computationally/RB inefficient/JJ ./.
We/PRP introduce/VBP an/DT approximate/JJ Metropolis/NNP -/HYPH Hastings/NNP rule/NN based/VBN on/IN a/DT sequential/JJ hypothesis/NN test/NN which/WDT allows/VBZ us/PRP to/TO accept/VB or/CC reject/VB samples/NNS with/IN high/JJ confidence/NN using/VBG only/RB a/DT fraction/NN of/IN the/DT data/NNS required/VBN for/IN the/DT exact/JJ MH/NNP rule/NN ./.
While/IN this/DT introduces/VBZ an/DT asymptotic/JJ bias/NN ,/, we/PRP show/VBP that/IN this/DT bias/NN can/MD be/VB controlled/VBN and/CC is/VBZ more/JJR than/IN offset/VBN by/IN a/DT decrease/NN in/IN variance/NN due/IN to/IN our/PRP$ ability/NN to/TO draw/VB more/JJR samples/NNS per/IN unit/NN of/IN time/NN ./.
We/PRP show/VBP that/IN the/DT same/JJ idea/NN can/MD also/RB be/VB applied/VBN to/IN Gibbs/NNP sampling/NN in/IN densely/RB connected/VBN graphs/NNS ./.
