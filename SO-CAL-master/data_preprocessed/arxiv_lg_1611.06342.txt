The/DT complexity/NN of/IN deep/JJ neural/JJ network/NN algorithms/NNS for/IN hardware/NN implementation/NN can/MD be/VB lowered/VBN either/RB by/IN scaling/VBG the/DT number/NN of/IN units/NNS or/CC reducing/VBG the/DT word/NN -/HYPH length/NN of/IN weights/NNS ./.
Both/DT approaches/NNS ,/, however/RB ,/, can/MD accompany/VB the/DT performance/NN degradation/NN although/IN many/JJ types/NNS of/IN research/NN are/VBP conducted/VBN to/TO relieve/VB this/DT problem/NN ./.
Thus/RB ,/, it/PRP is/VBZ an/DT important/JJ question/NN which/WDT one/CD ,/, between/IN the/DT network/NN size/NN scaling/NN and/CC the/DT weight/NN quantization/NN ,/, is/VBZ more/RBR effective/JJ for/IN hardware/NN optimization/NN ./.
For/IN this/DT study/NN ,/, the/DT performances/NNS of/IN fully/RB -/HYPH connected/VBN deep/JJ neural/JJ networks/NNS (/-LRB- FCDNNs/NNS )/-RRB- and/CC convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- are/VBP evaluated/VBN while/IN changing/VBG the/DT network/NN complexity/NN and/CC the/DT word/NN -/HYPH length/NN of/IN weights/NNS ./.
Based/VBN on/IN these/DT experiments/NNS ,/, we/PRP present/VBP the/DT effective/JJ compression/NN ratio/NN (/-LRB- ECR/NN )/-RRB- to/TO guide/VB the/DT trade/NN -/HYPH off/NN between/IN the/DT network/NN size/NN and/CC the/DT precision/NN of/IN weights/NNS when/WRB the/DT hardware/NN resource/NN is/VBZ limited/VBN ./.
