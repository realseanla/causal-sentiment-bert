Projected/VBN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- is/VBZ often/RB the/DT default/NN choice/NN for/IN large/JJ -/HYPH scale/NN optimization/NN in/IN machine/NN learning/NN ,/, but/CC requires/VBZ a/DT projection/NN after/IN each/DT update/NN ./.
For/IN heavily/RB -/HYPH constrained/VBN objectives/NNS ,/, we/PRP propose/VBP an/DT efficient/JJ extension/NN of/IN SGD/NNP that/WDT stays/VBZ close/JJ to/IN the/DT feasible/JJ region/NN while/IN only/RB applying/VBG constraints/NNS probabilistically/RB at/IN each/DT iteration/NN ./.
Theoretical/JJ analysis/NN shows/VBZ a/DT good/JJ trade/NN -/HYPH off/NN between/IN per/IN -/HYPH iteration/NN work/NN and/CC the/DT number/NN of/IN iterations/NNS needed/VBN ,/, indicating/VBG compelling/JJ advantages/NNS on/IN problems/NNS with/IN a/DT large/JJ number/NN of/IN constraints/NNS onto/IN which/WDT projecting/VBG is/VBZ expensive/JJ ./.
In/IN MATLAB/NNP experiments/NNS ,/, our/PRP$ algorithm/NN successfully/RB handles/VBZ a/DT large/JJ -/HYPH scale/NN real/JJ -/HYPH world/NN video/NN ranking/VBG problem/NN with/IN tens/NNS of/IN thousands/NNS of/IN linear/JJ inequality/NN constraints/NNS that/WDT was/VBD too/RB large/JJ for/IN projected/VBN SGD/NNP and/CC stochastic/JJ Frank/NNP -/HYPH Wolfe/NNP ./.
