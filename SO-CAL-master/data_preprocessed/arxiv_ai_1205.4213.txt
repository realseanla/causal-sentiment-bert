We/PRP propose/VBP Coactive/NNP Learning/NNP as/IN a/DT model/NN of/IN interaction/NN between/IN a/DT learning/NN system/NN and/CC a/DT human/JJ user/NN ,/, where/WRB both/DT have/VBP the/DT common/JJ goal/NN of/IN providing/VBG results/NNS of/IN maximum/JJ utility/NN to/IN the/DT user/NN ./.
At/IN each/DT step/NN ,/, the/DT system/NN (/-LRB- e.g./FW search/NN engine/NN )/-RRB- receives/VBZ a/DT context/NN (/-LRB- e.g./FW query/NN )/-RRB- and/CC predicts/VBZ an/DT object/NN (/-LRB- e.g./FW ranking/NN )/-RRB- ./.
The/DT user/NN responds/VBZ by/IN correcting/VBG the/DT system/NN if/IN necessary/JJ ,/, providing/VBG a/DT slightly/RB improved/VBN --/: but/CC not/RB necessarily/RB optimal/JJ --/: object/NN as/IN feedback/NN ./.
We/PRP argue/VBP that/IN such/JJ feedback/NN can/MD often/RB be/VB inferred/VBN from/IN observable/JJ user/NN behavior/NN ,/, for/IN example/NN ,/, from/IN clicks/VBZ in/IN web/NN -/HYPH search/NN ./.
Evaluating/VBG predictions/NNS by/IN their/PRP$ cardinal/JJ utility/NN to/IN the/DT user/NN ,/, we/PRP propose/VBP efficient/JJ learning/NN algorithms/NNS that/WDT have/VBP $/$ {/-LRB- \/SYM cal/NN O/NN }/-RRB- (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- }/-RRB- )/-RRB- $/$ average/JJ regret/NN ,/, even/RB though/IN the/DT learning/NN algorithm/NN never/RB observes/VBZ cardinal/JJ utility/NN values/NNS as/IN in/IN conventional/JJ online/JJ learning/NN ./.
We/PRP demonstrate/VBP the/DT applicability/NN of/IN our/PRP$ model/NN and/CC learning/NN algorithms/NNS on/IN a/DT movie/NN recommendation/NN task/NN ,/, as/RB well/RB as/IN ranking/VBG for/IN web/NN -/HYPH search/NN ./.
