Online/JJ model/NN -/HYPH free/JJ reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- methods/NNS with/IN continuous/JJ actions/NNS are/VBP playing/VBG a/DT prominent/JJ role/NN when/WRB dealing/VBG with/IN real/JJ -/HYPH world/NN applications/NNS such/JJ as/IN Robotics/NNP ./.
However/RB ,/, when/WRB confronted/VBN to/IN non-stationary/JJ environments/NNS ,/, these/DT methods/NNS crucially/RB rely/VBP on/IN an/DT exploration/NN -/HYPH exploitation/NN trade/NN -/HYPH off/NN which/WDT is/VBZ rarely/RB dynamically/RB and/CC automatically/RB adjusted/VBN to/IN changes/NNS in/IN the/DT environment/NN ./.
Here/RB we/PRP propose/VBP an/DT active/JJ exploration/NN algorithm/NN for/IN RL/NN in/IN structured/JJ (/-LRB- parameterized/JJ )/-RRB- continuous/JJ action/NN space/NN ./.
This/DT framework/NN deals/NNS with/IN a/DT set/NN of/IN discrete/JJ actions/NNS ,/, each/DT of/IN which/WDT is/VBZ parameterized/VBN with/IN continuous/JJ variables/NNS ./.
Discrete/JJ exploration/NN is/VBZ controlled/VBN through/IN a/DT Boltzmann/NNP softmax/NN function/NN with/IN an/DT inverse/JJ temperature/NN $/$ \/SYM beta/NN $/$ parameter/NN ./.
In/IN parallel/JJ ,/, a/DT Gaussian/JJ exploration/NN is/VBZ applied/VBN to/IN the/DT continuous/JJ action/NN parameters/NNS ./.
We/PRP apply/VBP a/DT meta/NN -/HYPH learning/VBG algorithm/NN based/VBN on/IN the/DT comparison/NN between/IN variations/NNS of/IN short/JJ -/HYPH term/NN and/CC long/JJ -/HYPH term/NN reward/NN running/VBG averages/NNS to/TO simultaneously/RB tune/VB $/$ \/SYM beta/NN $/$ and/CC the/DT width/NN of/IN the/DT Gaussian/JJ distribution/NN from/IN which/WDT continuous/JJ action/NN parameters/NNS are/VBP drawn/VBN ./.
When/WRB applied/VBN to/IN a/DT simple/JJ virtual/JJ human/JJ -/HYPH robot/NN interaction/NN task/NN ,/, we/PRP show/VBP that/IN this/DT algorithm/NN outperforms/VBZ continuous/JJ parameterized/JJ RL/NN both/CC without/IN active/JJ exploration/NN and/CC with/IN active/JJ exploration/NN based/VBN on/IN uncertainty/NN variations/NNS measured/VBN by/IN a/DT Kalman/NNP -/HYPH Q/NNP -/HYPH learning/VBG algorithm/NN ./.
