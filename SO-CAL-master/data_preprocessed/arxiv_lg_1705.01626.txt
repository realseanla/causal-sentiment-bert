Popular/NNP deep/RB learning/VBG frameworks/NNS require/VBP users/NNS to/IN fine/JJ -/HYPH tune/NN their/PRP$ memory/NN usage/NN so/IN that/IN the/DT training/NN data/NNS of/IN a/DT deep/JJ neural/JJ network/NN (/-LRB- DNN/NN )/-RRB- fits/VBZ within/IN the/DT GPU/NNP physical/JJ memory/NN ./.
Prior/JJ work/NN tries/VBZ to/TO address/VB this/DT restriction/NN by/IN virtualizing/VBG the/DT memory/NN usage/NN of/IN DNNs/NNS ,/, enabling/VBG both/DT CPU/NN and/CC GPU/NNP memory/NN to/TO be/VB utilized/VBN for/IN memory/NN allocations/NNS ./.
Despite/IN its/PRP$ merits/NNS ,/, virtualizing/VBG memory/NN can/MD incur/VB significant/JJ performance/NN overheads/NNS when/WRB the/DT time/NN needed/VBN to/TO copy/VB data/NNS back/RB and/CC forth/RB from/IN CPU/NN memory/NN is/VBZ higher/JJR than/IN the/DT latency/NN to/TO perform/VB the/DT computations/NNS required/VBN for/IN DNN/NNP forward/RB and/CC backward/RB propagation/NN ./.
We/PRP introduce/VBP a/DT high/JJ -/HYPH performance/NN virtualization/NN strategy/NN based/VBN on/IN a/DT "/`` compressing/VBG DMA/NN engine/NN "/'' (/-LRB- cDMA/NN )/-RRB- that/WDT drastically/RB reduces/VBZ the/DT size/NN of/IN the/DT data/NNS structures/NNS that/WDT are/VBP targeted/VBN for/IN CPU/NN -/HYPH side/NN allocations/NNS ./.
The/DT cDMA/NN engine/NN offers/VBZ an/DT average/JJ 2.6/CD x/SYM (/-LRB- maximum/NN 13.8/CD x/LS )/-RRB- compression/NN ratio/NN by/IN exploiting/VBG the/DT sparsity/NN inherent/JJ in/IN offloaded/VBN data/NNS ,/, improving/VBG the/DT performance/NN of/IN virtualized/VBN DNNs/NNS by/IN an/DT average/JJ 32/CD percent/NN (/-LRB- maximum/JJ 61/CD percent/NN )/-RRB- ./.
