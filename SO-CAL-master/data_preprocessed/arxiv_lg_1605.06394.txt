In/IN this/DT paper/NN ,/, we/PRP bridge/VBP the/DT gap/NN between/IN hyperparameter/NN optimization/NN and/CC ensemble/NN learning/NN by/IN performing/VBG Bayesian/JJ optimization/NN of/IN an/DT ensemble/NN with/IN regards/NNS to/IN its/PRP$ hyperparameters/NNS ./.
Our/PRP$ method/NN consists/VBZ in/IN building/VBG a/DT fixed/VBN -/HYPH size/NN ensemble/NN ,/, optimizing/VBG the/DT configuration/NN of/IN one/CD classifier/NN of/IN the/DT ensemble/NN at/IN each/DT iteration/NN of/IN the/DT hyperparameter/NN optimization/NN algorithm/NN ,/, taking/VBG into/IN consideration/NN the/DT interaction/NN with/IN the/DT other/JJ models/NNS when/WRB evaluating/VBG potential/JJ performances/NNS ./.
We/PRP also/RB consider/VBP the/DT case/NN where/WRB the/DT ensemble/NN is/VBZ to/TO be/VB reconstructed/VBN at/IN the/DT end/NN of/IN the/DT hyperparameter/NN optimization/NN phase/NN ,/, through/IN a/DT greedy/JJ selection/NN over/IN the/DT pool/NN of/IN models/NNS generated/VBN during/IN the/DT optimization/NN ./.
We/PRP study/VBP the/DT performance/NN of/IN our/PRP$ proposed/JJ method/NN on/IN three/CD different/JJ hyperparameter/NN spaces/NNS ,/, showing/VBG that/IN our/PRP$ approach/NN is/VBZ better/JJR than/IN both/CC the/DT best/JJS single/JJ model/NN and/CC a/DT greedy/JJ ensemble/NN construction/NN over/IN the/DT models/NNS produced/VBN by/IN a/DT standard/JJ Bayesian/JJ optimization/NN ./.
