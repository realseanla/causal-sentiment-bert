Deep/JJ dynamic/JJ generative/JJ models/NNS are/VBP developed/VBN to/TO learn/VB sequential/JJ dependencies/NNS in/IN time/NN -/HYPH series/NN data/NNS ./.
The/DT multi-layered/JJ model/NN is/VBZ designed/VBN by/IN constructing/VBG a/DT hierarchy/NN of/IN temporal/JJ sigmoid/NN belief/NN networks/NNS (/-LRB- TSBNs/NNS )/-RRB- ,/, defined/VBN as/IN a/DT sequential/JJ stack/NN of/IN sigmoid/NN belief/NN networks/NNS (/-LRB- SBNs/NNS )/-RRB- ./.
Each/DT SBN/NN has/VBZ a/DT contextual/JJ hidden/JJ state/NN ,/, inherited/VBN from/IN the/DT previous/JJ SBNs/NNS in/IN the/DT sequence/NN ,/, and/CC is/VBZ used/VBN to/TO regulate/VB its/PRP$ hidden/JJ bias/NN ./.
Scalable/JJ learning/NN and/CC inference/NN algorithms/NNS are/VBP derived/VBN by/IN introducing/VBG a/DT recognition/NN model/NN that/WDT yields/VBZ fast/JJ sampling/NN from/IN the/DT variational/JJ posterior/JJ ./.
This/DT recognition/NN model/NN is/VBZ trained/VBN jointly/RB with/IN the/DT generative/JJ model/NN ,/, by/IN maximizing/VBG its/PRP$ variational/JJ lower/JJR bound/VBN on/IN the/DT log/NN -/HYPH likelihood/NN ./.
Experimental/JJ results/NNS on/IN bouncing/VBG balls/NNS ,/, polyphonic/JJ music/NN ,/, motion/NN capture/NN ,/, and/CC text/NN streams/NNS show/VBP that/IN the/DT proposed/VBN approach/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN predictive/JJ performance/NN ,/, and/CC has/VBZ the/DT capacity/NN to/TO synthesize/VB various/JJ sequences/NNS ./.
