This/DT paper/NN investigates/VBZ the/DT scaling/NN properties/NNS of/IN Recurrent/JJ Neural/JJ Network/NNP Language/NNP Models/NNPS (/-LRB- RNNLMs/NNS )/-RRB- ./.
We/PRP discuss/VBP how/WRB to/TO train/VB very/RB large/JJ RNNs/NNS on/IN GPUs/NNS and/CC address/VB the/DT questions/NNS of/IN how/WRB RNNLMs/NNPS scale/NN with/IN respect/NN to/IN model/NN size/NN ,/, training/NN -/HYPH set/VBN size/NN ,/, computational/JJ costs/NNS and/CC memory/NN ./.
Our/PRP$ analysis/NN shows/VBZ that/IN despite/IN being/VBG more/RBR costly/JJ to/TO train/VB ,/, RNNLMs/NNPS obtain/VBP much/RB lower/JJR perplexities/NNS on/IN standard/JJ benchmarks/NNS than/IN n/NN -/HYPH gram/NN models/NNS ./.
We/PRP train/VBP the/DT largest/JJS known/VBN RNNs/NNS and/CC present/JJ relative/JJ word/NN error/NN rates/NNS gains/NNS of/IN 18/CD percent/NN on/IN an/DT ASR/NNP task/NN ./.
We/PRP also/RB present/VBP the/DT new/JJ lowest/JJS perplexities/NNS on/IN the/DT recently/RB released/VBN billion/CD word/NN language/NN modelling/NN benchmark/NN ,/, 1/CD BLEU/NN point/NN gain/NN on/IN machine/NN translation/NN and/CC a/DT 17/CD percent/NN relative/JJ hit/NN rate/NN gain/NN in/IN word/NN prediction/NN ./.
