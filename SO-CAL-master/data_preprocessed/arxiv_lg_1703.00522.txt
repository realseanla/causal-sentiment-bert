When/WRB training/VBG neural/JJ networks/NNS ,/, the/DT use/NN of/IN Synthetic/JJ Gradients/NNS (/-LRB- SG/NN )/-RRB- allows/VBZ layers/NNS or/CC modules/NNS to/TO be/VB trained/VBN without/IN update/NN locking/NN -/HYPH without/IN waiting/VBG for/IN a/DT true/JJ error/NN gradient/NN to/TO be/VB backpropagated/VBN -/HYPH resulting/VBG in/IN Decoupled/VBN Neural/JJ Interfaces/NNS (/-LRB- DNIs/NNS )/-RRB- ./.
This/DT unlocked/VBN ability/NN of/IN being/VBG able/JJ to/TO update/VB parts/NNS of/IN a/DT neural/JJ network/NN asynchronously/RB and/CC with/IN only/JJ local/JJ information/NN was/VBD demonstrated/VBN to/IN work/NN empirically/RB in/IN Jaderberg/NNP et/FW al/FW (/-LRB- 2016/CD )/-RRB- ./.
However/RB ,/, there/EX has/VBZ been/VBN very/RB little/JJ demonstration/NN of/IN what/WP changes/VBZ DNIs/NNS and/CC SGs/NNS impose/VB from/IN a/DT functional/JJ ,/, representational/JJ ,/, and/CC learning/VBG dynamics/NNS point/NN of/IN view/NN ./.
In/IN this/DT paper/NN ,/, we/PRP study/VBP DNIs/NNS through/IN the/DT use/NN of/IN synthetic/JJ gradients/NNS on/IN feed/NN -/HYPH forward/JJ networks/NNS to/TO better/RBR understand/VB their/PRP$ behaviour/NN and/CC elucidate/VB their/PRP$ effect/NN on/IN optimisation/NN ./.
We/PRP show/VBP that/IN the/DT incorporation/NN of/IN SGs/NNS does/VBZ not/RB affect/VB the/DT representational/JJ strength/NN of/IN the/DT learning/NN system/NN for/IN a/DT neural/JJ network/NN ,/, and/CC prove/VB the/DT convergence/NN of/IN the/DT learning/NN system/NN for/IN linear/JJ and/CC deep/JJ linear/JJ models/NNS ./.
On/IN practical/JJ problems/NNS we/PRP investigate/VBP the/DT mechanism/NN by/IN which/WDT synthetic/JJ gradient/NN estimators/NNS approximate/VBP the/DT true/JJ loss/NN ,/, and/CC ,/, surprisingly/RB ,/, how/WRB that/DT leads/VBZ to/IN drastically/RB different/JJ layer-wise/JJ representations/NNS ./.
Finally/RB ,/, we/PRP also/RB expose/VB the/DT relationship/NN of/IN using/VBG synthetic/JJ gradients/NNS to/IN other/JJ error/NN approximation/NN techniques/NNS and/CC find/VB a/DT unifying/JJ language/NN for/IN discussion/NN and/CC comparison/NN ./.
