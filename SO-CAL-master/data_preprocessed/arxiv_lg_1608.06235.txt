Robotic/JJ systems/NNS must/MD be/VB able/JJ to/TO quickly/RB and/CC robustly/RB make/VB decisions/NNS when/WRB operating/VBG in/IN uncertain/JJ and/CC dynamic/JJ environments/NNS ./.
While/IN Reinforcement/NN Learning/NN (/-LRB- RL/NN )/-RRB- can/MD be/VB used/VBN to/TO compute/VB optimal/JJ policies/NNS with/IN little/JJ prior/JJ knowledge/NN about/IN the/DT environment/NN ,/, it/PRP suffers/VBZ from/IN slow/JJ convergence/NN ./.
An/DT alternative/JJ approach/NN is/VBZ Model/NNP Predictive/NNP Control/NNP (/-LRB- MPC/NNP )/-RRB- ,/, which/WDT optimizes/VBZ policies/NNS quickly/RB ,/, but/CC also/RB requires/VBZ accurate/JJ models/NNS of/IN the/DT system/NN dynamics/NNS and/CC environment/NN ./.
In/IN this/DT paper/NN we/PRP propose/VBP a/DT new/JJ approach/NN ,/, adaptive/JJ probabilistic/JJ trajectory/NN optimization/NN ,/, that/IN combines/VBZ the/DT benefits/NNS of/IN RL/NNP and/CC MPC/NNP ./.
Our/PRP$ method/NN uses/VBZ scalable/JJ approximate/JJ inference/NN to/TO learn/VB and/CC updates/NNS probabilistic/JJ models/NNS in/IN an/DT online/JJ incremental/JJ fashion/NN while/IN also/RB computing/VBG optimal/JJ control/NN policies/NNS via/IN successive/JJ local/JJ approximations/NNS ./.
We/PRP present/VBP two/CD variations/NNS of/IN our/PRP$ algorithm/NN based/VBN on/IN the/DT Sparse/JJ Spectrum/NN Gaussian/JJ Process/NN (/-LRB- SSGP/NN )/-RRB- model/NN ,/, and/CC we/PRP test/VBP our/PRP$ algorithm/NN on/IN three/CD learning/NN tasks/NNS ,/, demonstrating/VBG the/DT effectiveness/NN and/CC efficiency/NN of/IN our/PRP$ approach/NN ./.
