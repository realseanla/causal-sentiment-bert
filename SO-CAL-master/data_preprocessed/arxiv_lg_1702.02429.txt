Recent/JJ research/NN in/IN neural/JJ machine/NN translation/NN has/VBZ largely/RB focused/VBN on/IN two/CD aspects/NNS ;/: neural/JJ network/NN architectures/NNS and/CC end/NN -/HYPH to/IN -/HYPH end/NN learning/NN algorithms/NNS ./.
The/DT problem/NN of/IN decoding/NN ,/, however/RB ,/, has/VBZ received/VBN relatively/RB little/JJ attention/NN from/IN the/DT research/NN community/NN ./.
In/IN this/DT paper/NN ,/, we/PRP solely/RB focus/VBP on/IN the/DT problem/NN of/IN decoding/VBG given/VBN a/DT trained/VBN neural/JJ machine/NN translation/NN model/NN ./.
Instead/RB of/IN trying/VBG to/TO build/VB a/DT new/JJ decoding/NN algorithm/NN for/IN any/DT specific/JJ decoding/NN objective/NN ,/, we/PRP propose/VBP the/DT idea/NN of/IN trainable/JJ decoding/NN algorithm/NN in/IN which/WDT we/PRP train/VBP a/DT decoding/NN algorithm/NN to/TO find/VB a/DT translation/NN that/WDT maximizes/VBZ an/DT arbitrary/JJ decoding/NN objective/NN ./.
More/RBR specifically/RB ,/, we/PRP design/VBP an/DT actor/NN that/WDT observes/VBZ and/CC manipulates/VBZ the/DT hidden/JJ state/NN of/IN the/DT neural/JJ machine/NN translation/NN decoder/NN and/CC propose/VB to/IN train/NN it/PRP using/VBG a/DT variant/NN of/IN deterministic/JJ policy/NN gradient/NN ./.
We/PRP extensively/RB evaluate/VB the/DT proposed/VBN algorithm/NN using/VBG four/CD language/NN pairs/NNS and/CC two/CD decoding/NN objectives/NNS and/CC show/VBP that/IN we/PRP can/MD indeed/RB train/VB a/DT trainable/JJ greedy/JJ decoder/NN that/WDT generates/VBZ a/DT better/JJR translation/NN (/-LRB- in/IN terms/NNS of/IN a/DT target/NN decoding/VBG objective/NN )/-RRB- with/IN minimal/JJ computational/JJ overhead/NN ./.
