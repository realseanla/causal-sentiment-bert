Reinforcement/NN Learning/NN is/VBZ gaining/VBG attention/NN by/IN the/DT wireless/JJ networking/NN community/NN due/IN to/IN its/PRP$ potential/NN to/TO learn/VB good/JJ -/HYPH performing/VBG configurations/NNS only/RB from/IN the/DT observed/VBN results/NNS ./.
In/IN this/DT work/NN we/PRP propose/VBP a/DT stateless/JJ variation/NN of/IN Q/NN -/HYPH learning/NN ,/, which/WDT we/PRP apply/VBP to/TO exploit/VB spatial/JJ reuse/VB in/IN a/DT wireless/JJ network/NN ./.
In/IN particular/JJ ,/, we/PRP allow/VBP networks/NNS to/TO modify/VB both/CC their/PRP$ transmission/NN power/NN and/CC the/DT channel/NN used/VBN solely/RB based/VBN on/IN the/DT experienced/JJ throughput/NN ./.
We/PRP concentrate/VBP in/IN a/DT completely/RB decentralized/VBN scenario/NN in/IN which/WDT no/DT information/NN about/IN neighbouring/VBG nodes/NNS is/VBZ available/JJ to/IN the/DT learners/NNS ./.
Our/PRP$ results/NNS show/VBP that/IN although/IN the/DT algorithm/NN is/VBZ able/JJ to/TO find/VB the/DT best/RBS -/HYPH performing/VBG actions/NNS to/TO enhance/VB aggregate/JJ throughput/NN ,/, there/EX is/VBZ high/JJ variability/NN in/IN the/DT throughput/NN experienced/VBN by/IN the/DT individual/JJ networks/NNS ./.
We/PRP identify/VBP the/DT cause/NN of/IN this/DT variability/NN as/IN the/DT adversarial/JJ setting/NN of/IN our/PRP$ setup/NN ,/, in/IN which/WDT the/DT most/RBS played/VBN actions/NNS provide/VBP intermittent/JJ good/JJ //HYPH poor/JJ performance/NN depending/VBG on/IN the/DT neighbouring/VBG decisions/NNS ./.
We/PRP also/RB evaluate/VBP the/DT effect/NN of/IN the/DT intrinsic/JJ learning/NN parameters/NNS of/IN the/DT algorithm/NN on/IN this/DT variability/NN ./.
