The/DT Frank/NNP -/HYPH Wolfe/NNP optimization/NN algorithm/NN has/VBZ recently/RB regained/VBN popularity/NN for/IN machine/NN learning/NN applications/NNS due/IN to/IN its/PRP$ projection/NN -/HYPH free/JJ property/NN and/CC its/PRP$ ability/NN to/TO handle/VB structured/JJ constraints/NNS ./.
However/RB ,/, in/IN the/DT stochastic/JJ learning/NN setting/NN ,/, it/PRP is/VBZ still/RB relatively/RB understudied/JJ compared/VBN to/IN the/DT gradient/NN descent/NN counterpart/NN ./.
In/IN this/DT work/NN ,/, leveraging/VBG a/DT recent/JJ variance/NN reduction/NN technique/NN ,/, we/PRP propose/VBP two/CD stochastic/JJ Frank/NNP -/HYPH Wolfe/NNP variants/NNS which/WDT substantially/RB improve/VBP previous/JJ results/NNS in/IN terms/NNS of/IN the/DT number/NN of/IN stochastic/JJ gradient/NN evaluations/NNS needed/VBN to/TO achieve/VB $/$ 1/CD -/HYPH \/SYM epsilon/NN $/$ accuracy/NN ./.
For/IN example/NN ,/, we/PRP improve/VBP from/IN $/$ O/UH (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN }/-RRB- )/-RRB- $/$ to/IN $/$ O/UH (/-LRB- \/SYM ln/NN \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN }/-RRB- )/-RRB- $/$ if/IN the/DT objective/JJ function/NN is/VBZ smooth/JJ and/CC strongly/RB convex/JJ ,/, and/CC from/IN $/$ O/UH (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN ^/SYM 2/CD }/-RRB- )/-RRB- $/$ to/IN $/$ O/UH (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN ^/SYM {/-LRB- 1.5/CD }/-RRB- }/-RRB- )/-RRB- $/$ if/IN the/DT objective/JJ function/NN is/VBZ smooth/JJ and/CC Lipschitz/NNP ./.
The/DT theoretical/JJ improvement/NN is/VBZ also/RB observed/VBN in/IN experiments/NNS on/IN real/JJ -/HYPH world/NN datasets/NNS for/IN a/DT multiclass/NN classification/NN application/NN ./.
