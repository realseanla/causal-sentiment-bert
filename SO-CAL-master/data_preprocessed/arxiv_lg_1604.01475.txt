We/PRP investigate/VBP the/DT $/$ \/CD ell/CD _/NFP \/SYM infty/JJ $/$ -/HYPH constrained/VBN representation/NN which/WDT demonstrates/VBZ robustness/NN to/IN quantization/NN errors/NNS ,/, utilizing/VBG the/DT tool/NN of/IN deep/JJ learning/NN ./.
Based/VBN on/IN the/DT Alternating/NNP Direction/NNP Method/NNP of/IN Multipliers/NNPS (/-LRB- ADMM/NNP )/-RRB- ,/, we/PRP formulate/VBP the/DT original/JJ convex/NN minimization/NN problem/NN as/IN a/DT feed/NN -/HYPH forward/JJ neural/JJ network/NN ,/, named/VBN \/SYM textit/FW {/-LRB- Deep/NNP $/$ \/CD ell/CD _/NFP \/SYM infty/JJ $/$ Encoder/CD }/-RRB- ,/, by/IN introducing/VBG the/DT novel/JJ Bounded/NNP Linear/NNP Unit/NNP (/-LRB- BLU/NNP )/-RRB- neuron/NN and/CC modeling/VBG the/DT Lagrange/NNP multipliers/NNS as/IN network/NN biases/NNS ./.
Such/PDT a/DT structural/JJ prior/JJ acts/NNS as/IN an/DT effective/JJ network/NN regularization/NN ,/, and/CC facilitates/VBZ the/DT model/NN initialization/NN ./.
We/PRP then/RB investigate/VB the/DT effective/JJ use/NN of/IN the/DT proposed/VBN model/NN in/IN the/DT application/NN of/IN hashing/VBG ,/, by/IN coupling/VBG the/DT proposed/VBN encoders/NNS under/IN a/DT supervised/JJ pairwise/JJ loss/NN ,/, to/TO develop/VB a/DT \/NN textit/NN {/-LRB- Deep/JJ Siamese/JJ $/$ \/CD ell/CD _/NFP \/SYM infty/JJ $/NN Network/NN }/-RRB- ,/, which/WDT can/MD be/VB optimized/VBN from/IN end/NN to/IN end/NN ./.
Extensive/JJ experiments/NNS demonstrate/VBP the/DT impressive/JJ performances/NNS of/IN the/DT proposed/VBN model/NN ./.
We/PRP also/RB provide/VBP an/DT in/IN -/HYPH depth/NN analysis/NN of/IN its/PRP$ behaviors/NNS against/IN the/DT competitors/NNS ./.
