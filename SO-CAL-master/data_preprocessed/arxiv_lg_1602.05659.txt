This/DT paper/NN proposes/VBZ a/DT universal/JJ method/NN ,/, Boost/VB Picking/VBG ,/, to/TO train/VB supervised/JJ classification/NN models/NNS mainly/RB by/IN un-labeled/JJ data/NNS ./.
Boost/VB Picking/VBG only/RB adopts/VBZ two/CD weak/JJ classifiers/NNS to/TO estimate/VB and/CC correct/VB the/DT error/NN ./.
It/PRP is/VBZ theoretically/RB proved/VBN that/IN Boost/NNP Picking/VBG could/MD train/VB a/DT supervised/JJ model/NN mainly/RB by/IN un-labeled/JJ data/NNS as/IN effectively/RB as/IN the/DT same/JJ model/NN trained/VBN by/IN 100/CD percent/NN labeled/VBN data/NNS ,/, only/RB if/IN recalls/NNS of/IN the/DT two/CD weak/JJ classifiers/NNS are/VBP all/RB greater/JJR than/IN zero/CD and/CC the/DT sum/NN of/IN precisions/NNS is/VBZ greater/JJR than/IN one/CD ./.
Based/VBN on/IN Boost/NNP Picking/VBG ,/, we/PRP present/VBP "/`` Test/NN along/IN with/IN Training/NN (/-LRB- TawT/NN )/-RRB- "/`` to/TO improve/VB the/DT generalization/NN of/IN supervised/JJ models/NNS ./.
Both/DT Boost/VBP Picking/VBG and/CC TawT/NN are/VBP successfully/RB tested/VBN in/IN varied/JJ little/JJ data/NNS sets/NNS ./.
