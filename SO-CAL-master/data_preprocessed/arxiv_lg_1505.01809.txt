Two/CD recent/JJ approaches/NNS have/VBP achieved/VBN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN image/NN captioning/NN ./.
The/DT first/JJ uses/VBZ a/DT pipelined/JJ process/NN where/WRB a/DT set/NN of/IN candidate/NN words/NNS is/VBZ generated/VBN by/IN a/DT convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- trained/VBN on/IN images/NNS ,/, and/CC then/RB a/DT maximum/JJ entropy/NN (/-LRB- ME/NNP )/-RRB- language/NN model/NN is/VBZ used/VBN to/TO arrange/VB these/DT words/NNS into/IN a/DT coherent/JJ sentence/NN ./.
The/DT second/JJ uses/VBZ the/DT penultimate/JJ activation/NN layer/NN of/IN the/DT CNN/NNP as/IN input/NN to/IN a/DT recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- that/WDT then/RB generates/VBZ the/DT caption/NN sequence/NN ./.
In/IN this/DT paper/NN ,/, we/PRP compare/VBP the/DT merits/NNS of/IN the/DT different/JJ language/NN modeling/NN approaches/VBZ for/IN the/DT first/JJ time/NN by/IN using/VBG the/DT same/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN CNN/NNP as/IN input/NN ./.
We/PRP examine/VBP issues/NNS in/IN the/DT different/JJ approaches/NNS ,/, including/VBG linguistic/JJ irregularities/NNS ,/, caption/NN repetition/NN ,/, and/CC data/NNS set/VBN overlap/NN ./.
By/IN combining/VBG key/JJ aspects/NNS of/IN both/CC the/DT ME/NNP and/CC RNN/NNP methods/NNS ,/, we/PRP achieve/VBP a/DT new/JJ record/NN performance/NN on/IN the/DT benchmark/NN COCO/NN dataset/NN ./.
