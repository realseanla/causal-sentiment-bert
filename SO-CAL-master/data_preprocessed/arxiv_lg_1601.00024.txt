We/PRP study/VBP a/DT novel/JJ machine/NN learning/NN (/-LRB- ML/NN )/-RRB- problem/NN setting/NN of/IN sequentially/RB allocating/VBG small/JJ subsets/NNS of/IN training/NN data/NNS amongst/IN a/DT large/JJ set/NN of/IN classifiers/NNS ./.
The/DT goal/NN is/VBZ to/TO select/VB a/DT classifier/NN that/WDT will/MD give/VB near/IN -/HYPH optimal/JJ accuracy/NN when/WRB trained/VBN on/IN all/DT data/NNS ,/, while/IN also/RB minimizing/VBG the/DT cost/NN of/IN misallocated/VBN samples/NNS ./.
This/DT is/VBZ motivated/VBN by/IN large/JJ modern/JJ datasets/NNS and/CC ML/NNP toolkits/NNS with/IN many/JJ combinations/NNS of/IN learning/VBG algorithms/NNS and/CC hyper/JJ -/HYPH parameters/NNS ./.
Inspired/VBN by/IN the/DT principle/NN of/IN "/`` optimism/NN under/IN uncertainty/NN ,/, "/'' we/PRP propose/VBP an/DT innovative/JJ strategy/NN ,/, Data/NNP Allocation/NNP using/VBG Upper/NNP Bounds/NNPS (/-LRB- DAUB/NNP )/-RRB- ,/, which/WDT robustly/RB achieves/VBZ these/DT objectives/NNS across/IN a/DT variety/NN of/IN real/JJ -/HYPH world/NN datasets/NNS ./.
