Distributional/JJ semantics/NNS models/NNS are/VBP known/VBN to/IN struggle/NN with/IN small/JJ data/NNS ./.
It/PRP is/VBZ generally/RB accepted/VBN that/IN in/IN order/NN to/TO learn/VB '/`` a/DT good/JJ vector/NN '/'' for/IN a/DT word/NN ,/, a/DT model/NN must/MD have/VB sufficient/JJ examples/NNS of/IN its/PRP$ usage/NN ./.
This/DT contradicts/VBZ the/DT fact/NN that/IN humans/NNS can/MD guess/VB the/DT meaning/NN of/IN a/DT word/NN from/IN a/DT few/JJ occurrences/NNS only/RB ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN a/DT neural/JJ language/NN model/NN such/JJ as/IN Word2Vec/NN only/RB necessitates/VBZ minor/JJ modifications/NNS to/IN its/PRP$ standard/JJ architecture/NN to/TO learn/VB new/JJ terms/NNS from/IN tiny/JJ data/NNS ,/, using/VBG background/NN knowledge/NN from/IN a/DT previously/RB learnt/VBN semantic/JJ space/NN ./.
We/PRP test/VBP our/PRP$ model/NN on/IN word/NN definitions/NNS and/CC on/IN a/DT nonce/JJ task/NN involving/VBG 2/CD -/HYPH 6/CD sentences/NNS '/POS worth/NN of/IN context/NN ,/, showing/VBG a/DT large/JJ increase/NN in/IN performance/NN over/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN models/NNS on/IN the/DT definitional/JJ task/NN ./.
