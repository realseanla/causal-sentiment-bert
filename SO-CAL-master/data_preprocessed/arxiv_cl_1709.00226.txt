Semantic/JJ composition/NN remains/VBZ an/DT open/JJ problem/NN for/IN vector/NN space/NN models/NNS of/IN semantics/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP explain/VBP how/WRB the/DT probabilistic/JJ graphical/JJ model/NN used/VBN in/IN the/DT framework/NN of/IN Functional/JJ Distributional/JJ Semantics/NNS can/MD be/VB interpreted/VBN as/IN a/DT probabilistic/JJ version/NN of/IN model/NN theory/NN ./.
Building/NN on/IN this/DT ,/, we/PRP explain/VBP how/WRB various/JJ semantic/JJ phenomena/NNS can/MD be/VB recast/JJ in/IN terms/NNS of/IN conditional/JJ probabilities/NNS in/IN the/DT graphical/JJ model/NN ./.
This/DT connection/NN between/IN formal/JJ semantics/NNS and/CC machine/NN learning/NN is/VBZ helpful/JJ in/IN both/CC directions/NNS :/: it/PRP gives/VBZ us/PRP an/DT explicit/JJ mechanism/NN for/IN modelling/NN context/NN -/HYPH dependent/JJ meanings/NNS (/-LRB- a/DT challenge/NN for/IN formal/JJ semantics/NNS )/-RRB- ,/, and/CC also/RB gives/VBZ us/PRP well/RB -/HYPH motivated/JJ techniques/NNS for/IN composing/VBG distributed/VBN representations/NNS (/-LRB- a/DT challenge/NN for/IN distributional/JJ semantics/NNS )/-RRB- ./.
We/PRP present/VBP results/NNS on/IN two/CD datasets/NNS that/WDT go/VBP beyond/IN word/NN similarity/NN ,/, showing/VBG how/WRB these/DT semantically/RB -/HYPH motivated/JJ techniques/NNS improve/VB on/IN the/DT performance/NN of/IN vector/NN models/NNS ./.
