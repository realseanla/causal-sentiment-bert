This/DT paper/NN establishes/VBZ a/DT statistical/JJ versus/CC computational/JJ trade/NN -/HYPH off/NN for/IN solving/VBG a/DT basic/JJ high/JJ -/HYPH dimensional/JJ machine/NN learning/NN problem/NN via/IN a/DT basic/JJ convex/NN relaxation/NN method/NN ./.
Specifically/RB ,/, we/PRP consider/VBP the/DT {/-LRB- \/SYM em/PRP Sparse/JJ Principal/NN Component/NN Analysis/NN }/-RRB- (/-LRB- Sparse/JJ PCA/NN )/-RRB- problem/NN ,/, and/CC the/DT family/NN of/IN {/-LRB- \/SYM em/PRP Sum/NNP -/HYPH of/IN -/HYPH Squares/NNS }/-RRB- (/-LRB- SoS/NN ,/, aka/RB Lasserre/NNP //HYPH Parillo/NNP )/-RRB- convex/NN relaxations/NNS ./.
It/PRP was/VBD well/RB known/VBN that/IN in/IN large/JJ dimension/NN $/$ p/NN $/$ ,/, a/DT planted/VBN $/$ k/CD $/$ -/HYPH sparse/JJ unit/NN vector/NN can/MD be/VB {/-LRB- \/NFP em/PRP in/IN principle/NN }/-RRB- detected/VBN using/VBG only/RB $/$ n/NN \/SYM approx/NN k/NN \/SYM log/NN p/NN $/$ (/-LRB- Gaussian/JJ or/CC Bernoulli/NNP )/-RRB- samples/NNS ,/, but/CC all/DT {/-LRB- \/NFP em/PRP efficient/JJ }/-RRB- (/-LRB- polynomial/JJ time/NN )/-RRB- algorithms/NNS known/VBN require/VBP $/$ n/NN \/SYM approx/NN k/CD ^/SYM 2/CD \/SYM log/NN p/NN $/$ samples/NNS ./.
It/PRP was/VBD also/RB known/VBN that/IN this/DT quadratic/JJ gap/NN can/MD not/RB be/VB improved/VBN by/IN the/DT the/DT most/RBS basic/JJ {/-LRB- \/SYM em/PRP semi-definite/JJ }/-RRB- (/-LRB- SDP/NN ,/, aka/RB spectral/JJ )/-RRB- relaxation/NN ,/, equivalent/JJ to/IN a/DT degree/NN -/HYPH 2/CD SoS/NN algorithms/NNS ./.
Here/RB we/PRP prove/VBP that/DT also/RB degree/NN -/HYPH 4/CD SoS/NN algorithms/NNS can/MD not/RB improve/VB this/DT quadratic/JJ gap/NN ./.
This/DT average/JJ -/HYPH case/NN lower/JJR bound/JJ adds/VBZ to/IN the/DT small/JJ collection/NN of/IN hardness/NN results/NNS in/IN machine/NN learning/NN for/IN this/DT powerful/JJ family/NN of/IN convex/NN relaxation/NN algorithms/NNS ./.
Moreover/RB ,/, our/PRP$ design/NN of/IN moments/NNS (/-LRB- or/CC "/`` pseudo-expectations/NNS "/'' )/-RRB- for/IN this/DT lower/JJR bound/JJ is/VBZ quite/RB different/JJ than/IN previous/JJ lower/JJR bounds/NNS ./.
Establishing/VBG lower/JJR bounds/NNS for/IN higher/JJR degree/NN SoS/NN algorithms/NNS for/IN remains/VBZ a/DT challenging/JJ problem/NN ./.
