We/PRP propose/VBP a/DT new/JJ framework/NN for/IN black/JJ -/HYPH box/NN convex/NN optimization/NN which/WDT is/VBZ well/RB -/HYPH suited/VBN for/IN situations/NNS where/WRB gradient/NN computations/NNS are/VBP expensive/JJ ./.
We/PRP derive/VBP a/DT new/JJ method/NN for/IN this/DT framework/NN which/WDT leverages/VBZ several/JJ concepts/NNS from/IN convex/NN optimization/NN ,/, from/IN standard/JJ first/JJ -/HYPH order/NN methods/NNS (/-LRB- e.g./FW gradient/NN descent/NN or/CC quasi-Newton/NN methods/NNS )/-RRB- to/IN analytical/JJ centers/NNS (/-LRB- i.e./FW minimizers/NNS of/IN self/NN -/HYPH concordant/JJ barriers/NNS )/-RRB- ./.
We/PRP demonstrate/VBP empirically/RB that/IN our/PRP$ new/JJ technique/NN compares/VBZ favorably/RB with/IN state/NN of/IN the/DT art/NN algorithms/NNS (/-LRB- such/JJ as/IN BFGS/NN )/-RRB- ./.
