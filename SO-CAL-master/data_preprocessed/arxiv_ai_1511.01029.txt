Recent/JJ works/NNS have/VBP highlighted/VBN scale/NN invariance/NN or/CC symmetry/NN present/NN in/IN the/DT weight/NN space/NN of/IN a/DT typical/JJ deep/JJ network/NN and/CC the/DT adverse/JJ effect/NN it/PRP has/VBZ on/IN the/DT Euclidean/JJ gradient/NN based/VBN stochastic/JJ gradient/NN descent/NN optimization/NN ./.
In/IN this/DT work/NN ,/, we/PRP show/VBP that/IN a/DT commonly/RB used/VBN deep/JJ network/NN ,/, which/WDT uses/VBZ convolution/NN ,/, batch/NN normalization/NN ,/, reLU/NN ,/, max/NN -/HYPH pooling/VBG ,/, and/CC sub-sampling/JJ pipeline/NN ,/, possess/VBP more/RBR complex/JJ forms/NNS of/IN symmetry/NN arising/VBG from/IN scaling/NN -/HYPH based/VBN reparameterization/NN of/IN the/DT network/NN weights/NNS ./.
We/PRP propose/VBP to/TO tackle/VB the/DT issue/NN of/IN the/DT weight/NN space/NN symmetry/NN by/IN constraining/VBG the/DT filters/NNS to/TO lie/VB on/IN the/DT unit/NN -/HYPH norm/NN manifold/NN ./.
Consequently/RB ,/, training/VBG the/DT network/NN boils/VBZ down/RP to/IN using/VBG stochastic/JJ gradient/NN descent/NN updates/NNS on/IN the/DT unit/NN -/HYPH norm/NN manifold/NN ./.
Our/PRP$ empirical/JJ evidence/NN based/VBN on/IN the/DT MNIST/NNP dataset/NN shows/VBZ that/IN the/DT proposed/VBN updates/NNS improve/VB the/DT test/NN performance/NN beyond/IN what/WP is/VBZ achieved/VBN with/IN batch/NN normalization/NN and/CC without/IN sacrificing/VBG the/DT computational/JJ efficiency/NN of/IN the/DT weight/NN updates/NNS ./.
