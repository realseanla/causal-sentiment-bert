We/PRP investigate/VBP the/DT generalizability/NN of/IN learned/VBN binary/JJ relations/NNS :/: functions/NNS that/WDT map/VBP pairs/NNS of/IN instances/NNS to/IN a/DT logical/JJ indicator/NN ./.
This/DT problem/NN has/VBZ application/NN in/IN numerous/JJ areas/NNS of/IN machine/NN learning/NN ,/, such/JJ as/IN ranking/VBG ,/, entity/NN resolution/NN and/CC link/NN prediction/NN ./.
Our/PRP$ learning/NN framework/NN incorporates/VBZ an/DT example/NN labeler/NN that/WDT ,/, given/VBN a/DT sequence/NN $/$ X$/CD of/IN $/$ n/NN $/$ instances/NNS and/CC a/DT desired/VBN training/NN size/NN $/$ m/CD $/$ ,/, subsamples/VBZ $/$ m/CD $/$ pairs/NNS from/IN $/$ X/CD \/SYM times/NNS X$/NNP without/IN replacement/NN ./.
The/DT challenge/NN in/IN analyzing/VBG this/DT learning/NN scenario/NN is/VBZ that/IN pairwise/JJ combinations/NNS of/IN random/JJ variables/NNS are/VBP inherently/RB dependent/JJ ,/, which/WDT prevents/VBZ us/PRP from/IN using/VBG traditional/JJ learning/NN -/HYPH theoretic/JJ arguments/NNS ./.
We/PRP present/VBP a/DT unified/VBN ,/, graph/NN -/HYPH based/VBN analysis/NN ,/, which/WDT allows/VBZ us/PRP to/TO analyze/VB this/DT dependence/NN using/VBG well/RB -/HYPH known/VBN graph/NN identities/NNS ./.
We/PRP are/VBP then/RB able/JJ to/TO bound/VBD the/DT generalization/NN error/NN of/IN learned/VBN binary/JJ relations/NNS using/VBG Rademacher/NNP complexity/NN and/CC algorithmic/JJ stability/NN ./.
The/DT rate/NN of/IN uniform/JJ convergence/NN is/VBZ partially/RB determined/VBN by/IN the/DT labeler/NN 's/POS subsampling/JJ process/NN ./.
We/PRP thus/RB examine/VBP how/WRB various/JJ assumptions/NNS about/IN subsampling/VBG affect/NN generalization/NN ;/: under/IN a/DT natural/JJ random/JJ subsampling/NN process/NN ,/, our/PRP$ bounds/NNS guarantee/VB $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- 1/CD //SYM \/SYM sqrt/SYM {/-LRB- n/NN }/-RRB- )/-RRB- $/$ uniform/JJ convergence/NN ./.
