Recently/RB ,/, several/JJ works/NNS have/VBP shown/VBN that/IN natural/JJ modifications/NNS of/IN the/DT classical/JJ conditional/JJ gradient/NN method/NN (/-LRB- aka/RB Frank/NNP -/HYPH Wolfe/NNP algorithm/NN )/-RRB- for/IN constrained/VBN convex/NN optimization/NN ,/, provably/RB converge/VBP with/IN a/DT linear/JJ rate/NN when/WRB :/: i/LS )/-RRB- the/DT feasible/JJ set/NN is/VBZ a/DT polytope/NN ,/, and/CC ii/LS )/-RRB- the/DT objective/NN is/VBZ smooth/JJ and/CC strongly/RB -/HYPH convex/NN ./.
However/RB ,/, all/DT of/IN these/DT results/NNS suffer/VBP from/IN two/CD significant/JJ shortcomings/NNS :/: large/JJ memory/NN requirement/NN due/IN to/IN the/DT need/NN to/TO store/VB an/DT explicit/JJ convex/NN decomposition/NN of/IN the/DT current/JJ iterate/NN ,/, and/CC as/IN a/DT consequence/NN ,/, large/JJ running/NN -/HYPH time/NN overhead/NN per/IN iteration/NN ,/, and/CC worst/JJS case/NN convergence/NN rate/NN that/WDT depends/VBZ unfavorably/RB on/IN the/DT dimension/NN ./.
