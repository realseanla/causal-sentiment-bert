Recent/JJ work/NN exhibited/VBD that/IN distributed/VBN word/NN representations/NNS are/VBP good/JJ at/IN capturing/VBG linguistic/JJ regularities/NNS in/IN language/NN ./.
This/DT allows/VBZ vector/NN -/HYPH oriented/VBN reasoning/NN based/VBN on/IN simple/JJ linear/JJ algebra/NN between/IN words/NNS ./.
Since/IN many/JJ different/JJ methods/NNS have/VBP been/VBN proposed/VBN for/IN learning/VBG document/NN representations/NNS ,/, it/PRP is/VBZ natural/JJ to/TO ask/VB whether/IN there/EX is/VBZ also/RB linear/JJ structure/NN in/IN these/DT learned/VBN representations/NNS to/TO allow/VB similar/JJ reasoning/NN at/IN document/NN level/NN ./.
To/TO answer/VB this/DT question/NN ,/, we/PRP design/VBP a/DT new/JJ document/NN analogy/NN task/NN for/IN testing/VBG the/DT semantic/JJ regularities/NNS in/IN document/NN representations/NNS ,/, and/CC conduct/VB empirical/JJ evaluations/NNS over/IN several/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN document/NN representation/NN models/NNS ./.
The/DT results/NNS reveal/VBP that/IN neural/JJ embedding/NN based/VBN document/NN representations/NNS work/VBP better/RBR on/IN this/DT analogy/NN task/NN than/IN conventional/JJ methods/NNS ,/, and/CC we/PRP provide/VBP some/DT preliminary/JJ explanations/NNS over/IN these/DT observations/NNS ./.
