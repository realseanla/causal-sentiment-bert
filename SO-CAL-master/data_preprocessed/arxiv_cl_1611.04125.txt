Joint/JJ representation/NN learning/NN of/IN text/NN and/CC knowledge/NN within/IN a/DT unified/JJ semantic/JJ space/NN enables/VBZ us/PRP to/TO perform/VB knowledge/NN graph/NN completion/NN more/RBR accurately/RB ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT novel/JJ framework/NN to/TO embed/VB words/NNS ,/, entities/NNS and/CC relations/NNS into/IN the/DT same/JJ continuous/JJ vector/NN space/NN ./.
In/IN this/DT model/NN ,/, both/CC entity/NN and/CC relation/NN embeddings/NNS are/VBP learned/VBN by/IN taking/VBG knowledge/NN graph/NN and/CC plain/JJ text/NN into/IN consideration/NN ./.
In/IN experiments/NNS ,/, we/PRP evaluate/VBP the/DT joint/JJ learning/NN model/NN on/IN three/CD tasks/NNS including/VBG entity/NN prediction/NN ,/, relation/NN prediction/NN and/CC relation/NN classification/NN from/IN text/NN ./.
The/DT experiment/NN results/NNS show/VBP that/IN our/PRP$ model/NN can/MD significantly/RB and/CC consistently/RB improve/VB the/DT performance/NN on/IN the/DT three/CD tasks/NNS as/IN compared/VBN with/IN other/JJ baselines/NNS ./.
