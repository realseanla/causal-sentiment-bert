Neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- has/VBZ achieved/VBN notable/JJ success/NN in/IN recent/JJ times/NNS ,/, however/RB it/PRP is/VBZ also/RB widely/RB recognized/VBN that/IN this/DT approach/NN has/VBZ limitations/NNS with/IN handling/VBG infrequent/JJ words/NNS and/CC word/NN pairs/NNS ./.
This/DT paper/NN presents/VBZ a/DT novel/JJ memory/NN -/HYPH augmented/VBN NMT/NN (/-LRB- M/NN -/HYPH NMT/NN )/-RRB- architecture/NN ,/, which/WDT stores/VBZ knowledge/NN about/IN how/WRB words/NNS (/-LRB- usually/RB infrequently/RB encountered/VBN ones/NNS )/-RRB- should/MD be/VB translated/VBN in/IN a/DT memory/NN and/CC then/RB utilizes/VBZ them/PRP to/TO assist/VB the/DT neural/JJ model/NN ./.
We/PRP use/VBP this/DT memory/NN mechanism/NN to/TO combine/VB the/DT knowledge/NN learned/VBD from/IN a/DT conventional/JJ statistical/JJ machine/NN translation/NN system/NN and/CC the/DT rules/NNS learned/VBN by/IN an/DT NMT/NN system/NN ,/, and/CC also/RB propose/VB a/DT solution/NN for/IN out/RB -/HYPH of/IN -/HYPH vocabulary/NN (/-LRB- OOV/NN )/-RRB- words/NNS based/VBN on/IN this/DT framework/NN ./.
Our/PRP$ experiments/NNS on/IN two/CD Chinese/JJ -/HYPH English/JJ translation/NN tasks/NNS demonstrated/VBD that/IN the/DT M/NN -/HYPH NMT/NN architecture/NN outperformed/VBD the/DT NMT/NN baseline/NN by/IN $/$ 9.0/CD $/$ and/CC $/$ 2.7/CD $/$ BLEU/CD points/NNS on/IN the/DT two/CD tasks/NNS ,/, respectively/RB ./.
Additionally/RB ,/, we/PRP found/VBD this/DT architecture/NN resulted/VBD in/IN a/DT much/RB more/RBR effective/JJ OOV/NN treatment/NN compared/VBN to/IN competitive/JJ methods/NNS ./.
