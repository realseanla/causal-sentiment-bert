We/PRP introduce/VBP the/DT "/`` exponential/JJ linear/JJ unit/NN "/'' (/-LRB- ELU/NNP )/-RRB- which/WDT speeds/VBZ up/RP learning/VBG in/IN deep/JJ neural/JJ networks/NNS and/CC leads/VBZ to/IN higher/JJR classification/NN accuracies/NNS ./.
Like/IN rectified/VBN linear/JJ units/NNS (/-LRB- ReLUs/NNS )/-RRB- ,/, leaky/JJ ReLUs/NNS (/-LRB- LReLUs/NNS )/-RRB- and/CC parameterized/JJ ReLUs/NNS (/-LRB- PReLUs/NNS )/-RRB- ,/, ELUs/NNS also/RB avoid/VBP a/DT vanishing/VBG gradient/NN via/IN the/DT identity/NN for/IN positive/JJ values/NNS ./.
However/RB ,/, ELUs/NNS have/VBP improved/VBN learning/VBG characteristics/NNS compared/VBN to/IN the/DT units/NNS with/IN other/JJ activation/NN functions/NNS ./.
In/IN contrast/NN to/IN ReLUs/NNS ,/, ELUs/NNS have/VBP negative/JJ values/NNS which/WDT allows/VBZ them/PRP to/TO push/VB mean/JJ unit/NN activations/NNS closer/RBR to/IN zero/CD ./.
Zero/CD means/NNS speed/VB up/RP learning/NN because/IN they/PRP bring/VBP the/DT gradient/NN closer/RBR to/IN the/DT unit/NN natural/JJ gradient/NN ./.
We/PRP show/VBP that/IN the/DT unit/NN natural/JJ gradient/NN differs/VBZ from/IN the/DT normal/JJ gradient/NN by/IN a/DT bias/NN shift/NN term/NN ,/, which/WDT is/VBZ proportional/JJ to/IN the/DT mean/NN activation/NN of/IN incoming/JJ units/NNS ./.
Like/IN batch/NN normalization/NN ,/, ELUs/NNS push/VBP the/DT mean/NN towards/IN zero/CD ,/, but/CC with/IN a/DT significantly/RB smaller/JJR computational/JJ footprint/NN ./.
While/IN other/JJ activation/NN functions/VBZ like/IN LReLUs/NNS and/CC PReLUs/NNS also/RB have/VBP negative/JJ values/NNS ,/, they/PRP do/VBP not/RB ensure/VB a/DT noise/NN -/HYPH robust/JJ deactivation/NN state/NN ./.
ELUs/NNS saturate/VBP to/IN a/DT negative/JJ value/NN with/IN smaller/JJR inputs/NNS and/CC thereby/RB decrease/VB the/DT propagated/VBN variation/NN and/CC information/NN ./.
Therefore/RB ,/, ELUs/NNS code/VBP the/DT degree/NN of/IN presence/NN of/IN particular/JJ phenomena/NN in/IN the/DT input/NN ,/, while/IN they/PRP do/VBP not/RB quantitatively/RB model/VB the/DT degree/NN of/IN their/PRP$ absence/NN ./.
Consequently/RB ,/, dependencies/NNS between/IN ELUs/NNS are/VBP much/RB easier/JJR to/TO model/VB and/CC distinct/JJ concepts/NNS are/VBP less/RBR likely/JJ to/TO interfere/VB ./.
We/PRP found/VBD that/IN ELUs/NNS lead/VBP not/RB only/RB to/IN faster/JJR learning/NN ,/, but/CC also/RB to/TO better/RBR generalization/NN performance/NN once/RB networks/NNS have/VBP many/JJ layers/NNS (/-LRB- &gt;/SYM =/SYM 5/CD )/-RRB- ./.
Using/VBG ELUs/NNS ,/, we/PRP obtained/VBD the/DT best/JJS published/VBN single/JJ -/HYPH crop/NN result/NN on/IN CIFAR/NN -/HYPH 100/CD and/CC CIFAR/NN -/HYPH 10/CD ./.
On/IN ImageNet/NNP ,/, ELU/NNP networks/NNS considerably/RB speed/VBP up/RP learning/VBG compared/VBN to/IN a/DT ReLU/NN network/NN with/IN similar/JJ classification/NN performance/NN ,/, obtaining/VBG less/JJR than/IN 10/CD percent/NN classification/NN error/NN for/IN a/DT single/JJ crop/NN ,/, single/JJ model/NN network/NN ./.
