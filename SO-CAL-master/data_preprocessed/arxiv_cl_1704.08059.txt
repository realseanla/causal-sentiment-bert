Skip/VB -/HYPH Gram/NNP Negative/JJ Sampling/NN (/-LRB- SGNS/NN )/-RRB- word/NN embedding/NN model/NN ,/, well/RB known/VBN by/IN its/PRP$ implementation/NN in/IN "/`` word2vec/NN "/'' software/NN ,/, is/VBZ usually/RB optimized/VBN by/IN stochastic/JJ gradient/NN descent/NN ./.
However/RB ,/, the/DT optimization/NN of/IN SGNS/NN objective/NN can/MD be/VB viewed/VBN as/IN a/DT problem/NN of/IN searching/VBG for/IN a/DT good/JJ matrix/NN with/IN the/DT low/JJ -/HYPH rank/NN constraint/NN ./.
The/DT most/RBS standard/JJ way/NN to/TO solve/VB this/DT type/NN of/IN problems/NNS is/VBZ to/TO apply/VB Riemannian/JJ optimization/NN framework/NN to/TO optimize/VB the/DT SGNS/NN objective/NN over/IN the/DT manifold/NN of/IN required/VBN low/JJ -/HYPH rank/NN matrices/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP an/DT algorithm/NN that/WDT optimizes/VBZ SGNS/NN objective/NN using/VBG Riemannian/NNP optimization/NN and/CC demonstrates/VBZ its/PRP$ superiority/NN over/IN popular/JJ competitors/NNS ,/, such/JJ as/IN the/DT original/JJ method/NN to/TO train/VB SGNS/NN and/CC SVD/NN over/IN SPPMI/NNP matrix/NN ./.
