Neural/JJ Machine/NN Translation/NN (/-LRB- NMT/NN )/-RRB- is/VBZ an/DT end/NN -/HYPH to/IN -/HYPH end/NN learning/NN approach/NN for/IN automated/VBN translation/NN ,/, with/IN the/DT potential/NN to/TO overcome/VB many/JJ of/IN the/DT weaknesses/NNS of/IN conventional/JJ phrase/NN -/HYPH based/VBN translation/NN systems/NNS ./.
Unfortunately/RB ,/, NMT/NN systems/NNS are/VBP known/VBN to/TO be/VB computationally/RB expensive/JJ both/CC in/IN training/NN and/CC in/IN translation/NN inference/NN ./.
Also/RB ,/, most/JJS NMT/NN systems/NNS have/VBP difficulty/NN with/IN rare/JJ words/NNS ./.
These/DT issues/NNS have/VBP hindered/VBN NMT/NNP 's/POS use/NN in/IN practical/JJ deployments/NNS and/CC services/NNS ,/, where/WRB both/DT accuracy/NN and/CC speed/NN are/VBP essential/JJ ./.
In/IN this/DT work/NN ,/, we/PRP present/VBP GNMT/NNP ,/, Google/NNP 's/POS Neural/JJ Machine/NN Translation/NN system/NN ,/, which/WDT attempts/VBZ to/TO address/VB many/JJ of/IN these/DT issues/NNS ./.
Our/PRP$ model/NN consists/VBZ of/IN a/DT deep/JJ LSTM/NN network/NN with/IN 8/CD encoder/NN and/CC 8/CD decoder/NN layers/NNS using/VBG attention/NN and/CC residual/JJ connections/NNS ./.
To/TO improve/VB parallelism/NN and/CC therefore/RB decrease/NN training/NN time/NN ,/, our/PRP$ attention/NN mechanism/NN connects/VBZ the/DT bottom/JJ layer/NN of/IN the/DT decoder/NN to/IN the/DT top/JJ layer/NN of/IN the/DT encoder/NN ./.
To/TO accelerate/VB the/DT final/JJ translation/NN speed/NN ,/, we/PRP employ/VBP low/JJ -/HYPH precision/NN arithmetic/NN during/IN inference/NN computations/NNS ./.
To/TO improve/VB handling/NN of/IN rare/JJ words/NNS ,/, we/PRP divide/VBP words/NNS into/IN a/DT limited/JJ set/NN of/IN common/JJ sub-word/JJ units/NNS (/-LRB- "/`` wordpieces/NNS "/'' )/-RRB- for/IN both/DT input/NN and/CC output/NN ./.
This/DT method/NN provides/VBZ a/DT good/JJ balance/NN between/IN the/DT flexibility/NN of/IN "/`` character/NN "/'' -/, delimited/VBN models/NNS and/CC the/DT efficiency/NN of/IN "/`` word/NN "/'' -/, delimited/VBN models/NNS ,/, naturally/RB handles/VBZ translation/NN of/IN rare/JJ words/NNS ,/, and/CC ultimately/RB improves/VBZ the/DT overall/JJ accuracy/NN of/IN the/DT system/NN ./.
Our/PRP$ beam/NN search/NN technique/NN employs/VBZ a/DT length/NN -/HYPH normalization/NN procedure/NN and/CC uses/VBZ a/DT coverage/NN penalty/NN ,/, which/WDT encourages/VBZ generation/NN of/IN an/DT output/NN sentence/NN that/WDT is/VBZ most/RBS likely/JJ to/TO cover/VB all/PDT the/DT words/NNS in/IN the/DT source/NN sentence/NN ./.
On/IN the/DT WMT/NNP '14/CD English/NNP -/HYPH to/IN -/HYPH French/JJ and/CC English/JJ -/HYPH to/IN -/HYPH German/JJ benchmarks/NNS ,/, GNMT/NNP achieves/VBZ competitive/JJ results/NNS to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
Using/VBG a/DT human/JJ side/NN -/HYPH by/IN -/HYPH side/NN evaluation/NN on/IN a/DT set/NN of/IN isolated/VBN simple/JJ sentences/NNS ,/, it/PRP reduces/VBZ translation/NN errors/NNS by/IN an/DT average/NN of/IN 60/CD percent/NN compared/VBN to/IN Google/NNP 's/POS phrase/NN -/HYPH based/VBN production/NN system/NN ./.
