This/DT paper/NN develops/VBZ a/DT model/NN that/WDT addresses/VBZ syntactic/JJ embedding/NN for/IN machine/NN comprehension/NN ,/, a/DT key/JJ task/NN of/IN natural/JJ language/NN understanding/NN ./.
Our/PRP$ proposed/VBN model/NN ,/, structural/JJ embedding/NN of/IN syntactic/JJ trees/NNS (/-LRB- SEST/NN )/-RRB- ,/, takes/VBZ each/DT word/NN in/IN a/DT sentence/NN ,/, constructs/NNS a/DT sequence/NN of/IN syntactic/JJ nodes/NNS extracted/VBN from/IN syntactic/JJ parse/VB trees/NNS ,/, and/CC encodes/VBZ the/DT sequence/NN into/IN a/DT vector/NN representation/NN ./.
The/DT learned/VBN vector/NN is/VBZ then/RB incorporated/VBN into/IN neural/JJ attention/NN models/NNS ,/, which/WDT allows/VBZ learning/VBG the/DT mapping/NN of/IN syntactic/JJ structures/NNS between/IN question/NN and/CC context/NN pairs/NNS ./.
We/PRP evaluate/VBP our/PRP$ approach/NN on/IN SQuAD/NN dataset/NN and/CC demonstrate/VBP that/IN our/PRP$ model/NN can/MD accurately/RB identify/VB the/DT syntactic/JJ boundaries/NNS of/IN the/DT sentences/NNS and/CC to/TO extract/VB answers/NNS that/WDT are/VBP syntactically/RB coherent/JJ over/IN the/DT baseline/NN methods/NNS ./.
