Our/PRP$ study/NN identifies/VBZ sentences/NNS in/IN Wikipedia/NNP articles/NNS that/WDT are/VBP either/RB identical/JJ or/CC highly/RB similar/JJ by/IN applying/VBG techniques/NNS for/IN near/JJ -/HYPH duplicate/JJ detection/NN of/IN web/NN pages/NNS ./.
This/DT is/VBZ accomplished/VBN with/IN a/DT MapReduce/NNP implementation/NN of/IN minhash/NN to/TO identify/VB clusters/NNS of/IN sentences/NNS with/IN high/JJ Jaccard/NNP similarity/NN ./.
We/PRP show/VBP that/IN these/DT clusters/NNS can/MD be/VB categorized/VBN into/IN six/CD different/JJ types/NNS ,/, two/CD of/IN which/WDT are/VBP particularly/RB interesting/JJ :/: identical/JJ sentences/NNS quantify/VBP the/DT extent/NN to/TO which/WDT content/NN in/IN Wikipedia/NNP is/VBZ copied/VBN and/CC pasted/VBN ,/, and/CC near/IN -/HYPH duplicate/JJ sentences/NNS that/WDT state/VBP contradictory/JJ facts/NNS point/VBP to/IN quality/NN issues/NNS in/IN Wikipedia/NNP ./.
