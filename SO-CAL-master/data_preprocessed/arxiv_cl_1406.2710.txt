In/IN this/DT paper/NN we/PRP propose/VBP a/DT general/JJ framework/NN for/IN learning/VBG distributed/VBN representations/NNS of/IN attributes/NNS :/: characteristics/NNS of/IN text/NN whose/WP$ representations/NNS can/MD be/VB jointly/RB learned/VBN with/IN word/NN embeddings/NNS ./.
Attributes/NNS can/MD correspond/VB to/IN document/NN indicators/NNS (/-LRB- to/TO learn/VB sentence/NN vectors/NNS )/-RRB- ,/, language/NN indicators/NNS (/-LRB- to/TO learn/VB distributed/VBN language/NN representations/NNS )/-RRB- ,/, meta/NN -/HYPH data/NNS and/CC side/NN information/NN (/-LRB- such/JJ as/IN the/DT age/NN ,/, gender/NN and/CC industry/NN of/IN a/DT blogger/NN )/-RRB- or/CC representations/NNS of/IN authors/NNS ./.
We/PRP describe/VBP a/DT third/JJ -/HYPH order/NN model/NN where/WRB word/NN context/NN and/CC attribute/NN vectors/NNS interact/VBP multiplicatively/RB to/TO predict/VB the/DT next/JJ word/NN in/IN a/DT sequence/NN ./.
This/DT leads/VBZ to/IN the/DT notion/NN of/IN conditional/JJ word/NN similarity/NN :/: how/WRB meanings/NNS of/IN words/NNS change/VBP when/WRB conditioned/VBN on/IN different/JJ attributes/NNS ./.
We/PRP perform/VBP several/JJ experimental/JJ tasks/NNS including/VBG sentiment/NN classification/NN ,/, cross-lingual/JJ document/NN classification/NN ,/, and/CC blog/NN authorship/NN attribution/NN ./.
We/PRP also/RB qualitatively/RB evaluate/VB conditional/JJ word/NN neighbours/NNS and/CC attribute/NN -/HYPH conditioned/VBN text/NN generation/NN ./.
