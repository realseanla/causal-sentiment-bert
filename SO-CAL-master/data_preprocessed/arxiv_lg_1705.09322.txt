Off/IN -/HYPH policy/NN learning/NN is/VBZ key/JJ to/IN scaling/VBG up/RP reinforcement/NN learning/NN as/IN it/PRP allows/VBZ to/TO learn/VB about/IN a/DT target/NN policy/NN from/IN the/DT experience/NN generated/VBN by/IN a/DT different/JJ behavior/NN policy/NN ./.
Unfortunately/RB ,/, it/PRP has/VBZ been/VBN challenging/VBG to/TO combine/VB off/RB -/HYPH policy/NN learning/NN with/IN function/NN approximation/NN and/CC multi-step/JJ bootstrapping/NN in/IN a/DT way/NN that/WDT leads/VBZ to/IN both/DT stable/JJ and/CC efficient/JJ algorithms/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN the/DT Tree/NNP Backup/JJ and/CC Retrace/VB algorithms/NNS are/VBP unstable/JJ with/IN linear/JJ function/NN approximation/NN ,/, both/CC in/IN theory/NN and/CC with/IN specific/JJ examples/NNS ./.
Based/VBN on/IN our/PRP$ analysis/NN ,/, we/PRP then/RB derive/VBP stable/JJ and/CC efficient/JJ gradient/NN -/HYPH based/VBN algorithms/NNS ,/, compatible/JJ with/IN accumulating/VBG or/CC Dutch/JJ traces/NNS ,/, using/VBG a/DT novel/JJ methodology/NN based/VBN on/IN proximal/JJ methods/NNS ./.
In/IN addition/NN to/IN convergence/NN proofs/NNS ,/, we/PRP provide/VBP sample/NN -/HYPH complexity/NN bounds/NNS ./.
