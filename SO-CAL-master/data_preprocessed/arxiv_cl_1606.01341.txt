In/IN this/DT work/NN ,/, we/PRP investigate/VBP several/JJ neural/JJ network/NN architectures/NNS for/IN fine/JJ -/HYPH grained/JJ entity/NN type/NN classification/NN ./.
Particularly/RB ,/, we/PRP consider/VBP extensions/NNS to/IN a/DT recently/RB proposed/VBN attentive/JJ neural/JJ architecture/NN and/CC make/VB three/CD key/JJ contributions/NNS ./.
Previous/JJ work/NN on/IN attentive/JJ neural/JJ architectures/NNS do/VBP not/RB consider/VB hand/NN -/HYPH crafted/VBN features/NNS ,/, we/PRP combine/VBP learnt/VBN and/CC hand/NN -/HYPH crafted/VBN features/NNS and/CC observe/VBP that/IN they/PRP complement/VBP each/DT other/JJ ./.
Additionally/RB ,/, through/IN quantitative/JJ analysis/NN we/PRP establish/VBP that/IN the/DT attention/NN mechanism/NN is/VBZ capable/JJ of/IN learning/VBG to/TO attend/VB over/IN syntactic/JJ heads/NNS and/CC the/DT phrase/NN containing/VBG the/DT mention/NN ,/, where/WRB both/DT are/VBP known/VBN strong/JJ hand/NN -/HYPH crafted/VBN features/NNS for/IN our/PRP$ task/NN ./.
We/PRP enable/VBP parameter/NN sharing/VBG through/IN a/DT hierarchical/JJ label/NN encoding/VBG method/NN ,/, that/IN in/IN low/JJ -/HYPH dimensional/JJ projections/NNS show/VBP clear/JJ clusters/NNS for/IN each/DT type/NN hierarchy/NN ./.
Lastly/RB ,/, despite/IN using/VBG the/DT same/JJ evaluation/NN dataset/NN ,/, the/DT literature/NN frequently/RB compare/VBP models/NNS trained/VBN using/VBG different/JJ data/NNS ./.
We/PRP establish/VBP that/IN the/DT choice/NN of/IN training/NN data/NNS has/VBZ a/DT drastic/JJ impact/NN on/IN performance/NN ,/, with/IN decreases/NNS by/IN as/RB much/JJ as/IN 9.85/CD percent/NN loose/JJ micro/NN F1/NN score/NN for/IN a/DT previously/RB proposed/VBN method/NN ./.
Despite/IN this/DT ,/, our/PRP$ best/JJS model/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS with/IN 75.36/CD percent/NN loose/JJ micro/NN F1/NN score/NN on/IN the/DT well/NN -/HYPH established/VBN FIGER/NNP (/-LRB- GOLD/NN )/-RRB- dataset/NN ./.
