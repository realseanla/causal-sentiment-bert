In/IN recent/JJ years/NNS significant/JJ progress/NN has/VBZ been/VBN made/VBN in/IN successfully/RB training/VBG recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- on/IN sequence/NN learning/NN problems/NNS involving/VBG long/JJ range/NN temporal/JJ dependencies/NNS ./.
The/DT progress/NN has/VBZ been/VBN made/VBN on/IN three/CD fronts/NNS :/: (/-LRB- a/LS )/-RRB- Algorithmic/JJ improvements/NNS involving/VBG sophisticated/JJ optimization/NN techniques/NNS ,/, (/-LRB- b/LS )/-RRB- network/NN design/NN involving/VBG complex/JJ hidden/JJ layer/NN nodes/NNS and/CC specialized/VBN recurrent/JJ layer/NN connections/NNS and/CC (/-LRB- c/LS )/-RRB- weight/NN initialization/NN methods/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP focus/VBP on/IN recently/RB proposed/VBN weight/NN initialization/NN with/IN identity/NN matrix/NN for/IN the/DT recurrent/JJ weights/NNS in/IN a/DT RNN/NN ./.
This/DT initialization/NN is/VBZ specifically/RB proposed/VBN for/IN hidden/JJ nodes/NNS with/IN Rectified/VBN Linear/NNP Unit/NNP (/-LRB- ReLU/NNP )/-RRB- non/AFX linearity/NN ./.
We/PRP offer/VBP a/DT simple/JJ dynamical/JJ systems/NNS perspective/NN on/IN weight/NN initialization/NN process/NN ,/, which/WDT allows/VBZ us/PRP to/TO propose/VB a/DT modified/VBN weight/NN initialization/NN strategy/NN ./.
We/PRP show/VBP that/IN this/DT initialization/NN technique/NN leads/VBZ to/IN successfully/RB training/VBG RNNs/NNS composed/VBN of/IN ReLUs/NNS ./.
We/PRP demonstrate/VBP that/IN our/PRP$ proposal/NN produces/VBZ comparable/JJ or/CC better/JJR solution/NN for/IN three/CD toy/NN problems/NNS involving/VBG long/JJ range/NN temporal/JJ structure/NN :/: the/DT addition/NN problem/NN ,/, the/DT multiplication/NN problem/NN and/CC the/DT MNIST/NN classification/NN problem/NN using/VBG sequence/NN of/IN pixels/NNS ./.
In/IN addition/NN ,/, we/PRP present/VBP results/NNS for/IN a/DT benchmark/NN action/NN recognition/NN problem/NN ./.
