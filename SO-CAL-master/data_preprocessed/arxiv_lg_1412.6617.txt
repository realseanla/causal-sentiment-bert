Energy/NNP -/HYPH based/VBN models/NNS are/VBP popular/JJ in/IN machine/NN learning/NN due/IN to/IN the/DT elegance/NN of/IN their/PRP$ formulation/NN and/CC their/PRP$ relationship/NN to/IN statistical/JJ physics/NN ./.
Among/IN these/DT ,/, the/DT Restricted/VBN Boltzmann/NNP Machine/NNP (/-LRB- RBM/NNP )/-RRB- has/VBZ been/VBN the/DT prototype/NN for/IN some/DT recent/JJ advancements/NNS in/IN the/DT unsupervised/JJ training/NN of/IN deep/JJ neural/JJ networks/NNS ./.
However/RB ,/, the/DT contrastive/JJ divergence/NN training/NN algorithm/NN ,/, so/RB often/RB used/VBN for/IN such/JJ models/NNS ,/, has/VBZ a/DT number/NN of/IN drawbacks/NNS and/CC ineligancies/NNS both/CC in/IN theory/NN and/CC in/IN practice/NN ./.
Here/RB ,/, we/PRP investigate/VBP the/DT performance/NN of/IN Minimum/NNP Probability/NNP Flow/NNP learning/VBG for/IN training/NN RBMs/NNS ./.
This/DT approach/NN reconceptualizes/VBZ the/DT nature/NN of/IN the/DT dynamics/NNS defined/VBN over/IN a/DT model/NN ,/, rather/RB than/IN thinking/VBG about/IN Gibbs/NNP sampling/NN ,/, and/CC derives/VBZ a/DT simple/JJ ,/, tractable/JJ ,/, and/CC elegant/JJ objective/JJ function/NN using/VBG a/DT Taylor/NNP expansion/NN ,/, allowing/VBG one/CD to/TO learn/VB the/DT parameters/NNS of/IN any/DT distribution/NN over/IN visible/JJ states/NNS ./.
In/IN the/DT paper/NN ,/, we/PRP expound/VBP the/DT Minimum/NNP Probability/NNP Flow/NNP learning/VBG algorithm/NN under/IN various/JJ dynamics/NNS ./.
We/PRP empirically/RB analyze/VB its/PRP$ performance/NN on/IN these/DT dynamics/NNS and/CC demonstrate/VBP that/IN MPF/NNP algorithms/NNS outperform/VBP CD/NN on/IN various/JJ RBM/NNP configurations/NNS ./.
