We/PRP consider/VBP the/DT problem/NN of/IN online/JJ learning/NN in/IN misspecified/VBN linear/JJ stochastic/JJ multi-armed/JJ bandit/NN problems/NNS ./.
Regret/NN guarantees/NNS for/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN linear/JJ bandit/NN algorithms/NNS such/JJ as/IN Optimism/NN in/IN the/DT Face/NNP of/IN Uncertainty/NN Linear/NNP bandit/NN (/-LRB- OFUL/NN )/-RRB- hold/VBP under/IN the/DT assumption/NN that/IN the/DT arms/NNS expected/VBN rewards/NNS are/VBP perfectly/RB linear/JJ in/IN their/PRP$ features/NNS ./.
It/PRP is/VBZ ,/, however/RB ,/, of/IN interest/NN to/TO investigate/VB the/DT impact/NN of/IN potential/JJ misspecification/NN in/IN linear/JJ bandit/NN models/NNS ,/, where/WRB the/DT expected/VBN rewards/NNS are/VBP perturbed/VBN away/RB from/IN the/DT linear/JJ subspace/NN determined/VBN by/IN the/DT arms/NNS features/NNS ./.
Although/IN OFUL/NNP has/VBZ recently/RB been/VBN shown/VBN to/TO be/VB robust/JJ to/IN relatively/RB small/JJ deviations/NNS from/IN linearity/NN ,/, we/PRP show/VBP that/IN any/DT linear/JJ bandit/NN algorithm/NN that/WDT enjoys/VBZ optimal/JJ regret/NN performance/NN in/IN the/DT perfectly/RB linear/JJ setting/NN (/-LRB- e.g./FW ,/, OFUL/NN )/-RRB- must/MD suffer/VB linear/JJ regret/NN under/IN a/DT sparse/JJ additive/JJ perturbation/NN of/IN the/DT linear/JJ model/NN ./.
In/IN an/DT attempt/NN to/TO overcome/VB this/DT negative/JJ result/NN ,/, we/PRP define/VBP a/DT natural/JJ class/NN of/IN bandit/NN models/NNS characterized/VBN by/IN a/DT non-sparse/JJ deviation/NN from/IN linearity/NN ./.
We/PRP argue/VBP that/IN the/DT OFUL/NNP algorithm/NN can/MD fail/VB to/TO achieve/VB sublinear/NN regret/NN even/RB under/IN models/NNS that/WDT have/VBP non-sparse/JJ deviation.We/NN finally/RB develop/VB a/DT novel/JJ bandit/NN algorithm/NN ,/, comprising/VBG a/DT hypothesis/NN test/NN for/IN linearity/NN followed/VBN by/IN a/DT decision/NN to/TO use/VB either/CC the/DT OFUL/NNP or/CC Upper/NNP Confidence/NN Bound/NN (/-LRB- UCB/NNP )/-RRB- algorithm/NN ./.
For/IN perfectly/RB linear/JJ bandit/NN models/NNS ,/, the/DT algorithm/NN provably/RB exhibits/VBZ OFULs/NNS favorable/JJ regret/NN performance/NN ,/, while/IN for/IN misspecified/VBN models/NNS satisfying/VBG the/DT non-sparse/JJ deviation/NN property/NN ,/, the/DT algorithm/NN avoids/VBZ the/DT linear/JJ regret/NN phenomenon/NN and/CC falls/VBZ back/RB on/IN UCBs/NNS sublinear/JJ regret/NN scaling/NN ./.
Numerical/NNP experiments/NNS on/IN synthetic/JJ data/NNS ,/, and/CC on/IN recommendation/NN data/NNS from/IN the/DT public/JJ Yahoo!/NNP Learning/NNP to/IN Rank/NNP Challenge/NNP dataset/NN ,/, empirically/RB support/VBP our/PRP$ findings/NNS ./.
