Several/JJ variants/NNS of/IN the/DT Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- architecture/NN for/IN recurrent/JJ neural/JJ networks/NNS have/VBP been/VBN proposed/VBN since/IN its/PRP$ inception/NN in/IN 1995/CD ./.
In/IN recent/JJ years/NNS ,/, these/DT networks/NNS have/VBP become/VBN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN models/NNS for/IN a/DT variety/NN of/IN machine/NN learning/NN problems/NNS ./.
This/DT has/VBZ led/VBN to/IN a/DT renewed/VBN interest/NN in/IN understanding/VBG the/DT role/NN and/CC utility/NN of/IN various/JJ computational/JJ components/NNS of/IN typical/JJ LSTM/NN variants/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP the/DT first/JJ large/JJ -/HYPH scale/NN analysis/NN of/IN eight/CD LSTM/NN variants/NNS on/IN three/CD representative/JJ tasks/NNS :/: speech/NN recognition/NN ,/, handwriting/NN recognition/NN ,/, and/CC polyphonic/JJ music/NN modeling/NN ./.
The/DT hyperparameters/NNS of/IN all/DT LSTM/NN variants/NNS for/IN each/DT task/NN were/VBD optimized/VBN separately/RB using/VBG random/JJ search/NN and/CC their/PRP$ importance/NN was/VBD assessed/VBN using/VBG the/DT powerful/JJ fANOVA/NN framework/NN ./.
In/IN total/JJ ,/, we/PRP summarize/VBP the/DT results/NNS of/IN 5400/CD experimental/JJ runs/NNS (/-LRB- about/RB 15/CD years/NNS of/IN CPU/NN time/NN )/-RRB- ,/, which/WDT makes/VBZ our/PRP$ study/NN the/DT largest/JJS of/IN its/PRP$ kind/NN on/RB LSTM/NNP networks/NNS ./.
Our/PRP$ results/NNS show/VBP that/IN none/NN of/IN the/DT variants/NNS can/MD improve/VB upon/IN the/DT standard/JJ LSTM/NNP architecture/NN significantly/RB ,/, and/CC demonstrate/VBP the/DT forget/VB gate/NN and/CC the/DT output/NN activation/NN function/NN to/TO be/VB its/PRP$ most/RBS critical/JJ components/NNS ./.
We/PRP further/RB observe/VBP that/IN the/DT studied/VBN hyperparameters/NNS are/VBP virtually/RB independent/JJ and/CC derive/VBP guidelines/NNS for/IN their/PRP$ efficient/JJ adjustment/NN ./.
