A/DT number/NN of/IN recent/JJ works/NNS have/VBP proposed/VBN attention/NN models/NNS for/IN Visual/JJ Question/NN Answering/VBG (/-LRB- VQA/NN )/-RRB- that/WDT generate/VBP spatial/JJ maps/NNS highlighting/VBG image/NN regions/NNS relevant/JJ to/IN answering/VBG the/DT question/NN ./.
In/IN this/DT paper/NN ,/, we/PRP argue/VBP that/IN in/IN addition/NN to/IN modeling/NN "/'' where/WRB to/TO look/VB "/'' or/CC visual/JJ attention/NN ,/, it/PRP is/VBZ equally/RB important/JJ to/TO model/VB "/`` what/WP words/NNS to/TO listen/VB to/IN "/'' or/CC question/NN attention/NN ./.
We/PRP present/VBP a/DT novel/JJ co-attention/NN model/NN for/IN VQA/NN that/WDT jointly/RB reasons/NNS about/IN image/NN and/CC question/NN attention/NN ./.
In/IN addition/NN ,/, our/PRP$ model/NN reasons/NNS about/IN the/DT question/NN and/CC consequently/RB the/DT image/NN via/IN the/DT co-attention/NN mechanism/NN in/IN a/DT hierarchical/JJ fashion/NN via/IN a/DT novel/JJ 1/CD -/HYPH dimensional/JJ convolution/NN neural/JJ networks/NNS (/-LRB- CNN/NNP )/-RRB- model/NN ./.
Our/PRP$ final/JJ model/NN outperforms/VBZ all/DT reported/VBN methods/NNS ,/, improving/VBG the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN on/IN the/DT VQA/NNP dataset/NN from/IN 60.4/CD percent/NN to/IN 62.1/CD percent/NN ,/, and/CC from/IN 61.6/CD percent/NN to/IN 65.4/CD percent/NN on/IN the/DT COCO/NN -/HYPH QA/NN dataset/NN ./.
