We/PRP present/VBP a/DT unified/VBN approach/NN for/IN learning/VBG the/DT parameters/NNS of/IN Sum/NNP -/HYPH Product/NNP networks/NNS (/-LRB- SPNs/NNS )/-RRB- ./.
We/PRP prove/VBP that/IN any/DT complete/JJ and/CC decomposable/JJ SPN/NN is/VBZ equivalent/JJ to/IN a/DT mixture/NN of/IN trees/NNS where/WRB each/DT tree/NN corresponds/VBZ to/IN a/DT product/NN of/IN univariate/JJ distributions/NNS ./.
Based/VBN on/IN the/DT mixture/NN model/NN perspective/NN ,/, we/PRP characterize/VBP the/DT objective/JJ function/NN when/WRB learning/VBG SPNs/NNS based/VBN on/IN the/DT maximum/JJ likelihood/NN estimation/NN (/-LRB- MLE/NNP )/-RRB- principle/NN and/CC show/VBP that/IN the/DT optimization/NN problem/NN can/MD be/VB formulated/VBN as/IN a/DT signomial/JJ program/NN ./.
Both/CC the/DT projected/VBN gradient/NN descent/NN (/-LRB- PGD/NN )/-RRB- and/CC the/DT exponentiated/VBN gradient/NN (/-LRB- EG/NN )/-RRB- in/IN this/DT setting/NN can/MD be/VB viewed/VBN as/IN first/JJ order/NN approximations/NNS of/IN the/DT signomial/JJ program/NN after/IN proper/JJ transformation/NN of/IN the/DT objective/JJ function/NN ./.
Based/VBN on/IN the/DT signomial/JJ program/NN formulation/NN ,/, we/PRP construct/VBP two/CD parameter/NN learning/NN algorithms/NNS for/IN SPNs/NNS by/IN using/VBG sequential/JJ monomial/JJ approximations/NNS (/-LRB- SMA/NN )/-RRB- and/CC the/DT concave/NN -/HYPH convex/NN procedure/NN (/-LRB- CCCP/NN )/-RRB- ,/, respectively/RB ./.
The/DT two/CD proposed/VBN methods/NNS naturally/RB admit/VBP multiplicative/JJ updates/NNS ,/, hence/RB effectively/RB avoiding/VBG the/DT projection/NN operation/NN ./.
With/IN the/DT help/NN of/IN the/DT a/DT unified/VBN framework/NN ,/, we/PRP also/RB show/VBP an/DT intrinsic/JJ connection/NN between/IN CCCP/NN and/CC Expectation/NN Maximization/NN (/-LRB- EM/NN )/-RRB- ,/, where/WRB EM/NNP turns/VBZ out/RP to/TO be/VB another/DT relaxation/NN of/IN the/DT signomial/JJ program/NN ./.
Extensive/JJ experiments/NNS on/IN 20/CD data/NNS sets/NNS demonstrate/VBP the/DT effectiveness/NN and/CC efficiency/NN of/IN the/DT two/CD proposed/VBN approaches/NNS for/IN learning/VBG SPNs/NNS ./.
We/PRP also/RB show/VBP that/IN the/DT proposed/VBN methods/NNS can/MD improve/VB the/DT performance/NN of/IN structure/NN learning/NN and/CC yield/NN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS ./.
