We/PRP describe/VBP and/CC study/VBP a/DT model/NN for/IN an/DT Automated/NNP Online/NNP Recommendation/NNP System/NNP (/-LRB- AORS/NNP )/-RRB- in/IN which/WDT a/DT user/NN 's/POS preferences/NNS can/MD be/VB time/NN -/HYPH dependent/JJ and/CC can/MD also/RB depend/VB on/IN the/DT history/NN of/IN past/JJ recommendations/NNS and/CC play/VB -/HYPH outs/NNS ./.
The/DT three/CD key/JJ features/NNS of/IN the/DT model/NN that/WDT makes/VBZ it/PRP more/RBR realistic/JJ compared/VBN to/IN existing/VBG models/NNS for/IN recommendation/NN systems/NNS are/VBP (/-LRB- 1/CD )/-RRB- user/NN preference/NN is/VBZ inherently/RB latent/JJ ,/, (/-LRB- 2/LS )/-RRB- current/JJ recommendations/NNS can/MD affect/VB future/JJ preferences/NNS ,/, and/CC (/-LRB- 3/LS )/-RRB- it/PRP allows/VBZ for/IN the/DT development/NN of/IN learning/VBG algorithms/NNS with/IN provable/JJ performance/NN guarantees/NNS ./.
The/DT problem/NN is/VBZ cast/VBN as/IN an/DT average/JJ -/HYPH cost/NN restless/JJ multi-armed/JJ bandit/NN for/IN a/DT given/VBN user/NN ,/, with/IN an/DT independent/JJ partially/RB observable/JJ Markov/NNP decision/NN process/NN (/-LRB- POMDP/NN )/-RRB- for/IN each/DT item/NN of/IN content/NN ./.
We/PRP analyze/VBP the/DT POMDP/NN for/IN a/DT single/JJ arm/NN ,/, describe/VB its/PRP$ structural/JJ properties/NNS ,/, and/CC characterize/VB its/PRP$ optimal/JJ policy/NN ./.
We/PRP then/RB develop/VB a/DT Thompson/NNP sampling/NN -/HYPH based/VBN online/JJ reinforcement/NN learning/VBG algorithm/NN to/TO learn/VB the/DT parameters/NNS of/IN the/DT model/NN and/CC optimize/NN utility/NN from/IN the/DT binary/JJ responses/NNS of/IN the/DT users/NNS to/IN continuous/JJ recommendations/NNS ./.
We/PRP then/RB analyze/VB the/DT performance/NN of/IN the/DT learning/NN algorithm/NN and/CC characterize/VB the/DT regret/NN ./.
Illustrative/JJ numerical/JJ results/NNS and/CC directions/NNS for/IN extension/NN to/IN the/DT restless/JJ hidden/JJ Markov/NNP multi-armed/JJ bandit/NN problem/NN are/VBP also/RB presented/VBN ./.
