Attention/NN mechanisms/NNS in/IN neural/JJ networks/NNS have/VBP proved/VBN useful/JJ for/IN problems/NNS in/IN which/WDT the/DT input/NN and/CC output/NN do/VBP not/RB have/VB fixed/VBN dimension/NN ./.
Often/RB there/RB exist/VBP features/NNS that/WDT are/VBP locally/RB translation/NN invariant/JJ and/CC would/MD be/VB valuable/JJ for/IN directing/VBG the/DT model/NN 's/POS attention/NN ,/, but/CC previous/JJ attentional/JJ architectures/NNS are/VBP not/RB constructed/VBN to/TO learn/VB such/JJ features/NNS specifically/RB ./.
We/PRP introduce/VBP an/DT attentional/JJ neural/JJ network/NN that/WDT employs/VBZ convolution/NN on/IN the/DT input/NN tokens/NNS to/TO detect/VB local/JJ time/NN -/HYPH invariant/JJ and/CC long/JJ -/HYPH range/NN topical/JJ attention/NN features/NNS in/IN a/DT context/NN -/HYPH dependent/JJ way/NN ./.
We/PRP apply/VBP this/DT architecture/NN to/IN the/DT problem/NN of/IN extreme/JJ summarization/NN of/IN source/NN code/NN snippets/NNS into/IN short/JJ ,/, descriptive/JJ function/NN name/NN -/HYPH like/JJ summaries/NNS ./.
Using/VBG those/DT features/NNS ,/, the/DT model/NN sequentially/RB generates/VBZ a/DT summary/NN by/IN marginalizing/VBG over/IN two/CD attention/NN mechanisms/NNS :/: one/CD that/WDT predicts/VBZ the/DT next/JJ summary/NN token/NN based/VBN on/IN the/DT attention/NN weights/NNS of/IN the/DT input/NN tokens/NNS and/CC another/DT that/WDT is/VBZ able/JJ to/TO copy/VB a/DT code/NN token/NN as/IN -/HYPH is/VBZ directly/RB into/IN the/DT summary/NN ./.
We/PRP demonstrate/VBP our/PRP$ convolutional/JJ attention/NN neural/JJ network/NN 's/POS performance/NN on/IN 10/CD popular/JJ Java/NNP projects/NNS showing/VBG that/IN it/PRP achieves/VBZ better/JJR performance/NN compared/VBN to/IN previous/JJ attentional/JJ mechanisms/NNS ./.
