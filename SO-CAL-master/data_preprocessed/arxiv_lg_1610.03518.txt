Developing/VBG control/NN policies/NNS in/IN simulation/NN is/VBZ often/RB more/RBR practical/JJ and/CC safer/JJR than/IN directly/RB running/VBG experiments/NNS in/IN the/DT real/JJ world/NN ./.
This/DT applies/VBZ to/IN policies/NNS obtained/VBN from/IN planning/NN and/CC optimization/NN ,/, and/CC even/RB more/RBR so/RB to/IN policies/NNS obtained/VBN from/IN reinforcement/NN learning/NN ,/, which/WDT is/VBZ often/RB very/RB data/NNS demanding/VBG ./.
However/RB ,/, a/DT policy/NN that/WDT succeeds/VBZ in/IN simulation/NN often/RB does/VBZ n't/RB work/VB when/WRB deployed/VBN on/IN a/DT real/JJ robot/NN ./.
Nevertheless/RB ,/, often/RB the/DT overall/JJ gist/NN of/IN what/WP the/DT policy/NN does/VBZ in/IN simulation/NN remains/VBZ valid/JJ in/IN the/DT real/JJ world/NN ./.
In/IN this/DT paper/NN we/PRP investigate/VBP such/JJ settings/NNS ,/, where/WRB the/DT sequence/NN of/IN states/NNS traversed/VBN in/IN simulation/NN remains/VBZ reasonable/JJ for/IN the/DT real/JJ world/NN ,/, even/RB if/IN the/DT details/NNS of/IN the/DT controls/NNS are/VBP not/RB ,/, as/RB could/MD be/VB the/DT case/NN when/WRB the/DT key/JJ differences/NNS lie/VBP in/IN detailed/JJ friction/NN ,/, contact/NN ,/, mass/NN and/CC geometry/NN properties/NNS ./.
During/IN execution/NN ,/, at/IN each/DT time/NN step/VB our/PRP$ approach/NN computes/VBZ what/WP the/DT simulation/NN -/HYPH based/VBN control/NN policy/NN would/MD do/VB ,/, but/CC then/RB ,/, rather/RB than/IN executing/VBG these/DT controls/NNS on/IN the/DT real/JJ robot/NN ,/, our/PRP$ approach/NN computes/VBZ what/WP the/DT simulation/NN expects/VBZ the/DT resulting/VBG next/JJ state/NN (/-LRB- s/AFX )/-RRB- will/MD be/VB ,/, and/CC then/RB relies/VBZ on/IN a/DT learned/VBN deep/JJ inverse/JJ dynamics/NNS model/NN to/TO decide/VB which/WDT real/RB -/HYPH world/NN action/NN is/VBZ most/RBS suitable/JJ to/TO achieve/VB those/DT next/JJ states/NNS ./.
Deep/JJ models/NNS are/VBP only/RB as/RB good/JJ as/IN their/PRP$ training/NN data/NNS ,/, and/CC we/PRP also/RB propose/VBP an/DT approach/NN for/IN data/NNS collection/NN to/IN (/-LRB- incrementally/RB )/-RRB- learn/VB the/DT deep/JJ inverse/JJ dynamics/NNS model/NN ./.
Our/PRP$ experiments/NNS shows/VBZ our/PRP$ approach/NN compares/VBZ favorably/RB with/IN various/JJ baselines/NNS that/WDT have/VBP been/VBN developed/VBN for/IN dealing/VBG with/IN simulation/NN to/IN real/JJ world/NN model/NN discrepancy/NN ,/, including/VBG output/NN error/NN control/NN and/CC Gaussian/JJ dynamics/NNS adaptation/NN ./.
