The/DT sparsity/NN of/IN natural/JJ signals/NNS in/IN a/DT transform/VB domain/NN or/CC dictionary/NN has/VBZ been/VBN extensively/RB exploited/VBN in/IN several/JJ applications/NNS ./.
More/RBR recently/RB ,/, the/DT data/NN -/HYPH driven/VBN adaptation/NN of/IN synthesis/NN dictionaries/NNS has/VBZ shown/VBN promise/NN in/IN many/JJ applications/NNS compared/VBN to/IN fixed/VBN or/CC analytical/JJ dictionaries/NNS ./.
However/RB ,/, dictionary/NN learning/NN problems/NNS are/VBP typically/RB non-convex/JJ and/CC NP/NNP -/HYPH hard/JJ ,/, and/CC the/DT alternating/VBG minimization/NN approaches/NNS usually/RB adopted/VBN to/TO solve/VB these/DT problems/NNS are/VBP often/RB computationally/RB expensive/JJ ,/, with/IN the/DT computations/NNS dominated/VBN by/IN the/DT NP/NNP -/HYPH hard/JJ synthesis/NN sparse/JJ coding/NN step/NN ./.
In/IN this/DT work/NN ,/, we/PRP investigate/VBP an/DT efficient/JJ method/NN for/IN dictionary/NN learning/NN by/IN first/JJ decomposing/VBG the/DT training/NN data/NNS set/VBN into/IN a/DT sum/NN of/IN sparse/JJ rank/NN -/HYPH one/CD matrices/NNS and/CC then/RB using/VBG a/DT block/NN coordinate/NN descent/NN approach/NN to/TO estimate/VB the/DT rank/NN -/HYPH one/CD terms/NNS ./.
The/DT proposed/VBN algorithm/NN involves/VBZ efficient/JJ closed/JJ -/HYPH form/NN solutions/NNS ./.
In/IN particular/JJ ,/, the/DT sparse/JJ coding/NN step/NN involves/VBZ a/DT simple/JJ form/NN of/IN thresholding/NN ./.
We/PRP provide/VBP a/DT convergence/NN analysis/NN for/IN the/DT proposed/VBN block/NN coordinate/NN descent/NN method/NN that/WDT solves/VBZ a/DT highly/RB non-convex/JJ problem/NN ./.
Our/PRP$ experiments/NNS show/VBP the/DT promising/JJ performance/NN and/CC significant/JJ speed/NN -/HYPH ups/NNS provided/VBN by/IN our/PRP$ method/NN over/IN the/DT classical/JJ K/NN -/HYPH SVD/NN scheme/NN in/IN sparse/JJ signal/NN representation/NN and/CC image/NN denoising/NN ./.
