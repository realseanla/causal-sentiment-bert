The/DT use/NN of/IN M/NN -/HYPH estimators/NNS in/IN generalized/VBN linear/JJ regression/NN models/NNS in/IN high/JJ dimensional/JJ settings/NNS requires/VBZ risk/NN minimization/NN with/IN hard/JJ $/$ L_0/CD $/$ constraints/NNS ./.
Of/IN the/DT known/VBN methods/NNS ,/, the/DT class/NN of/IN projected/VBN gradient/NN descent/NN (/-LRB- also/RB known/VBN as/IN iterative/JJ hard/JJ thresholding/NN (/-LRB- IHT/NN )/-RRB- )/-RRB- methods/NNS is/VBZ known/VBN to/TO offer/VB the/DT fastest/JJS and/CC most/RBS scalable/JJ solutions/NNS ./.
However/RB ,/, the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN is/VBZ only/RB able/JJ to/TO analyze/VB these/DT methods/NNS in/IN extremely/RB restrictive/JJ settings/NNS which/WDT do/VBP not/RB hold/VB in/RP high/JJ dimensional/JJ statistical/JJ models/NNS ./.
In/IN this/DT work/NN we/PRP bridge/VBP this/DT gap/NN by/IN providing/VBG the/DT first/JJ analysis/NN for/IN IHT/NNP -/HYPH style/NN methods/NNS in/IN the/DT high/JJ dimensional/JJ statistical/JJ setting/NN ./.
Our/PRP$ bounds/NNS are/VBP tight/JJ and/CC match/NN known/VBN minimax/NN lower/JJR bounds/NNS ./.
Our/PRP$ results/NNS rely/VBP on/IN a/DT general/JJ analysis/NN framework/NN that/WDT enables/VBZ us/PRP to/TO analyze/VB several/JJ popular/JJ hard/JJ thresholding/NN style/NN algorithms/NNS (/-LRB- such/JJ as/IN HTP/NNP ,/, CoSaMP/NNP ,/, SP/NN )/-RRB- in/IN the/DT high/JJ dimensional/JJ regression/NN setting/NN ./.
We/PRP also/RB extend/VBP our/PRP$ analysis/NN to/IN large/JJ family/NN of/IN "/`` fully/RB corrective/JJ methods/NNS "/'' that/WDT includes/VBZ two/CD -/HYPH stage/NN and/CC partial/JJ hard/JJ -/HYPH thresholding/NN algorithms/NNS ./.
We/PRP show/VBP that/IN our/PRP$ results/NNS hold/VBP for/IN the/DT problem/NN of/IN sparse/JJ regression/NN ,/, as/RB well/RB as/IN low/JJ -/HYPH rank/NN matrix/NN recovery/NN ./.
