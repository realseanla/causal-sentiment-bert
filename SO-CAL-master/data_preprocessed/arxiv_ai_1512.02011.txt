Using/VBG deep/JJ neural/JJ nets/NNS as/IN function/NN approximator/NN for/IN reinforcement/NN learning/VBG tasks/NNS have/VBP recently/RB been/VBN shown/VBN to/TO be/VB very/RB powerful/JJ for/IN solving/VBG problems/NNS approaching/VBG real/JJ -/HYPH world/NN complexity/NN ./.
Using/VBG these/DT results/NNS as/IN a/DT benchmark/NN ,/, we/PRP discuss/VBP the/DT role/NN that/IN the/DT discount/NN factor/NN may/MD play/VB in/IN the/DT quality/NN of/IN the/DT learning/NN process/NN of/IN a/DT deep/JJ Q/NN -/HYPH network/NN (/-LRB- DQN/NN )/-RRB- ./.
When/WRB the/DT discount/NN factor/NN progressively/RB increases/VBZ up/RP to/IN its/PRP$ final/JJ value/NN ,/, we/PRP empirically/RB show/VBP that/IN it/PRP is/VBZ possible/JJ to/TO significantly/RB reduce/VB the/DT number/NN of/IN learning/VBG steps/NNS ./.
When/WRB used/VBN in/IN conjunction/NN with/IN a/DT varying/VBG learning/NN rate/NN ,/, we/PRP empirically/RB show/VBP that/IN it/PRP outperforms/VBZ original/JJ DQN/NNP on/IN several/JJ experiments/NNS ./.
We/PRP relate/VBP this/DT phenomenon/NN with/IN the/DT instabilities/NNS of/IN neural/JJ networks/NNS when/WRB they/PRP are/VBP used/VBN in/IN an/DT approximate/JJ Dynamic/JJ Programming/NN setting/NN ./.
We/PRP also/RB describe/VBP the/DT possibility/NN to/TO fall/VB within/IN a/DT local/JJ optimum/JJ during/IN the/DT learning/NN process/NN ,/, thus/RB connecting/VBG our/PRP$ discussion/NN with/IN the/DT exploration/NN //HYPH exploitation/NN dilemma/NN ./.
