We/PRP propose/VBP a/DT multimodal/JJ deep/JJ learning/NN framework/NN that/WDT can/MD transfer/VB the/DT knowledge/NN obtained/VBN from/IN a/DT single/JJ -/HYPH modal/JJ neural/JJ network/NN to/IN a/DT network/NN with/IN a/DT different/JJ modality/NN ./.
For/IN instance/NN ,/, we/PRP show/VBP that/IN we/PRP can/MD leverage/VB the/DT speech/NN data/NNS to/IN fine/JJ -/HYPH tune/NN the/DT network/NN trained/VBN for/IN video/NN recognition/NN ,/, given/VBN an/DT initial/JJ set/NN of/IN audio/JJ -/HYPH video/NN parallel/JJ dataset/NN within/IN the/DT same/JJ semantics/NNS ./.
Our/PRP$ approach/NN learns/VBZ the/DT analogy/NN -/HYPH preserving/VBG embeddings/NNS between/IN the/DT abstract/JJ representations/NNS learned/VBN from/IN each/DT network/NN ,/, allowing/VBG for/IN semantics/NNS -/HYPH level/NN transfer/NN or/CC reconstruction/NN of/IN the/DT data/NNS among/IN different/JJ modalities/NNS ./.
Our/PRP$ method/NN is/VBZ thus/RB specifically/RB useful/JJ when/WRB one/CD of/IN the/DT modalities/NNS is/VBZ more/RBR scarce/JJ in/IN labeled/VBN data/NNS than/IN other/JJ modalities/NNS ./.
While/IN we/PRP mainly/RB focus/VBP on/IN applying/VBG transfer/NN learning/NN on/IN the/DT audio/NN -/HYPH visual/JJ recognition/NN task/NN as/IN an/DT application/NN of/IN our/PRP$ approach/NN ,/, our/PRP$ framework/NN is/VBZ flexible/JJ and/CC thus/RB can/MD work/VB with/IN any/DT multimodal/JJ datasets/NNS ./.
In/IN this/DT work/NN -/HYPH in/IN -/HYPH progress/NN report/NN ,/, we/PRP show/VBP our/PRP$ preliminary/JJ results/NNS on/IN the/DT AV/NN -/HYPH Letters/NNPS dataset/NN ./.
