A/DT key/JJ problem/NN in/IN structured/JJ output/NN prediction/NN is/VBZ direct/JJ optimization/NN of/IN the/DT task/NN reward/NN function/NN that/WDT matters/VBZ for/IN test/NN evaluation/NN ./.
This/DT paper/NN presents/VBZ a/DT simple/JJ and/CC computationally/RB efficient/JJ approach/NN to/TO incorporate/VB task/NN reward/NN into/IN a/DT maximum/JJ likelihood/NN framework/NN ./.
We/PRP establish/VBP a/DT connection/NN between/IN the/DT log/NN -/HYPH likelihood/NN and/CC regularized/VBN expected/VBN reward/NN objectives/NNS ,/, showing/VBG that/IN at/IN a/DT zero/CD temperature/NN ,/, they/PRP are/VBP approximately/RB equivalent/JJ in/IN the/DT vicinity/NN of/IN the/DT optimal/JJ solution/NN ./.
We/PRP show/VBP that/IN optimal/JJ regularized/VBN expected/VBN reward/NN is/VBZ achieved/VBN when/WRB the/DT conditional/JJ distribution/NN of/IN the/DT outputs/NNS given/VBN the/DT inputs/NNS is/VBZ proportional/JJ to/IN their/PRP$ exponentiated/VBN (/-LRB- temperature/NN adjusted/VBN )/-RRB- rewards/NNS ./.
Based/VBN on/IN this/DT observation/NN ,/, we/PRP optimize/VBP conditional/JJ log/NN -/HYPH probability/NN of/IN edited/VBN outputs/NNS that/WDT are/VBP sampled/VBN proportionally/RB to/IN their/PRP$ scaled/VBN exponentiated/VBN reward/NN ./.
We/PRP apply/VBP this/DT framework/NN to/TO optimize/VB edit/NN distance/NN in/IN the/DT output/NN label/NN space/NN ./.
Experiments/NNS on/IN speech/NN recognition/NN and/CC machine/NN translation/NN for/IN neural/JJ sequence/NN to/IN sequence/NN models/NNS show/VBP notable/JJ improvements/NNS over/IN a/DT maximum/JJ likelihood/NN baseline/NN by/IN using/VBG edit/NN distance/NN augmented/VBN maximum/JJ likelihood/NN ./.
