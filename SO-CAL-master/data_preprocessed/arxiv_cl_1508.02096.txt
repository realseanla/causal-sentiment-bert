We/PRP introduce/VBP a/DT model/NN for/IN constructing/VBG vector/NN representations/NNS of/IN words/NNS by/IN composing/VBG characters/NNS using/VBG bidirectional/JJ LSTMs/NNS ./.
Relative/JJ to/IN traditional/JJ word/NN representation/NN models/NNS that/WDT have/VBP independent/JJ vectors/NNS for/IN each/DT word/NN type/NN ,/, our/PRP$ model/NN requires/VBZ only/RB a/DT single/JJ vector/NN per/IN character/NN type/NN and/CC a/DT fixed/VBN set/NN of/IN parameters/NNS for/IN the/DT compositional/JJ model/NN ./.
Despite/IN the/DT compactness/NN of/IN this/DT model/NN and/CC ,/, more/RBR importantly/RB ,/, the/DT arbitrary/JJ nature/NN of/IN the/DT form/NN -/HYPH function/NN relationship/NN in/IN language/NN ,/, our/PRP$ "/`` composed/VBN "/'' word/NN representations/NNS yield/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN language/NN modeling/NN and/CC part/NN -/HYPH of/IN -/HYPH speech/NN tagging/NN ./.
Benefits/NNS over/IN traditional/JJ baselines/NNS are/VBP particularly/RB pronounced/VBN in/IN morphologically/RB rich/JJ languages/NNS (/-LRB- e.g./FW ,/, Turkish/JJ )/-RRB- ./.
