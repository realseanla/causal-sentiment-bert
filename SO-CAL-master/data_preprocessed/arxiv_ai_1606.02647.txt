In/IN this/DT work/NN ,/, we/PRP take/VBP a/DT fresh/JJ look/NN at/IN some/DT old/JJ and/CC new/JJ algorithms/NNS for/IN off/RB -/HYPH policy/NN ,/, return/NN -/HYPH based/VBN reinforcement/NN learning/NN ./.
Expressing/VBG these/DT in/IN a/DT common/JJ form/NN ,/, we/PRP derive/VBP a/DT novel/JJ algorithm/NN ,/, Retrace/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- ,/, with/IN three/CD desired/VBN properties/NNS :/: (/-LRB- 1/LS )/-RRB- low/JJ variance/NN ;/: (/-LRB- 2/LS )/-RRB- safety/NN ,/, as/IN it/PRP safely/RB uses/VBZ samples/NNS collected/VBN from/IN any/DT behaviour/NN policy/NN ,/, whatever/WDT its/PRP$ degree/NN of/IN "/`` off/RB -/HYPH policyness/NN "/'' ;/: and/CC (/-LRB- 3/LS )/-RRB- efficiency/NN ,/, as/IN it/PRP makes/VBZ the/DT best/JJS use/NN of/IN samples/NNS collected/VBN from/IN near/RB on/IN -/HYPH policy/NN behaviour/NN policies/NNS ./.
We/PRP analyse/VBP the/DT contractive/JJ nature/NN of/IN the/DT related/JJ operator/NN under/IN both/DT off/IN -/HYPH policy/NN policy/NN evaluation/NN and/CC control/NN settings/NNS and/CC derive/VBP online/JJ sample/NN -/HYPH based/VBN algorithms/NNS ./.
To/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ return/NN -/HYPH based/VBN off/IN -/HYPH policy/NN control/NN algorithm/NN converging/VBG a.s./NN to/IN $/$ Q/NN ^/SYM */NFP $/$ without/IN the/DT GLIE/NNP assumption/NN (/-LRB- Greedy/NNP in/IN the/DT Limit/NN with/IN Infinite/JJ Exploration/NN )/-RRB- ./.
As/IN a/DT corollary/NN ,/, we/PRP prove/VBP the/DT convergence/NN of/IN Watkins/NNP '/POS Q/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- ,/, which/WDT was/VBD still/RB an/DT open/JJ problem/NN ./.
We/PRP illustrate/VBP the/DT benefits/NNS of/IN Retrace/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- on/IN a/DT standard/JJ suite/NN of/IN Atari/NNP 2600/CD games/NNS ./.
