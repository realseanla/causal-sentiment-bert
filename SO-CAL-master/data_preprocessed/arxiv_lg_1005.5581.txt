Many/JJ classical/JJ bounds/NNS on/IN the/DT sample/NN complexity/NN of/IN active/JJ learning/NN based/VBN on/IN the/DT realizability/NN assumption/NN have/VBP been/VBN derived/VBN ,/, which/WDT show/VBP that/IN active/JJ learning/NN can/MD exponentially/RB improve/VB the/DT sample/NN complexity/NN over/IN passive/JJ learning/NN ./.
However/RB ,/, this/DT realizability/NN assumption/NN could/MD not/RB be/VB met/VBN in/IN practice/NN and/CC few/JJ results/NNS on/IN the/DT exponential/JJ improvement/NN in/IN the/DT sample/NN complexity/NN in/IN the/DT non-realizable/JJ case/NN has/VBZ been/VBN obtained/VBN ./.
In/IN this/DT paper/NN ,/, we/PRP theoretically/RB characterize/VBP the/DT sample/NN complexity/NN of/IN active/JJ learning/NN in/IN the/DT non-realizable/JJ case/NN with/IN Tsybakov/NNP noise/NN condition/NN under/IN the/DT multi-view/JJ setting/NN ./.
We/PRP prove/VBP that/IN the/DT sample/NN complexity/NN of/IN active/JJ learning/NN with/IN unbounded/JJ Tsybakov/NNP noise/NN can/MD be/VB $/$ \/CD widetilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM log/NN \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN }/-RRB- )/-RRB- $/$ ,/, contrasting/VBG to/IN that/DT polynomial/JJ improvement/NN is/VBZ the/DT best/JJS possible/JJ achievement/NN with/IN the/DT same/JJ noise/NN condition/NN in/IN single/JJ -/HYPH view/NN setting/NN ./.
We/PRP also/RB prove/VBP that/IN ,/, contrasting/VBG to/IN that/DT in/IN previous/JJ polynomial/JJ bounds/NNS the/DT order/NN of/IN $/$ 1/CD //SYM \/SYM epsilon/SYM $/$ is/VBZ related/VBN to/IN the/DT Tsybakov/NNP noise/NN condition/NN ,/, in/IN general/JJ multi-view/NN setting/VBG the/DT sample/NN complexity/NN of/IN active/JJ learning/NN with/IN unbounded/JJ Tsybakov/NNP noise/NN is/VBZ $/$ \/CD widetilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN }/-RRB- )/-RRB- $/$ ,/, where/WRB the/DT order/NN of/IN $/$ 1/CD //SYM \/SYM epsilon/SYM $/$ is/VBZ independent/JJ of/IN the/DT Tsybakov/NNP noise/NN condition/NN ./.
