We/PRP present/VBP a/DT Character/NN -/HYPH Word/NN Long/NNP Short/NNP -/HYPH Term/NNP Memory/NNP Language/NNP Model/NNP which/WDT both/DT reduces/VBZ the/DT perplexity/NN with/IN respect/NN to/IN a/DT baseline/NN word/NN -/HYPH level/NN language/NN model/NN and/CC reduces/VBZ the/DT number/NN of/IN parameters/NNS of/IN the/DT model/NN ./.
Character/NN information/NN can/MD reveal/VB structural/JJ (/-LRB- dis/FW )/-RRB- similarities/NNS between/IN words/NNS and/CC can/MD even/RB be/VB used/VBN when/WRB a/DT word/NN is/VBZ out/RB -/HYPH of/IN -/HYPH vocabulary/NN ,/, thus/RB improving/VBG the/DT modeling/NN of/IN infrequent/JJ and/CC unknown/JJ words/NNS ./.
By/IN concatenating/VBG word/NN and/CC character/NN embeddings/NNS ,/, we/PRP achieve/VBP up/RP to/IN 2.77/CD percent/NN relative/JJ improvement/NN on/IN English/NNP compared/VBN to/IN a/DT baseline/NN model/NN with/IN a/DT similar/JJ amount/NN of/IN parameters/NNS and/CC 4.57/CD percent/NN on/IN Dutch/NNP ./.
Moreover/RB ,/, we/PRP also/RB outperform/VBP baseline/JJ word/NN -/HYPH level/NN models/NNS with/IN a/DT larger/JJR number/NN of/IN parameters/NNS ./.
