In/IN this/DT paper/NN we/PRP propose/VBP a/DT data/NN intensive/JJ approach/NN for/IN inferring/VBG sentence/NN -/HYPH internal/JJ temporal/JJ relations/NNS ./.
Temporal/JJ inference/NN is/VBZ relevant/JJ for/IN practical/JJ NLP/NN applications/NNS which/WDT either/CC extract/NN or/CC synthesize/VBP temporal/JJ information/NN (/-LRB- e.g./FW ,/, summarisation/NN ,/, question/NN answering/NN )/-RRB- ./.
Our/PRP$ method/NN bypasses/VBZ the/DT need/NN for/IN manual/JJ coding/NN by/IN exploiting/VBG the/DT presence/NN of/IN markers/NNS like/IN after/IN "/`` ,/, which/WDT overtly/RB signal/VBP a/DT temporal/JJ relation/NN ./.
We/PRP first/RB show/VBP that/IN models/NNS trained/VBN on/IN main/JJ and/CC subordinate/JJ clauses/NNS connected/VBN with/IN a/DT temporal/JJ marker/NN achieve/VB good/JJ performance/NN on/IN a/DT pseudo-disambiguation/NN task/NN simulating/VBG temporal/JJ inference/NN (/-LRB- during/IN testing/VBG the/DT temporal/JJ marker/NN is/VBZ treated/VBN as/IN unseen/JJ and/CC the/DT models/NNS must/MD select/VB the/DT right/JJ marker/NN from/IN a/DT set/NN of/IN possible/JJ candidates/NNS )/-RRB- ./.
Secondly/RB ,/, we/PRP assess/VBP whether/IN the/DT proposed/VBN approach/NN holds/VBZ promise/NN for/IN the/DT semi-automatic/JJ creation/NN of/IN temporal/JJ annotations/NNS ./.
Specifically/RB ,/, we/PRP use/VBP a/DT model/NN trained/VBN on/IN noisy/JJ and/CC approximate/JJ data/NNS (/-LRB- i.e./FW ,/, main/JJ and/CC subordinate/JJ clauses/NNS )/-RRB- to/TO predict/VB intra-sentential/JJ relations/NNS present/JJ in/IN TimeBank/NNP ,/, a/DT corpus/NN annotated/VBN rich/JJ temporal/JJ information/NN ./.
Our/PRP$ experiments/NNS compare/VBP and/CC contrast/NN several/JJ probabilistic/JJ models/NNS differing/VBG in/IN their/PRP$ feature/NN space/NN ,/, linguistic/JJ assumptions/NNS and/CC data/NNS requirements/NNS ./.
We/PRP evaluate/VBP performance/NN against/IN gold/NN standard/JJ corpora/NN and/CC also/RB against/IN human/JJ subjects/NNS ./.
