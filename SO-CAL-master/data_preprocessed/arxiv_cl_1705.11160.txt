In/IN the/DT past/JJ few/JJ years/NNS ,/, attention/NN mechanisms/NNS have/VBP become/VBN an/DT indispensable/JJ component/NN of/IN end/NN -/HYPH to/IN -/HYPH end/NN neural/JJ machine/NN translation/NN models/NNS ./.
However/RB ,/, previous/JJ attention/NN models/NNS always/RB refer/VBP to/IN some/DT source/NN words/NNS when/WRB predicting/VBG a/DT target/NN word/NN ,/, which/WDT contradicts/VBZ with/IN the/DT fact/NN that/IN some/DT target/NN words/NNS have/VBP no/DT corresponding/VBG source/NN words/NNS ./.
Motivated/VBN by/IN this/DT observation/NN ,/, we/PRP propose/VBP a/DT novel/JJ attention/NN model/NN that/WDT has/VBZ the/DT capability/NN of/IN determining/VBG when/WRB a/DT decoder/NN should/MD attend/VB to/IN source/NN words/NNS and/CC when/WRB it/PRP should/MD not/RB ./.
Experimental/JJ results/NNS on/IN NIST/NNP Chinese/NNP -/HYPH English/NNP translation/NN tasks/NNS show/VBP that/IN the/DT new/JJ model/NN achieves/VBZ an/DT improvement/NN of/IN 0.8/CD BLEU/NN score/NN over/IN a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baseline/NN ./.
