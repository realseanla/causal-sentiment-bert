We/PRP design/VBP a/DT new/JJ approach/NN that/WDT allows/VBZ robot/NN learning/NN of/IN new/JJ activities/NNS from/IN unlabeled/JJ human/JJ example/NN videos/NNS ./.
Given/VBN videos/NNS of/IN humans/NNS executing/VBG the/DT same/JJ activity/NN from/IN a/DT human/NN 's/POS viewpoint/NN (/-LRB- i.e./FW ,/, first/RB -/HYPH person/NN videos/NNS )/-RRB- ,/, our/PRP$ objective/NN is/VBZ to/TO make/VB the/DT robot/NN learn/VB the/DT temporal/JJ structure/NN of/IN the/DT activity/NN as/IN its/PRP$ future/JJ regression/NN network/NN ,/, and/CC learn/VB to/TO transfer/VB such/JJ model/NN for/IN its/PRP$ own/JJ motor/NN execution/NN ./.
We/PRP present/VBP a/DT new/JJ deep/JJ learning/NN model/NN :/: We/PRP extend/VBP the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN convolutional/JJ object/NN detection/NN network/NN for/IN the/DT detection/NN of/IN human/JJ hands/NNS in/IN training/NN videos/NNS based/VBN on/IN image/NN information/NN ,/, and/CC newly/RB introduce/VB the/DT concept/NN of/IN using/VBG a/DT fully/RB convolutional/JJ network/NN to/TO regress/VB (/-LRB- i.e./FW ,/, predict/VBP )/-RRB- the/DT intermediate/JJ scene/NN representation/NN corresponding/VBG to/IN the/DT future/JJ frame/NN (/-LRB- e.g./FW ,/, 1/CD -/SYM 2/CD seconds/NNS later/RB )/-RRB- ./.
Combining/VBG these/DT allows/VBZ direct/JJ prediction/NN of/IN future/JJ locations/NNS of/IN human/JJ hands/NNS and/CC objects/NNS ,/, which/WDT enables/VBZ the/DT robot/NN to/IN infer/VB the/DT motor/NN control/NN plan/NN using/VBG our/PRP$ manipulation/NN network/NN ./.
We/PRP experimentally/RB confirm/VBP that/IN our/PRP$ approach/NN makes/VBZ learning/NN of/IN robot/NN activities/NNS from/IN unlabeled/JJ human/JJ interaction/NN videos/NNS possible/JJ ,/, and/CC demonstrate/VBP that/IN our/PRP$ robot/NN is/VBZ able/JJ to/TO execute/VB the/DT learned/VBN collaborative/JJ activities/NNS in/IN real/JJ -/HYPH time/NN directly/RB based/VBN on/IN its/PRP$ camera/NN input/NN ./.
