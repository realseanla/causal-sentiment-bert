The/DT expressive/JJ power/NN of/IN neural/JJ networks/NNS is/VBZ important/JJ for/IN understanding/VBG deep/JJ learning/NN ./.
Most/JJS existing/JJ works/NNS consider/VBP this/DT problem/NN from/IN the/DT view/NN of/IN the/DT depth/NN of/IN a/DT network/NN ./.
In/IN this/DT paper/NN ,/, we/PRP study/VBP how/WRB width/NN affects/VBZ the/DT expressiveness/NN of/IN neural/JJ networks/NNS ./.
Classical/JJ results/NNS state/NN that/WDT \/SYM emph/NN {/-LRB- depth/NN -/HYPH bounded/VBN }/-RRB- (/-LRB- e.g./FW depth/NN -/: $/$ 2/CD $/$ )/-RRB- networks/NNS with/IN suitable/JJ activation/NN functions/NNS are/VBP universal/JJ approximators/NNS ./.
We/PRP show/VBP a/DT universal/JJ approximation/NN theorem/NN for/IN \/SYM emph/NN {/-LRB- width/NN -/HYPH bounded/VBN }/-RRB- ReLU/NN networks/NNS :/: width/NN -/: $/$ (/-LRB- n/NN 4/CD )/-RRB- $/$ ReLU/CD networks/NNS ,/, where/WRB $/$ n/NN $/$ is/VBZ the/DT input/NN dimension/NN ,/, are/VBP universal/JJ approximators/NNS ./.
Moreover/RB ,/, except/IN for/IN a/DT measure/NN zero/CD set/NN ,/, all/DT functions/NNS can/MD not/RB be/VB approximated/VBN by/IN width/NN -/, $/$ n/NN $/$ ReLU/CD networks/NNS ,/, which/WDT exhibits/VBZ a/DT phase/NN transition/NN ./.
Several/JJ recent/JJ works/NNS demonstrate/VBP the/DT benefits/NNS of/IN depth/NN by/IN proving/VBG the/DT depth/NN -/HYPH efficiency/NN of/IN neural/JJ networks/NNS ./.
That/DT is/VBZ ,/, there/EX are/VBP classes/NNS of/IN deep/JJ networks/NNS which/WDT can/MD not/RB be/VB realized/VBN by/IN any/DT shallow/JJ network/NN whose/WP$ size/NN is/VBZ no/DT more/JJR than/IN an/DT \/NN emph/NN {/-LRB- exponential/JJ }/-RRB- bound/VBN ./.
Here/RB we/PRP pose/VBP the/DT dual/JJ question/NN on/IN the/DT width/NN -/HYPH efficiency/NN of/IN ReLU/NNP networks/NNS :/: Are/VBP there/RB wide/JJ networks/NNS that/WDT can/MD not/RB be/VB realized/VBN by/IN narrow/JJ networks/NNS whose/WP$ size/NN is/VBZ not/RB substantially/RB larger/JJR ?/.
We/PRP show/VBP that/IN there/EX exist/VBP classes/NNS of/IN wide/JJ networks/NNS which/WDT can/MD not/RB be/VB realized/VBN by/IN any/DT narrow/JJ network/NN whose/WP$ depth/NN is/VBZ no/DT more/JJR than/IN a/DT \/SYM emph/NN {/-LRB- polynomial/JJ }/-RRB- bound/VBN ./.
On/IN the/DT other/JJ hand/NN ,/, we/PRP demonstrate/VBP by/IN extensive/JJ experiments/NNS that/WDT narrow/VBP networks/NNS whose/WP$ size/NN exceed/VBP the/DT polynomial/JJ bound/VBN by/IN a/DT constant/JJ factor/NN can/MD approximate/VB wide/RB and/CC shallow/JJ network/NN with/IN high/JJ accuracy/NN ./.
Our/PRP$ results/NNS provide/VBP more/JJR comprehensive/JJ evidence/NN that/IN depth/NN is/VBZ more/RBR effective/JJ than/IN width/NN for/IN the/DT expressiveness/NN of/IN ReLU/NNP networks/NNS ./.
