Federated/NNP Learning/NNP is/VBZ a/DT machine/NN learning/VBG setting/NN where/WRB the/DT goal/NN is/VBZ to/TO train/VB a/DT high/JJ -/HYPH quality/NN centralized/JJ model/NN with/IN training/NN data/NNS distributed/VBN over/IN a/DT large/JJ number/NN of/IN clients/NNS each/DT with/IN unreliable/JJ and/CC relatively/RB slow/JJ network/NN connections/NNS ./.
We/PRP consider/VBP learning/VBG algorithms/NNS for/IN this/DT setting/NN where/WRB on/IN each/DT round/NN ,/, each/DT client/NN independently/RB computes/VBZ an/DT update/NN to/IN the/DT current/JJ model/NN based/VBN on/IN its/PRP$ local/JJ data/NNS ,/, and/CC communicates/VBZ this/DT update/NN to/IN a/DT central/JJ server/NN ,/, where/WRB the/DT client/NN -/HYPH side/NN updates/NNS are/VBP aggregated/VBN to/IN compute/VB a/DT new/JJ global/JJ model/NN ./.
The/DT typical/JJ clients/NNS in/IN this/DT setting/NN are/VBP mobile/JJ phones/NNS ,/, and/CC communication/NN efficiency/NN is/VBZ of/IN utmost/JJ importance/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP two/CD ways/NNS to/TO reduce/VB the/DT uplink/NN communication/NN costs/NNS ./.
The/DT proposed/VBN methods/NNS are/VBP evaluated/VBN on/IN the/DT application/NN of/IN training/VBG a/DT deep/JJ neural/JJ network/NN to/TO perform/VB image/NN classification/NN ./.
Our/PRP$ best/JJS approach/NN reduces/VBZ the/DT upload/NN communication/NN required/VBN to/TO train/VB a/DT reasonable/JJ model/NN by/IN two/CD orders/NNS of/IN magnitude/NN ./.
