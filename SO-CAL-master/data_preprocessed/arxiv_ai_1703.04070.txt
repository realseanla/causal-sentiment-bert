We/PRP introduce/VBP a/DT method/NN for/IN learning/VBG the/DT dynamics/NNS of/IN complex/JJ nonlinear/JJ systems/NNS based/VBN on/IN deep/JJ generative/NN models/NNS over/IN temporal/JJ segments/NNS of/IN states/NNS and/CC actions/NNS ./.
Unlike/IN dynamics/NNS models/NNS that/WDT operate/VBP over/IN individual/JJ discrete/JJ timesteps/NNS ,/, we/PRP learn/VBP the/DT distribution/NN over/IN future/JJ state/NN trajectories/NNS conditioned/VBN on/IN past/JJ state/NN ,/, past/JJ action/NN ,/, and/CC planned/VBD future/JJ action/NN trajectories/NNS ,/, as/RB well/RB as/IN a/DT latent/NN prior/RB over/IN action/NN trajectories/NNS ./.
Our/PRP$ approach/NN is/VBZ based/VBN on/IN convolutional/JJ autoregressive/JJ models/NNS and/CC variational/JJ autoencoders/NNS ./.
It/PRP makes/VBZ stable/JJ and/CC accurate/JJ predictions/NNS over/IN long/JJ horizons/NNS for/IN complex/NN ,/, stochastic/JJ systems/NNS ,/, effectively/RB expressing/VBG uncertainty/NN and/CC modeling/VBG the/DT effects/NNS of/IN collisions/NNS ,/, sensory/JJ noise/NN ,/, and/CC action/NN delays/NNS ./.
The/DT learned/VBN dynamics/NNS model/NN and/CC action/NN prior/RB can/MD be/VB used/VBN for/IN end/NN -/HYPH to/IN -/HYPH end/NN ,/, fully/RB differentiable/JJ trajectory/NN optimization/NN and/CC model/NN -/HYPH based/VBN policy/NN optimization/NN ,/, which/WDT we/PRP use/VBP to/TO evaluate/VB the/DT performance/NN and/CC sample/NN -/HYPH efficiency/NN of/IN our/PRP$ method/NN ./.
