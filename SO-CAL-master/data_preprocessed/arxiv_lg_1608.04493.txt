Deep/JJ learning/NN has/VBZ become/VBN a/DT ubiquitous/JJ technology/NN to/TO improve/VB machine/NN intelligence/NN ./.
However/RB ,/, most/JJS of/IN the/DT existing/VBG deep/JJ models/NNS are/VBP structurally/RB very/RB complex/JJ ,/, making/VBG them/PRP difficult/JJ to/TO be/VB deployed/VBN on/IN the/DT mobile/JJ platforms/NNS with/IN limited/JJ computational/JJ power/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ network/NN compression/NN method/NN called/VBN dynamic/JJ network/NN surgery/NN ,/, which/WDT can/MD remarkably/RB reduce/VB the/DT network/NN complexity/NN by/IN making/VBG on/IN -/HYPH the/DT -/HYPH fly/NN connection/NN pruning/NN ./.
Unlike/IN the/DT previous/JJ methods/NNS which/WDT accomplish/VBP this/DT task/NN in/IN a/DT greedy/JJ way/NN ,/, we/PRP properly/RB incorporate/VB connection/NN splicing/NN into/IN the/DT whole/JJ process/NN to/TO avoid/VB incorrect/JJ pruning/NN and/CC make/VB it/PRP as/IN a/DT continual/JJ network/NN maintenance/NN ./.
The/DT effectiveness/NN of/IN our/PRP$ method/NN is/VBZ proved/VBN with/IN experiments/NNS ./.
Without/IN any/DT accuracy/NN loss/NN ,/, our/PRP$ method/NN can/MD efficiently/RB compress/VB the/DT number/NN of/IN parameters/NNS in/IN LeNet/NNP -/HYPH 5/CD and/CC AlexNet/NNP by/IN a/DT factor/NN of/IN $/$ \/CD bm/NNP {/-LRB- 108/CD }/-RRB- \/NN times/NNS $/$ and/CC $/$ \/CD bm/NNP {/-LRB- 17.7/CD }/-RRB- \/NN times/NNS $/$ respectively/RB ,/, proving/VBG that/IN it/PRP outperforms/VBZ the/DT recent/JJ pruning/NN method/NN by/IN considerable/JJ margins/NNS ./.
Code/NNP will/MD be/VB made/VBN publicly/RB available/JJ ./.
