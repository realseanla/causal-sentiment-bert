In/IN this/DT paper/NN we/PRP propose/VBP a/DT generative/JJ model/NN ,/, the/DT Temporal/JJ Generative/JJ Adversarial/NNP Network/NNP (/-LRB- TGAN/NNP )/-RRB- ,/, which/WDT can/MD learn/VB a/DT semantic/JJ representation/NN of/IN unlabelled/JJ videos/NNS ,/, and/CC is/VBZ capable/JJ of/IN generating/VBG consistent/JJ videos/NNS ./.
Unlike/IN an/DT existing/VBG GAN/NNP that/WDT generates/VBZ videos/NNS with/IN a/DT generator/NN consisting/VBG of/IN 3D/JJ deconvolutional/JJ layers/NNS ,/, our/PRP$ model/NN exploits/NNS two/CD types/NNS of/IN generators/NNS :/: a/DT temporal/JJ generator/NN and/CC an/DT image/NN generator/NN ./.
The/DT temporal/JJ generator/NN consists/VBZ of/IN 1D/NN deconvolutional/JJ layers/NNS and/CC outputs/NNS a/DT set/NN of/IN latent/JJ variables/NNS ,/, each/DT of/IN which/WDT corresponds/VBZ to/IN a/DT frame/NN in/IN the/DT generated/VBN video/NN ,/, and/CC the/DT image/NN generator/NN transforms/VBZ them/PRP into/IN a/DT video/NN with/IN 2D/NN deconvolutional/JJ layers/NNS ./.
This/DT representation/NN allows/VBZ efficient/JJ training/NN of/IN the/DT network/NN parameters/NNS ./.
Moreover/RB ,/, it/PRP can/MD handle/VB a/DT wider/JJR range/NN of/IN applications/NNS including/VBG the/DT generation/NN of/IN a/DT long/JJ sequence/NN ,/, frame/NN interpolation/NN ,/, and/CC the/DT use/NN of/IN pre-trained/JJ models/NNS ./.
Experimental/JJ results/NNS demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ method/NN ./.
