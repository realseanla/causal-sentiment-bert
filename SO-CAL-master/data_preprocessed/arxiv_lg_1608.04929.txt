A/DT recent/JJ goal/NN in/IN the/DT Reinforcement/NN Learning/NN (/-LRB- RL/NN )/-RRB- framework/NN is/VBZ to/TO choose/VB a/DT sequence/NN of/IN actions/NNS or/CC a/DT policy/NN to/TO maximize/VB the/DT reward/NN collected/VBN or/CC minimize/VB the/DT regret/NN incurred/VBN in/IN a/DT finite/JJ time/NN horizon/NN ./.
For/IN several/JJ RL/NN problems/NNS in/IN operation/NN research/NN and/CC optimal/JJ control/NN ,/, the/DT optimal/JJ policy/NN of/IN the/DT underlying/VBG Markov/NNP Decision/NN Process/NN (/-LRB- MDP/NN )/-RRB- is/VBZ characterized/VBN by/IN a/DT known/JJ structure/NN ./.
The/DT current/JJ state/NN of/IN the/DT art/NN algorithms/NNS do/VBP not/RB utilize/VB this/DT known/JJ structure/NN of/IN the/DT optimal/JJ policy/NN while/IN minimizing/VBG regret/NN ./.
In/IN this/DT work/NN ,/, we/PRP develop/VBP new/JJ RL/NNP algorithms/NNS that/WDT exploit/VBP the/DT structure/NN of/IN the/DT optimal/JJ policy/NN to/TO minimize/VB regret/NN ./.
Numerical/NNP experiments/NNS on/IN MDPs/NNS with/IN structured/JJ optimal/JJ policies/NNS show/VBP that/IN our/PRP$ algorithms/NNS have/VBP better/JJR performance/NN ,/, are/VBP easy/JJ to/TO implement/VB ,/, have/VB a/DT smaller/JJR run/NN -/HYPH time/NN and/CC require/VBP less/JJR number/NN of/IN random/JJ number/NN generations/NNS ./.
