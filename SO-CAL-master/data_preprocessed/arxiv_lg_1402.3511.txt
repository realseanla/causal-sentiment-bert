Sequence/NN prediction/NN and/CC classification/NN are/VBP ubiquitous/JJ and/CC challenging/JJ problems/NNS in/IN machine/NN learning/NN that/WDT can/MD require/VB identifying/VBG complex/JJ dependencies/NNS between/IN temporally/RB distant/JJ inputs/NNS ./.
Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNNs/NNS )/-RRB- have/VBP the/DT ability/NN ,/, in/IN theory/NN ,/, to/TO cope/VB with/IN these/DT temporal/JJ dependencies/NNS by/IN virtue/NN of/IN the/DT short/JJ -/HYPH term/NN memory/NN implemented/VBN by/IN their/PRP$ recurrent/JJ (/-LRB- feedback/NN )/-RRB- connections/NNS ./.
However/RB ,/, in/IN practice/NN they/PRP are/VBP difficult/JJ to/TO train/VB successfully/RB when/WRB the/DT long/JJ -/HYPH term/NN memory/NN is/VBZ required/VBN ./.
This/DT paper/NN introduces/VBZ a/DT simple/JJ ,/, yet/RB powerful/JJ modification/NN to/IN the/DT standard/JJ RNN/NNP architecture/NN ,/, the/DT Clockwork/NNP RNN/NNP (/-LRB- CW/NNP -/HYPH RNN/NNP )/-RRB- ,/, in/IN which/WDT the/DT hidden/JJ layer/NN is/VBZ partitioned/VBN into/IN separate/JJ modules/NNS ,/, each/DT processing/NN inputs/NNS at/IN its/PRP$ own/JJ temporal/JJ granularity/NN ,/, making/VBG computations/NNS only/RB at/IN its/PRP$ prescribed/JJ clock/NN rate/NN ./.
Rather/RB than/IN making/VBG the/DT standard/JJ RNN/NN models/NNS more/RBR complex/JJ ,/, CW/NNP -/HYPH RNN/NNP reduces/VBZ the/DT number/NN of/IN RNN/NN parameters/NNS ,/, improves/VBZ the/DT performance/NN significantly/RB in/IN the/DT tasks/NNS tested/VBN ,/, and/CC speeds/NNS up/IN the/DT network/NN evaluation/NN ./.
The/DT network/NN is/VBZ demonstrated/VBN in/IN preliminary/JJ experiments/NNS involving/VBG two/CD tasks/NNS :/: audio/JJ signal/NN generation/NN and/CC TIMIT/NN spoken/VBN word/NN classification/NN ,/, where/WRB it/PRP outperforms/VBZ both/CC RNN/NNP and/CC LSTM/NNP networks/NNS ./.
