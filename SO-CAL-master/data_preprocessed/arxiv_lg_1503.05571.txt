We/PRP introduce/VBP a/DT novel/JJ training/NN principle/NN for/IN probabilistic/JJ models/NNS that/WDT is/VBZ an/DT alternative/NN to/IN maximum/JJ likelihood/NN ./.
The/DT proposed/VBN Generative/NNP Stochastic/NNP Networks/NNP (/-LRB- GSN/NNP )/-RRB- framework/NN is/VBZ based/VBN on/IN learning/VBG the/DT transition/NN operator/NN of/IN a/DT Markov/NNP chain/NN whose/WP$ stationary/JJ distribution/NN estimates/VBZ the/DT data/NNS distribution/NN ./.
Because/IN the/DT transition/NN distribution/NN is/VBZ a/DT conditional/JJ distribution/NN generally/RB involving/VBG a/DT small/JJ move/NN ,/, it/PRP has/VBZ fewer/JJR dominant/JJ modes/NNS ,/, being/VBG unimodal/JJ in/IN the/DT limit/NN of/IN small/JJ moves/NNS ./.
Thus/RB ,/, it/PRP is/VBZ easier/JJR to/TO learn/VB ,/, more/RBR like/IN learning/VBG to/TO perform/VB supervised/JJ function/NN approximation/NN ,/, with/IN gradients/NNS that/WDT can/MD be/VB obtained/VBN by/IN back/RB -/HYPH propagation/NN ./.
The/DT theorems/NNS provided/VBN here/RB generalize/VB recent/JJ work/NN on/IN the/DT probabilistic/JJ interpretation/NN of/IN denoising/VBG auto/NN -/HYPH encoders/NNS and/CC provide/VB an/DT interesting/JJ justification/NN for/IN dependency/NN networks/NNS and/CC generalized/VBN pseudolikelihood/NN (/-LRB- along/IN with/IN defining/VBG an/DT appropriate/JJ joint/JJ distribution/NN and/CC sampling/NN mechanism/NN ,/, even/RB when/WRB the/DT conditionals/NNS are/VBP not/RB consistent/JJ )/-RRB- ./.
We/PRP study/VBP how/WRB GSNs/NNS can/MD be/VB used/VBN with/IN missing/VBG inputs/NNS and/CC can/MD be/VB used/VBN to/TO sample/VB subsets/NNS of/IN variables/NNS given/VBN the/DT rest/NN ./.
Successful/JJ experiments/NNS are/VBP conducted/VBN ,/, validating/VBG these/DT theoretical/JJ results/NNS ,/, on/IN two/CD image/NN datasets/NNS and/CC with/IN a/DT particular/JJ architecture/NN that/WDT mimics/VBZ the/DT Deep/JJ Boltzmann/NNP Machine/NNP Gibbs/NNP sampler/NN but/CC allows/VBZ training/NN to/TO proceed/VB with/IN backprop/NN ,/, without/IN the/DT need/NN for/IN layerwise/NN pretraining/NN ./.
