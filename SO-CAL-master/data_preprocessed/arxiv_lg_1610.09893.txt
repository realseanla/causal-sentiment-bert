Recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- have/VBP achieved/VBN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performances/NNS in/IN many/JJ natural/JJ language/NN processing/NN tasks/NNS ,/, such/JJ as/IN language/NN modeling/NN and/CC machine/NN translation/NN ./.
However/RB ,/, when/WRB the/DT vocabulary/NN is/VBZ large/JJ ,/, the/DT RNN/NN model/NN will/MD become/VB very/RB big/JJ (/-LRB- e.g./FW ,/, possibly/RB beyond/IN the/DT memory/NN capacity/NN of/IN a/DT GPU/NNP device/NN )/-RRB- and/CC its/PRP$ training/NN will/MD become/VB very/RB inefficient/JJ ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT novel/JJ technique/NN to/TO tackle/VB this/DT challenge/NN ./.
The/DT key/JJ idea/NN is/VBZ to/TO use/VB 2/CD -/HYPH Component/NN (/-LRB- 2C/NN )/-RRB- shared/VBN embedding/NN for/IN word/NN representations/NNS ./.
We/PRP allocate/VBP every/DT word/NN in/IN the/DT vocabulary/NN into/IN a/DT table/NN ,/, each/DT row/NN of/IN which/WDT is/VBZ associated/VBN with/IN a/DT vector/NN ,/, and/CC each/DT column/NN associated/VBN with/IN another/DT vector/NN ./.
Depending/VBG on/IN its/PRP$ position/NN in/IN the/DT table/NN ,/, a/DT word/NN is/VBZ jointly/RB represented/VBN by/IN two/CD components/NNS :/: a/DT row/NN vector/NN and/CC a/DT column/NN vector/NN ./.
Since/IN the/DT words/NNS in/IN the/DT same/JJ row/NN share/VB the/DT row/NN vector/NN and/CC the/DT words/NNS in/IN the/DT same/JJ column/NN share/NN the/DT column/NN vector/NN ,/, we/PRP only/RB need/VBP $/$ 2/CD \/SYM sqrt/NN {/-LRB- |/NFP V/NNP |/NFP }/-RRB- $/$ vectors/NNS to/TO represent/VB a/DT vocabulary/NN of/IN $/$ |/NFP V/NN |/NFP $/$ unique/JJ words/NNS ,/, which/WDT are/VBP far/RB less/JJR than/IN the/DT $/$ |/NFP V/NN |/NFP $/$ vectors/NNS required/VBN by/IN existing/VBG approaches/NNS ./.
Based/VBN on/IN the/DT 2/CD -/HYPH Component/NN shared/VBD embedding/NN ,/, we/PRP design/VBP a/DT new/JJ RNN/NNP algorithm/NN and/CC evaluate/VB it/PRP using/VBG the/DT language/NN modeling/NN task/NN on/IN several/JJ benchmark/NN datasets/NNS ./.
The/DT results/NNS show/VBP that/IN our/PRP$ algorithm/NN significantly/RB reduces/VBZ the/DT model/NN size/NN and/CC speeds/NNS up/IN the/DT training/NN process/NN ,/, without/IN sacrifice/NN of/IN accuracy/NN (/-LRB- it/PRP achieves/VBZ similar/JJ ,/, if/IN not/RB better/JJR ,/, perplexity/NN as/IN compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN language/NN models/NNS )/-RRB- ./.
Remarkably/RB ,/, on/IN the/DT One/CD -/HYPH Billion/CD -/HYPH Word/NN benchmark/NN Dataset/NN ,/, our/PRP$ algorithm/NN achieves/VBZ comparable/JJ perplexity/NN to/IN previous/JJ language/NN models/NNS ,/, whilst/IN reducing/VBG the/DT model/NN size/NN by/IN a/DT factor/NN of/IN 40/CD -/HYPH 100/CD ,/, and/CC speeding/VBG up/RP the/DT training/NN process/NN by/IN a/DT factor/NN of/IN 2/CD ./.
We/PRP name/VBP our/PRP$ proposed/VBN algorithm/NN \/SYM emph/NN {/-LRB- LightRNN/NN }/-RRB- to/TO reflect/VB its/PRP$ very/RB small/JJ model/NN size/NN and/CC very/RB high/JJ training/NN speed/NN ./.
