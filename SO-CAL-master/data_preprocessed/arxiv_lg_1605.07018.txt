We/PRP study/VBP an/DT online/JJ learning/NN framework/NN introduced/VBN by/IN Mannor/NNP and/CC Shamir/NNP (/-LRB- 2011/CD )/-RRB- in/IN which/WDT the/DT feedback/NN is/VBZ specified/VBN by/IN a/DT graph/NN ,/, in/IN a/DT setting/NN where/WRB the/DT graph/NN may/MD vary/VB from/IN round/NN to/IN round/NN and/CC is/VBZ \/SYM emph/NN {/-LRB- never/RB fully/RB revealed/VBN }/-RRB- to/IN the/DT learner/NN ./.
We/PRP show/VBP a/DT large/JJ gap/NN between/IN the/DT adversarial/JJ and/CC the/DT stochastic/JJ cases/NNS ./.
In/IN the/DT adversarial/JJ case/NN ,/, we/PRP prove/VBP that/IN even/RB for/IN dense/JJ feedback/NN graphs/NNS ,/, the/DT learner/NN can/MD not/RB improve/VB upon/IN a/DT trivial/JJ regret/NN bound/VBN obtained/VBN by/IN ignoring/VBG any/DT additional/JJ feedback/NN besides/IN her/PRP$ own/JJ loss/NN ./.
In/IN contrast/NN ,/, in/IN the/DT stochastic/JJ case/NN we/PRP give/VBP an/DT algorithm/NN that/WDT achieves/VBZ $/$ \/CD widetilde/NN \/SYM Theta/NNP (/-LRB- \/SYM sqrt/NN {/-LRB- \/SYM alpha/NN T/NN }/-RRB- )/-RRB- $/$ regret/NN over/IN $/$ T$/CD rounds/NNS ,/, provided/VBD that/IN the/DT independence/NN numbers/NNS of/IN the/DT hidden/JJ feedback/NN graphs/NNS are/VBP at/IN most/RBS $/$ \/SYM alpha/NN $/$ ./.
We/PRP also/RB extend/VBP our/PRP$ results/NNS to/IN a/DT more/RBR general/JJ feedback/NN model/NN ,/, in/IN which/WDT the/DT learner/NN does/VBZ not/RB necessarily/RB observe/VB her/PRP$ own/JJ loss/NN ,/, and/CC show/VBP that/IN ,/, even/RB in/IN simple/JJ cases/NNS ,/, concealing/VBG the/DT feedback/NN graphs/NNS might/MD render/VB a/DT learnable/JJ problem/NN unlearnable/JJ ./.
