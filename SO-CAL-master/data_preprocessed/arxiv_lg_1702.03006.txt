To/TO estimate/VB the/DT value/NN functions/NNS of/IN policies/NNS from/IN exploratory/JJ data/NNS ,/, most/JJS model/NN -/HYPH free/JJ off/IN -/HYPH policy/NN algorithms/NNS rely/VBP on/IN importance/NN sampling/NN ,/, where/WRB the/DT use/NN of/IN importance/NN sampling/NN ratios/NNS often/RB leads/VBZ to/IN estimates/NNS with/IN severe/JJ variance/NN ./.
It/PRP is/VBZ thus/RB desirable/JJ to/TO learn/VB off/IN -/HYPH policy/NN without/IN using/VBG the/DT ratios/NNS ./.
However/RB ,/, such/PDT an/DT algorithm/NN does/VBZ not/RB exist/VB for/IN multi-step/JJ learning/NN with/IN function/NN approximation/NN ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP the/DT first/JJ such/JJ algorithm/NN based/VBN on/IN temporal/JJ -/HYPH difference/NN (/-LRB- TD/NN )/-RRB- learning/NN updates/NNS ./.
We/PRP show/VBP that/IN an/DT explicit/JJ use/NN of/IN importance/NN sampling/NN ratios/NNS can/MD be/VB eliminated/VBN by/IN varying/VBG the/DT amount/NN of/IN bootstrapping/VBG in/IN TD/NN updates/NNS in/IN an/DT action/NN -/HYPH dependent/JJ manner/NN ./.
Our/PRP$ new/JJ algorithm/NN achieves/VBZ stability/NN using/VBG a/DT two/CD -/HYPH timescale/NN gradient/NN -/HYPH based/VBN TD/NNP update/NN ./.
A/DT prior/JJ algorithm/NN based/VBN on/IN lookup/NN table/NN representation/NN called/VBN Tree/NNP Backup/JJ can/MD also/RB be/VB retrieved/VBN using/VBG action/NN -/HYPH dependent/JJ bootstrapping/NN ,/, becoming/VBG a/DT special/JJ case/NN of/IN our/PRP$ algorithm/NN ./.
In/IN two/CD challenging/JJ off/IN -/HYPH policy/NN tasks/NNS ,/, we/PRP demonstrate/VBP that/IN our/PRP$ algorithm/NN is/VBZ stable/JJ ,/, effectively/RB avoids/VBZ the/DT large/JJ variance/NN issue/NN ,/, and/CC can/MD perform/VB substantially/RB better/JJR than/IN its/PRP$ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN counterpart/NN ./.
