This/DT paper/NN presents/VBZ an/DT empirical/JJ study/NN of/IN two/CD widely/RB -/HYPH used/VBN sequence/NN prediction/NN models/NNS ,/, Conditional/JJ Random/NNP Fields/NNPS (/-LRB- CRFs/NNS )/-RRB- and/CC Long/JJ Short/JJ -/HYPH Term/NN Memory/NN Networks/NNS (/-LRB- LSTMs/NNS )/-RRB- ,/, on/IN two/CD fundamental/JJ tasks/NNS for/IN Vietnamese/JJ text/NN processing/NN ,/, including/VBG part/NN -/HYPH of/IN -/HYPH speech/NN tagging/NN and/CC named/VBN entity/NN recognition/NN ./.
We/PRP show/VBP that/IN a/DT strong/JJ lower/JJR bound/VBN for/IN labeling/NN accuracy/NN can/MD be/VB obtained/VBN by/IN relying/VBG only/RB on/IN simple/JJ word/NN -/HYPH based/VBN features/NNS with/IN minimal/JJ hand/NN -/HYPH crafted/VBN feature/NN engineering/NN ,/, of/IN 90.65/CD \/SYM percent/NN and/CC 86.03/CD \/SYM percent/NN performance/NN scores/NNS on/IN the/DT standard/JJ test/NN sets/NNS for/IN the/DT two/CD tasks/NNS respectively/RB ./.
In/IN particular/JJ ,/, we/PRP demonstrate/VBP empirically/RB the/DT surprising/JJ efficiency/NN of/IN word/NN embeddings/NNS in/IN both/DT of/IN the/DT two/CD tasks/NNS ,/, with/IN both/DT of/IN the/DT two/CD models/NNS ./.
We/PRP point/VBP out/RP that/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN LSTMs/NNPS model/NN does/VBZ not/RB always/RB outperform/VBP significantly/RB the/DT traditional/JJ CRFs/NNS model/NN ,/, especially/RB on/IN moderate/JJ -/HYPH sized/JJ data/NNS sets/NNS ./.
Finally/RB ,/, we/PRP give/VBP some/DT suggestions/NNS and/CC discussions/NNS for/IN efficient/JJ use/NN of/IN sequence/NN labeling/NN models/NNS in/IN practical/JJ applications/NNS ./.
