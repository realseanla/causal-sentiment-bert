Opponent/NN modeling/NN is/VBZ necessary/JJ in/IN multi-agent/JJ settings/NNS where/WRB secondary/JJ agents/NNS with/IN competing/VBG goals/NNS also/RB adapt/VBP their/PRP$ strategies/NNS ,/, yet/CC it/PRP remains/VBZ challenging/JJ because/IN strategies/NNS interact/VBP with/IN each/DT other/JJ and/CC change/NN ./.
Most/JJS previous/JJ work/NN focuses/VBZ on/IN developing/VBG probabilistic/JJ models/NNS or/CC parameterized/JJ strategies/NNS for/IN specific/JJ applications/NNS ./.
Inspired/VBN by/IN the/DT recent/JJ success/NN of/IN deep/JJ reinforcement/NN learning/NN ,/, we/PRP present/VBP neural/JJ -/HYPH based/VBN models/NNS that/WDT jointly/RB learn/VBP a/DT policy/NN and/CC the/DT behavior/NN of/IN opponents/NNS ./.
Instead/RB of/IN explicitly/RB predicting/VBG the/DT opponent/NN 's/POS action/NN ,/, we/PRP encode/VBP observation/NN of/IN the/DT opponents/NNS into/IN a/DT deep/JJ Q/NN -/HYPH Network/NN (/-LRB- DQN/NN )/-RRB- ;/: however/RB ,/, we/PRP retain/VBP explicit/JJ modeling/NN (/-LRB- if/IN desired/VBN )/-RRB- using/VBG multitasking/NN ./.
By/IN using/VBG a/DT Mixture/NN -/HYPH of/IN -/HYPH Experts/NNS architecture/NN ,/, our/PRP$ model/NN automatically/RB discovers/VBZ different/JJ strategy/NN patterns/NNS of/IN opponents/NNS without/IN extra/JJ supervision/NN ./.
We/PRP evaluate/VBP our/PRP$ models/NNS on/IN a/DT simulated/JJ soccer/NN game/NN and/CC a/DT popular/JJ trivia/NNS game/NN ,/, showing/VBG superior/JJ performance/NN over/IN DQN/NNP and/CC its/PRP$ variants/NNS ./.
