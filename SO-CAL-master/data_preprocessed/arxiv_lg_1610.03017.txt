Most/JJS existing/VBG machine/NN translation/NN systems/NNS operate/VBP at/IN the/DT level/NN of/IN words/NNS ,/, relying/VBG on/IN explicit/JJ segmentation/NN to/TO extract/VB tokens/NNS ./.
We/PRP introduce/VBP a/DT neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- model/NN that/WDT maps/VBZ a/DT source/NN character/NN sequence/NN to/IN a/DT target/NN character/NN sequence/NN without/IN any/DT segmentation/NN ./.
We/PRP employ/VBP a/DT character/NN -/HYPH level/NN convolutional/JJ network/NN with/IN max/NN -/HYPH pooling/VBG at/IN the/DT encoder/NN to/TO reduce/VB the/DT length/NN of/IN source/NN representation/NN ,/, allowing/VBG the/DT model/NN to/TO be/VB trained/VBN at/IN a/DT speed/NN comparable/JJ to/IN subword/NN -/HYPH level/NN models/NNS while/IN capturing/VBG local/JJ regularities/NNS ./.
Our/PRP$ character/NN -/HYPH to/IN -/HYPH character/NN model/NN outperforms/VBZ a/DT recently/RB proposed/VBN baseline/NN with/IN a/DT subword/NN -/HYPH level/NN encoder/NN on/IN WMT/NNP '15/CD DE-EN/NN and/CC CS/NN -/HYPH EN/NN ,/, and/CC gives/VBZ comparable/JJ performance/NN on/IN FI/NN -/HYPH EN/NN and/CC RU/NNP -/HYPH EN/NNP ./.
We/PRP then/RB demonstrate/VBP that/IN it/PRP is/VBZ possible/JJ to/TO share/VB a/DT single/JJ character/NN -/HYPH level/NN encoder/NN across/IN multiple/JJ languages/NNS by/IN training/VBG a/DT model/NN on/IN a/DT many/JJ -/HYPH to/IN -/HYPH one/CD translation/NN task/NN ./.
In/IN this/DT multilingual/JJ setting/NN ,/, the/DT character/NN -/HYPH level/NN encoder/NN significantly/RB outperforms/VBZ the/DT subword/NN -/HYPH level/NN encoder/NN on/IN all/PDT the/DT language/NN pairs/NNS ./.
We/PRP also/RB observe/VBP that/IN the/DT quality/NN of/IN the/DT multilingual/JJ character/NN -/HYPH level/NN translation/NN even/RB surpasses/VBZ the/DT models/NNS trained/VBN and/CC tuned/VBN on/IN one/CD language/NN pair/NN ,/, namely/RB on/IN CS/NN -/HYPH EN/NN ,/, FI/NN -/HYPH EN/NN and/CC RU/NNP -/HYPH EN/NNP ./.
