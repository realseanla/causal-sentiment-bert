We/PRP consider/VBP an/DT online/JJ learning/NN to/IN rank/NN setting/NN in/IN which/WDT ,/, at/IN each/DT round/NN ,/, an/DT oblivious/JJ adversary/NN generates/VBZ a/DT list/NN of/IN $/$ m/CD $/$ documents/NNS ,/, pertaining/VBG to/IN a/DT query/NN ,/, and/CC the/DT learner/NN produces/VBZ scores/NNS to/TO rank/VB the/DT documents/NNS ./.
The/DT adversary/NN then/RB generates/VBZ a/DT relevance/NN vector/NN and/CC the/DT learner/NN updates/NNS its/PRP$ ranker/NN according/VBG to/IN the/DT feedback/NN received/VBD ./.
We/PRP consider/VBP the/DT setting/NN where/WRB the/DT feedback/NN is/VBZ restricted/VBN to/TO be/VB the/DT relevance/NN levels/NNS of/IN only/RB the/DT top/JJ $/$ k/CD $/$ documents/NNS in/IN the/DT ranked/VBN list/NN for/IN $/$ k/CD \/SYM ll/MD m/VB $/$ ./.
However/RB ,/, the/DT performance/NN of/IN learner/NN is/VBZ judged/VBN based/VBN on/IN the/DT unrevealed/JJ full/JJ relevance/NN vectors/NNS ,/, using/VBG an/DT appropriate/JJ learning/NN to/IN rank/NN loss/NN function/NN ./.
We/PRP develop/VBP efficient/JJ algorithms/NNS for/IN well/RB known/JJ losses/NNS in/IN the/DT pointwise/NN ,/, pairwise/JJ and/CC listwise/JJ families/NNS ./.
We/PRP also/RB prove/VBP that/IN no/DT online/JJ algorithm/NN can/MD have/VB sublinear/NN regret/NN ,/, with/IN top/JJ -/HYPH 1/CD feedback/NN ,/, for/IN any/DT loss/NN that/WDT is/VBZ calibrated/VBN with/IN respect/NN to/IN NDCG/NNP ./.
We/PRP apply/VBP our/PRP$ algorithms/NNS on/IN benchmark/NN datasets/NNS demonstrating/VBG efficient/JJ online/JJ learning/NN of/IN a/DT ranking/VBG function/NN from/IN highly/RB restricted/JJ feedback/NN ./.
