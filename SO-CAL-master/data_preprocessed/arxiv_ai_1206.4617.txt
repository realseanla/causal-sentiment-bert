Inverse/JJ optimal/JJ control/NN ,/, also/RB known/VBN as/IN inverse/JJ reinforcement/NN learning/NN ,/, is/VBZ the/DT problem/NN of/IN recovering/VBG an/DT unknown/JJ reward/NN function/NN in/IN a/DT Markov/NNP decision/NN process/NN from/IN expert/JJ demonstrations/NNS of/IN the/DT optimal/JJ policy/NN ./.
We/PRP introduce/VBP a/DT probabilistic/JJ inverse/JJ optimal/JJ control/NN algorithm/NN that/WDT scales/VBZ gracefully/RB with/IN task/NN dimensionality/NN ,/, and/CC is/VBZ suitable/JJ for/IN large/JJ ,/, continuous/JJ domains/NNS where/WRB even/RB computing/VBG a/DT full/JJ policy/NN is/VBZ impractical/JJ ./.
By/IN using/VBG a/DT local/JJ approximation/NN of/IN the/DT reward/NN function/NN ,/, our/PRP$ method/NN can/MD also/RB drop/VB the/DT assumption/NN that/IN the/DT demonstrations/NNS are/VBP globally/RB optimal/JJ ,/, requiring/VBG only/RB local/JJ optimality/NN ./.
This/DT allows/VBZ it/PRP to/TO learn/VB from/IN examples/NNS that/WDT are/VBP unsuitable/JJ for/IN prior/JJ methods/NNS ./.
