Despite/IN recent/JJ breakthroughs/NNS in/IN the/DT applications/NNS of/IN deep/JJ neural/JJ networks/NNS ,/, one/CD setting/NN that/WDT presents/VBZ a/DT persistent/JJ challenge/NN is/VBZ that/IN of/IN "/`` one/CD -/HYPH shot/NN learning/NN ./. "/''
Traditional/JJ gradient/NN -/HYPH based/VBN networks/NNS require/VBP a/DT lot/NN of/IN data/NNS to/TO learn/VB ,/, often/RB through/IN extensive/JJ iterative/JJ training/NN ./.
When/WRB new/JJ data/NNS is/VBZ encountered/VBN ,/, the/DT models/NNS must/MD inefficiently/RB relearn/VB their/PRP$ parameters/NNS to/IN adequately/RB incorporate/VB the/DT new/JJ information/NN without/IN catastrophic/JJ interference/NN ./.
Architectures/NNS with/IN augmented/VBN memory/NN capacities/NNS ,/, such/JJ as/IN Neural/JJ Turing/NN Machines/NNS (/-LRB- NTMs/NNS )/-RRB- ,/, offer/VBP the/DT ability/NN to/TO quickly/RB encode/VB and/CC retrieve/VB new/JJ information/NN ,/, and/CC hence/RB can/MD potentially/RB obviate/VB the/DT downsides/NNS of/IN conventional/JJ models/NNS ./.
Here/RB ,/, we/PRP demonstrate/VBP the/DT ability/NN of/IN a/DT memory/NN -/HYPH augmented/VBN neural/JJ network/NN to/TO rapidly/RB assimilate/VB new/JJ data/NNS ,/, and/CC leverage/NN this/DT data/NN to/TO make/VB accurate/JJ predictions/NNS after/IN only/RB a/DT few/JJ samples/NNS ./.
We/PRP also/RB introduce/VBP a/DT new/JJ method/NN for/IN accessing/VBG an/DT external/JJ memory/NN that/WDT focuses/VBZ on/IN memory/NN content/NN ,/, unlike/IN previous/JJ methods/NNS that/WDT additionally/RB use/VBP memory/NN location/NN -/HYPH based/VBN focusing/VBG mechanisms/NNS ./.
