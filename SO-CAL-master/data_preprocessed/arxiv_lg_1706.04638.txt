We/PRP offer/VBP a/DT generalized/VBN point/NN of/IN view/NN on/IN the/DT backpropagation/NN algorithm/NN ,/, currently/RB the/DT most/RBS common/JJ technique/NN to/TO train/VB neural/JJ networks/NNS via/IN stochastic/JJ gradient/NN descent/NN and/CC variants/NNS thereof/RB ./.
Specifically/RB ,/, we/PRP show/VBP that/IN backpropagation/NN of/IN a/DT prediction/NN error/NN is/VBZ equivalent/JJ to/IN sequential/JJ gradient/NN descent/NN steps/NNS on/IN a/DT quadratic/JJ penalty/NN energy/NN ./.
This/DT energy/NN comprises/VBZ the/DT network/NN activations/NNS as/IN variables/NNS of/IN the/DT optimization/NN and/CC couples/VBZ them/PRP to/IN the/DT network/NN parameters/NNS ./.
Based/VBN on/IN this/DT viewpoint/NN ,/, we/PRP illustrate/VBP the/DT limitations/NNS on/IN step/NN sizes/NNS when/WRB optimizing/VBG a/DT nested/VBN function/NN with/IN gradient/NN descent/NN ./.
Rather/RB than/IN taking/VBG explicit/JJ gradient/NN steps/NNS ,/, where/WRB step/NN size/NN restrictions/NNS are/VBP an/DT impediment/NN for/IN optimization/NN ,/, we/PRP propose/VBP proximal/JJ backpropagation/NN (/-LRB- ProxProp/NNP )/-RRB- as/IN a/DT novel/JJ algorithm/NN that/WDT takes/VBZ implicit/JJ gradient/NN steps/NNS to/TO update/VB the/DT network/NN parameters/NNS ./.
We/PRP experimentally/RB demonstrate/VBP that/IN our/PRP$ algorithm/NN is/VBZ robust/JJ in/IN the/DT sense/NN that/IN it/PRP decreases/VBZ the/DT objective/JJ function/NN for/IN a/DT wide/JJ range/NN of/IN parameter/NN values/NNS ./.
In/IN a/DT systematic/JJ quantitative/JJ analysis/NN ,/, we/PRP compare/VBP to/IN related/JJ approaches/NNS on/IN a/DT supervised/JJ visual/JJ learning/NN task/NN (/-LRB- CIFAR/NN -/HYPH 10/CD )/-RRB- for/IN fully/RB connected/VBN as/RB well/RB as/IN convolutional/JJ neural/JJ networks/NNS and/CC for/IN an/DT unsupervised/JJ autoencoder/NN (/-LRB- USPS/NNP dataset/NN )/-RRB- ./.
We/PRP demonstrate/VBP that/IN ProxProp/NNP leads/VBZ to/IN a/DT significant/JJ speed/NN up/RP in/IN training/NN performance/NN ./.
