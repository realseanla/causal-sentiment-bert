We/PRP present/VBP a/DT neural/JJ network/NN architecture/NN based/VBN on/IN bidirectional/JJ LSTMs/NNPS to/TO compute/VB representations/NNS of/IN words/NNS in/IN the/DT sentential/JJ contexts/NNS ./.
These/DT context/NN -/HYPH sensitive/JJ word/NN representations/NNS are/VBP suitable/JJ for/IN ,/, e.g./FW ,/, distinguishing/VBG different/JJ word/NN senses/NNS and/CC other/JJ context/NN -/HYPH modulated/VBN variations/NNS in/IN meaning/NN ./.
To/TO learn/VB the/DT parameters/NNS of/IN our/PRP$ model/NN ,/, we/PRP use/VBP cross-lingual/JJ supervision/NN ,/, hypothesizing/VBG that/IN a/DT good/JJ representation/NN of/IN a/DT word/NN in/IN context/NN will/MD be/VB one/CD that/WDT is/VBZ sufficient/JJ for/IN selecting/VBG the/DT correct/JJ translation/NN into/IN a/DT second/JJ language/NN ./.
We/PRP evaluate/VBP the/DT quality/NN of/IN our/PRP$ representations/NNS as/IN features/NNS in/IN three/CD downstream/JJ tasks/NNS :/: prediction/NN of/IN semantic/JJ supersenses/NNS (/-LRB- which/WDT assign/VBP nouns/NNS and/CC verbs/NNS into/IN a/DT few/JJ dozen/NN semantic/JJ classes/NNS )/-RRB- ,/, low/JJ resource/NN machine/NN translation/NN ,/, and/CC a/DT lexical/JJ substitution/NN task/NN ,/, and/CC obtain/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN all/DT of/IN these/DT ./.
