We/PRP consider/VBP learning/VBG representations/NNS of/IN entities/NNS and/CC relations/NNS in/IN KBs/NNS using/VBG the/DT neural/JJ -/HYPH embedding/NN approach/NN ./.
We/PRP show/VBP that/IN most/JJS existing/VBG models/NNS ,/, including/VBG NTN/NNP and/CC TransE/NNP ,/, can/MD be/VB generalized/VBN under/IN a/DT unified/JJ learning/NN framework/NN ,/, where/WRB entities/NNS are/VBP low/JJ -/HYPH dimensional/JJ vectors/NNS learned/VBN from/IN a/DT neural/JJ network/NN and/CC relations/NNS are/VBP bilinear/JJ and/CC //HYPH or/CC linear/JJ mapping/NN functions/NNS ./.
Under/IN this/DT framework/NN ,/, we/PRP compare/VBP a/DT variety/NN of/IN embedding/NN models/NNS on/IN the/DT link/NN prediction/NN task/NN ./.
We/PRP show/VBP that/IN a/DT simple/JJ bilinear/NN formulation/NN achieves/VBZ new/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS for/IN the/DT task/NN (/-LRB- achieving/VBG a/DT top/JJ -/HYPH 10/CD accuracy/NN of/IN 73.2/CD percent/NN vs./FW 54.7/CD percent/NN by/IN TransE/NN when/WRB evaluated/VBN on/IN Freebase/NN )/-RRB- ./.
Furthermore/RB ,/, we/PRP introduce/VBP a/DT novel/JJ approach/NN that/WDT utilizes/VBZ the/DT learned/VBN relation/NN embeddings/NNS to/IN mine/PRP logical/JJ rules/NNS from/IN the/DT KB/NNP ./.
We/PRP demonstrate/VBP that/IN embeddings/NNS trained/VBN from/IN the/DT bilinear/NN objective/NN can/MD effectively/RB capture/VB relation/NN composition/NN via/IN matrix/NN multiplication/NN ./.
We/PRP also/RB show/VBP that/IN our/PRP$ embedding/NN -/HYPH based/VBN approach/NN can/MD extract/VB rules/NNS that/WDT involve/VBP relation/NN transitivity/NN more/RBR effectively/RB than/IN a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN rule/NN mining/NN approach/NN that/WDT is/VBZ tailored/VBN for/IN large/JJ -/HYPH scale/NN KBs/NNS ./.
