In/IN the/DT neural/JJ network/NN domain/NN ,/, methods/NNS for/IN hyperparameter/NN optimization/NN and/CC meta/NN -/HYPH modeling/NN are/VBP computationally/RB expensive/JJ due/IN to/IN the/DT need/NN to/TO train/VB a/DT large/JJ number/NN of/IN neural/JJ network/NN configurations/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN a/DT simple/JJ regression/NN model/NN ,/, based/VBN on/IN support/NN vector/NN machines/NNS ,/, can/MD predict/VB the/DT final/JJ performance/NN of/IN partially/RB trained/VBN neural/JJ network/NN configurations/NNS using/VBG features/NNS based/VBN on/IN network/NN architectures/NNS ,/, hyperparameters/NNS ,/, and/CC time/NN -/HYPH series/NN validation/NN performance/NN data/NNS ./.
We/PRP use/VBP this/DT regression/NN model/NN to/TO develop/VB an/DT early/JJ stopping/VBG strategy/NN for/IN neural/JJ network/NN configurations/NNS ./.
With/IN this/DT early/JJ stopping/VBG strategy/NN ,/, we/PRP obtain/VBP significant/JJ speedups/NNS in/IN both/DT hyperparameter/NN optimization/NN and/CC meta/NN -/HYPH modeling/NN ./.
Particularly/RB in/IN the/DT context/NN of/IN meta/NN -/HYPH modeling/NN ,/, our/PRP$ method/NN can/MD learn/VB to/TO predict/VB the/DT performance/NN of/IN drastically/RB different/JJ architectures/NNS and/CC is/VBZ seamlessly/RB incorporated/VBN into/IN reinforcement/NN learning/NN -/HYPH based/VBN architecture/NN selection/NN algorithms/NNS ./.
Finally/RB ,/, we/PRP show/VBP that/IN our/PRP$ method/NN is/VBZ simpler/JJR ,/, faster/RBR ,/, and/CC more/RBR accurate/JJ than/IN Bayesian/JJ methods/NNS for/IN learning/VBG curve/NN prediction/NN ./.
