This/DT paper/NN presents/VBZ incremental/JJ network/NN quantization/NN (/-LRB- INQ/NN )/-RRB- ,/, a/DT novel/JJ method/NN ,/, targeting/VBG to/TO efficiently/RB convert/VB any/DT pre-trained/JJ full/JJ -/HYPH precision/NN convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- model/NN into/IN a/DT low/JJ -/HYPH precision/NN version/NN whose/WP$ weights/NNS are/VBP constrained/VBN to/TO be/VB either/CC powers/NNS of/IN two/CD or/CC zero/CD ./.
Unlike/IN existing/VBG methods/NNS which/WDT are/VBP struggled/VBN in/IN noticeable/JJ accuracy/NN loss/NN ,/, our/PRP$ INQ/NN has/VBZ the/DT potential/JJ to/TO resolve/VB this/DT issue/NN ,/, as/RB benefiting/VBG from/IN two/CD innovations/NNS ./.
On/IN one/CD hand/NN ,/, we/PRP introduce/VBP three/CD interdependent/JJ operations/NNS ,/, namely/RB weight/NN partition/NN ,/, group-wise/JJ quantization/NN and/CC re-training/NN ./.
A/DT well/RB -/HYPH proven/VBN measure/NN is/VBZ employed/VBN to/TO divide/VB the/DT weights/NNS in/IN each/DT layer/NN of/IN a/DT pre-trained/JJ CNN/NNP model/NN into/IN two/CD disjoint/NN groups/NNS ./.
The/DT weights/NNS in/IN the/DT first/JJ group/NN are/VBP responsible/JJ to/TO form/VB a/DT low/JJ -/HYPH precision/NN base/NN ,/, thus/RB they/PRP are/VBP quantized/VBN by/IN a/DT variable/JJ -/HYPH length/NN encoding/NN method/NN ./.
The/DT weights/NNS in/IN the/DT other/JJ group/NN are/VBP responsible/JJ to/TO compensate/VB for/IN the/DT accuracy/NN loss/NN from/IN the/DT quantization/NN ,/, thus/RB they/PRP are/VBP the/DT ones/NNS to/TO be/VB re-trained/VBN ./.
On/IN the/DT other/JJ hand/NN ,/, these/DT three/CD operations/NNS are/VBP repeated/VBN on/IN the/DT latest/JJS re-trained/VBN group/NN in/IN an/DT iterative/JJ manner/NN until/IN all/PDT the/DT weights/NNS are/VBP converted/VBN into/IN low/JJ -/HYPH precision/NN ones/NNS ,/, acting/VBG as/IN an/DT incremental/JJ network/NN quantization/NN and/CC accuracy/NN enhancement/NN procedure/NN ./.
Extensive/JJ experiments/NNS on/IN the/DT ImageNet/NNP classification/NN task/NN using/VBG almost/RB all/DT known/VBN deep/JJ CNN/NNP architectures/NNS including/VBG AlexNet/NNP ,/, VGG/NNP -/HYPH 16/CD ,/, GoogleNet/NNP and/CC ResNets/NNP well/RB testify/VB the/DT efficacy/NN of/IN the/DT proposed/JJ method/NN ./.
Specifically/RB ,/, at/IN 5/CD -/HYPH bit/NN quantization/NN ,/, our/PRP$ models/NNS have/VBP improved/VBN accuracy/NN than/IN the/DT 32/CD -/HYPH bit/NN floating/JJ -/HYPH point/NN references/NNS ./.
Taking/VBG ResNet/NNP -/HYPH 18/CD as/IN an/DT example/NN ,/, we/PRP further/RB show/VBP that/IN our/PRP$ quantized/VBN models/NNS with/IN 4/CD -/HYPH bit/NN ,/, 3/CD -/HYPH bit/NN and/CC 2/CD -/HYPH bit/NN ternary/JJ weights/NNS have/VBP improved/VBN or/CC very/RB similar/JJ accuracy/NN against/IN its/PRP$ 32/CD -/HYPH bit/NN floating/JJ -/HYPH point/NN baseline/NN ./.
Besides/RB ,/, impressive/JJ results/NNS with/IN the/DT combination/NN of/IN network/NN pruning/NN and/CC INQ/NN are/VBP also/RB reported/VBN ./.
The/DT code/NN will/MD be/VB made/VBN publicly/RB available/JJ ./.
