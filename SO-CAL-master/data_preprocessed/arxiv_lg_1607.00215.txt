Computational/JJ results/NNS demonstrate/VBP that/IN posterior/JJ sampling/NN for/IN reinforcement/NN learning/NN (/-LRB- PSRL/NNP )/-RRB- dramatically/RB outperforms/VBZ algorithms/NNS driven/VBN by/IN optimism/NN ,/, such/JJ as/IN UCRL2/NN ./.
We/PRP provide/VBP insight/NN into/IN the/DT extent/NN of/IN this/DT performance/NN boost/NN and/CC the/DT phenomenon/NN that/WDT drives/VBZ it/PRP ./.
We/PRP leverage/VBP this/DT insight/NN to/TO establish/VB an/DT $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- H/NN \/SYM sqrt/NN {/-LRB- SAT/NN }/-RRB- )/-RRB- $/$ expected/VBN regret/NN bound/VBN for/IN PSRL/NNP in/IN finite/NN -/HYPH horizon/NN episodic/JJ Markov/NNP decision/NN processes/NNS ,/, where/WRB $/$ H$/CD is/VBZ the/DT horizon/NN ,/, $/$ S$/CD is/VBZ the/DT number/NN of/IN states/NNS ,/, $/$ A$/$ is/VBZ the/DT number/NN of/IN actions/NNS and/CC $/$ T$/CD is/VBZ the/DT time/NN elapsed/VBN ./.
This/DT improves/VBZ upon/IN the/DT best/JJS previous/JJ bound/VBN of/IN $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- H/NN S/NN \/SYM sqrt/NN {/-LRB- AT/NN }/-RRB- )/-RRB- $/$ for/IN any/DT reinforcement/NN learning/VBG algorithm/NN ./.
