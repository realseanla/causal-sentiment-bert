The/DT general/JJ perception/NN is/VBZ that/IN kernel/NN methods/NNS are/VBP not/RB scalable/JJ ,/, and/CC neural/JJ nets/NNS are/VBP the/DT methods/NNS of/IN choice/NN for/IN nonlinear/JJ learning/NN problems/NNS ./.
Or/CC have/VBP we/PRP simply/RB not/RB tried/VBN hard/RB enough/RB for/IN kernel/NN methods/NNS ?/.
Here/RB we/PRP propose/VBP an/DT approach/NN that/WDT scales/VBZ up/RP kernel/NN methods/NNS using/VBG a/DT novel/JJ concept/NN called/VBN "/`` doubly/RB stochastic/JJ functional/JJ gradients/NNS "/'' ./.
Our/PRP$ approach/NN relies/VBZ on/IN the/DT fact/NN that/IN many/JJ kernel/NN methods/NNS can/MD be/VB expressed/VBN as/IN convex/NN optimization/NN problems/NNS ,/, and/CC we/PRP solve/VB the/DT problems/NNS by/IN making/VBG two/CD unbiased/JJ stochastic/JJ approximations/NNS to/IN the/DT functional/JJ gradient/NN ,/, one/CD using/VBG random/JJ training/NN points/NNS and/CC another/DT using/VBG random/JJ functions/NNS associated/VBN with/IN the/DT kernel/NN ,/, and/CC then/RB descending/VBG using/VBG this/DT noisy/JJ functional/JJ gradient/NN ./.
We/PRP show/VBP that/IN a/DT function/NN produced/VBN by/IN this/DT procedure/NN after/IN $/$ t/CD $/$ iterations/NNS converges/VBZ to/IN the/DT optimal/JJ function/NN in/IN the/DT reproducing/VBG kernel/NN Hilbert/NNP space/NN in/IN rate/NN $/$ O/UH (/-LRB- 1/CD //SYM t/NN )/-RRB- $/$ ,/, and/CC achieves/VBZ a/DT generalization/NN performance/NN of/IN $/$ O/UH (/-LRB- 1/CD //SYM \/SYM sqrt/SYM {/-LRB- t/NN }/-RRB- )/-RRB- $/$ ./.
This/DT doubly/RB stochasticity/NN also/RB allows/VBZ us/PRP to/TO avoid/VB keeping/VBG the/DT support/NN vectors/NNS and/CC to/TO implement/VB the/DT algorithm/NN in/IN a/DT small/JJ memory/NN footprint/NN ,/, which/WDT is/VBZ linear/JJ in/IN number/NN of/IN iterations/NNS and/CC independent/JJ of/IN data/NNS dimension/NN ./.
Our/PRP$ approach/NN can/MD readily/RB scale/VB kernel/NN methods/NNS up/IN to/IN the/DT regimes/NNS which/WDT are/VBP dominated/VBN by/IN neural/JJ nets/NNS ./.
We/PRP show/VBP that/IN our/PRP$ method/NN can/MD achieve/VB competitive/JJ performance/NN to/IN neural/JJ nets/NNS in/IN datasets/NNS such/JJ as/IN 8/CD million/CD handwritten/JJ digits/NNS from/IN MNIST/NNP ,/, 2.3/CD million/CD energy/NN materials/NNS from/IN MolecularSpace/NNP ,/, and/CC 1/CD million/CD photos/NNS from/IN ImageNet/NNP ./.
