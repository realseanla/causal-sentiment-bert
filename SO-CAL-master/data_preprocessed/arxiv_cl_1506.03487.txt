The/DT Paraphrase/NN Database/NN (/-LRB- PPDB/NN ;/: Ganitkevitch/NNP et/FW al./FW ,/, 2013/CD )/-RRB- is/VBZ an/DT extensive/JJ semantic/JJ resource/NN ,/, consisting/VBG of/IN a/DT list/NN of/IN phrase/NN pairs/NNS with/IN (/-LRB- heuristic/NN )/-RRB- confidence/NN estimates/NNS ./.
However/RB ,/, it/PRP is/VBZ still/RB unclear/JJ how/WRB it/PRP can/MD best/RB be/VB used/VBN ,/, due/IN to/IN the/DT heuristic/NN nature/NN of/IN the/DT confidences/NNS and/CC its/PRP$ necessarily/RB incomplete/JJ coverage/NN ./.
We/PRP propose/VBP models/NNS to/IN leverage/NN the/DT phrase/NN pairs/NNS from/IN the/DT PPDB/NN to/TO build/VB parametric/JJ paraphrase/NN models/NNS that/WDT score/VBP paraphrase/NN pairs/NNS more/RBR accurately/RB than/IN the/DT PPDB/NNP 's/POS internal/JJ scores/NNS while/IN simultaneously/RB improving/VBG its/PRP$ coverage/NN ./.
They/PRP allow/VBP for/IN learning/VBG phrase/NN embeddings/NNS as/RB well/RB as/IN improved/VBN word/NN embeddings/NNS ./.
Moreover/RB ,/, we/PRP introduce/VBP two/CD new/JJ ,/, manually/RB annotated/VBN datasets/NNS to/TO evaluate/VB short/JJ -/HYPH phrase/NN paraphrasing/NN models/NNS ./.
Using/VBG our/PRP$ paraphrase/NN model/NN trained/VBN using/VBG PPDB/NN ,/, we/PRP achieve/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN standard/JJ word/NN and/CC bigram/NN similarity/NN tasks/NNS and/CC beat/VBD strong/JJ baselines/NNS on/IN our/PRP$ new/JJ short/JJ phrase/NN paraphrase/NN tasks/NNS ./.
