We/PRP simulate/VBP the/DT training/NN of/IN a/DT set/NN of/IN state/NN of/IN the/DT art/NN neural/JJ networks/NNS ,/, the/DT Maxout/NNP networks/NNS (/-LRB- Goodfellow/NNP et/FW al./FW ,/, 2013a/NN )/-RRB- ,/, on/IN three/CD benchmark/NN datasets/NNS :/: the/DT MNIST/NNP ,/, CIFAR10/NN and/CC SVHN/NN ,/, with/IN three/CD distinct/JJ arithmetics/NNS :/: floating/VBG point/NN ,/, fixed/VBN point/NN and/CC dynamic/JJ fixed/VBN point/NN ./.
For/IN each/DT of/IN those/DT datasets/NNS and/CC for/IN each/DT of/IN those/DT arithmetics/NNS ,/, we/PRP assess/VBP the/DT impact/NN of/IN the/DT precision/NN of/IN the/DT computations/NNS on/IN the/DT final/JJ error/NN of/IN the/DT training/NN ./.
We/PRP find/VBP that/IN very/RB low/JJ precision/NN computation/NN is/VBZ sufficient/JJ not/RB just/RB for/IN running/VBG trained/VBN networks/NNS but/CC also/RB for/IN training/VBG them/PRP ./.
For/IN example/NN ,/, almost/RB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS were/VBD obtained/VBN on/IN most/JJS datasets/NNS with/IN around/RB 10/CD bits/NNS for/IN computing/VBG activations/NNS and/CC gradients/NNS ,/, and/CC 12/CD bits/NNS for/IN storing/VBG updated/VBN parameters/NNS ./.
