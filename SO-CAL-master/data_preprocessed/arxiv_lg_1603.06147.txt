The/DT existing/VBG machine/NN translation/NN systems/NNS ,/, whether/IN phrase/NN -/HYPH based/VBN or/CC neural/JJ ,/, have/VBP relied/VBN almost/RB exclusively/RB on/IN word/NN -/HYPH level/NN modelling/NN with/IN explicit/JJ segmentation/NN ./.
In/IN this/DT paper/NN ,/, we/PRP ask/VBP a/DT fundamental/JJ question/NN :/: can/MD neural/JJ machine/NN translation/NN generate/VBP a/DT character/NN sequence/NN without/IN any/DT explicit/JJ segmentation/NN ?/.
To/TO answer/VB this/DT question/NN ,/, we/PRP evaluate/VBP an/DT attention/NN -/HYPH based/VBN encoder/NN -/HYPH decoder/NN with/IN a/DT subword/NN -/HYPH level/NN encoder/NN and/CC a/DT character/NN -/HYPH level/NN decoder/NN on/IN four/CD language/NN pairs/NNS --/: En/NNP -/HYPH Cs/NNP ,/, En/NNP -/HYPH De/NNP ,/, En/NNP -/HYPH Ru/NNP and/CC En/NNP -/HYPH Fi/NNP --/: using/VBG the/DT parallel/JJ corpora/NNS from/IN WMT/NNP '/POS 15/CD ./.
Our/PRP$ experiments/NNS show/VBP that/IN the/DT models/NNS with/IN a/DT character/NN -/HYPH level/NN decoder/NN outperform/VBP the/DT ones/NNS with/IN a/DT subword/NN -/HYPH level/NN decoder/NN on/IN all/DT of/IN the/DT four/CD language/NN pairs/NNS ./.
Furthermore/RB ,/, the/DT ensembles/NNS of/IN neural/JJ models/NNS with/IN a/DT character/NN -/HYPH level/NN decoder/NN outperform/VBP the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN non-neural/JJ machine/NN translation/NN systems/NNS on/IN En/NNP -/HYPH Cs/NNP ,/, En/NNP -/HYPH De/NNP and/CC En/NNP -/HYPH Fi/NNP and/CC perform/VB comparably/RB on/IN En/NNP -/HYPH Ru/NNP ./.
