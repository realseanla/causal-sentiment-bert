Recurrent/JJ neural/JJ networks/NNS have/VBP recently/RB been/VBN used/VBN for/IN learning/VBG to/TO describe/VB images/NNS using/VBG natural/JJ language/NN ./.
However/RB ,/, it/PRP has/VBZ been/VBN observed/VBN that/IN these/DT models/NNS generalize/VB poorly/RB to/IN scenes/NNS that/WDT were/VBD not/RB observed/VBN during/IN training/NN ,/, possibly/RB depending/VBG too/RB strongly/RB on/IN the/DT statistics/NNS of/IN the/DT text/NN in/IN the/DT training/NN data/NNS ./.
Here/RB we/PRP propose/VBP to/TO describe/VB images/NNS using/VBG short/JJ structured/JJ representations/NNS ,/, aiming/VBG to/TO capture/VB the/DT crux/NN of/IN a/DT description/NN ./.
These/DT structured/JJ representations/NNS allow/VBP us/PRP to/TO tease/VB -/HYPH out/RP and/CC evaluate/VB separately/RB two/CD types/NNS of/IN generalization/NN :/: standard/JJ generalization/NN to/IN new/JJ images/NNS with/IN similar/JJ scenes/NNS ,/, and/CC generalization/NN to/IN new/JJ combinations/NNS of/IN known/VBN entities/NNS ./.
We/PRP compare/VBP two/CD learning/VBG approaches/NNS on/IN the/DT MS/NN -/HYPH COCO/NN dataset/NN :/: a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN recurrent/JJ network/NN based/VBN on/IN an/DT LSTM/NN (/-LRB- Show/NN ,/, Attend/VB and/CC Tell/VB )/-RRB- ,/, and/CC a/DT simple/JJ structured/JJ prediction/NN model/NN on/IN top/NN of/IN a/DT deep/JJ network/NN ./.
We/PRP find/VBP that/IN the/DT structured/JJ model/NN generalizes/VBZ to/IN new/JJ compositions/NNS substantially/RB better/JJR than/IN the/DT LSTM/NNP ,/, ~/SYM 7/CD times/NNS the/DT accuracy/NN of/IN predicting/VBG structured/JJ representations/NNS ./.
By/IN providing/VBG a/DT concrete/JJ method/NN to/TO quantify/VB generalization/NN for/IN unseen/JJ combinations/NNS ,/, we/PRP argue/VBP that/IN structured/JJ representations/NNS and/CC compositional/JJ splits/NNS are/VBP a/DT useful/JJ benchmark/NN for/IN image/NN captioning/NN ,/, and/CC advocate/VB compositional/JJ models/NNS that/WDT capture/VBP linguistic/JJ and/CC visual/JJ structure/NN ./.
