Pre-trained/JJ word/NN embeddings/NNS learned/VBN from/IN unlabeled/JJ text/NN have/VBP become/VBN a/DT standard/JJ component/NN of/IN neural/JJ network/NN architectures/NNS for/IN NLP/NN tasks/NNS ./.
However/RB ,/, in/IN most/JJS cases/NNS ,/, the/DT recurrent/JJ network/NN that/WDT operates/VBZ on/IN word/NN -/HYPH level/NN representations/NNS to/TO produce/VB context/NN sensitive/JJ representations/NNS is/VBZ trained/VBN on/IN relatively/RB little/JJ labeled/VBN data/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP demonstrate/VBP a/DT general/JJ semi-supervised/VBN approach/NN for/IN adding/VBG pre/AFX -/HYPH trained/VBN context/NN embeddings/NNS from/IN bidirectional/JJ language/NN models/NNS to/IN NLP/NN systems/NNS and/CC apply/VB it/PRP to/IN sequence/NN labeling/NN tasks/NNS ./.
We/PRP evaluate/VBP our/PRP$ model/NN on/IN two/CD standard/JJ datasets/NNS for/IN named/VBN entity/NN recognition/NN (/-LRB- NER/NN )/-RRB- and/CC chunking/VBG ,/, and/CC in/IN both/DT cases/NNS achieve/VBP state/NN of/IN the/DT art/NN results/NNS ,/, surpassing/VBG previous/JJ systems/NNS that/WDT use/VBP other/JJ forms/NNS of/IN transfer/NN or/CC joint/JJ learning/NN with/IN additional/JJ labeled/VBN data/NNS and/CC task/NN specific/JJ gazetteers/NNS ./.
