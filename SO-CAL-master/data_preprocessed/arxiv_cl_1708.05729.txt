Neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- approaches/NNS have/VBP improved/VBN the/DT state/NN of/IN the/DT art/NN in/IN many/JJ machine/NN translation/NN settings/NNS over/IN the/DT last/JJ couple/NN of/IN years/NNS ,/, but/CC they/PRP require/VBP large/JJ amounts/NNS of/IN training/NN data/NNS to/TO produce/VB sensible/JJ output/NN ./.
We/PRP demonstrate/VBP that/IN NMT/NNP can/MD be/VB used/VBN for/IN low/JJ -/HYPH resource/NN languages/NNS as/RB well/RB ,/, by/IN introducing/VBG more/JJR local/JJ dependencies/NNS and/CC using/VBG word/NN alignments/NNS to/TO learn/VB sentence/NN reordering/NN during/IN translation/NN ./.
In/IN addition/NN to/IN our/PRP$ novel/JJ model/NN ,/, we/PRP also/RB present/VBP an/DT empirical/JJ evaluation/NN of/IN low/JJ -/HYPH resource/NN phrase/NN -/HYPH based/VBN statistical/JJ machine/NN translation/NN (/-LRB- SMT/NN )/-RRB- and/CC NMT/NN to/TO investigate/VB the/DT lower/JJR limits/NNS of/IN the/DT respective/JJ technologies/NNS ./.
We/PRP find/VBP that/IN while/IN SMT/NN remains/VBZ the/DT best/JJS option/NN for/IN low/JJ -/HYPH resource/NN settings/NNS ,/, our/PRP$ method/NN can/MD produce/VB acceptable/JJ translations/NNS with/IN only/RB 70000/CD tokens/NNS of/IN training/NN data/NNS ,/, a/DT level/NN where/WRB the/DT baseline/NN NMT/NN system/NN fails/VBZ completely/RB ./.
