In/IN this/DT work/NN we/PRP explore/VBP recent/JJ advances/NNS in/IN Recurrent/JJ Neural/JJ Networks/NNS for/IN large/JJ scale/NN Language/NNP Modeling/VBG ,/, a/DT task/NN central/JJ to/IN language/NN understanding/NN ./.
We/PRP extend/VBP current/JJ models/NNS to/TO deal/VB with/IN two/CD key/JJ challenges/NNS present/JJ in/IN this/DT task/NN :/: corpora/NNS and/CC vocabulary/NN sizes/NNS ,/, and/CC complex/JJ ,/, long/JJ term/NN structure/NN of/IN language/NN ./.
We/PRP perform/VBP an/DT exhaustive/JJ study/NN on/IN techniques/NNS such/JJ as/IN character/NN Convolutional/JJ Neural/JJ Networks/NNS or/CC Long/JJ -/HYPH Short/JJ Term/NN Memory/NN ,/, on/IN the/DT One/CD Billion/CD Word/NNP Benchmark/NNP ./.
Our/PRP$ best/JJS single/JJ model/NN significantly/RB improves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN perplexity/NN from/IN 51.3/CD down/RB to/IN 30.0/CD (/-LRB- whilst/IN reducing/VBG the/DT number/NN of/IN parameters/NNS by/IN a/DT factor/NN of/IN 20/CD )/-RRB- ,/, while/IN an/DT ensemble/NN of/IN models/NNS sets/VBZ a/DT new/JJ record/NN by/IN improving/VBG perplexity/NN from/IN 41.0/CD down/RB to/IN 24.2/CD ./.
We/PRP also/RB release/VBP these/DT models/NNS for/IN the/DT NLP/NN and/CC ML/NNP community/NN to/TO study/VB and/CC improve/VB upon/IN ./.
