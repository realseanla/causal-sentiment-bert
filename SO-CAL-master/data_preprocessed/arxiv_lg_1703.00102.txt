In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT StochAstic/NNP Recursive/JJ grAdient/NN algoritHm/NN (/-LRB- SARAH/NNP )/-RRB- ,/, as/RB well/RB as/IN its/PRP$ practical/JJ variant/NN SARAH/NNP ,/, as/IN a/DT novel/JJ approach/NN to/IN the/DT finite/NN -/HYPH sum/NN minimization/NN problems/NNS ./.
Different/JJ from/IN the/DT vanilla/NN SGD/NN and/CC other/JJ modern/JJ stochastic/JJ methods/NNS such/JJ as/IN SVRG/NNP ,/, S2GD/NN ,/, SAG/NNP and/CC SAGA/NNP ,/, SARAH/NNP admits/VBZ a/DT simple/JJ recursive/JJ framework/NN for/IN updating/VBG stochastic/JJ gradient/NN estimates/NNS ;/: when/WRB comparing/VBG to/IN SAG/NNP //HYPH SAGA/NNP ,/, SARAH/NNP does/VBZ not/RB require/VB a/DT storage/NN of/IN past/JJ gradients/NNS ./.
The/DT linear/JJ convergence/NN rate/NN of/IN SARAH/NNP is/VBZ proven/VBN under/IN strong/JJ convexity/NN assumption/NN ./.
We/PRP also/RB prove/VBP a/DT linear/JJ convergence/NN rate/NN (/-LRB- in/IN the/DT strongly/RB convex/JJ case/NN )/-RRB- for/IN an/DT inner/JJ loop/NN of/IN SARAH/NNP ,/, the/DT property/NN that/WDT SVRG/NNP does/VBZ not/RB possess/VB ./.
Numerical/NNP experiments/NNS demonstrate/VBP the/DT efficiency/NN of/IN our/PRP$ algorithm/NN ./.
