Potential/JJ -/HYPH based/VBN reward/NN shaping/NN (/-LRB- PBRS/NN )/-RRB- is/VBZ an/DT effective/JJ and/CC popular/JJ technique/NN to/TO speed/VB up/RP reinforcement/NN learning/NN by/IN leveraging/VBG domain/NN knowledge/NN ./.
While/IN PBRS/NNP is/VBZ proven/VBN to/TO always/RB preserve/VB optimal/JJ policies/NNS ,/, its/PRP$ effect/NN on/IN learning/VBG speed/NN is/VBZ determined/VBN by/IN the/DT quality/NN of/IN its/PRP$ potential/JJ function/NN ,/, which/WDT ,/, in/IN turn/NN ,/, depends/VBZ on/IN both/CC the/DT underlying/VBG heuristic/NN and/CC the/DT scale/NN ./.
Knowing/VBG which/WDT heuristic/NN will/MD prove/VB effective/JJ requires/VBZ testing/VBG the/DT options/NNS beforehand/RB ,/, and/CC determining/VBG the/DT appropriate/JJ scale/NN requires/VBZ tuning/NN ,/, both/DT of/IN which/WDT introduce/VBP additional/JJ sample/NN complexity/NN ./.
We/PRP formulate/VBP a/DT PBRS/NNP framework/NN that/WDT reduces/VBZ learning/VBG speed/NN ,/, but/CC does/VBZ not/RB incur/VB extra/JJ sample/NN complexity/NN ./.
For/IN this/DT ,/, we/PRP propose/VBP to/IN simultaneously/RB learn/VB an/DT ensemble/NN of/IN policies/NNS ,/, shaped/VBN w.r.t./RB many/JJ heuristics/NNS and/CC on/IN a/DT range/NN of/IN scales/NNS ./.
The/DT target/NN policy/NN is/VBZ then/RB obtained/VBN by/IN voting/NN ./.
The/DT ensemble/NN needs/VBZ to/TO be/VB able/JJ to/TO efficiently/RB and/CC reliably/RB learn/VB off/IN -/HYPH policy/NN :/: requirements/NNS fulfilled/VBN by/IN the/DT recent/JJ Horde/NNP architecture/NN ,/, which/WDT we/PRP take/VBP as/IN our/PRP$ basis/NN ./.
We/PRP demonstrate/VBP empirically/RB that/IN (/-LRB- 1/CD )/-RRB- our/PRP$ ensemble/NN policy/NN outperforms/VBZ both/CC the/DT base/NN policy/NN ,/, and/CC its/PRP$ single/JJ -/HYPH heuristic/NN components/NNS ,/, and/CC (/-LRB- 2/LS )/-RRB- an/DT ensemble/NN over/IN a/DT general/JJ range/NN of/IN scales/NNS performs/VBZ at/IN least/JJS as/RB well/RB as/IN one/CD with/IN optimally/RB tuned/VBN components/NNS ./.
