The/DT performance/NN of/IN Neural/JJ Network/NN (/-LRB- NN/NNP )/-RRB- -/HYPH based/VBN language/NN models/NNS is/VBZ steadily/RB improving/VBG due/IN to/IN the/DT emergence/NN of/IN new/JJ architectures/NNS ,/, which/WDT are/VBP able/JJ to/TO learn/VB different/JJ natural/JJ language/NN characteristics/NNS ./.
This/DT paper/NN presents/VBZ a/DT novel/JJ framework/NN ,/, which/WDT shows/VBZ that/IN a/DT significant/JJ improvement/NN can/MD be/VB achieved/VBN by/IN combining/VBG different/JJ existing/VBG heterogeneous/JJ models/NNS in/IN a/DT single/JJ architecture/NN ./.
This/DT is/VBZ done/VBN through/IN 1/CD )/-RRB- a/DT feature/NN layer/NN ,/, which/WDT separately/RB learns/VBZ different/JJ NN/NNP -/HYPH based/VBN models/NNS and/CC 2/CD )/-RRB- a/DT mixture/NN layer/NN ,/, which/WDT merges/VBZ the/DT resulting/VBG model/NN features/NNS ./.
In/IN doing/VBG so/RB ,/, this/DT architecture/NN benefits/NNS from/IN the/DT learning/NN capabilities/NNS of/IN each/DT model/NN with/IN no/DT noticeable/JJ increase/NN in/IN the/DT number/NN of/IN model/NN parameters/NNS or/CC the/DT training/NN time/NN ./.
Extensive/JJ experiments/NNS conducted/VBN on/IN the/DT Penn/NNP Treebank/NNP (/-LRB- PTB/NNP )/-RRB- and/CC the/DT Large/JJ Text/VB Compression/NNP Benchmark/NNP (/-LRB- LTCB/NNP )/-RRB- corpus/NN showed/VBD a/DT significant/JJ reduction/NN of/IN the/DT perplexity/NN when/WRB compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN feedforward/NN as/RB well/RB as/IN recurrent/JJ neural/JJ network/NN architectures/NNS ./.
