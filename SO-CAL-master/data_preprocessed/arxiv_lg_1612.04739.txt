We/PRP present/VBP an/DT architecture/NN which/WDT lets/VBZ us/PRP train/NN deep/RB ,/, directed/VBN generative/JJ models/NNS with/IN many/JJ layers/NNS of/IN latent/JJ variables/NNS ./.
We/PRP include/VBP deterministic/JJ paths/NNS between/IN all/DT latent/JJ variables/NNS and/CC the/DT generated/VBN output/NN ,/, and/CC provide/VB a/DT richer/JJR set/NN of/IN connections/NNS between/IN computations/NNS for/IN inference/NN and/CC generation/NN ,/, which/WDT enables/VBZ more/JJR effective/JJ communication/NN of/IN information/NN throughout/IN the/DT model/NN during/IN training/NN ./.
To/TO improve/VB performance/NN on/IN natural/JJ images/NNS ,/, we/PRP incorporate/VBP a/DT lightweight/JJ autoregressive/JJ model/NN in/IN the/DT reconstruction/NN distribution/NN ./.
These/DT techniques/NNS permit/VBP end/NN -/HYPH to/IN -/HYPH end/NN training/NN of/IN models/NNS with/IN 10/CD layers/NNS of/IN latent/JJ variables/NNS ./.
Experiments/NNS show/VBP that/IN our/PRP$ approach/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN standard/JJ image/NN modelling/NN benchmarks/NNS ,/, can/MD expose/VB latent/JJ class/NN structure/NN in/IN the/DT absence/NN of/IN label/NN information/NN ,/, and/CC can/MD provide/VB convincing/JJ imputations/NNS of/IN occluded/VBN regions/NNS in/IN natural/JJ images/NNS ./.
