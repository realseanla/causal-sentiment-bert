Recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- are/VBP very/RB good/JJ at/IN modelling/VBG the/DT flow/NN of/IN text/NN ,/, but/CC typically/RB need/VBP to/TO be/VB trained/VBN on/IN a/DT far/RB larger/JJR corpus/NN than/IN is/VBZ available/JJ for/IN the/DT PAN/NN 2015/CD Author/NN Identification/NN task/NN ./.
This/DT paper/NN describes/VBZ a/DT novel/JJ approach/NN where/WRB the/DT output/NN layer/NN of/IN a/DT character/NN -/HYPH level/NN RNN/NN language/NN model/NN is/VBZ split/VBN into/IN several/JJ independent/JJ predictive/JJ sub-models/NNS ,/, each/DT representing/VBG an/DT author/NN ,/, while/IN the/DT recurrent/JJ layer/NN is/VBZ shared/VBN by/IN all/DT ./.
This/DT allows/VBZ the/DT recurrent/JJ layer/NN to/TO model/VB the/DT language/NN as/IN a/DT whole/NN without/IN over-fitting/VBG ,/, while/IN the/DT outputs/NNS select/VB aspects/NNS of/IN the/DT underlying/VBG model/NN that/WDT reflect/VBP their/PRP$ author/NN 's/POS style/NN ./.
The/DT method/NN proves/VBZ competitive/JJ ,/, ranking/VBG first/RB in/IN two/CD of/IN the/DT four/CD languages/NNS ./.
