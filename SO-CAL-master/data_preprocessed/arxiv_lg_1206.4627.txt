We/PRP study/VBP the/DT convergence/NN rate/NN of/IN stochastic/JJ optimization/NN of/IN exact/JJ (/-LRB- NP/NNP -/HYPH hard/JJ )/-RRB- objectives/NNS ,/, for/IN which/WDT only/RB biased/JJ estimates/NNS of/IN the/DT gradient/NN are/VBP available/JJ ./.
We/PRP motivate/VBP this/DT problem/NN in/IN the/DT context/NN of/IN learning/VBG the/DT structure/NN and/CC parameters/NNS of/IN Ising/NN models/NNS ./.
We/PRP first/RB provide/VBP a/DT convergence/NN -/HYPH rate/NN analysis/NN of/IN deterministic/JJ errors/NNS for/IN forward/RB -/HYPH backward/JJ splitting/NN (/-LRB- FBS/NN )/-RRB- ./.
We/PRP then/RB extend/VBP our/PRP$ analysis/NN to/IN biased/JJ stochastic/JJ errors/NNS ,/, by/IN first/JJ characterizing/VBG a/DT family/NN of/IN samplers/NNS and/CC providing/VBG a/DT high/JJ probability/NN bound/VBD that/IN allows/VBZ understanding/VBG not/RB only/RB FBS/NN ,/, but/CC also/RB proximal/JJ gradient/NN (/-LRB- PG/NN )/-RRB- methods/NNS ./.
We/PRP derive/VBP some/DT interesting/JJ conclusions/NNS :/: FBS/NN requires/VBZ only/RB a/DT logarithmically/RB increasing/VBG number/NN of/IN random/JJ samples/NNS in/IN order/NN to/TO converge/VB (/-LRB- although/IN at/IN a/DT very/RB low/JJ rate/NN )/-RRB- ;/: the/DT required/VBN number/NN of/IN random/JJ samples/NNS is/VBZ the/DT same/JJ for/IN the/DT deterministic/JJ and/CC the/DT biased/JJ stochastic/JJ setting/NN for/IN FBS/NN and/CC basic/JJ PG/NN ;/: accelerated/VBN PG/NN is/VBZ not/RB guaranteed/VBN to/TO converge/VB in/IN the/DT biased/JJ stochastic/JJ setting/NN ./.
