When/WRB faced/VBN with/IN complex/JJ choices/NNS ,/, users/NNS refine/VBP their/PRP$ own/JJ preference/NN criteria/NNS as/IN they/PRP explore/VBP the/DT catalogue/NN of/IN options/NNS ./.
In/IN this/DT paper/NN we/PRP propose/VBP an/DT approach/NN to/IN preference/NN elicitation/NN suited/VBN for/IN this/DT scenario/NN ./.
We/PRP extend/VBP Coactive/NNP Learning/NNP ,/, which/WDT iteratively/RB collects/VBZ manipulative/JJ feedback/NN ,/, to/TO optionally/RB query/VB example/NN critiques/NNS ./.
User/NN critiques/NNS are/VBP integrated/VBN into/IN the/DT learning/NN model/NN by/IN dynamically/RB extending/VBG the/DT feature/NN space/NN ./.
Our/PRP$ formulation/NN natively/RB supports/VBZ constructive/JJ learning/NN tasks/NNS ,/, where/WRB the/DT option/NN catalogue/NN is/VBZ generated/VBN on/IN -/HYPH the/DT -/HYPH fly/NN ./.
We/PRP present/VBP an/DT upper/JJ bound/VBN on/IN the/DT average/JJ regret/NN suffered/VBN by/IN the/DT learner/NN ./.
Our/PRP$ empirical/JJ analysis/NN highlights/VBZ the/DT promise/NN of/IN our/PRP$ approach/NN ./.
