Recently/RB ,/, many/JJ variance/NN reduced/VBN stochastic/JJ alternating/VBG direction/NN method/NN of/IN multipliers/NNS (/-LRB- ADMM/NN )/-RRB- methods/NNS (/-LRB- e.g./FW \/SYM SAG/NN -/HYPH ADMM/NN ,/, SDCA/NN -/HYPH ADMM/NN and/CC SVRG/NN -/HYPH ADMM/NN )/-RRB- have/VBP made/VBN exciting/JJ progress/NN such/JJ as/IN linear/JJ convergence/NN rates/NNS for/IN strongly/RB convex/JJ problems/NNS ./.
However/RB ,/, the/DT best/RBS known/VBN convergence/NN rate/NN for/IN general/JJ convex/NN problems/NNS is/VBZ O/NN (/-LRB- 1/CD //SYM T/NN )/-RRB- as/IN opposed/VBN to/IN O/NN (/-LRB- 1/CD //SYM T/NN ^/SYM 2/CD )/-RRB- of/IN accelerated/VBN batch/NN algorithms/NNS ,/, where/WRB $/$ T$/CD is/VBZ the/DT number/NN of/IN iterations/NNS ./.
Thus/RB ,/, there/EX still/RB remains/VBZ a/DT gap/NN in/IN convergence/NN rates/NNS between/IN existing/VBG stochastic/JJ ADMM/NN and/CC batch/NN algorithms/NNS ./.
To/TO bridge/VB this/DT gap/NN ,/, we/PRP introduce/VBP the/DT momentum/NN acceleration/NN trick/NN for/IN batch/NN optimization/NN into/IN the/DT stochastic/JJ variance/NN reduced/VBN gradient/NN based/VBN ADMM/NNP (/-LRB- SVRG/NNP -/HYPH ADMM/NNP )/-RRB- ,/, which/WDT leads/VBZ to/IN an/DT accelerated/VBN (/-LRB- ASVRG/NN -/HYPH ADMM/NN )/-RRB- method/NN ./.
Then/RB we/PRP design/VB two/CD different/JJ momentum/NN term/NN update/NN rules/NNS for/IN strongly/RB convex/JJ and/CC general/JJ convex/NN cases/NNS ./.
We/PRP prove/VBP that/IN ASVRG/NN -/HYPH ADMM/NN converges/VBZ linearly/RB for/IN strongly/RB convex/JJ problems/NNS ./.
Besides/IN having/VBG a/DT low/JJ per/IN -/HYPH iteration/NN complexity/NN as/IN existing/VBG stochastic/JJ ADMM/NN methods/NNS ,/, ASVRG/NN -/HYPH ADMM/NN improves/VBZ the/DT convergence/NN rate/NN on/IN general/JJ convex/NN problems/NNS from/IN O/NN (/-LRB- 1/CD //SYM T/NN )/-RRB- to/IN O/NN (/-LRB- 1/CD //SYM T/NN ^/SYM 2/CD )/-RRB- ./.
Our/PRP$ experimental/JJ results/NNS show/VBP the/DT effectiveness/NN of/IN ASVRG/NN -/HYPH ADMM/NN ./.
