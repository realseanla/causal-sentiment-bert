We/PRP propose/VBP a/DT new/JJ approach/NN to/IN the/DT problem/NN of/IN searching/VBG a/DT space/NN of/IN policies/NNS for/IN a/DT Markov/NNP decision/NN process/NN (/-LRB- MDP/NN )/-RRB- or/CC a/DT partially/RB observable/JJ Markov/NNP decision/NN process/NN (/-LRB- POMDP/NN )/-RRB- ,/, given/VBN a/DT model/NN ./.
Our/PRP$ approach/NN is/VBZ based/VBN on/IN the/DT following/VBG observation/NN :/: Any/DT (/-LRB- PO/NN )/-RRB- MDP/NN can/MD be/VB transformed/VBN into/IN an/DT "/`` equivalent/JJ "/'' POMDP/NN in/IN which/WDT all/DT state/NN transitions/NNS (/-LRB- given/VBN the/DT current/JJ state/NN and/CC action/NN )/-RRB- are/VBP deterministic/JJ ./.
This/DT reduces/VBZ the/DT general/JJ problem/NN of/IN policy/NN search/NN to/IN one/CD in/IN which/WDT we/PRP need/VBP only/RB consider/VB POMDPs/NNS with/IN deterministic/JJ transitions/NNS ./.
We/PRP give/VBP a/DT natural/JJ way/NN of/IN estimating/VBG the/DT value/NN of/IN all/DT policies/NNS in/IN these/DT transformed/VBN POMDPs/NNS ./.
Policy/NN search/NN is/VBZ then/RB simply/RB performed/VBN by/IN searching/VBG for/IN a/DT policy/NN with/IN high/JJ estimated/VBN value/NN ./.
We/PRP also/RB establish/VBP conditions/NNS under/IN which/WDT our/PRP$ value/NN estimates/NNS will/MD be/VB good/JJ ,/, recovering/VBG theoretical/JJ results/NNS similar/JJ to/IN those/DT of/IN Kearns/NNP ,/, Mansour/NNP and/CC Ng/NNP (/-LRB- 1999/CD )/-RRB- ,/, but/CC with/IN "/`` sample/NN complexity/NN "/'' bounds/NNS that/WDT have/VBP only/RB a/DT polynomial/JJ rather/RB than/IN exponential/JJ dependence/NN on/IN the/DT horizon/NN time/NN ./.
Our/PRP$ method/NN applies/VBZ to/IN arbitrary/JJ POMDPs/NNS ,/, including/VBG ones/NNS with/IN infinite/JJ state/NN and/CC action/NN spaces/NNS ./.
We/PRP also/RB present/JJ empirical/JJ results/NNS for/IN our/PRP$ approach/NN on/IN a/DT small/JJ discrete/JJ problem/NN ,/, and/CC on/IN a/DT complex/JJ continuous/JJ state/NN //HYPH continuous/JJ action/NN problem/NN involving/VBG learning/NN to/TO ride/VB a/DT bicycle/NN ./.
