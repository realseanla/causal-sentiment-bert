We/PRP describe/VBP a/DT simple/JJ neural/JJ language/NN model/NN that/WDT relies/VBZ only/RB on/IN character/NN -/HYPH level/NN inputs/NNS ./.
Predictions/NNS are/VBP still/RB made/VBN at/IN the/DT word/NN -/HYPH level/NN ./.
Our/PRP$ model/NN employs/VBZ a/DT convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- over/IN characters/NNS ,/, whose/WP$ output/NN is/VBZ given/VBN to/IN a/DT long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- recurrent/JJ neural/JJ network/NN language/NN model/NN (/-LRB- RNN/NN -/HYPH LM/NN )/-RRB- ./.
On/IN the/DT English/NNP Penn/NNP Treebank/NNP the/DT model/NN is/VBZ on/IN par/NN with/IN the/DT existing/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN despite/IN having/VBG 60/CD percent/NN fewer/JJR parameters/NNS ./.
On/IN languages/NNS with/IN rich/JJ morphology/NN (/-LRB- Czech/NNP ,/, German/NNP ,/, French/NNP ,/, Spanish/NNP ,/, Russian/NNP )/-RRB- ,/, the/DT model/NN consistently/RB outperforms/VBZ a/DT Kneser/NN -/HYPH Ney/NN baseline/NN (/-LRB- by/IN 30/CD -/SYM 35/CD percent/NN )/-RRB- and/CC a/DT word/NN -/HYPH level/NN LSTM/NN baseline/NN (/-LRB- by/IN 15/CD -/SYM 25/CD percent/NN )/-RRB- ,/, again/RB with/IN far/RB fewer/JJR parameters/NNS ./.
Our/PRP$ results/NNS suggest/VBP that/IN on/IN many/JJ languages/NNS ,/, character/NN inputs/NNS are/VBP sufficient/JJ for/IN language/NN modeling/NN ./.
