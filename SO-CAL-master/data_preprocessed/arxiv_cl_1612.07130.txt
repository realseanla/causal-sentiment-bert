In/IN this/DT paper/NN we/PRP propose/VBP and/CC carefully/RB evaluate/VB a/DT sequence/NN labeling/NN framework/NN which/WDT solely/RB utilizes/VBZ sparse/JJ indicator/NN features/NNS derived/VBN from/IN dense/JJ distributed/VBN word/NN representations/NNS ./.
The/DT proposed/VBN model/NN obtains/VBZ (/-LRB- near/RB )/-RRB- state/NN -/HYPH of/IN -/HYPH the/DT art/NN performance/NN for/IN both/DT part/NN -/HYPH of/IN -/HYPH speech/NN tagging/NN and/CC named/VBN entity/NN recognition/NN for/IN a/DT variety/NN of/IN languages/NNS ./.
Our/PRP$ model/NN relies/VBZ only/RB on/IN a/DT few/JJ thousand/CD sparse/JJ coding/NN -/HYPH derived/VBN features/NNS ,/, without/IN applying/VBG any/DT modification/NN of/IN the/DT word/NN representations/NNS employed/VBN for/IN the/DT different/JJ tasks/NNS ./.
The/DT proposed/VBN model/NN has/VBZ favorable/JJ generalization/NN properties/NNS as/IN it/PRP retains/VBZ over/IN 89.8/CD percent/NN of/IN its/PRP$ average/JJ POS/NN tagging/VBG accuracy/NN when/WRB trained/VBN at/IN 1.2/CD percent/NN of/IN the/DT total/JJ available/JJ training/NN data/NNS ,/, i.e./FW ~/SYM 150/CD sentences/NNS per/IN language/NN ./.
