The/DT goal/NN of/IN sentence/NN and/CC document/NN modeling/NN is/VBZ to/TO accurately/RB represent/VB the/DT meaning/NN of/IN sentences/NNS and/CC documents/NNS for/IN various/JJ Natural/NNP Language/NNP Processing/NNP tasks/NNS ./.
In/IN this/DT work/NN ,/, we/PRP present/VBP Dependency/NN Sensitive/JJ Convolutional/JJ Neural/JJ Networks/NNS (/-LRB- DSCNN/NNP )/-RRB- as/IN a/DT general/JJ -/HYPH purpose/NN classification/NN system/NN for/IN both/DT sentences/NNS and/CC documents/NNS ./.
DSCNN/NNP hierarchically/RB builds/VBZ textual/JJ representations/NNS by/IN processing/VBG pretrained/JJ word/NN embeddings/NNS via/IN Long/JJ Short/JJ -/HYPH Term/NN Memory/NN networks/NNS and/CC subsequently/RB extracting/VBG features/NNS with/IN convolution/NN operators/NNS ./.
Compared/VBN with/IN existing/VBG recursive/JJ neural/JJ models/NNS with/IN tree/NN structures/NNS ,/, DSCNN/NNP does/VBZ not/RB rely/VB on/IN parsers/NNS and/CC expensive/JJ phrase/NN labeling/NN ,/, and/CC thus/RB is/VBZ not/RB restricted/VBN to/IN sentence/NN -/HYPH level/NN tasks/NNS ./.
Moreover/RB ,/, unlike/IN other/JJ CNN/NNP -/HYPH based/VBN models/NNS that/WDT analyze/VBP sentences/NNS locally/RB by/IN sliding/VBG windows/NNS ,/, our/PRP$ system/NN captures/VBZ both/CC the/DT dependency/NN information/NN within/IN each/DT sentence/NN and/CC relationships/NNS across/IN sentences/NNS in/IN the/DT same/JJ document/NN ./.
Experiment/NN results/NNS demonstrate/VBP that/IN our/PRP$ approach/NN is/VBZ achieving/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN several/JJ tasks/NNS ,/, including/VBG sentiment/NN analysis/NN ,/, question/NN type/NN classification/NN ,/, and/CC subjectivity/NN classification/NN ./.
