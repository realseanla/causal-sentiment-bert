We/PRP present/VBP an/DT adaptive/JJ online/JJ gradient/NN descent/NN algorithm/NN to/TO solve/VB online/JJ convex/NN optimization/NN problems/NNS with/IN long/JJ -/HYPH term/NN constraints/NNS ,/, which/WDT are/VBP constraints/NNS that/WDT need/VBP to/TO be/VB satisfied/JJ when/WRB accumulated/VBN over/IN a/DT finite/JJ number/NN of/IN rounds/NNS T/NN ,/, but/CC can/MD be/VB violated/VBN in/IN intermediate/JJ rounds/NNS ./.
For/IN some/DT user/NN -/HYPH defined/VBN trade/NN -/HYPH off/NN parameter/NN $/$ \/SYM beta/NN $/$ $/$ \/CD in/IN $/$ (/-LRB- 0/CD ,/, 1/CD )/-RRB- ,/, the/DT proposed/VBN algorithm/NN achieves/VBZ cumulative/JJ regret/NN bounds/NNS of/IN O/NN (/-LRB- T/NN ^/SYM max/NN {/-LRB- $/$ \/SYM beta/NN $/$ ,1/CD --/: $/$ \/SYM beta/NN $/$ }/-RRB- )/-RRB- and/CC O/NN (/-LRB- T/NN ^/SYM (/-LRB- 1/CD --/: $/$ \/SYM beta/NN $/$ //SYM 2/CD )/-RRB- )/-RRB- for/IN the/DT loss/NN and/CC the/DT constraint/NN violations/NNS respectively/RB ./.
Our/PRP$ results/NNS hold/VBP for/IN convex/JJ losses/NNS and/CC can/MD handle/VB arbitrary/JJ convex/NN constraints/NNS without/IN requiring/VBG knowledge/NN of/IN the/DT number/NN of/IN rounds/NNS in/IN advance/NN ./.
Our/PRP$ contributions/NNS improve/VB over/IN the/DT best/RBS known/VBN cumulative/JJ regret/NN bounds/NNS by/IN Mahdavi/NNP ,/, et/FW al./FW (/-LRB- 2012/CD )/-RRB- that/WDT are/VBP respectively/RB O/NN (/-LRB- T/NN ^/SYM 1/2/CD )/-RRB- and/CC O/NN (/-LRB- T/NN ^/SYM 3/4/CD )/-RRB- for/IN general/JJ convex/NN domains/NNS ,/, and/CC respectively/RB O/NN (/-LRB- T/NN ^/SYM 2/3/CD )/-RRB- and/CC O/NN (/-LRB- T/NN ^/SYM 2/3/CD )/-RRB- when/WRB further/JJ restricting/VBG to/IN polyhedral/NN domains/NNS ./.
We/PRP supplement/VBP the/DT analysis/NN with/IN experiments/NNS validating/VBG the/DT performance/NN of/IN our/PRP$ algorithm/NN in/IN practice/NN ./.
