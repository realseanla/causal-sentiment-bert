Deep/JJ neural/JJ networks/NNS (/-LRB- DNN/NN )/-RRB- have/VBP achieved/VBN huge/JJ practical/JJ success/NN in/IN recent/JJ years/NNS ./.
However/RB ,/, its/PRP$ theoretical/JJ properties/NNS (/-LRB- in/IN particular/JJ generalization/NN ability/NN )/-RRB- are/VBP not/RB yet/RB very/RB clear/JJ ,/, since/IN existing/VBG error/NN bounds/NNS for/IN neural/JJ networks/NNS can/MD not/RB be/VB directly/RB used/VBN to/TO explain/VB the/DT statistical/JJ behaviors/NNS of/IN practically/RB adopted/VBN DNN/NNP models/NNS (/-LRB- which/WDT are/VBP multi-class/JJ in/IN their/PRP$ nature/NN and/CC may/MD contain/VB convolutional/JJ layers/NNS )/-RRB- ./.
To/TO tackle/VB the/DT challenge/NN ,/, we/PRP derive/VBP a/DT new/JJ margin/NN bound/VBN for/IN DNN/NNP in/IN this/DT paper/NN ,/, in/IN which/WDT the/DT expected/VBN 0/CD -/HYPH 1/CD error/NN of/IN a/DT DNN/NN model/NN is/VBZ upper/JJ bounded/VBN by/IN its/PRP$ empirical/JJ margin/NN error/NN plus/CC a/DT Rademacher/NNP Average/NNP based/VBN capacity/NN term/NN ./.
This/DT new/JJ bound/JJ is/VBZ very/RB general/JJ and/CC is/VBZ consistent/JJ with/IN the/DT empirical/JJ behaviors/NNS of/IN DNN/NN models/NNS observed/VBN in/IN our/PRP$ experiments/NNS ./.
According/VBG to/IN the/DT new/JJ bound/JJ ,/, minimizing/VBG the/DT empirical/JJ margin/NN error/NN can/MD effectively/RB improve/VB the/DT test/NN performance/NN of/IN DNN/NNP ./.
We/PRP therefore/RB propose/VBP large/JJ margin/NN DNN/NNP algorithms/NNS ,/, which/WDT impose/VBP margin/NN penalty/NN terms/NNS to/IN the/DT cross/NN entropy/NN loss/NN of/IN DNN/NNP ,/, so/RB as/IN to/TO reduce/VB the/DT margin/NN error/NN during/IN the/DT training/NN process/NN ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT proposed/VBN algorithms/NNS can/MD achieve/VB significantly/RB smaller/JJR empirical/JJ margin/NN errors/NNS ,/, as/RB well/RB as/IN better/JJR test/NN performances/NNS than/IN the/DT standard/JJ DNN/NNP algorithm/NN ./.
