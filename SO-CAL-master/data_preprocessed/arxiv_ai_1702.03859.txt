Usually/RB bilingual/JJ word/NN vectors/NNS are/VBP trained/VBN "/`` online/RB "/'' ./.
Mikolov/NNP et/FW al./FW showed/VBD they/PRP can/MD also/RB be/VB found/VBN "/`` offline/RB "/'' ,/, whereby/WRB two/CD pre-trained/JJ embeddings/NNS are/VBP aligned/VBN with/IN a/DT linear/JJ transformation/NN ,/, using/VBG dictionaries/NNS compiled/VBN from/IN expert/JJ knowledge/NN ./.
In/IN this/DT work/NN ,/, we/PRP prove/VBP that/IN the/DT linear/JJ transformation/NN between/IN two/CD spaces/NNS should/MD be/VB orthogonal/JJ ./.
This/DT transformation/NN can/MD be/VB obtained/VBN using/VBG the/DT singular/JJ value/NN decomposition/NN ./.
We/PRP introduce/VBP a/DT novel/JJ "/`` inverted/VBN softmax/NN "/'' for/IN identifying/VBG translation/NN pairs/NNS ,/, with/IN which/WDT we/PRP improve/VBP the/DT precision/NN @/IN 1/CD of/IN Mikolov/NNP 's/POS original/JJ mapping/NN from/IN 34/CD percent/NN to/IN 43/CD percent/NN ,/, when/WRB translating/VBG a/DT test/NN set/VBN composed/VBN of/IN both/DT common/JJ and/CC rare/JJ English/JJ words/NNS into/IN Italian/JJ ./.
Orthogonal/JJ transformations/NNS are/VBP more/RBR robust/JJ to/IN noise/NN ,/, enabling/VBG us/PRP to/TO learn/VB the/DT transformation/NN without/IN expert/NN bilingual/JJ signal/NN by/IN constructing/VBG a/DT "/`` pseudo-dictionary/JJ "/'' from/IN the/DT identical/JJ character/NN strings/NNS which/WDT appear/VBP in/IN both/DT languages/NNS ,/, achieving/VBG 40/CD percent/NN precision/NN on/IN the/DT same/JJ test/NN set/NN ./.
Finally/RB ,/, we/PRP extend/VBP our/PRP$ method/NN to/TO retrieve/VB the/DT true/JJ translations/NNS of/IN English/NNP sentences/NNS from/IN a/DT corpus/NN of/IN 200k/CD Italian/JJ sentences/NNS with/IN a/DT precision/NN @/IN 1/CD of/IN 68/CD percent/NN ./.
