Most/JJS of/IN the/DT existing/VBG neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- models/NNS focus/VBP on/IN the/DT conversion/NN of/IN sequential/JJ data/NNS and/CC do/VBP not/RB directly/RB take/VB syntax/NN into/IN consideration/NN ./.
We/PRP propose/VBP a/DT novel/JJ end/NN -/HYPH to/IN -/HYPH end/NN syntactic/JJ NMT/NN model/NN ,/, extending/VBG a/DT sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN with/IN the/DT source/NN -/HYPH side/NN phrase/NN structure/NN ./.
Our/PRP$ model/NN has/VBZ an/DT attention/NN mechanism/NN that/WDT enables/VBZ the/DT decoder/NN to/TO generate/VB a/DT translated/VBN word/NN while/IN softly/RB aligning/VBG it/PRP with/IN phrases/NNS as/RB well/RB as/IN words/NNS of/IN the/DT source/NN sentence/NN ./.
Experimental/JJ results/NNS on/IN the/DT WAT/NNP '15/CD English/NNP -/HYPH to/IN -/HYPH Japanese/JJ dataset/NN demonstrate/VBP that/IN our/PRP$ proposed/VBN model/NN outperforms/VBZ sequence/NN -/HYPH to/IN -/HYPH sequence/NN attentional/JJ NMT/NN models/NNS and/CC compares/VBZ favorably/RB with/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN tree/NN -/HYPH to/IN -/HYPH string/NN SMT/NN system/NN ./.
