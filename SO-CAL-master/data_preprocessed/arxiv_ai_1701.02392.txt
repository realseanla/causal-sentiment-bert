Deep/JJ Reinforcement/NN Learning/NN has/VBZ enabled/VBN the/DT learning/NN of/IN policies/NNS for/IN complex/JJ tasks/NNS in/IN partially/RB observable/JJ environments/NNS ,/, without/IN explicitly/RB learning/VBG the/DT underlying/VBG model/NN of/IN the/DT tasks/NNS ./.
While/IN such/JJ model/NN -/HYPH free/JJ methods/NNS achieve/VBP considerable/JJ performance/NN ,/, they/PRP often/RB ignore/VBP the/DT structure/NN of/IN task/NN ./.
We/PRP present/VBP a/DT natural/JJ representation/NN of/IN to/IN Reinforcement/NN Learning/NN (/-LRB- RL/NN )/-RRB- problems/NNS using/VBG Recurrent/JJ Convolutional/JJ Neural/JJ Networks/NNS (/-LRB- RCNNs/NNS )/-RRB- ,/, to/TO better/RBR exploit/VB this/DT inherent/JJ structure/NN ./.
We/PRP define/VBP 3/CD such/JJ RCNNs/NNS ,/, whose/WP$ forward/JJ passes/NNS execute/VB an/DT efficient/JJ Value/NN Iteration/NN ,/, propagate/VB beliefs/NNS of/IN state/NN in/IN partially/RB observable/JJ environments/NNS ,/, and/CC choose/VB optimal/JJ actions/NNS respectively/RB ./.
Backpropagating/VBG gradients/NNS through/IN these/DT RCNNs/NNS allows/VBZ the/DT system/NN to/TO explicitly/RB learn/VB the/DT Transition/NN Model/NN and/CC Reward/NN Function/NN associated/VBN with/IN the/DT underlying/VBG MDP/NNP ,/, serving/VBG as/IN an/DT elegant/JJ alternative/NN to/IN classical/JJ model/NN -/HYPH based/VBN RL/NNP ./.
We/PRP evaluate/VBP the/DT proposed/VBN algorithms/NNS in/IN simulation/NN ,/, considering/VBG a/DT robot/NN planning/NN problem/NN ./.
We/PRP demonstrate/VBP the/DT capability/NN of/IN our/PRP$ framework/NN to/TO reduce/VB the/DT cost/NN of/IN replanning/NN ,/, learn/VB accurate/JJ MDP/NN models/NNS ,/, and/CC finally/RB re-plan/VB with/IN learnt/VBN models/NNS to/TO achieve/VB near/IN -/HYPH optimal/JJ policies/NNS ./.
