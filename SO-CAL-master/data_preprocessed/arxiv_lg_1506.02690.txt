This/DT paper/NN proposes/VBZ a/DT set/NN of/IN new/JJ error/NN criteria/NNS and/CC learning/VBG approaches/NNS ,/, Adaptive/JJ Normalized/VBN Risk/NN -/HYPH Averting/VBG Training/NN (/-LRB- ANRAT/NN )/-RRB- ,/, to/TO attack/VB the/DT non-convex/JJ optimization/NN problem/NN in/IN training/NN deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- ./.
Theoretically/RB ,/, we/PRP demonstrate/VBP its/PRP$ effectiveness/NN on/IN global/JJ and/CC local/JJ convexity/NN lower/RBR -/HYPH bounded/VBN by/IN the/DT standard/JJ $/$ L_p/CD $/$ -/HYPH norm/NN error/NN ./.
By/IN analyzing/VBG the/DT gradient/NN on/IN the/DT convexity/NN index/NN $/$ \/CD lambda/NN $/$ ,/, we/PRP explain/VBP the/DT reason/NN why/WRB to/TO learn/VB $/$ \/CD lambda/NN $/$ adaptively/RB using/VBG gradient/NN descent/NN works/VBZ ./.
In/IN practice/NN ,/, we/PRP show/VBP how/WRB this/DT method/NN improves/VBZ training/NN of/IN deep/JJ neural/JJ networks/NNS to/TO solve/VB visual/JJ recognition/NN tasks/NNS on/IN the/DT MNIST/NN and/CC CIFAR/NN -/HYPH 10/CD datasets/NNS ./.
Without/IN using/VBG pretraining/NN or/CC other/JJ tricks/NNS ,/, we/PRP obtain/VBP results/NNS comparable/JJ or/CC superior/JJ to/IN those/DT reported/VBN in/IN recent/JJ literature/NN on/IN the/DT same/JJ tasks/NNS using/VBG standard/JJ ConvNets/NNP MSE/NNP //HYPH cross/NNP entropy/NN ./.
Performance/NN on/IN deep/JJ //HYPH shallow/JJ multilayer/JJ perceptrons/NNS and/CC Denoised/VBN Auto/NN -/HYPH encoders/NNS is/VBZ also/RB explored/VBN ./.
ANRAT/NNP can/MD be/VB combined/VBN with/IN other/JJ quasi-Newton/JJ training/NN methods/NNS ,/, innovative/JJ network/NN variants/NNS ,/, regularization/NN techniques/NNS and/CC other/JJ specific/JJ tricks/NNS in/IN DNNs/NNS ./.
Other/JJ than/IN unsupervised/JJ pretraining/NN ,/, it/PRP provides/VBZ a/DT new/JJ perspective/NN to/TO address/VB the/DT non-convex/JJ optimization/NN problem/NN in/IN DNNs/NNS ./.
