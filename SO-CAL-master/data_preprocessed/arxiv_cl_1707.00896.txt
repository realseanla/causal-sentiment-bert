Hierarchical/JJ attention/NN networks/NNS have/VBP recently/RB achieved/VBN remarkable/JJ performance/NN for/IN document/NN classification/NN in/IN a/DT given/VBN language/NN ./.
However/RB ,/, when/WRB multilingual/JJ document/NN collections/NNS are/VBP considered/VBN ,/, training/VBG such/JJ models/NNS separately/RB for/IN each/DT language/NN entails/VBZ linear/JJ parameter/NN growth/NN and/CC lack/NN of/IN cross-language/JJ transfer/NN ./.
Learning/VBG a/DT single/JJ multilingual/JJ model/NN with/IN fewer/JJR parameters/NNS is/VBZ therefore/RB a/DT challenging/JJ but/CC potentially/RB beneficial/JJ objective/NN ./.
To/IN this/DT end/NN ,/, we/PRP propose/VBP multilingual/JJ hierarchical/JJ attention/NN networks/NNS for/IN learning/VBG document/NN structures/NNS ,/, with/IN shared/VBN encoders/NNS and/CC //HYPH or/CC attention/NN mechanisms/NNS across/IN languages/NNS ,/, using/VBG multi-task/VB learning/NN and/CC an/DT aligned/VBN semantic/JJ space/NN as/IN input/NN ./.
We/PRP evaluate/VBP the/DT proposed/VBN models/NNS on/IN multilingual/JJ document/NN classification/NN with/IN disjoint/NN label/NN sets/NNS ,/, on/IN a/DT large/JJ dataset/NN which/WDT we/PRP provide/VBP ,/, with/IN 600k/CD news/NN documents/NNS in/IN 8/CD languages/NNS ,/, and/CC 5k/CD labels/NNS ./.
The/DT multilingual/JJ models/NNS outperform/VBP strong/JJ monolingual/JJ ones/NNS in/IN low/JJ -/HYPH resource/NN as/RB well/RB as/IN full/JJ -/HYPH resource/NN settings/NNS ,/, and/CC use/VB fewer/JJR parameters/NNS ,/, thus/RB confirming/VBG their/PRP$ computational/JJ efficiency/NN and/CC the/DT utility/NN of/IN cross-language/JJ transfer/NN ./.
