We/PRP study/VBP the/DT problem/NN of/IN cooperative/JJ multi-agent/JJ reinforcement/NN learning/VBG with/IN a/DT single/JJ joint/NN reward/NN signal/NN ./.
This/DT class/NN of/IN learning/NN problems/NNS is/VBZ difficult/JJ because/IN of/IN the/DT often/RB large/JJ combined/VBN action/NN and/CC observation/NN spaces/NNS ./.
In/IN the/DT fully/RB centralized/JJ and/CC decentralized/JJ approaches/NNS ,/, we/PRP find/VBP the/DT problem/NN of/IN spurious/JJ rewards/NNS and/CC a/DT phenomenon/NN we/PRP call/VBP the/DT "/`` lazy/JJ agent/NN "/'' problem/NN ,/, which/WDT arises/VBZ due/IN to/IN partial/JJ observability/NN ./.
We/PRP address/VBP these/DT problems/NNS by/IN training/VBG individual/JJ agents/NNS with/IN a/DT novel/JJ value/NN decomposition/NN network/NN architecture/NN ,/, which/WDT learns/VBZ to/IN decompose/VB the/DT team/NN value/NN function/NN into/IN agent-wise/JJ value/NN functions/NNS ./.
We/PRP perform/VBP an/DT experimental/JJ evaluation/NN across/IN a/DT range/NN of/IN partially/RB -/HYPH observable/JJ multi-agent/NN domains/NNS and/CC show/VBP that/IN learning/VBG such/JJ value/NN -/HYPH decompositions/NNS leads/VBZ to/IN superior/JJ results/NNS ,/, in/IN particular/JJ when/WRB combined/VBN with/IN weight/NN sharing/NN ,/, role/NN information/NN and/CC information/NN channels/NNS ./.
