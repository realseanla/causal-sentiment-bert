Reinforcement/NN learning/NN systems/NNS are/VBP often/RB concerned/VBN with/IN balancing/VBG exploration/NN of/IN untested/JJ actions/NNS against/IN exploitation/NN of/IN actions/NNS that/WDT are/VBP known/VBN to/TO be/VB good/JJ ./.
The/DT benefit/NN of/IN exploration/NN can/MD be/VB estimated/VBN using/VBG the/DT classical/JJ notion/NN of/IN Value/NN of/IN Information/NN -/HYPH the/DT expected/VBN improvement/NN in/IN future/JJ decision/NN quality/NN arising/VBG from/IN the/DT information/NN acquired/VBN by/IN exploration/NN ./.
Estimating/VBG this/DT quantity/NN requires/VBZ an/DT assessment/NN of/IN the/DT agent/NN 's/POS uncertainty/NN about/IN its/PRP$ current/JJ value/NN estimates/VBZ for/IN states/NNS ./.
In/IN this/DT paper/NN we/PRP investigate/VBP ways/NNS of/IN representing/VBG and/CC reasoning/NN about/IN this/DT uncertainty/NN in/IN algorithms/NNS where/WRB the/DT system/NN attempts/VBZ to/TO learn/VB a/DT model/NN of/IN its/PRP$ environment/NN ./.
We/PRP explicitly/RB represent/VBP uncertainty/NN about/IN the/DT parameters/NNS of/IN the/DT model/NN and/CC build/VB probability/NN distributions/NNS over/IN Q/NN -/HYPH values/NNS based/VBN on/IN these/DT ./.
These/DT distributions/NNS are/VBP used/VBN to/TO compute/VB a/DT myopic/JJ approximation/NN to/IN the/DT value/NN of/IN information/NN for/IN each/DT action/NN and/CC hence/RB to/TO select/VB the/DT action/NN that/WDT best/JJS balances/NNS exploration/NN and/CC exploitation/NN ./.
