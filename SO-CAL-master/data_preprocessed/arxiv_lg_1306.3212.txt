The/DT L1/NN -/HYPH regularized/VBN Gaussian/JJ maximum/JJ likelihood/NN estimator/NN (/-LRB- MLE/NNP )/-RRB- has/VBZ been/VBN shown/VBN to/TO have/VB strong/JJ statistical/JJ guarantees/NNS in/IN recovering/VBG a/DT sparse/JJ inverse/JJ covariance/NN matrix/NN ,/, or/CC alternatively/RB the/DT underlying/JJ graph/NN structure/NN of/IN a/DT Gaussian/NNP Markov/NNP Random/NNP Field/NNP ,/, from/IN very/RB limited/JJ samples/NNS ./.
We/PRP propose/VBP a/DT novel/JJ algorithm/NN for/IN solving/VBG the/DT resulting/VBG optimization/NN problem/NN which/WDT is/VBZ a/DT regularized/JJ log/NN -/HYPH determinant/NN program/NN ./.
In/IN contrast/NN to/IN recent/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS that/WDT largely/RB use/VBP first/JJ order/NN gradient/NN information/NN ,/, our/PRP$ algorithm/NN is/VBZ based/VBN on/IN Newton/NNP 's/POS method/NN and/CC employs/VBZ a/DT quadratic/JJ approximation/NN ,/, but/CC with/IN some/DT modifications/NNS that/WDT leverage/VBP the/DT structure/NN of/IN the/DT sparse/JJ Gaussian/NNP MLE/NNP problem/NN ./.
We/PRP show/VBP that/IN our/PRP$ method/NN is/VBZ superlinearly/JJ convergent/JJ ,/, and/CC present/JJ experimental/JJ results/NNS using/VBG synthetic/JJ and/CC real/JJ -/HYPH world/NN application/NN data/NNS that/WDT demonstrate/VBP the/DT considerable/JJ improvements/NNS in/IN performance/NN of/IN our/PRP$ method/NN when/WRB compared/VBN to/IN other/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS ./.
