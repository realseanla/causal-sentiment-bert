Minimizing/VBG a/DT convex/NN function/NN over/IN the/DT spectrahedron/NN ,/, i.e./FW ,/, the/DT set/NN of/IN all/DT positive/JJ semidefinite/NN matrices/NNS with/IN unit/NN trace/NN ,/, is/VBZ an/DT important/JJ optimization/NN task/NN with/IN many/JJ applications/NNS in/IN optimization/NN ,/, machine/NN learning/NN ,/, and/CC signal/NN processing/NN ./.
It/PRP is/VBZ also/RB notoriously/RB difficult/JJ to/TO solve/VB in/IN large/JJ -/HYPH scale/NN since/IN standard/JJ techniques/NNS require/VBP expensive/JJ matrix/NN decompositions/NNS ./.
An/DT alternative/NN ,/, is/VBZ the/DT conditional/JJ gradient/NN method/NN (/-LRB- aka/RB Frank/NNP -/HYPH Wolfe/NNP algorithm/NN )/-RRB- that/WDT regained/VBD much/JJ interest/NN in/IN recent/JJ years/NNS ,/, mostly/RB due/IN to/IN its/PRP$ application/NN to/IN this/DT specific/JJ setting/NN ./.
The/DT key/JJ benefit/NN of/IN the/DT CG/NN method/NN is/VBZ that/IN it/PRP avoids/VBZ expensive/JJ matrix/NN decompositions/NNS all/RB together/RB ,/, and/CC simply/RB requires/VBZ a/DT single/JJ eigenvector/NN computation/NN per/IN iteration/NN ,/, which/WDT is/VBZ much/RB more/RBR efficient/JJ ./.
On/IN the/DT downside/NN ,/, the/DT CG/NN method/NN ,/, in/IN general/JJ ,/, converges/VBZ with/IN an/DT inferior/JJ rate/NN ./.
The/DT error/NN for/IN minimizing/VBG a/DT $/$ \/SYM beta/NN $/$ -/HYPH smooth/JJ function/NN after/IN $/$ t/CD $/$ iterations/CD scales/NNS like/IN $/$ \/CD beta/NN //SYM t/NN $/$ ./.
This/DT convergence/NN rate/NN does/VBZ not/RB improve/VB even/RB if/IN the/DT function/NN is/VBZ also/RB strongly/RB convex/JJ ./.
