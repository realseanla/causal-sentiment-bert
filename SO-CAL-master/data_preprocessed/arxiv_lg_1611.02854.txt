Recent/JJ work/NN has/VBZ demonstrated/VBN the/DT effectiveness/NN of/IN employing/VBG explicit/JJ external/JJ memory/NN structures/NNS in/IN conjunction/NN with/IN deep/JJ neural/JJ models/NNS for/IN algorithmic/JJ learning/NN (/-LRB- Graves/NNP et/FW al./FW 2014/CD ;/: Weston/NNP et/FW al./FW 2014/CD )/-RRB- ./.
These/DT models/NNS utilize/VBP differentiable/JJ versions/NNS of/IN traditional/JJ discrete/JJ memory/NN -/HYPH access/NN structures/NNS (/-LRB- random/JJ access/NN ,/, stacks/NNS ,/, tapes/NNS )/-RRB- to/TO provide/VB the/DT variable/JJ -/HYPH length/NN storage/NN necessary/JJ for/IN computational/JJ tasks/NNS ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP an/DT alternative/JJ model/NN ,/, Lie/NN -/HYPH access/NN memory/NN ,/, that/DT is/VBZ explicitly/RB designed/VBN for/IN the/DT neural/JJ setting/NN ./.
In/IN this/DT paradigm/NN ,/, memory/NN is/VBZ accessed/VBN using/VBG a/DT continuous/JJ head/NN in/IN a/DT key/JJ -/HYPH space/NN manifold/NN ./.
The/DT head/NN is/VBZ moved/VBN via/IN Lie/NN group/NN actions/NNS ,/, such/JJ as/IN shifts/NNS or/CC rotations/NNS ,/, generated/VBN by/IN a/DT controller/NN ,/, and/CC soft/JJ memory/NN access/NN is/VBZ performed/VBN by/IN considering/VBG the/DT distance/NN to/IN keys/NNS associated/VBN with/IN each/DT memory/NN ./.
We/PRP argue/VBP that/IN Lie/NN groups/NNS provide/VBP a/DT natural/JJ generalization/NN of/IN discrete/JJ memory/NN structures/NNS ,/, such/JJ as/IN Turing/NNP machines/NNS ,/, as/IN they/PRP provide/VBP inverse/JJ and/CC identity/NN operators/NNS while/IN maintain/VBP differentiability/NN ./.
To/TO experiment/VB with/IN this/DT approach/NN ,/, we/PRP implement/VBP several/JJ simplified/VBN Lie/NN -/HYPH access/NN neural/JJ Turing/NN machine/NN (/-LRB- LANTM/NN )/-RRB- with/IN different/JJ Lie/NN groups/NNS ./.
We/PRP find/VBP that/IN this/DT approach/NN is/VBZ able/JJ to/TO perform/VB well/RB on/IN a/DT range/NN of/IN algorithmic/JJ tasks/NNS ./.
