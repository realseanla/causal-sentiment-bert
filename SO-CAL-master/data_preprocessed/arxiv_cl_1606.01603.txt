Most/JJS existing/VBG approaches/NNS for/IN zero/CD pronoun/NN resolution/NN are/VBP supervised/JJ approaches/NNS ,/, where/WRB annotated/VBN data/NNS are/VBP released/VBN by/IN shared/VBN task/NN organizers/NNS ./.
Therefore/RB ,/, the/DT lack/NN of/IN annotated/VBN data/NNS becomes/VBZ a/DT major/JJ obstacle/NN in/IN zero/CD pronoun/NN resolution/NN task/NN ./.
The/DT existing/VBG approaches/NNS mainly/RB face/VBP the/DT challenge/NN of/IN costing/VBG manpower/NN on/IN labeling/VBG the/DT extended/VBN data/NNS for/IN better/JJR training/NN performance/NN and/CC domain/NN adaption/NN ./.
To/TO alleviate/VB the/DT problem/NN above/RB ,/, in/IN this/DT paper/NN we/PRP propose/VBP a/DT simple/JJ but/CC novel/JJ approach/NN to/IN automatically/RB produce/VB large/JJ -/HYPH scale/NN pseudo/JJ training/NN data/NNS for/IN zero/CD pronoun/NN resolution/NN ./.
Furthermore/RB ,/, to/TO avoid/VB the/DT drawbacks/NNS of/IN the/DT feature/NN engineering/NN based/VBN approaches/NNS ,/, we/PRP proposed/VBD an/DT attention/NN -/HYPH based/VBN LSTM/NNP model/NN for/IN this/DT task/NN ./.
Experimental/JJ results/NNS show/VBP that/IN our/PRP$ proposed/VBN approach/NN outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS significantly/RB with/IN an/DT absolute/JJ improvement/NN of/IN 5.1/CD percent/NN F/NN -/HYPH score/NN in/IN OntoNotes/NNP 5.0/CD corpus/NN ./.
