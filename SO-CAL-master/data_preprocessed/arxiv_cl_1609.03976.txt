The/DT attention/NN mechanism/NN is/VBZ an/DT important/JJ part/NN of/IN the/DT neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- where/WRB it/PRP was/VBD reported/VBN to/TO produce/VB richer/JJR source/NN representation/NN compared/VBN to/IN fixed/VBN -/HYPH length/NN encoding/NN sequence/NN -/HYPH to/IN -/HYPH sequence/NN models/NNS ./.
Recently/RB ,/, the/DT effectiveness/NN of/IN attention/NN has/VBZ also/RB been/VBN explored/VBN in/IN the/DT context/NN of/IN image/NN captioning/NN ./.
In/IN this/DT work/NN ,/, we/PRP assess/VBP the/DT feasibility/NN of/IN a/DT multimodal/JJ attention/NN mechanism/NN that/WDT simultaneously/RB focus/VBP over/IN an/DT image/NN and/CC its/PRP$ natural/JJ language/NN description/NN for/IN generating/VBG a/DT description/NN in/IN another/DT language/NN ./.
We/PRP train/VBP several/JJ variants/NNS of/IN our/PRP$ proposed/VBN attention/NN mechanism/NN on/IN the/DT Multi30k/NN multilingual/JJ image/NN captioning/VBG dataset/NN ./.
We/PRP show/VBP that/IN a/DT dedicated/JJ attention/NN for/IN each/DT modality/NN achieves/VBZ up/RP to/IN 1.6/CD points/NNS in/IN BLEU/NN and/CC METEOR/NN compared/VBN to/IN a/DT textual/JJ NMT/NN baseline/NN ./.
