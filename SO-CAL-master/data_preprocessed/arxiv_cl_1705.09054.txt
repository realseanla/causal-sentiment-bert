Recognizing/VBG textual/JJ entailment/NN is/VBZ a/DT fundamental/JJ task/NN in/IN a/DT variety/NN of/IN text/NN mining/NN or/CC natural/JJ language/NN processing/NN applications/NNS ./.
This/DT paper/NN proposes/VBZ a/DT simple/JJ neural/JJ model/NN for/IN RTE/NNP problem/NN ./.
It/PRP first/RB matches/VBZ each/DT word/NN in/IN the/DT hypothesis/NN with/IN its/PRP$ most/RBS -/HYPH similar/JJ word/NN in/IN the/DT premise/NN ,/, producing/VBG an/DT augmented/VBN representation/NN of/IN the/DT hypothesis/NN conditioned/VBN on/IN the/DT premise/NN as/IN a/DT sequence/NN of/IN word/NN pairs/NNS ./.
The/DT LSTM/NNP model/NN is/VBZ then/RB used/VBN to/TO model/VB this/DT augmented/VBN sequence/NN ,/, and/CC the/DT final/JJ output/NN from/IN the/DT LSTM/NNP is/VBZ fed/VBN into/IN a/DT softmax/NN layer/NN to/TO make/VB the/DT prediction/NN ./.
Besides/IN the/DT base/NN model/NN ,/, in/IN order/NN to/TO enhance/VB its/PRP$ performance/NN ,/, we/PRP also/RB proposed/VBD three/CD techniques/NNS :/: the/DT integration/NN of/IN multiple/JJ word/NN -/HYPH embedding/NN library/NN ,/, bi-way/NN integration/NN ,/, and/CC ensemble/NN based/VBN on/IN model/NN averaging/NN ./.
Experimental/JJ results/NNS on/IN the/DT SNLI/NNP dataset/NN have/VBP shown/VBN that/IN the/DT three/CD techniques/NNS are/VBP effective/JJ in/IN boosting/VBG the/DT predicative/JJ accuracy/NN and/CC that/IN our/PRP$ method/NN outperforms/VBZ several/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH state/NN ones/NNS ./.
