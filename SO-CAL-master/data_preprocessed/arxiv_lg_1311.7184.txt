In/IN the/DT mixture/NN models/NNS problem/NN it/PRP is/VBZ assumed/VBN that/IN there/EX are/VBP $/$ K$/CD distributions/NNS $/$ \/SYM theta/NN _/NFP {/-LRB- 1/CD }/-RRB- ,/, \/SYM ldots/NNS ,/, \/SYM theta/NN _/NFP {/-LRB- K/NN }/-RRB- $/$ and/CC one/CD gets/VBZ to/TO observe/VB a/DT sample/NN from/IN a/DT mixture/NN of/IN these/DT distributions/NNS with/IN unknown/JJ coefficients/NNS ./.
The/DT goal/NN is/VBZ to/TO associate/VB instances/NNS with/IN their/PRP$ generating/VBG distributions/NNS ,/, or/CC to/TO identify/VB the/DT parameters/NNS of/IN the/DT hidden/JJ distributions/NNS ./.
In/IN this/DT work/NN we/PRP make/VBP the/DT assumption/NN that/IN we/PRP have/VBP access/NN to/IN several/JJ samples/NNS drawn/VBN from/IN the/DT same/JJ $/$ K$/CD underlying/VBG distributions/NNS ,/, but/CC with/IN different/JJ mixing/VBG weights/NNS ./.
As/IN with/IN topic/NN modeling/NN ,/, having/VBG multiple/JJ samples/NNS is/VBZ often/RB a/DT reasonable/JJ assumption/NN ./.
Instead/RB of/IN pooling/VBG the/DT data/NNS into/IN one/CD sample/NN ,/, we/PRP prove/VBP that/IN it/PRP is/VBZ possible/JJ to/TO use/VB the/DT differences/NNS between/IN the/DT samples/NNS to/TO better/RBR recover/VB the/DT underlying/JJ structure/NN ./.
We/PRP present/VBP algorithms/NNS that/WDT recover/VBP the/DT underlying/JJ structure/NN under/IN milder/JJR assumptions/NNS than/IN the/DT current/JJ state/NN of/IN art/NN when/WRB either/CC the/DT dimensionality/NN or/CC the/DT separation/NN is/VBZ high/JJ ./.
The/DT methods/NNS ,/, when/WRB applied/VBN to/IN topic/NN modeling/NN ,/, allow/VB generalization/NN to/IN words/NNS not/RB present/JJ in/IN the/DT training/NN data/NNS ./.
