We/PRP propose/VBP a/DT novel/JJ neural/JJ attention/NN architecture/NN to/TO tackle/VB machine/NN comprehension/NN tasks/NNS ,/, such/JJ as/IN answering/VBG Cloze/NNP -/HYPH style/NN queries/NNS with/IN respect/NN to/IN a/DT document/NN ./.
Unlike/IN previous/JJ models/NNS ,/, we/PRP do/VBP not/RB collapse/VB the/DT query/NN into/IN a/DT single/JJ vector/NN ,/, instead/RB we/PRP deploy/VBP an/DT iterative/JJ alternating/VBG attention/NN mechanism/NN that/WDT allows/VBZ a/DT fine/JJ -/HYPH grained/JJ exploration/NN of/IN both/CC the/DT query/NN and/CC the/DT document/NN ./.
Our/PRP$ model/NN outperforms/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baselines/NNS in/IN standard/JJ machine/NN comprehension/NN benchmarks/NNS such/JJ as/IN CNN/NNP news/NN articles/NNS and/CC the/DT Children/NNPS 's/POS Book/NNP Test/NNP (/-LRB- CBT/NNP )/-RRB- dataset/NN ./.
