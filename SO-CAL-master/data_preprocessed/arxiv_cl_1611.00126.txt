Most/JJS of/IN existing/VBG work/NN learn/VB sentiment/NN -/HYPH specific/JJ word/NN representation/NN for/IN improving/VBG Twitter/NNP sentiment/NN classification/NN ,/, which/WDT encoded/VBD both/CC n/NN -/HYPH gram/NN and/CC distant/JJ supervised/JJ tweet/NN sentiment/NN information/NN in/IN learning/NN process/NN ./.
They/PRP assume/VBP all/DT words/NNS within/IN a/DT tweet/NN have/VBP the/DT same/JJ sentiment/NN polarity/NN as/IN the/DT whole/JJ tweet/NN ,/, which/WDT ignores/VBZ the/DT word/NN its/PRP$ own/JJ sentiment/NN polarity/NN ./.
To/TO address/VB this/DT problem/NN ,/, we/PRP propose/VBP to/TO learn/VB sentiment/NN -/HYPH specific/JJ word/NN embedding/NN by/IN exploiting/VBG both/DT lexicon/NN resource/NN and/CC distant/JJ supervised/JJ information/NN ./.
We/PRP develop/VBP a/DT multi-level/JJ sentiment/NN -/HYPH enriched/JJ word/NN embedding/NN learning/NN method/NN ,/, which/WDT uses/VBZ parallel/JJ asymmetric/JJ neural/JJ network/NN to/TO model/VB n/NN -/HYPH gram/NN ,/, word/NN level/NN sentiment/NN and/CC tweet/NN level/NN sentiment/NN in/IN learning/NN process/NN ./.
Experiments/NNS on/IN standard/JJ benchmarks/NNS show/VBP our/PRP$ approach/NN outperforms/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS ./.
