In/IN this/DT paper/NN we/PRP introduce/VBP the/DT idea/NN of/IN improving/VBG the/DT performance/NN of/IN parametric/JJ temporal/JJ -/HYPH difference/NN (/-LRB- TD/NN )/-RRB- learning/NN algorithms/NNS by/IN selectively/RB emphasizing/VBG or/CC de-emphasizing/VBG their/PRP$ updates/NNS on/IN different/JJ time/NN steps/NNS ./.
In/IN particular/JJ ,/, we/PRP show/VBP that/IN varying/VBG the/DT emphasis/NN of/IN linear/JJ TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- 's/POS updates/NNS in/IN a/DT particular/JJ way/NN causes/VBZ its/PRP$ expected/VBN update/NN to/TO become/VB stable/JJ under/IN off/IN -/HYPH policy/NN training/NN ./.
The/DT only/JJ prior/JJ model/NN -/HYPH free/JJ TD/NN methods/NNS to/TO achieve/VB this/DT with/IN per/IN -/HYPH step/NN computation/NN linear/JJ in/IN the/DT number/NN of/IN function/NN approximation/NN parameters/NNS are/VBP the/DT gradient/NN -/HYPH TD/NN family/NN of/IN methods/NNS including/VBG TDC/NN ,/, GTD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- ,/, and/CC GQ/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- ./.
Compared/VBN to/IN these/DT methods/NNS ,/, our/PRP$ _/NFP emphatic/JJ TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- _/NFP is/VBZ simpler/JJR and/CC easier/JJR to/TO use/VB ;/: it/PRP has/VBZ only/RB one/CD learned/VBD parameter/NN vector/NN and/CC one/CD step/NN -/HYPH size/NN parameter/NN ./.
On/IN the/DT other/JJ hand/NN ,/, the/DT range/NN of/IN problems/NNS for/IN which/WDT it/PRP is/VBZ stable/JJ but/CC does/VBZ not/RB converge/VB with/IN probability/NN one/CD is/VBZ larger/JJR than/IN for/IN gradient/NN -/HYPH TD/NN methods/NNS ./.
Our/PRP$ treatment/NN includes/VBZ general/JJ state/NN -/HYPH dependent/JJ discounting/NN and/CC bootstrapping/NN functions/NNS ,/, and/CC a/DT way/NN of/IN specifying/VBG varying/VBG degrees/NNS of/IN interest/NN in/IN accurately/RB valuing/VBG different/JJ states/NNS ./.
