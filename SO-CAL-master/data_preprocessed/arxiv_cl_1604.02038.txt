We/PRP propose/VBP Sentence/NNP Level/NNP Recurrent/NNP Topic/NNP Model/NNP (/-LRB- SLRTM/NNP )/-RRB- ,/, a/DT new/JJ topic/NN model/NN that/WDT assumes/VBZ the/DT generation/NN of/IN each/DT word/NN within/IN a/DT sentence/NN to/TO depend/VB on/IN both/PDT the/DT topic/NN of/IN the/DT sentence/NN and/CC the/DT whole/JJ history/NN of/IN its/PRP$ preceding/VBG words/NNS in/IN the/DT sentence/NN ./.
Different/JJ from/IN conventional/JJ topic/NN models/NNS that/WDT largely/RB ignore/VBP the/DT sequential/JJ order/NN of/IN words/NNS or/CC their/PRP$ topic/NN coherence/NN ,/, SLRTM/NN gives/VBZ full/JJ characterization/NN to/IN them/PRP by/IN using/VBG a/DT Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNN/NN )/-RRB- based/VBN framework/NN ./.
Experimental/JJ results/NNS have/VBP shown/VBN that/IN SLRTM/NNP outperforms/VBZ several/JJ strong/JJ baselines/NNS on/IN various/JJ tasks/NNS ./.
Furthermore/RB ,/, SLRTM/NNP can/MD automatically/RB generate/VB sentences/NNS given/VBN a/DT topic/NN (/-LRB- i.e./FW ,/, topics/NNS to/IN sentences/NNS )/-RRB- ,/, which/WDT is/VBZ a/DT key/JJ technology/NN for/IN real/JJ world/NN applications/NNS such/JJ as/IN personalized/VBN short/JJ text/NN conversation/NN ./.
