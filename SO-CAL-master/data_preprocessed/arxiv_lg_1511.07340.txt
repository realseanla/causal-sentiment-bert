We/PRP introduce/VBP the/DT concept/NN of/IN a/DT Modular/JJ Autoencoder/NN (/-LRB- MAE/NN )/-RRB- ,/, capable/JJ of/IN learning/VBG a/DT set/NN of/IN diverse/JJ but/CC complementary/JJ representations/NNS from/IN unlabelled/JJ data/NNS ,/, that/DT can/MD later/RB be/VB used/VBN for/IN supervised/JJ tasks/NNS ./.
The/DT learning/NN of/IN the/DT representations/NNS is/VBZ controlled/VBN by/IN a/DT trade/NN off/IN parameter/NN ,/, and/CC we/PRP show/VBP on/IN six/CD benchmark/NN datasets/NNS the/DT optimum/JJ lies/NNS between/IN two/CD extremes/NNS :/: a/DT set/NN of/IN smaller/JJR ,/, independent/JJ autoencoders/NNS each/DT with/IN low/JJ capacity/NN ,/, versus/CC a/DT single/JJ monolithic/JJ encoding/NN ,/, outperforming/VBG an/DT appropriate/JJ baseline/NN ./.
In/IN the/DT present/JJ paper/NN we/PRP explore/VBP the/DT special/JJ case/NN of/IN linear/JJ MAE/NN ,/, and/CC derive/VBP an/DT SVD/NN -/HYPH based/VBN algorithm/NN which/WDT converges/VBZ several/JJ orders/NNS of/IN magnitude/NN faster/RBR than/IN gradient/NN descent/NN ./.
