We/PRP consider/VBP the/DT problem/NN of/IN outlier/NN rejection/NN in/IN single/JJ subspace/NN learning/NN ./.
Classical/JJ approaches/NNS work/VBP directly/RB with/IN a/DT low/JJ -/HYPH dimensional/JJ representation/NN of/IN the/DT subspace/NN ./.
Our/PRP$ approach/NN works/VBZ with/IN a/DT dual/JJ representation/NN of/IN the/DT subspace/NN and/CC hence/RB aims/VBZ to/TO find/VB its/PRP$ orthogonal/JJ complement/NN ./.
We/PRP pose/VBP this/DT problem/NN as/IN an/DT $/$ \/CD ell_1/CD $/$ -/HYPH minimization/NN problem/NN on/IN the/DT sphere/NN and/CC show/VBP that/IN ,/, under/IN certain/JJ conditions/NNS on/IN the/DT distribution/NN of/IN the/DT data/NNS ,/, any/DT global/JJ minimizer/NN of/IN this/DT non-convex/JJ problem/NN gives/VBZ a/DT vector/NN orthogonal/JJ to/IN the/DT subspace/NN ./.
Moreover/RB ,/, we/PRP show/VBP that/IN such/PDT a/DT vector/NN can/MD still/RB be/VB found/VBN by/IN relaxing/VBG the/DT non-convex/JJ problem/NN with/IN a/DT sequence/NN of/IN linear/JJ programs/NNS ./.
Experiments/NNS on/IN synthetic/JJ and/CC real/JJ data/NNS show/VBP that/IN the/DT proposed/VBN approach/NN ,/, which/WDT we/PRP call/VBP Dual/JJ Principal/NN Component/NN Pursuit/NN (/-LRB- DPCP/NN )/-RRB- ,/, outperforms/VBZ state/NN -/HYPH of/IN -/HYPH the/DT art/NN methods/NNS ,/, especially/RB in/IN the/DT case/NN of/IN high/JJ -/HYPH dimensional/JJ subspaces/NNS ./.
