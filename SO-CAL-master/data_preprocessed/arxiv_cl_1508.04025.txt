An/DT attentional/JJ mechanism/NN has/VBZ been/VBN used/VBN in/IN neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- to/IN selectively/RB focus/VB on/IN parts/NNS of/IN the/DT source/NN sentence/NN during/IN translation/NN ./.
However/RB ,/, there/EX has/VBZ been/VBN little/JJ work/NN exploring/VBG useful/JJ architectures/NNS for/IN attention/NN -/HYPH based/VBN NMT/NNP ./.
This/DT paper/NN examines/VBZ two/CD simple/JJ and/CC effective/JJ classes/NNS of/IN attentional/JJ mechanism/NN :/: a/DT global/JJ approach/NN which/WDT always/RB attends/VBZ to/IN all/DT source/NN words/NNS and/CC a/DT local/JJ one/NN that/WDT only/RB looks/VBZ at/IN a/DT subset/NN of/IN source/NN words/NNS at/IN a/DT time/NN ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN of/IN both/DT approaches/NNS over/IN the/DT WMT/NNP translation/NN tasks/NNS between/IN English/NNP and/CC German/NNP in/IN both/DT directions/NNS ./.
Our/PRP$ attentional/JJ NMTs/NNS provide/VBP a/DT boost/NN of/IN up/RB to/IN 5.0/CD BLEU/NN points/NNS over/IN non-attentional/JJ systems/NNS which/WDT already/RB incorporate/VBP known/JJ techniques/NNS such/JJ as/IN dropout/NN ./.
For/IN the/DT English/NNP to/IN German/JJ direction/NN ,/, we/PRP have/VBP established/VBN new/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS of/IN 23.0/CD BLEU/NN for/IN WMT/NNP '14/CD and/CC 25.9/CD BLEU/NN for/IN WMT/NNP '/POS 15/CD ./.
Our/PRP$ in/IN -/HYPH depth/NN analysis/NN sheds/VBZ light/NN on/IN which/WDT architectures/NNS are/VBP best/JJS and/CC we/PRP are/VBP first/JJ to/TO assess/VB attentional/JJ models/NNS using/VBG alignment/NN error/NN rates/NNS ./.
