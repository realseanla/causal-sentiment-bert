We/PRP describe/VBP a/DT new/JJ spatio/JJ -/HYPH temporal/JJ video/NN autoencoder/NN ,/, based/VBN on/IN a/DT classic/JJ spatial/JJ image/NN autoencoder/NN and/CC a/DT novel/JJ nested/VBN temporal/JJ autoencoder/NN ./.
The/DT temporal/JJ encoder/NN is/VBZ represented/VBN by/IN a/DT differentiable/JJ visual/JJ memory/NN composed/VBN of/IN convolutional/JJ long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- cells/NNS that/WDT integrate/VBP changes/NNS over/IN time/NN ./.
Here/RB we/PRP target/VBP motion/NN changes/NNS and/CC use/VB as/IN temporal/JJ decoder/NN a/DT robust/JJ optical/JJ flow/NN prediction/NN module/NN together/RB with/IN an/DT image/NN sampler/NN serving/VBG as/IN built/VBN -/HYPH in/RP feedback/NN loop/NN ./.
The/DT architecture/NN is/VBZ end/NN -/HYPH to/IN -/HYPH end/NN differentiable/NN ./.
At/IN each/DT time/NN step/NN ,/, the/DT system/NN receives/VBZ as/IN input/NN a/DT video/NN frame/NN ,/, predicts/VBZ the/DT optical/JJ flow/NN based/VBN on/IN the/DT current/JJ observation/NN and/CC the/DT LSTM/NNP memory/NN state/NN as/IN a/DT dense/JJ transformation/NN map/NN ,/, and/CC applies/VBZ it/PRP to/IN the/DT current/JJ frame/NN to/TO generate/VB the/DT next/JJ frame/NN ./.
By/IN minimising/VBG the/DT reconstruction/NN error/NN between/IN the/DT predicted/VBN next/JJ frame/NN and/CC the/DT corresponding/JJ ground/NN truth/NN next/JJ frame/NN ,/, we/PRP train/VBP the/DT whole/JJ system/NN to/TO extract/VB features/NNS useful/JJ for/IN motion/NN estimation/NN without/IN any/DT supervision/NN effort/NN ./.
We/PRP believe/VBP these/DT features/NNS can/MD in/IN turn/NN facilitate/VBP learning/VBG high/JJ -/HYPH level/NN tasks/NNS such/JJ as/IN path/NN planning/NN ,/, semantic/JJ segmentation/NN ,/, or/CC action/NN recognition/NN ,/, reducing/VBG the/DT overall/JJ supervision/NN effort/NN ./.
