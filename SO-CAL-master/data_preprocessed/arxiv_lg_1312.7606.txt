We/PRP apply/VBP diffusion/NN strategies/NNS to/TO develop/VB fully/RB -/HYPH distributed/VBN cooperative/JJ reinforcement/NN learning/VBG solutions/NNS ,/, in/IN which/WDT agents/NNS in/IN a/DT network/NN communicate/VBP only/RB with/IN their/PRP$ immediate/JJ neighbors/NNS to/TO improve/VB predictions/NNS about/IN their/PRP$ environment/NN ./.
The/DT algorithm/NN can/MD also/RB be/VB applied/VBN to/IN off/RB -/HYPH policy/NN sampling/NN ,/, meaning/VBG that/IN the/DT agents/NNS can/MD learn/VB to/TO predict/VB the/DT response/NN to/IN a/DT behavior/NN different/JJ from/IN the/DT actual/JJ policies/NNS they/PRP are/VBP following/VBG ./.
The/DT proposed/VBN distributed/VBN strategy/NN is/VBZ efficient/JJ ,/, with/IN linear/JJ complexity/NN in/IN both/CC computation/NN time/NN and/CC memory/NN footprint/NN ./.
We/PRP provide/VBP a/DT mean/NN -/HYPH square/NN -/HYPH error/NN performance/NN analysis/NN and/CC establish/VB convergence/NN under/IN constant/JJ step/NN -/HYPH size/NN updates/NNS ,/, which/WDT endow/VBP the/DT network/NN with/IN continuous/JJ learning/NN capabilities/NNS ./.
The/DT results/NNS show/VBP a/DT clear/JJ gain/NN from/IN cooperation/NN :/: when/WRB the/DT individual/JJ agents/NNS can/MD estimate/VB the/DT solution/NN ,/, cooperation/NN increases/VBZ stability/NN and/CC reduces/VBZ bias/NN and/CC variance/NN of/IN the/DT prediction/NN error/NN ;/: but/CC ,/, more/RBR importantly/RB ,/, the/DT network/NN is/VBZ able/JJ to/TO approach/VB the/DT optimal/JJ solution/NN even/RB when/WRB none/NN of/IN the/DT individual/JJ agents/NNS could/MD (/-LRB- e.g./FW ,/, when/WRB the/DT individual/JJ behavior/NN policies/NNS restrict/VBP each/DT agent/NN to/IN sample/NN a/DT small/JJ portion/NN of/IN the/DT state/NN space/NN )/-RRB- ./.
