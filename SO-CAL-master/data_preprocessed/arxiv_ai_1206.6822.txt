The/DT paper/NN analyzes/VBZ theoretically/RB and/CC empirically/RB the/DT performance/NN of/IN likelihood/NN weighting/NN (/-LRB- LW/NNP )/-RRB- on/IN a/DT subset/NN of/IN nodes/NNS in/IN Bayesian/JJ networks/NNS ./.
The/DT proposed/VBN scheme/NN requires/VBZ fewer/JJR samples/NNS to/TO converge/VB due/IN to/IN reduction/NN in/IN sampling/NN variance/NN ./.
The/DT method/NN exploits/VBZ the/DT structure/NN of/IN the/DT network/NN to/IN bound/VBN the/DT complexity/NN of/IN exact/JJ inference/NN used/VBN to/TO compute/VB sampling/NN distributions/NNS ,/, similar/JJ to/IN Gibbs/NNP cutset/NN sampling/NN ./.
Yet/RB ,/, the/DT extension/NN of/IN the/DT previosly/RB proposed/VBN cutset/NN sampling/NN principles/NNS to/IN likelihood/NN weighting/NN is/VBZ non-trivial/JJ due/IN to/IN differences/NNS in/IN the/DT sampling/NN processes/NNS of/IN Gibbs/NNP sampler/NN and/CC LW/NNP ./.
We/PRP demonstrate/VBP empirically/RB that/IN likelihood/NN weighting/NN on/IN a/DT cutset/NN (/-LRB- LWLC/NN )/-RRB- is/VBZ effective/JJ time-wise/RB and/CC has/VBZ a/DT lower/JJR rejection/NN rate/NN than/IN LW/NNP when/WRB applied/VBN to/IN networks/NNS with/IN many/JJ deterministic/JJ probabilities/NNS ./.
Finally/RB ,/, we/PRP show/VBP that/IN the/DT performance/NN of/IN likelihood/NN weighting/NN on/IN a/DT cutset/NN can/MD be/VB improved/VBN further/RB by/IN caching/VBG computed/VBN sampling/NN distributions/NNS and/CC ,/, consequently/RB ,/, learning/VBG '/'' zeros/NNS '/'' of/IN the/DT target/NN distribution/NN ./.
