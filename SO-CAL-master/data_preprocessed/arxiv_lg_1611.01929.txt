The/DT commonly/RB used/VBN Q/NN -/HYPH learning/VBG algorithm/NN combined/VBN with/IN function/NN approximation/NN induces/VBZ systematic/JJ overestimations/NNS of/IN state/NN -/HYPH action/NN values/NNS ./.
These/DT systematic/JJ errors/NNS might/MD cause/VB instability/NN ,/, poor/JJ performance/NN and/CC sometimes/RB divergence/NN of/IN learning/NN ./.
In/IN this/DT work/NN ,/, we/PRP present/VBP the/DT \/NN textsc/NN {/-LRB- Averaged/VBD Target/NNP DQN/NNP }/-RRB- (/-LRB- ADQN/NN )/-RRB- algorithm/NN ,/, an/DT adaptation/NN to/IN the/DT DQN/NNP class/NN of/IN algorithms/NNS which/WDT uses/VBZ a/DT weighted/JJ average/NN over/IN past/JJ learned/VBN networks/NNS to/TO reduce/VB generalization/NN noise/NN variance/NN ./.
As/IN a/DT consequence/NN ,/, this/DT leads/VBZ to/IN reduced/VBN overestimations/NNS ,/, more/RBR stable/JJ learning/NN process/NN and/CC improved/VBN performance/NN ./.
Additionally/RB ,/, we/PRP analyze/VBP ADQN/NN variance/NN reduction/NN along/IN trajectories/NNS and/CC demonstrate/VBP the/DT performance/NN of/IN ADQN/NNP on/IN a/DT toy/NN Gridworld/NNP problem/NN ,/, as/RB well/RB as/IN on/IN several/JJ of/IN the/DT Atari/NNP 2600/CD games/NNS from/IN the/DT Arcade/NNP Learning/NNP Environment/NNP ./.
