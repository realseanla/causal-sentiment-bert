We/PRP consider/VBP the/DT problem/NN of/IN recovering/VBG the/DT sparsest/JJS vector/NN in/IN a/DT subspace/NN $/$ \/CD mathcal/NN {/-LRB- S/NN }/-RRB- \/SYM subseteq/NN \/SYM mathbb/NN {/-LRB- R/NN }/-RRB- ^/SYM p/NN $/$ with/IN $/$ \/CD mathrm/NN {/-LRB- dim/NN }/-RRB- (/-LRB- \/SYM mathcal/NN {/-LRB- S/NN }/-RRB- )/-RRB- =/SYM n/NN &lt;/SYM p/NN $/$ ./.
This/DT problem/NN can/MD be/VB considered/VBN a/DT homogeneous/JJ variant/NN of/IN the/DT sparse/JJ recovery/NN problem/NN ,/, and/CC finds/VBZ applications/NNS in/IN sparse/JJ dictionary/NN learning/NN ,/, sparse/JJ PCA/NN ,/, and/CC other/JJ problems/NNS in/IN signal/NN processing/NN and/CC machine/NN learning/NN ./.
Simple/JJ convex/NN heuristics/NNS for/IN this/DT problem/NN provably/RB break/VB down/RP when/WRB the/DT fraction/NN of/IN nonzero/JJ entries/NNS in/IN the/DT target/NN sparse/JJ vector/NN substantially/RB exceeds/VBZ $/$ 1/CD //SYM \/SYM sqrt/SYM {/-LRB- n/NN }/-RRB- $/$ ./.
In/IN contrast/NN ,/, we/PRP exhibit/VBP a/DT relatively/RB simple/JJ nonconvex/JJ approach/NN based/VBN on/IN alternating/VBG directions/NNS ,/, which/WDT provably/RB succeeds/VBZ even/RB when/WRB the/DT fraction/NN of/IN nonzero/JJ entries/NNS is/VBZ $/$ \/SYM Omega/NN (/-LRB- 1/CD )/-RRB- $/$ ./.
To/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ practical/JJ algorithm/NN to/TO achieve/VB this/DT linear/JJ scaling/NN ./.
This/DT result/NN assumes/VBZ a/DT planted/VBN sparse/JJ model/NN ,/, in/IN which/WDT the/DT target/NN sparse/JJ vector/NN is/VBZ embedded/VBN in/IN an/DT otherwise/RB random/JJ subspace/NN ./.
Empirically/RB ,/, our/PRP$ proposed/VBN algorithm/NN also/RB succeeds/VBZ in/IN more/JJR challenging/JJ data/NNS models/NNS arising/VBG ,/, e.g./FW ,/, from/IN sparse/JJ dictionary/NN learning/NN ./.
