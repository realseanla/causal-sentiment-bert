Stochastic/JJ discriminative/JJ EM/NN (/-LRB- sdEM/NN )/-RRB- is/VBZ an/DT online/JJ -/HYPH EM/NN -/HYPH type/NN algorithm/NN for/IN discriminative/JJ training/NN of/IN probabilistic/JJ generative/JJ models/NNS belonging/VBG to/IN the/DT exponential/JJ family/NN ./.
In/IN this/DT work/NN ,/, we/PRP introduce/VBP and/CC justify/VBP this/DT algorithm/NN as/IN a/DT stochastic/JJ natural/JJ gradient/NN descent/NN method/NN ,/, i.e./FW a/DT method/NN which/WDT accounts/VBZ for/IN the/DT information/NN geometry/NN in/IN the/DT parameter/NN space/NN of/IN the/DT statistical/JJ model/NN ./.
We/PRP show/VBP how/WRB this/DT learning/NN algorithm/NN can/MD be/VB used/VBN to/TO train/VB probabilistic/JJ generative/JJ models/NNS by/IN minimizing/VBG different/JJ discriminative/JJ loss/NN functions/NNS ,/, such/JJ as/IN the/DT negative/JJ conditional/JJ log/NN -/HYPH likelihood/NN and/CC the/DT Hinge/NNP loss/NN ./.
The/DT resulting/VBG models/NNS trained/VBN by/IN sdEM/NN are/VBP always/RB generative/JJ (/-LRB- i.e./FW they/PRP define/VBP a/DT joint/JJ probability/NN distribution/NN )/-RRB- and/CC ,/, in/IN consequence/NN ,/, allows/VBZ to/TO deal/VB with/IN missing/VBG data/NNS and/CC latent/JJ variables/NNS in/IN a/DT principled/JJ way/NN either/CC when/WRB being/VBG learned/VBN or/CC when/WRB making/VBG predictions/NNS ./.
The/DT performance/NN of/IN this/DT method/NN is/VBZ illustrated/VBN by/IN several/JJ text/NN classification/NN problems/NNS for/IN which/WDT a/DT multinomial/JJ naive/JJ Bayes/NNS and/CC a/DT latent/JJ Dirichlet/NNP allocation/NN based/VBN classifier/NN are/VBP learned/VBN using/VBG different/JJ discriminative/JJ loss/NN functions/NNS ./.
