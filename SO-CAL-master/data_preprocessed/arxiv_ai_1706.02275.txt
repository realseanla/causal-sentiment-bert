We/PRP explore/VBP deep/JJ reinforcement/NN learning/VBG methods/NNS for/IN multi-agent/JJ domains/NNS ./.
We/PRP begin/VBP by/IN analyzing/VBG the/DT difficulty/NN of/IN traditional/JJ algorithms/NNS in/IN the/DT multi-agent/JJ case/NN :/: Q/NN -/HYPH learning/NN is/VBZ challenged/VBN by/IN an/DT inherent/JJ non-stationarity/NN of/IN the/DT environment/NN ,/, while/IN policy/NN gradient/NN suffers/VBZ from/IN a/DT variance/NN that/WDT increases/VBZ as/IN the/DT number/NN of/IN agents/NNS grows/VBZ ./.
We/PRP then/RB present/VBP an/DT adaptation/NN of/IN actor/NN -/HYPH critic/NN methods/NNS that/WDT considers/VBZ action/NN policies/NNS of/IN other/JJ agents/NNS and/CC is/VBZ able/JJ to/TO successfully/RB learn/VB policies/NNS that/WDT require/VBP complex/JJ multi-agent/JJ coordination/NN ./.
Additionally/RB ,/, we/PRP introduce/VBP a/DT training/NN regimen/NN utilizing/VBG an/DT ensemble/NN of/IN policies/NNS for/IN each/DT agent/NN that/WDT leads/VBZ to/IN more/JJR robust/JJ multi-agent/JJ policies/NNS ./.
We/PRP show/VBP the/DT strength/NN of/IN our/PRP$ approach/NN compared/VBN to/IN existing/VBG methods/NNS in/IN cooperative/JJ as/RB well/RB as/IN competitive/JJ scenarios/NNS ,/, where/WRB agent/NN populations/NNS are/VBP able/JJ to/TO discover/VB various/JJ physical/JJ and/CC informational/JJ coordination/NN strategies/NNS ./.
