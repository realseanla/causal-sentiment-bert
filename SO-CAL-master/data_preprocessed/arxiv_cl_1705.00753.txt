While/IN end/NN -/HYPH to/IN -/HYPH end/NN neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- has/VBZ made/VBN remarkable/JJ progress/NN recently/RB ,/, it/PRP still/RB suffers/VBZ from/IN the/DT data/NNS scarcity/NN problem/NN for/IN low/JJ -/HYPH resource/NN language/NN pairs/NNS and/CC domains/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT method/NN for/IN zero/CD -/HYPH resource/NN NMT/NN by/IN assuming/VBG that/IN parallel/JJ sentences/NNS have/VBP close/JJ probabilities/NNS of/IN generating/VBG a/DT sentence/NN in/IN a/DT third/JJ language/NN ./.
Based/VBN on/IN this/DT assumption/NN ,/, our/PRP$ method/NN is/VBZ able/JJ to/TO train/VB a/DT source/NN -/HYPH to/IN -/HYPH target/NN NMT/NN model/NN (/-LRB- "/`` student/NN "/'' )/-RRB- without/IN parallel/JJ corpora/NNS available/JJ ,/, guided/VBN by/IN an/DT existing/VBG pivot/NN -/HYPH to/IN -/HYPH target/NN NMT/NN model/NN (/-LRB- "/`` teacher/NN "/'' )/-RRB- on/IN a/DT source/NN -/HYPH pivot/NN parallel/JJ corpus/NN ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT proposed/JJ method/NN significantly/RB improves/VBZ over/IN a/DT baseline/NN pivot/NN -/HYPH based/VBN model/NN by/IN 3.0/CD BLEU/NN points/NNS across/IN various/JJ language/NN pairs/NNS ./.
