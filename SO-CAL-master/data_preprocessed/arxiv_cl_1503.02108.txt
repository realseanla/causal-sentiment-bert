We/PRP present/VBP a/DT Bayesian/JJ approach/NN to/IN adapting/VBG parameters/NNS of/IN a/DT well/RB -/HYPH trained/VBN context/NN -/HYPH dependent/JJ deep/JJ -/HYPH neural/JJ -/HYPH network/NN hid/VBD -/HYPH den/NN Markov/NNP models/NNS (/-LRB- CD/NN -/HYPH DNN/NN -/HYPH HMMs/NNS )/-RRB- to/TO improve/VB automatic/JJ speech/NN recognition/NN performance/NN ./.
Due/IN to/IN an/DT abundance/NN of/IN DNN/NN parameters/NNS but/CC with/IN only/RB a/DT limited/JJ amount/NN of/IN adaptation/NN data/NNS ,/, the/DT posterior/JJ probabilities/NNS of/IN unseen/JJ CD/NN states/NNS (/-LRB- senones/NNS )/-RRB- are/VBP often/RB pushed/VBN towards/IN zero/CD during/IN adaptation/NN ,/, and/CC consequently/RB the/DT ability/NN to/TO model/VB these/DT senones/NNS can/MD be/VB degraded/VBN ./.
We/PRP formulate/VBP maximum/NN a/DT posteriori/NN (/-LRB- MAP/NN )/-RRB- adaptation/NN of/IN parameters/NNS of/IN a/DT specially/RB designed/VBN CD/NN -/HYPH DNN/NN -/HYPH HMM/NN with/IN an/DT augmented/VBN linear/JJ hidden/JJ networks/NNS connected/VBN to/IN the/DT output/NN senones/NNS and/CC compare/VB it/PRP to/IN the/DT feature/NN space/NN maximum/NN a/DT posteriori/JJ linear/JJ regression/NN previously/RB proposed/VBN ./.
Experimental/JJ evidences/NNS on/IN the/DT 20,000/CD -/HYPH word/NN open/JJ vocabulary/NN Wall/NNP Street/NNP Journal/NNP task/NN demonstrate/VBP the/DT feasibility/NN of/IN the/DT proposed/VBN framework/NN ./.
In/IN supervised/JJ adaptation/NN ,/, the/DT proposed/VBN MAP/NN adaptation/NN provides/VBZ more/JJR than/IN 10/CD percent/NN relative/JJ error/NN reduction/NN and/CC consistently/RB outperforms/VBZ the/DT conventional/JJ transformation/NN based/VBN methods/NNS ./.
Furthermore/RB ,/, we/PRP present/VBP an/DT initial/JJ attempt/NN to/TO generate/VB hierarchical/JJ priors/NNS to/TO improve/VB adaptation/NN efficiency/NN and/CC effectiveness/NN with/IN limited/JJ adaptation/NN data/NNS by/IN exploiting/VBG similarities/NNS among/IN senones/NNS ./.
