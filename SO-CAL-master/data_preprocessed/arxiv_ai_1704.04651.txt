In/IN this/DT work/NN we/PRP present/VBP a/DT new/JJ reinforcement/NN learning/VBG agent/NN ,/, called/VBN Reactor/NNP (/-LRB- for/IN Retrace/NNP -/HYPH actor/NN )/-RRB- ,/, based/VBN on/IN an/DT off/RB -/HYPH policy/NN multi-step/JJ return/NN actor/NN -/HYPH critic/NN architecture/NN ./.
The/DT agent/NN uses/VBZ a/DT deep/JJ recurrent/JJ neural/JJ network/NN for/IN function/NN approximation/NN ./.
The/DT network/NN outputs/NNS a/DT target/NN policy/NN {/-LRB- \/SYM pi/NN }/-RRB- (/-LRB- the/DT actor/NN )/-RRB- ,/, an/DT action/NN -/HYPH value/NN Q/NN -/HYPH function/NN (/-LRB- the/DT critic/NN )/-RRB- evaluating/VBG the/DT current/JJ policy/NN {/-LRB- \/SYM pi/NN }/-RRB- ,/, and/CC an/DT estimated/VBN behavioral/JJ policy/NN {/-LRB- \/SYM hat/NN \/SYM mu/NNS }/-RRB- which/WDT we/PRP use/VBP for/IN off/RB -/HYPH policy/NN correction/NN ./.
The/DT agent/NN maintains/VBZ a/DT memory/NN buffer/NN filled/VBN with/IN past/JJ experiences/NNS ./.
The/DT critic/NN is/VBZ trained/VBN by/IN the/DT multi-step/JJ off/NN -/HYPH policy/NN Retrace/NNP algorithm/NN and/CC the/DT actor/NN is/VBZ trained/VBN by/IN a/DT novel/NN {/-LRB- \/SYM beta/NN }/-RRB- -/HYPH leave/NN -/HYPH one/CD -/HYPH out/RP policy/NN gradient/NN estimate/NN (/-LRB- which/WDT uses/VBZ both/CC the/DT off/NN -/HYPH policy/NN corrected/VBN return/NN and/CC the/DT estimated/VBN Q/NN -/HYPH function/NN )/-RRB- ./.
The/DT Reactor/NNP is/VBZ sample/NN -/HYPH efficient/JJ thanks/NNS to/IN the/DT use/NN of/IN memory/NN replay/NN ,/, and/CC numerical/JJ efficient/JJ since/IN it/PRP uses/VBZ multi-step/JJ returns/NNS ./.
Also/RB both/DT acting/NN and/CC learning/NN can/MD be/VB parallelized/VBN ./.
We/PRP evaluated/VBD our/PRP$ algorithm/NN on/IN 57/CD Atari/NNP 2600/CD games/NNS and/CC demonstrate/VBP that/IN it/PRP achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN ./.
