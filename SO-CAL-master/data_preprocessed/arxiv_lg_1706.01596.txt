We/PRP consider/VBP PAC/NN learning/NN of/IN probability/NN distributions/NNS (/-LRB- a.k.a./RB density/NN estimation/NN )/-RRB- ,/, where/WRB we/PRP are/VBP given/VBN an/DT i.i.d./NN sample/NN generated/VBN from/IN an/DT unknown/JJ target/NN distribution/NN ,/, and/CC want/VBP to/IN output/NN a/DT distribution/NN that/WDT is/VBZ close/JJ to/IN the/DT target/NN in/IN total/JJ variation/NN distance/NN ./.
Let/VB $/$ \/CD mathcal/JJ F$/NN be/VB an/DT arbitrary/JJ class/NN of/IN probability/NN distributions/NNS ,/, and/CC let/VB $/$ \/CD mathcal/NN {/-LRB- F/NN }/-RRB- ^/SYM k/CD $/$ denote/VB the/DT class/NN of/IN $/$ k/CD $/$ -/HYPH mixtures/NNS of/IN elements/NNS of/IN $/$ \/CD mathcal/JJ F$/NN ./.
Assuming/VBG the/DT existence/NN of/IN a/DT method/NN for/IN learning/VBG $/$ \/CD mathcal/JJ F$/NN with/IN sample/NN complexity/NN $/$ m/CD _/NFP {/-LRB- \/SYM mathcal/NN {/-LRB- F/NN }/-RRB- }/-RRB- (/-LRB- \/SYM varepsilon/NN )/-RRB- $/$ in/IN the/DT realizable/JJ setting/NN ,/, we/PRP provide/VBP a/DT method/NN for/IN learning/VBG $/$ \/CD mathcal/JJ F/NN ^/SYM k/CD $/$ with/IN sample/NN complexity/NN $/$ O/UH (/-LRB- {/-LRB- k/CD \/SYM log/NN k/CD \/SYM cdot/CD m/NN _/NFP {/-LRB- \/SYM mathcal/JJ F/NN }/-RRB- (/-LRB- \/SYM varepsilon/NN )/-RRB- }/-RRB- //, {/-LRB- \/SYM varepsilon/NN ^/SYM {/-LRB- 2/CD }/-RRB- }/-RRB- )/-RRB- $/$ in/IN the/DT agnostic/JJ setting/NN ./.
Our/PRP$ mixture/NN learning/VBG algorithm/NN has/VBZ the/DT property/NN that/WDT ,/, if/IN the/DT $/$ \/CD mathcal/JJ F$/NN -/HYPH learner/NN is/VBZ proper/JJ ,/, then/RB the/DT $/$ \/CD mathcal/JJ F/NN ^/SYM k/CD $/$ -/HYPH learner/NN is/VBZ proper/JJ as/RB well/RB ./.
