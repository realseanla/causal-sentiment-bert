Most/JJS neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- models/NNS are/VBP based/VBN on/IN the/DT sequential/JJ encoder/NN -/HYPH decoder/NN framework/NN ,/, which/WDT makes/VBZ no/DT use/NN of/IN syntactic/JJ information/NN ./.
In/IN this/DT paper/NN ,/, we/PRP improve/VBP this/DT model/NN by/IN explicitly/RB incorporating/VBG source/NN -/HYPH side/NN syntactic/JJ trees/NNS ./.
More/RBR specifically/RB ,/, we/PRP propose/VBP (/-LRB- 1/CD )/-RRB- a/DT bidirectional/JJ tree/NN encoder/NN which/WDT learns/VBZ both/CC sequential/JJ and/CC tree/NN structured/VBN representations/NNS ;/: (/-LRB- 2/LS )/-RRB- a/DT tree/NN -/HYPH coverage/NN model/NN that/WDT lets/VBZ the/DT attention/NN depend/VB on/IN the/DT source/NN -/HYPH side/NN syntax/NN ./.
Experiments/NNS on/IN Chinese/JJ -/HYPH English/JJ translation/NN demonstrate/VBP that/IN our/PRP$ proposed/VBN models/NNS outperform/VBP the/DT sequential/JJ attentional/JJ model/NN as/RB well/RB as/IN a/DT stronger/JJR baseline/NN with/IN a/DT bottom/NN -/HYPH up/NN tree/NN encoder/NN and/CC word/NN coverage/NN ./.
