Very/RB deep/JJ CNNs/NNS with/IN small/JJ 3x3/NN kernels/NNS have/VBP recently/RB been/VBN shown/VBN to/TO achieve/VB very/RB strong/JJ performance/NN as/IN acoustic/JJ models/NNS in/IN hybrid/NN NN/NNP -/HYPH HMM/NNP speech/NN recognition/NN systems/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP demonstrate/VBP that/IN the/DT accuracy/NN gains/NNS of/IN these/DT deep/JJ CNNs/NNS are/VBP retained/VBN both/DT on/IN larger/JJR scale/NN data/NNS ,/, and/CC after/IN sequence/NN training/NN ./.
We/PRP show/VBP this/DT by/IN carrying/VBG out/RP sequence/NN training/NN on/IN both/CC the/DT 300h/NN switchboard/NN -/HYPH 1/CD and/CC the/DT 2000h/NN switchboard/NN dataset/NN ./.
Furthermore/RB ,/, we/PRP investigate/VBP how/WRB pooling/VBG and/CC padding/NN in/IN time/NN influences/VBZ performance/NN ,/, both/CC in/IN terms/NNS of/IN word/NN error/NN rate/NN and/CC computational/JJ cost/NN ./.
We/PRP argue/VBP that/IN designing/VBG CNNs/NNS without/IN timepadding/VBG and/CC without/IN timepooling/VBG ,/, though/IN slightly/RB suboptimal/JJ for/IN accuracy/NN ,/, has/VBZ two/CD significant/JJ consequences/NNS ./.
Firstly/RB ,/, the/DT proposed/VBN design/NN allows/VBZ for/IN efficient/JJ evaluation/NN at/IN sequence/NN training/NN and/CC test/NN (/-LRB- deployment/NN )/-RRB- time/NN ./.
Secondly/RB ,/, this/DT design/NN principle/NN allows/VBZ for/IN batch/NN normalization/NN to/TO be/VB adopted/VBN to/IN CNNs/NNS on/IN sequence/NN data/NNS ./.
Our/PRP$ very/RB deep/JJ CNN/NNP model/NN sequence/NN trained/VBN on/IN the/DT 2000h/NN switchboard/NN dataset/NN obtains/VBZ 9.4/CD word/NN error/NN rate/NN on/IN the/DT Hub5/NN test/NN -/HYPH set/NN ,/, matching/VBG with/IN a/DT single/JJ model/NN the/DT performance/NN of/IN 2015/CD IBM/NNP system/NN combination/NN ,/, which/WDT was/VBD the/DT previous/JJ best/JJS published/VBN result/NN ./.
