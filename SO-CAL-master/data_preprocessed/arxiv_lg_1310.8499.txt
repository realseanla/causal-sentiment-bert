We/PRP introduce/VBP a/DT multilayer/JJ deep/JJ generative/JJ model/NN capable/JJ of/IN learning/VBG hierarchies/NNS of/IN sparse/JJ distributed/VBN representations/NNS from/IN data/NNS ./.
The/DT model/NN consists/VBZ of/IN several/JJ layers/NNS of/IN stochastic/JJ units/NNS ,/, with/IN autoregressive/JJ connections/NNS within/IN each/DT layer/NN ,/, which/WDT allows/VBZ for/IN efficient/JJ exact/JJ sampling/NN ./.
We/PRP train/VBP the/DT model/NN efficiently/RB using/VBG an/DT algorithm/NN derived/VBN from/IN the/DT Minimum/NNP Description/NN Length/NN (/-LRB- MDL/NN )/-RRB- principle/NN ,/, which/WDT minimizes/VBZ the/DT amount/NN of/IN information/NN contained/VBN in/IN the/DT joint/NN vector/NN of/IN data/NNS and/CC hidden/VBN unit/NN configurations/NNS for/IN the/DT training/NN set/NN ./.
As/IN we/PRP are/VBP not/RB given/VBN the/DT hidden/JJ unit/NN configurations/NNS corresponding/VBG to/IN the/DT training/NN data/NNS ,/, we/PRP use/VBP a/DT feedforward/JJ network/NN to/TO map/VB data/NN vectors/NNS to/IN configurations/NNS of/IN hidden/VBN units/NNS that/WDT are/VBP jointly/RB probable/JJ with/IN them/PRP and/CC train/VB it/PRP jointly/RB with/IN the/DT model/NN ./.
Our/PRP$ approach/NN can/MD also/RB be/VB seen/VBN as/IN maximizing/VBG a/DT lower/JJR bound/VBN on/IN the/DT log/NN -/HYPH likelihood/NN ,/, with/IN the/DT feedforward/JJ network/NN implementing/VBG approximate/JJ inference/NN ./.
