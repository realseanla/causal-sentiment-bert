We/PRP present/VBP a/DT solution/NN to/IN the/DT problem/NN of/IN paraphrase/NN identification/NN of/IN questions/NNS ./.
We/PRP focus/VBP on/IN a/DT recent/JJ dataset/NN of/IN question/NN pairs/NNS annotated/VBN with/IN binary/JJ paraphrase/NN labels/NNS and/CC show/VBP that/IN a/DT variant/NN of/IN the/DT decomposable/JJ attention/NN model/NN (/-LRB- Parikh/NNP et/FW al./FW ,/, 2016/CD )/-RRB- results/VBZ in/IN accurate/JJ performance/NN on/IN this/DT task/NN ,/, while/IN being/VBG far/RB simpler/JJR than/IN many/JJ competing/VBG neural/JJ architectures/NNS ./.
Furthermore/RB ,/, when/WRB the/DT model/NN is/VBZ pretrained/VBN on/IN a/DT noisy/JJ dataset/NN of/IN automatically/RB collected/VBN question/NN paraphrases/NNS ,/, it/PRP obtains/VBZ the/DT best/JJS reported/VBN performance/NN on/IN the/DT dataset/NN ./.
