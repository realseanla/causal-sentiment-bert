In/IN the/DT convex/NN optimization/NN approach/NN to/IN online/JJ regret/NN minimization/NN ,/, many/JJ methods/NNS have/VBP been/VBN developed/VBN to/TO guarantee/VB a/DT $/$ O/UH (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ regret/NN bound/VBN for/IN subdifferentiable/JJ convex/NN loss/NN functions/VBZ with/IN bounded/VBN subgradients/NNS by/IN means/NNS of/IN a/DT reduction/NN to/TO bounded/VBD linear/JJ loss/NN functions/NNS ./.
This/DT suggests/VBZ that/IN the/DT latter/JJ tend/VBP to/TO be/VB the/DT hardest/JJS loss/NN functions/NNS to/TO learn/VB against/IN ./.
We/PRP investigate/VBP this/DT question/NN in/IN a/DT systematic/JJ fashion/NN by/IN establishing/VBG $/$ \/SYM Omega/NN (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ lower/JJR bounds/NNS on/IN the/DT minimum/JJ achievable/JJ regret/NN for/IN a/DT class/NN of/IN piecewise/JJ linear/JJ loss/NN functions/VBZ that/IN subsumes/VBZ the/DT class/NN of/IN bounded/VBD linear/JJ loss/NN functions/NNS ./.
These/DT results/NNS hold/VBP in/IN a/DT completely/RB adversarial/JJ setting/NN ./.
In/IN contrast/NN ,/, we/PRP show/VBP that/IN the/DT minimum/JJ achievable/JJ regret/NN can/MD be/VB significantly/RB smaller/JJR when/WRB the/DT opponent/NN is/VBZ greedy/JJ ./.
