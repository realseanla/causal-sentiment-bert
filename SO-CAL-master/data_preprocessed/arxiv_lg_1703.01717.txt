Approximate/JJ Markov/NNP chain/NN Monte/NNP Carlo/NNP (/-LRB- MCMC/NNP )/-RRB- offers/VBZ the/DT promise/NN of/IN more/RBR rapid/JJ sampling/NN at/IN the/DT cost/NN of/IN more/JJR biased/JJ inference/NN ./.
Since/IN standard/JJ MCMC/NNP diagnostics/NNS fail/VBP to/TO detect/VB these/DT biases/NNS ,/, researchers/NNS have/VBP developed/VBN computable/JJ Stein/NNP discrepancy/NN measures/NNS that/WDT provably/RB determine/VBP the/DT convergence/NN of/IN a/DT sample/NN to/IN its/PRP$ target/NN distribution/NN ./.
This/DT approach/NN was/VBD recently/RB combined/VBN with/IN the/DT theory/NN of/IN reproducing/VBG kernels/NNS to/TO define/VB a/DT closed/JJ -/HYPH form/NN kernel/NN Stein/NNP discrepancy/NN (/-LRB- KSD/NN )/-RRB- computable/JJ by/IN summing/VBG kernel/NN evaluations/NNS across/IN pairs/NNS of/IN sample/NN points/NNS ./.
We/PRP develop/VBP a/DT theory/NN of/IN weak/JJ convergence/NN for/IN KSDs/NNS based/VBN on/IN Stein/NNP 's/POS method/NN ,/, demonstrate/VBP that/IN commonly/RB used/VBN KSDs/NNS fail/VBP to/TO detect/VB non-convergence/NN even/RB for/IN Gaussian/JJ targets/NNS ,/, and/CC show/VBP that/IN kernels/NNS with/IN slowly/RB decaying/VBG tails/NNS provably/RB determine/VBP convergence/NN for/IN a/DT large/JJ class/NN of/IN target/NN distributions/NNS ./.
The/DT resulting/VBG convergence/NN -/HYPH determining/VBG KSDs/NNS are/VBP suitable/JJ for/IN comparing/VBG biased/JJ ,/, exact/JJ ,/, and/CC deterministic/JJ sample/NN sequences/NNS and/CC simpler/JJR to/TO compute/VB and/CC parallelize/VB than/IN alternative/JJ Stein/NNP discrepancies/NNS ./.
We/PRP use/VBP our/PRP$ tools/NNS to/TO compare/VB biased/JJ samplers/NNS ,/, select/VB sampler/NN hyperparameters/NNS ,/, and/CC improve/VB upon/IN existing/VBG KSD/NNP approaches/VBZ to/IN one/CD -/HYPH sample/NN hypothesis/NN testing/NN and/CC sample/NN quality/NN improvement/NN ./.
