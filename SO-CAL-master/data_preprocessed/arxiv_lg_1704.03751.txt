When/WRB you/PRP need/VBP to/TO enable/VB deep/JJ learning/NN on/IN low/JJ -/HYPH cost/NN embedded/VBN SoCs/NNS ,/, is/VBZ it/PRP better/JJR to/IN port/NN an/DT existing/VBG deep/JJ learning/NN framework/NN or/CC should/MD you/PRP build/VB one/CD from/IN scratch/NN ?/.
In/IN this/DT paper/NN ,/, we/PRP share/VBP our/PRP$ practical/JJ experiences/NNS of/IN building/VBG an/DT embedded/VBN inference/NN engine/NN using/VBG ARM/NN Compute/VB Library/NNP (/-LRB- ACL/NN )/-RRB- ./.
The/DT results/NNS show/VBP that/IN ,/, contradictory/JJ to/IN conventional/JJ wisdoms/NNS ,/, for/IN simple/JJ models/NNS ,/, it/PRP takes/VBZ much/RB less/JJR development/NN time/NN to/TO build/VB an/DT inference/NN engine/NN from/IN scratch/NN compared/VBN to/IN porting/VBG existing/VBG frameworks/NNS ./.
In/IN addition/NN ,/, by/IN utilizing/VBG ACL/NN ,/, we/PRP managed/VBD to/TO build/VB an/DT inference/NN engine/NN that/WDT outperforms/VBZ TensorFlow/NNP by/IN 25/CD percent/NN ./.
Our/PRP$ conclusion/NN is/VBZ that/IN ,/, on/IN embedded/VBN devices/NNS ,/, we/PRP most/RBS likely/RB will/MD use/VB very/RB simple/JJ deep/JJ learning/NN models/NNS for/IN inference/NN ,/, and/CC with/IN well/RB -/HYPH developed/VBN building/NN blocks/NNS such/JJ as/IN ACL/NN ,/, it/PRP may/MD be/VB better/JJR in/IN both/DT performance/NN and/CC development/NN time/NN to/TO build/VB the/DT engine/NN from/IN scratch/NN ./.
