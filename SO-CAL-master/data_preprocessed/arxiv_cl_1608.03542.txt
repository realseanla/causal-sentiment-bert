We/PRP present/VBP WikiReading/NNP ,/, a/DT large/JJ -/HYPH scale/NN natural/JJ language/NN understanding/NN task/NN and/CC publicly/RB -/HYPH available/JJ dataset/NN with/IN 18/CD million/CD instances/NNS ./.
The/DT task/NN is/VBZ to/TO predict/VB textual/JJ values/NNS from/IN the/DT structured/JJ knowledge/NN base/NN Wikidata/NNP by/IN reading/VBG the/DT text/NN of/IN the/DT corresponding/VBG Wikipedia/NNP articles/NNS ./.
The/DT task/NN contains/VBZ a/DT rich/JJ variety/NN of/IN challenging/JJ classification/NN and/CC extraction/NN sub-tasks/NNS ,/, making/VBG it/PRP well/RB -/HYPH suited/VBN for/IN end/NN -/HYPH to/IN -/HYPH end/NN models/NNS such/JJ as/IN deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- ./.
We/PRP compare/VBP various/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN DNN/NN -/HYPH based/VBN architectures/NNS for/IN document/NN classification/NN ,/, information/NN extraction/NN ,/, and/CC question/NN answering/NN ./.
We/PRP find/VBP that/IN models/NNS supporting/VBG a/DT rich/JJ answer/NN space/NN ,/, such/JJ as/IN word/NN or/CC character/NN sequences/NNS ,/, perform/VB best/JJS ./.
Our/PRP$ best/RBS -/HYPH performing/VBG model/NN ,/, a/DT word/NN -/HYPH level/NN sequence/NN to/IN sequence/NN model/NN with/IN a/DT mechanism/NN to/TO copy/VB out/RB -/HYPH of/IN -/HYPH vocabulary/NN words/NNS ,/, obtains/VBZ an/DT accuracy/NN of/IN 71.8/CD percent/NN ./.
