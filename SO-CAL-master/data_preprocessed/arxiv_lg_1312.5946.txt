In/IN this/DT paper/NN ,/, we/PRP consider/VBP simple/JJ and/CC fast/JJ approaches/NNS to/IN initialize/VB the/DT Expectation/NN -/HYPH Maximization/NN algorithm/NN (/-LRB- EM/NN )/-RRB- for/IN multivariate/JJ Gaussian/JJ mixture/NN models/NNS ./.
We/PRP present/VBP new/JJ initialization/NN methods/NNS based/VBN on/IN the/DT well/NN -/HYPH known/VBN $/$ K$/CD -/HYPH means/NN algorithm/NN and/CC the/DT Gonzalez/NNP algorithm/NN ./.
These/DT methods/NNS close/VBP the/DT gap/NN between/IN simple/JJ uniform/JJ initialization/NN techniques/NNS and/CC complex/JJ methods/NNS ,/, that/WDT have/VBP been/VBN specifically/RB designed/VBN for/IN Gaussian/JJ mixture/NN models/NNS and/CC depend/VB on/IN the/DT right/JJ choice/NN of/IN hyperparameters/NNS ./.
In/IN our/PRP$ evaluation/NN we/PRP compare/VBP our/PRP$ methods/NNS with/IN a/DT commonly/RB used/VBN random/JJ initialization/NN method/NN ,/, an/DT approach/NN based/VBN on/IN agglomerative/JJ hierarchical/JJ clustering/NN ,/, and/CC a/DT known/VBN ,/, plain/JJ adaption/NN of/IN the/DT Gonzalez/NNP algorithm/NN ./.
Our/PRP$ results/NNS indicate/VBP that/IN algorithms/NNS based/VBN on/IN $/$ K$/CD -/HYPH means/NNS outperform/VB the/DT other/JJ methods/NNS ./.
