Short/JJ text/NN messages/NNS such/JJ as/IN tweets/NNS are/VBP very/RB noisy/JJ and/CC sparse/JJ in/IN their/PRP$ use/NN of/IN vocabulary/NN ./.
Traditional/JJ textual/JJ representations/NNS ,/, such/JJ as/IN tf/NN -/HYPH idf/NN ,/, have/VBP difficulty/NN grasping/VBG the/DT semantic/JJ meaning/NN of/IN such/JJ texts/NNS ,/, which/WDT is/VBZ important/JJ in/IN applications/NNS such/JJ as/IN event/NN detection/NN ,/, opinion/NN mining/NN ,/, news/NN recommendation/NN ,/, etc/FW ./.
We/PRP constructed/VBD a/DT method/NN based/VBN on/IN semantic/JJ word/NN embeddings/NNS and/CC frequency/NN information/NN to/TO arrive/VB at/IN low/JJ -/HYPH dimensional/JJ representations/NNS for/IN short/JJ texts/NNS designed/VBN to/TO capture/VB semantic/JJ similarity/NN ./.
For/IN this/DT purpose/NN we/PRP designed/VBD a/DT weight/NN -/HYPH based/VBN model/NN and/CC a/DT learning/NN procedure/NN based/VBN on/IN a/DT novel/JJ median/NN -/HYPH based/VBN loss/NN function/NN ./.
This/DT paper/NN discusses/VBZ the/DT details/NNS of/IN our/PRP$ model/NN and/CC the/DT optimization/NN methods/NNS ,/, together/RB with/IN the/DT experimental/JJ results/NNS on/IN both/DT Wikipedia/NNP and/CC Twitter/NNP data/NNS ./.
We/PRP find/VBP that/IN our/PRP$ method/NN outperforms/VBZ the/DT baseline/NN approaches/VBZ in/IN the/DT experiments/NNS ,/, and/CC that/IN it/PRP generalizes/VBZ well/RB on/IN different/JJ word/NN embeddings/NNS without/IN retraining/VBG ./.
Our/PRP$ method/NN is/VBZ therefore/RB capable/JJ of/IN retaining/VBG most/JJS of/IN the/DT semantic/JJ information/NN in/IN the/DT text/NN ,/, and/CC is/VBZ applicable/JJ out/IN -/HYPH of/IN -/HYPH the/DT -/HYPH box/NN ./.
