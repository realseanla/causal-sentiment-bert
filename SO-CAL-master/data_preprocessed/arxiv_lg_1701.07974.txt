Standard/JJ error/NN backpropagation/NN is/VBZ used/VBN in/IN almost/RB all/DT modern/JJ deep/JJ network/NN training/NN ./.
However/RB ,/, it/PRP typically/RB suffers/VBZ from/IN proliferation/NN of/IN saddle/NN points/NNS in/IN high/JJ -/HYPH dimensional/JJ parameter/NN space/NN ./.
Therefore/RB ,/, it/PRP is/VBZ highly/RB desirable/JJ to/TO design/VB an/DT efficient/JJ algorithm/NN to/TO escape/VB from/IN these/DT saddle/NN points/NNS and/CC reach/VB a/DT good/JJ parameter/NN region/NN of/IN better/JJR generalization/NN capabilities/NNS ,/, especially/RB based/VBN on/IN rough/JJ insights/NNS about/IN the/DT landscape/NN of/IN the/DT error/NN surface/NN ./.
Here/RB ,/, we/PRP propose/VBP a/DT simple/JJ extension/NN of/IN the/DT backpropagation/NN ,/, namely/RB reinforced/VBN backpropagation/NN ,/, which/WDT simply/RB adds/VBZ previous/JJ first/RB -/HYPH order/NN gradients/NNS in/IN a/DT stochastic/JJ manner/NN with/IN a/DT probability/NN that/WDT increases/VBZ with/IN learning/NN time/NN ./.
Extensive/JJ numerical/JJ simulations/NNS on/IN a/DT toy/NN deep/JJ learning/NN model/NN verify/VB its/PRP$ excellent/JJ performance/NN ./.
The/DT reinforced/VBN backpropagation/NN can/MD significantly/RB improve/VB test/NN performance/NN of/IN the/DT deep/JJ network/NN training/NN ,/, especially/RB when/WRB the/DT data/NNS are/VBP scarce/JJ ./.
The/DT performance/NN is/VBZ even/RB better/JJR than/IN that/DT of/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN stochastic/JJ optimization/NN algorithm/NN called/VBN Adam/NNP ,/, with/IN an/DT extra/JJ advantage/NN of/IN less/JJR computer/NN memory/NN required/VBN ./.
