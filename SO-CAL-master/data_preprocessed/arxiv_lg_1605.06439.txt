We/PRP consider/VBP online/JJ learning/NN algorithms/NNS that/WDT guarantee/VBP worst/JJS -/HYPH case/NN regret/NN rates/NNS in/IN adversarial/JJ environments/NNS (/-LRB- so/RB they/PRP can/MD be/VB deployed/VBN safely/RB and/CC will/MD perform/VB robustly/RB )/-RRB- ,/, yet/CC adapt/VB optimally/RB to/IN favorable/JJ stochastic/JJ environments/NNS (/-LRB- so/RB they/PRP will/MD perform/VB well/RB in/IN a/DT variety/NN of/IN settings/NNS of/IN practical/JJ importance/NN )/-RRB- ./.
We/PRP quantify/VBP the/DT friendliness/NN of/IN stochastic/JJ environments/NNS by/IN means/NNS of/IN the/DT well/NN -/HYPH known/VBN Bernstein/NNP (/-LRB- a.k.a./RB generalized/VBN Tsybakov/NNP margin/NN )/-RRB- condition/NN ./.
For/IN two/CD recent/JJ algorithms/NNS (/-LRB- Squint/NNP for/IN the/DT Hedge/NNP setting/NN and/CC MetaGrad/NNP for/IN online/JJ convex/NN optimization/NN )/-RRB- we/PRP show/VBP that/IN the/DT particular/JJ form/NN of/IN their/PRP$ data/NNS -/HYPH dependent/JJ individual/JJ -/HYPH sequence/NN regret/NN guarantees/NNS implies/VBZ that/IN they/PRP adapt/VBP automatically/RB to/IN the/DT Bernstein/NNP parameters/NNS of/IN the/DT stochastic/JJ environment/NN ./.
We/PRP prove/VBP that/IN these/DT algorithms/NNS attain/VB fast/JJ rates/NNS in/IN their/PRP$ respective/JJ settings/NNS both/CC in/IN expectation/NN and/CC with/IN high/JJ probability/NN ./.
