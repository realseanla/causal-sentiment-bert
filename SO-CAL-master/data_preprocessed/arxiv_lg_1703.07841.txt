We/PRP report/VBP the/DT results/NNS of/IN our/PRP$ classification/NN -/HYPH based/VBN machine/NN translation/NN model/NN ,/, built/VBN upon/IN the/DT framework/NN of/IN a/DT recurrent/JJ neural/JJ network/NN using/VBG gated/VBN recurrent/JJ units/NNS ./.
Unlike/IN other/JJ RNN/NN models/NNS that/WDT attempt/VBP to/TO maximize/VB the/DT overall/JJ conditional/JJ log/NN probability/NN of/IN sentences/NNS against/IN sentences/NNS ,/, our/PRP$ model/NN focuses/VBZ a/DT classification/NN approach/NN of/IN estimating/VBG the/DT conditional/JJ probability/NN of/IN the/DT next/JJ word/NN given/VBN the/DT input/NN sequence/NN ./.
This/DT simpler/JJR approach/NN using/VBG GRUs/NNS was/VBD hoped/VBN to/TO be/VB comparable/JJ with/IN more/RBR complicated/JJ RNN/NN models/NNS ,/, but/CC achievements/NNS in/IN this/DT implementation/NN were/VBD modest/JJ and/CC there/RB remains/VBZ a/DT lot/NN of/IN room/NN for/IN improving/VBG this/DT classification/NN approach/NN ./.
