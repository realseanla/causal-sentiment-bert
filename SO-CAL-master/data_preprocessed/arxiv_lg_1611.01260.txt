We/PRP propose/VBP a/DT technique/NN to/TO augment/VB network/NN layers/NNS by/IN adding/VBG a/DT linear/JJ gating/NN mechanism/NN ,/, which/WDT provides/VBZ a/DT way/NN to/TO learn/VB identity/NN mappings/NNS by/IN optimizing/VBG only/RB one/CD parameter/NN ./.
We/PRP also/RB introduce/VBP a/DT new/JJ metric/JJ which/WDT served/VBD as/IN basis/NN for/IN the/DT technique/NN ./.
It/PRP captures/VBZ the/DT difficulty/NN involved/VBN in/IN learning/VBG identity/NN mappings/NNS for/IN different/JJ types/NNS of/IN network/NN models/NNS ,/, and/CC provides/VBZ a/DT new/JJ theoretical/JJ intuition/NN for/IN the/DT increased/VBN depths/NNS of/IN models/NNS such/JJ as/IN Highway/NN and/CC Residual/JJ Networks/NNS ./.
We/PRP propose/VBP a/DT new/JJ model/NN ,/, the/DT Gated/JJ Residual/JJ Network/NN ,/, which/WDT is/VBZ the/DT result/NN when/WRB augmenting/VBG Residual/JJ Networks/NNS ./.
Experimental/JJ results/NNS show/VBP that/IN augmenting/VBG layers/NNS grants/NNS increased/VBD performance/NN ,/, less/JJR issues/NNS with/IN depth/NN ,/, and/CC more/JJR layer/NN independence/NN --/: fully/RB removing/VBG them/PRP does/VBZ not/RB cripple/VB the/DT model/NN ./.
We/PRP evaluate/VBP our/PRP$ method/NN on/IN MNIST/NN using/VBG fully/RB -/HYPH connected/VBN networks/NNS and/CC on/IN CIFAR/NN -/HYPH 10/CD using/VBG Wide/JJ ResNets/NNS ,/, achieving/VBG a/DT relative/JJ error/NN reduction/NN of/IN more/JJR than/IN 8/CD percent/NN in/IN the/DT latter/JJ when/WRB compared/VBN to/IN the/DT original/JJ model/NN ./.
