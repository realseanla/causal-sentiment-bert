Stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- ,/, which/WDT updates/NNS the/DT model/NN parameters/NNS by/IN adding/VBG a/DT local/JJ gradient/NN times/NNS a/DT learning/NN rate/NN at/IN each/DT step/NN ,/, is/VBZ widely/RB used/VBN in/IN model/NN training/NN of/IN machine/NN learning/NN algorithms/NNS such/JJ as/IN neural/JJ networks/NNS ./.
It/PRP is/VBZ observed/VBN that/IN the/DT models/NNS trained/VBN by/IN SGD/NNP are/VBP sensitive/JJ to/IN learning/VBG rates/NNS and/CC good/JJ learning/NN rates/NNS are/VBP problem/NN specific/JJ ./.
We/PRP propose/VBP an/DT algorithm/NN to/TO automatically/RB learn/VB learning/NN rates/NNS using/VBG neural/JJ network/NN based/VBN actor/NN -/HYPH critic/NN methods/NNS from/IN deep/JJ reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- ./.
In/IN particular/JJ ,/, we/PRP train/VBP a/DT policy/NN network/NN called/VBN actor/NN to/TO decide/VB the/DT learning/NN rate/NN at/IN each/DT step/NN during/IN training/NN ,/, and/CC a/DT value/NN network/NN called/VBN critic/NN to/TO give/VB feedback/NN about/IN quality/NN of/IN the/DT decision/NN (/-LRB- e.g./FW ,/, the/DT goodness/NN of/IN the/DT learning/NN rate/NN outputted/VBN by/IN the/DT actor/NN )/-RRB- that/IN the/DT actor/NN made/VBN ./.
The/DT introduction/NN of/IN auxiliary/JJ actor/NN and/CC critic/NN networks/NNS helps/VBZ the/DT main/JJ network/NN achieve/VB better/JJR performance/NN ./.
Experiments/NNS on/IN different/JJ datasets/NNS and/CC network/NN architectures/NNS show/VBP that/IN our/PRP$ approach/NN leads/VBZ to/IN better/JJR convergence/NN of/IN SGD/NNP than/IN human/JJ -/HYPH designed/VBN competitors/NNS ./.
