Recursive/JJ neural/JJ models/NNS ,/, which/WDT use/VBP syntactic/JJ parse/VB trees/NNS to/TO recursively/RB generate/VB representations/NNS bottom/JJ -/HYPH up/RB from/IN parse/VB children/NNS ,/, are/VBP a/DT popular/JJ new/JJ architecture/NN ,/, promising/VBG to/TO capture/VB structural/JJ properties/NNS like/IN the/DT scope/NN of/IN negation/NN or/CC long/JJ -/HYPH distance/NN semantic/JJ dependencies/NNS ./.
But/CC understanding/VBG exactly/RB which/WDT tasks/NNS this/DT parse/VB -/HYPH based/VBN method/NN is/VBZ appropriate/JJ for/IN remains/VBZ an/DT open/JJ question/NN ./.
In/IN this/DT paper/NN we/PRP benchmark/NN recursive/JJ neural/JJ models/NNS against/IN sequential/JJ recurrent/JJ neural/JJ models/NNS ,/, which/WDT are/VBP structured/VBN solely/RB on/IN word/NN sequences/NNS ./.
We/PRP investigate/VBP 5/CD tasks/NNS :/: sentiment/NN classification/NN on/IN (/-LRB- 1/CD )/-RRB- sentences/NNS and/CC (/-LRB- 2/LS )/-RRB- syntactic/JJ phrases/NNS ;/: (/-LRB- 3/LS )/-RRB- question/NN answering/NN ;/: (/-LRB- 4/LS )/-RRB- discourse/NN parsing/VBG ;/: (/-LRB- 5/LS )/-RRB- semantic/JJ relations/NNS (/-LRB- e.g./FW ,/, component/NN -/HYPH whole/NN between/IN nouns/NNS )/-RRB- ;/: We/PRP find/VBP that/IN recurrent/JJ models/NNS have/VBP equal/JJ or/CC superior/JJ performance/NN to/IN recursive/JJ models/NNS on/IN all/DT tasks/NNS except/IN one/CD :/: semantic/JJ relations/NNS between/IN nominals/NNS ./.
Our/PRP$ analysis/NN suggests/VBZ that/IN tasks/NNS relying/VBG on/IN the/DT scope/NN of/IN negation/NN (/-LRB- like/IN sentiment/NN )/-RRB- are/VBP well/RB -/HYPH handled/VBN by/IN sequential/JJ models/NNS ./.
Recursive/JJ models/NNS help/VBP only/RB with/IN tasks/NNS that/WDT require/VBP representing/VBG long/RB -/HYPH distance/NN relations/NNS between/IN words/NNS ./.
Our/PRP$ results/NNS offer/VBP insights/NNS on/IN the/DT design/NN of/IN neural/JJ architectures/NNS for/IN representation/NN learning/NN ./.
