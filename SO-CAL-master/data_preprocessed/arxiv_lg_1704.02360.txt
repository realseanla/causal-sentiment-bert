Voice/NNP conversion/NN (/-LRB- VC/NNP )/-RRB- using/VBG sequence/NN -/HYPH to/IN -/HYPH sequence/NN learning/NN of/IN context/NN posterior/JJ probabilities/NNS is/VBZ proposed/VBN ./.
Conventional/JJ VC/NNP using/VBG shared/VBN context/NN posterior/JJ probabilities/NNS predicts/VBZ target/NN speech/NN parameters/NNS from/IN the/DT context/NN posterior/JJ probabilities/NNS estimated/VBN from/IN the/DT source/NN speech/NN parameters/NNS ./.
Although/IN conventional/JJ VC/NNP can/MD be/VB built/VBN from/IN non-parallel/JJ data/NNS ,/, it/PRP is/VBZ difficult/JJ to/TO convert/VB speaker/NN individuality/NN such/JJ as/IN phonetic/JJ property/NN and/CC speaking/VBG rate/NN contained/VBN in/IN the/DT posterior/JJ probabilities/NNS because/IN the/DT source/NN posterior/JJ probabilities/NNS are/VBP directly/RB used/VBN for/IN predicting/VBG target/NN speech/NN parameters/NNS ./.
In/IN this/DT work/NN ,/, we/PRP assume/VBP that/IN the/DT training/NN data/NNS partly/RB include/VBP parallel/JJ speech/NN data/NNS and/CC propose/VB sequence/NN -/HYPH to/IN -/HYPH sequence/NN learning/NN between/IN the/DT source/NN and/CC target/NN posterior/JJ probabilities/NNS ./.
The/DT conversion/NN models/NNS perform/VBP non-linear/JJ and/CC variable/JJ -/HYPH length/NN transformation/NN from/IN the/DT source/NN probability/NN sequence/NN to/IN the/DT target/NN one/CD ./.
Further/RB ,/, we/PRP propose/VBP a/DT joint/JJ training/NN algorithm/NN for/IN the/DT modules/NNS ./.
In/IN contrast/NN to/IN conventional/JJ VC/NNP ,/, which/WDT separately/RB trains/VBZ the/DT speech/NN recognition/NN that/WDT estimates/VBZ posterior/JJ probabilities/NNS and/CC the/DT speech/NN synthesis/NN that/WDT predicts/VBZ target/NN speech/NN parameters/NNS ,/, our/PRP$ proposed/JJ method/NN jointly/RB trains/VBZ these/DT modules/NNS along/IN with/IN the/DT proposed/VBN probability/NN conversion/NN modules/NNS ./.
Experimental/JJ results/NNS demonstrate/VBP that/IN our/PRP$ approach/NN outperforms/VBZ the/DT conventional/JJ VC/NNP ./.
