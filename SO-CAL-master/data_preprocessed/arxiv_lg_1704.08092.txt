We/PRP introduce/VBP an/DT attention/NN -/HYPH based/VBN Bi-LSTM/NN for/IN Chinese/JJ implicit/JJ discourse/NN relations/NNS and/CC demonstrate/VBP that/IN modeling/VBG argument/NN pairs/NNS as/IN a/DT joint/JJ sequence/NN can/MD outperform/VB word/NN order/NN -/HYPH agnostic/JJ approaches/NNS ./.
Our/PRP$ model/NN benefits/NNS from/IN a/DT partial/JJ sampling/NN scheme/NN and/CC is/VBZ conceptually/RB simple/JJ ,/, yet/CC achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN the/DT Chinese/JJ Discourse/NN Treebank/NNP ./.
We/PRP also/RB visualize/VBP its/PRP$ attention/NN activity/NN to/TO illustrate/VB the/DT model/NN 's/POS ability/NN to/TO selectively/RB focus/VB on/IN the/DT relevant/JJ parts/NNS of/IN an/DT input/NN sequence/NN ./.
