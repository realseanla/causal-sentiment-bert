We/PRP propose/VBP a/DT simple/JJ ,/, elegant/JJ solution/NN to/TO use/VB a/DT single/JJ Neural/JJ Machine/NN Translation/NN (/-LRB- NMT/NN )/-RRB- model/NN to/TO translate/VB between/IN multiple/JJ languages/NNS ./.
Our/PRP$ solution/NN requires/VBZ no/DT change/NN in/IN the/DT model/NN architecture/NN from/IN our/PRP$ base/NN system/NN but/CC instead/RB introduces/VBZ an/DT artificial/JJ token/NN at/IN the/DT beginning/NN of/IN the/DT input/NN sentence/NN to/TO specify/VB the/DT required/VBN target/NN language/NN ./.
The/DT rest/NN of/IN the/DT model/NN ,/, which/WDT includes/VBZ encoder/NN ,/, decoder/NN and/CC attention/NN ,/, remains/VBZ unchanged/JJ and/CC is/VBZ shared/VBN across/IN all/DT languages/NNS ./.
Using/VBG a/DT shared/VBN wordpiece/NN vocabulary/NN ,/, our/PRP$ approach/NN enables/VBZ Multilingual/JJ NMT/NN using/VBG a/DT single/JJ model/NN without/IN any/DT increase/NN in/IN parameters/NNS ,/, which/WDT is/VBZ significantly/RB simpler/JJR than/IN previous/JJ proposals/NNS for/IN Multilingual/JJ NMT/NN ./.
Our/PRP$ method/NN often/RB improves/VBZ the/DT translation/NN quality/NN of/IN all/DT involved/JJ language/NN pairs/NNS ,/, even/RB while/IN keeping/VBG the/DT total/JJ number/NN of/IN model/NN parameters/NNS constant/JJ ./.
On/IN the/DT WMT/NNP '14/CD benchmarks/NNS ,/, a/DT single/JJ multilingual/JJ model/NN achieves/VBZ comparable/JJ performance/NN for/IN English/NNP $/$ \/CD rightarrow/RB $/$ French/NNPS and/CC surpasses/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS for/IN English/NNP $/$ \/CD rightarrow/RB $/$ German/JJ ./.
Similarly/RB ,/, a/DT single/JJ multilingual/JJ model/NN surpasses/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS for/IN French/JJ $/$ \/CD rightarrow/RB $/$ English/NNP and/CC German/JJ $/$ \/CD rightarrow/RB $/$ English/NNP on/IN WMT/NNP '14/CD and/CC WMT/NNP '15/CD benchmarks/NNS respectively/RB ./.
On/IN production/NN corpora/NNS ,/, multilingual/JJ models/NNS of/IN up/RB to/IN twelve/CD language/NN pairs/NNS allow/VBP for/IN better/JJR translation/NN of/IN many/JJ individual/JJ pairs/NNS ./.
In/IN addition/NN to/IN improving/VBG the/DT translation/NN quality/NN of/IN language/NN pairs/NNS that/WDT the/DT model/NN was/VBD trained/VBN with/IN ,/, our/PRP$ models/NNS can/MD also/RB learn/VB to/TO perform/VB implicit/JJ bridging/NN between/IN language/NN pairs/NNS never/RB seen/VBN explicitly/RB during/IN training/NN ,/, showing/VBG that/IN transfer/NN learning/NN and/CC zero/CD -/HYPH shot/NN translation/NN is/VBZ possible/JJ for/IN neural/JJ translation/NN ./.
Finally/RB ,/, we/PRP show/VBP analyses/NNS that/WDT hints/VBZ at/IN a/DT universal/JJ interlingua/NN representation/NN in/IN our/PRP$ models/NNS and/CC show/VB some/DT interesting/JJ examples/NNS when/WRB mixing/VBG languages/NNS ./.
