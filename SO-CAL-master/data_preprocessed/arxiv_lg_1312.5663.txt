Recently/RB ,/, it/PRP has/VBZ been/VBN observed/VBN that/IN when/WRB representations/NNS are/VBP learnt/VBN in/IN a/DT way/NN that/WDT encourages/VBZ sparsity/NN ,/, improved/VBN performance/NN is/VBZ obtained/VBN on/IN classification/NN tasks/NNS ./.
These/DT methods/NNS involve/VBP combinations/NNS of/IN activation/NN functions/NNS ,/, sampling/NN steps/NNS and/CC different/JJ kinds/NNS of/IN penalties/NNS ./.
To/TO investigate/VB the/DT effectiveness/NN of/IN sparsity/NN by/IN itself/PRP ,/, we/PRP propose/VBP the/DT k/NN -/HYPH sparse/JJ autoencoder/NN ,/, which/WDT is/VBZ a/DT linear/JJ model/NN ,/, but/CC where/WRB in/IN hidden/JJ layers/NNS only/RB the/DT k/NN highest/JJS activities/NNS are/VBP kept/VBN ./.
When/WRB applied/VBN to/IN the/DT MNIST/NNP and/CC NORB/NNP datasets/NNS ,/, we/PRP find/VBP that/IN this/DT method/NN achieves/VBZ better/JJR classification/NN results/NNS than/IN denoising/VBG autoencoders/NNS ,/, networks/NNS trained/VBN with/IN dropout/NN ,/, and/CC restricted/VBN Boltzmann/JJ machines/NNS ./.
k/CD -/HYPH sparse/JJ autoencoders/NNS are/VBP simple/JJ to/TO train/VB and/CC the/DT encoding/VBG stage/NN is/VBZ very/RB fast/RB ,/, making/VBG them/PRP well/RB -/HYPH suited/VBN to/IN large/JJ problem/NN sizes/NNS ,/, where/WRB conventional/JJ sparse/JJ coding/NN algorithms/NNS can/MD not/RB be/VB applied/VBN ./.
