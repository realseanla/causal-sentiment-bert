Neural/JJ networks/NNS are/VBP powerful/JJ and/CC flexible/JJ models/NNS that/WDT work/VBP well/RB for/IN many/JJ difficult/JJ learning/NN tasks/NNS in/IN image/NN ,/, speech/NN and/CC natural/JJ language/NN understanding/NN ./.
Despite/IN their/PRP$ success/NN ,/, neural/JJ networks/NNS are/VBP still/RB hard/JJ to/TO design/VB ./.
In/IN this/DT paper/NN ,/, we/PRP use/VBP a/DT recurrent/JJ network/NN to/TO generate/VB the/DT model/NN descriptions/NNS of/IN neural/JJ networks/NNS and/CC train/VB this/DT RNN/NN with/IN reinforcement/NN learning/NN to/TO maximize/VB the/DT expected/VBN accuracy/NN of/IN the/DT generated/VBN architectures/NNS on/IN a/DT validation/NN set/NN ./.
On/IN the/DT CIFAR/NN -/HYPH 10/CD dataset/NN ,/, our/PRP$ method/NN ,/, starting/VBG from/IN scratch/NN ,/, can/MD design/VB a/DT novel/JJ network/NN architecture/NN that/WDT rivals/VBZ the/DT best/JJS human/JJ -/HYPH invented/VBN architecture/NN in/IN terms/NNS of/IN test/NN set/VBN accuracy/NN ./.
Our/PRP$ CIFAR/NN -/HYPH 10/CD model/NN achieves/VBZ a/DT test/NN error/NN rate/NN of/IN 3.84/CD ,/, which/WDT is/VBZ only/RB 0.1/CD percent/NN worse/JJR and/CC 1.2/CD x/SYM faster/JJR than/IN the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN model/NN ./.
On/IN the/DT Penn/NNP Treebank/NNP dataset/NN ,/, our/PRP$ model/NN can/MD compose/VB a/DT novel/JJ recurrent/JJ cell/NN that/WDT outperforms/VBZ the/DT widely/RB -/HYPH used/VBN LSTM/NNP cell/NN ,/, and/CC other/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baselines/NNS ./.
Our/PRP$ cell/NN achieves/VBZ a/DT test/NN set/VBN perplexity/NN of/IN 62.4/CD on/IN the/DT Penn/NNP Treebank/NNP ,/, which/WDT is/VBZ 3.6/CD perplexity/NN better/JJR than/IN the/DT previous/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
