In/IN this/DT paper/NN we/PRP present/VBP a/DT technique/NN to/TO train/VB neural/JJ network/NN models/NNS on/IN small/JJ amounts/NNS of/IN data/NNS ./.
Current/JJ methods/NNS for/IN training/NN neural/JJ networks/NNS on/IN small/JJ amounts/NNS of/IN rich/JJ data/NNS typically/RB rely/VBP on/IN strategies/NNS such/JJ as/IN fine/JJ -/HYPH tuning/NN a/DT pre-trained/JJ neural/JJ network/NN or/CC the/DT use/NN of/IN domain/NN -/HYPH specific/JJ hand/NN -/HYPH engineered/VBN features/NNS ./.
Here/RB we/PRP take/VBP the/DT approach/NN of/IN treating/VBG network/NN layers/NNS ,/, or/CC entire/JJ networks/NNS ,/, as/IN modules/NNS and/CC combine/VB pre-trained/JJ modules/NNS with/IN untrained/JJ modules/NNS ,/, to/TO learn/VB the/DT shift/NN in/IN distributions/NNS between/IN data/NNS sets/NNS ./.
The/DT central/JJ impact/NN of/IN using/VBG a/DT modular/JJ approach/NN comes/VBZ from/IN adding/VBG new/JJ representations/NNS to/IN a/DT network/NN ,/, as/IN opposed/VBN to/IN replacing/VBG representations/NNS via/IN fine/JJ -/HYPH tuning/NN ./.
Using/VBG this/DT technique/NN ,/, we/PRP are/VBP able/JJ surpass/VBP results/NNS using/VBG standard/JJ fine/JJ -/HYPH tuning/NN transfer/NN learning/VBG approaches/NNS ,/, and/CC we/PRP are/VBP also/RB able/JJ to/TO significantly/RB increase/VB performance/NN over/IN such/JJ approaches/NNS when/WRB using/VBG smaller/JJR amounts/NNS of/IN data/NNS ./.
