We/PRP analyze/VBP the/DT performance/NN of/IN encoder/NN -/HYPH decoder/NN neural/JJ models/NNS and/CC compare/VB them/PRP with/IN well/RB -/HYPH known/VBN established/VBN methods/NNS ./.
The/DT latter/JJ represent/VBP different/JJ classes/NNS of/IN traditional/JJ approaches/NNS that/WDT are/VBP applied/VBN to/IN the/DT monotone/JJ sequence/NN -/HYPH to/IN -/HYPH sequence/NN tasks/NNS OCR/NNP post-correction/NN ,/, spelling/NN correction/NN ,/, grapheme/NN -/HYPH to/IN -/HYPH phoneme/NN conversion/NN ,/, and/CC lemmatization/NN ./.
Such/JJ tasks/NNS are/VBP of/IN practical/JJ relevance/NN for/IN various/JJ higher/JJR -/HYPH level/NN research/NN fields/NNS including/VBG digital/JJ humanities/NNS ,/, automatic/JJ text/NN correction/NN ,/, and/CC speech/NN recognition/NN ./.
We/PRP investigate/VBP how/WRB well/RB generic/JJ deep/JJ -/HYPH learning/NN approaches/NNS adapt/VBP to/IN these/DT tasks/NNS ,/, and/CC how/WRB they/PRP perform/VBP in/IN comparison/NN with/IN established/VBN and/CC more/JJR specialized/JJ methods/NNS ,/, including/VBG our/PRP$ own/JJ adaptation/NN of/IN pruned/VBN CRFs/NNS ./.
