Textual/JJ information/NN is/VBZ considered/VBN as/IN significant/JJ supplement/NN to/IN knowledge/NN representation/NN learning/NN (/-LRB- KRL/NN )/-RRB- ./.
There/EX are/VBP two/CD main/JJ challenges/NNS for/IN constructing/VBG knowledge/NN representations/NNS from/IN plain/JJ texts/NNS :/: (/-LRB- 1/LS )/-RRB- How/WRB to/TO take/VB full/JJ advantages/NNS of/IN sequential/JJ contexts/NNS of/IN entities/NNS in/IN plain/JJ texts/NNS for/IN KRL/NNP ./.
(/-LRB- 2/LS )/-RRB- How/WRB to/TO dynamically/RB select/VB those/DT informative/JJ sentences/NNS of/IN the/DT corresponding/VBG entities/NNS for/IN KRL/NNP ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP the/DT Sequential/NNP Text/VB -/HYPH embodied/VBN Knowledge/NN Representation/NN Learning/VBG to/TO build/VB knowledge/NN representations/NNS from/IN multiple/JJ sentences/NNS ./.
Given/VBN each/DT reference/NN sentence/NN of/IN an/DT entity/NN ,/, we/PRP first/RB utilize/VBP recurrent/JJ neural/JJ network/NN with/IN pooling/VBG or/CC long/RB short/JJ -/HYPH term/NN memory/NN network/NN to/TO encode/VB the/DT semantic/JJ information/NN of/IN the/DT sentence/NN with/IN respect/NN to/IN the/DT entity/NN ./.
Then/RB we/PRP further/RB design/VB an/DT attention/NN model/NN to/TO measure/VB the/DT informativeness/NN of/IN each/DT sentence/NN ,/, and/CC build/VB text/NN -/HYPH based/VBN representations/NNS of/IN entities/NNS ./.
We/PRP evaluate/VBP our/PRP$ method/NN on/IN two/CD tasks/NNS ,/, including/VBG triple/JJ classification/NN and/CC link/NN prediction/NN ./.
Experimental/JJ results/NNS demonstrate/VBP that/IN our/PRP$ method/NN outperforms/VBZ other/JJ baselines/NNS on/IN both/DT tasks/NNS ,/, which/WDT indicates/VBZ that/IN our/PRP$ method/NN is/VBZ capable/JJ of/IN selecting/VBG informative/JJ sentences/NNS and/CC encoding/VBG the/DT textual/JJ information/NN well/RB into/IN knowledge/NN representations/NNS ./.
