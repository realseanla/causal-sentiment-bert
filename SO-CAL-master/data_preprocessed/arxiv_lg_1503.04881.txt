The/DT chain/NN -/HYPH structured/VBN long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- has/VBZ showed/VBN to/TO be/VB effective/JJ in/IN a/DT wide/JJ range/NN of/IN problems/NNS such/JJ as/IN speech/NN recognition/NN and/CC machine/NN translation/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO extend/VB it/PRP to/IN tree/NN structures/NNS ,/, in/IN which/WDT a/DT memory/NN cell/NN can/MD reflect/VB the/DT history/NN memories/NNS of/IN multiple/JJ child/NN cells/NNS or/CC multiple/JJ descendant/NN cells/NNS in/IN a/DT recursive/JJ process/NN ./.
We/PRP call/VBP the/DT model/NN S/NN -/HYPH LSTM/NN ,/, which/WDT provides/VBZ a/DT principled/JJ way/NN of/IN considering/VBG long/JJ -/HYPH distance/NN interaction/NN over/IN hierarchies/NNS ,/, e.g./FW ,/, language/NN or/CC image/NN parse/VBP structures/NNS ./.
We/PRP leverage/VBP the/DT models/NNS for/IN semantic/JJ composition/NN to/TO understand/VB the/DT meaning/NN of/IN text/NN ,/, a/DT fundamental/JJ problem/NN in/IN natural/JJ language/NN understanding/NN ,/, and/CC show/VBP that/IN it/PRP outperforms/VBZ a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN recursive/JJ model/NN by/IN replacing/VBG its/PRP$ composition/NN layers/NNS with/IN the/DT S/NN -/HYPH LSTM/NN memory/NN blocks/NNS ./.
We/PRP also/RB show/VBP that/IN utilizing/VBG the/DT given/VBN structures/NNS is/VBZ helpful/JJ in/IN achieving/VBG a/DT performance/NN better/JJR than/IN that/DT without/IN considering/VBG the/DT structures/NNS ./.
