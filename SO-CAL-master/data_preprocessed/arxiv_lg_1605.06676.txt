We/PRP consider/VBP the/DT problem/NN of/IN multiple/JJ agents/NNS sensing/VBG and/CC acting/VBG in/IN environments/NNS with/IN the/DT goal/NN of/IN maximising/VBG their/PRP$ shared/VBN utility/NN ./.
In/IN these/DT environments/NNS ,/, agents/NNS must/MD learn/VB communication/NN protocols/NNS in/IN order/NN to/TO share/VB information/NN that/WDT is/VBZ needed/VBN to/TO solve/VB the/DT tasks/NNS ./.
By/IN embracing/VBG deep/JJ neural/JJ networks/NNS ,/, we/PRP are/VBP able/JJ to/TO demonstrate/VB end/NN -/HYPH to/IN -/HYPH end/NN learning/NN of/IN protocols/NNS in/IN complex/JJ environments/NNS inspired/VBN by/IN communication/NN riddles/NNS and/CC multi-agent/JJ computer/NN vision/NN problems/NNS with/IN partial/JJ observability/NN ./.
We/PRP propose/VBP two/CD approaches/NNS for/IN learning/VBG in/IN these/DT domains/NNS :/: Reinforced/NNP Inter-Agent/NNP Learning/NNP (/-LRB- RIAL/NNP )/-RRB- and/CC Differentiable/NNP Inter-Agent/NNP Learning/NNP (/-LRB- DIAL/NNP )/-RRB- ./.
The/DT former/JJ uses/VBZ deep/JJ Q/NN -/HYPH learning/NN ,/, while/IN the/DT latter/JJ exploits/NNS the/DT fact/NN that/IN ,/, during/IN learning/NN ,/, agents/NNS can/MD propagate/VB error/NN derivatives/NNS through/IN (/-LRB- noisy/JJ )/-RRB- communication/NN channels/NNS ./.
Hence/RB ,/, this/DT approach/NN uses/VBZ centralised/JJ learning/NN but/CC decentralised/VBN execution/NN ./.
Our/PRP$ experiments/NNS introduce/VBP new/JJ environments/NNS for/IN studying/VBG the/DT learning/NN of/IN communication/NN protocols/NNS and/CC present/VB a/DT set/NN of/IN engineering/NN innovations/NNS that/WDT are/VBP essential/JJ for/IN success/NN in/IN these/DT domains/NNS ./.
