This/DT paper/NN is/VBZ concerned/VBN with/IN learning/VBG binary/JJ classifiers/NNS under/IN adversarial/JJ label/NN -/HYPH noise/NN ./.
We/PRP introduce/VBP the/DT problem/NN of/IN error/NN -/HYPH correction/NN in/IN learning/NN where/WRB the/DT goal/NN is/VBZ to/TO recover/VB the/DT original/JJ clean/JJ data/NNS from/IN a/DT label/NN -/HYPH manipulated/VBN version/NN of/IN it/PRP ,/, given/VBN (/-LRB- i/LS )/-RRB- no/DT constraints/NNS on/IN the/DT adversary/NN other/JJ than/IN an/DT upper/JJ -/HYPH bound/VBN on/IN the/DT number/NN of/IN errors/NNS ,/, and/CC (/-LRB- ii/LS )/-RRB- some/DT regularity/NN properties/NNS for/IN the/DT original/JJ data/NNS ./.
We/PRP present/VBP a/DT simple/JJ and/CC practical/JJ error/NN -/HYPH correction/NN algorithm/NN called/VBN SubSVMs/NNPS that/WDT learns/VBZ individual/JJ SVMs/NNS on/IN several/JJ small/JJ -/HYPH size/NN (/-LRB- log/NN -/HYPH size/NN )/-RRB- ,/, class/NN -/HYPH balanced/VBN ,/, random/JJ subsets/NNS of/IN the/DT data/NNS and/CC then/RB reclassifies/VBZ the/DT training/NN points/NNS using/VBG a/DT majority/NN vote/NN ./.
Our/PRP$ analysis/NN reveals/VBZ the/DT need/NN for/IN the/DT two/CD main/JJ ingredients/NNS of/IN SubSVMs/NNPS ,/, namely/RB class/NN -/HYPH balanced/VBN sampling/NN and/CC subsampled/JJ bagging/NN ./.
Experimental/JJ results/NNS on/IN synthetic/JJ as/RB well/RB as/IN benchmark/NN UCI/NNP data/NNS demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ approach/NN ./.
In/IN addition/NN to/IN noise/NN -/HYPH tolerance/NN ,/, log/NN -/HYPH size/NN subsampled/JJ bagging/NN also/RB yields/VBZ significant/JJ run/NN -/HYPH time/NN benefits/NNS over/IN standard/JJ SVMs/NNS ./.
