Structure/NN and/CC parameters/NNS in/IN a/DT Bayesian/JJ network/NN uniquely/RB specify/VB the/DT probability/NN distribution/NN of/IN the/DT modeled/VBN domain/NN ./.
The/DT locality/NN of/IN both/DT structure/NN and/CC probabilistic/JJ information/NN are/VBP the/DT great/JJ benefits/NNS of/IN Bayesian/JJ networks/NNS and/CC require/VBP the/DT modeler/NN to/TO only/RB specify/VB local/JJ information/NN ./.
On/IN the/DT other/JJ hand/NN this/DT locality/NN of/IN information/NN might/MD prevent/VB the/DT modeler/NN -/HYPH and/CC even/RB more/RBR any/DT other/JJ person/NN -/HYPH from/IN obtaining/VBG a/DT general/JJ overview/NN of/IN the/DT important/JJ relationships/NNS within/IN the/DT domain/NN ./.
The/DT goal/NN of/IN the/DT work/NN presented/VBN in/IN this/DT paper/NN is/VBZ to/TO provide/VB an/DT "/`` alternative/NN "/'' view/NN on/IN the/DT knowledge/NN encoded/VBN in/IN a/DT Bayesian/JJ network/NN which/WDT might/MD sometimes/RB be/VB very/RB helpful/JJ for/IN providing/VBG insights/NNS into/IN the/DT underlying/VBG domain/NN ./.
The/DT basic/JJ idea/NN is/VBZ to/TO calculate/VB a/DT mixture/NN approximation/NN to/IN the/DT probability/NN distribution/NN represented/VBN by/IN the/DT Bayesian/JJ network/NN ./.
The/DT mixture/NN component/NN densities/NNS can/MD be/VB thought/VBN of/IN as/IN representing/VBG typical/JJ scenarios/NNS implied/VBN by/IN the/DT Bayesian/JJ model/NN ,/, providing/VBG intuition/NN about/IN the/DT basic/JJ relationships/NNS ./.
As/IN an/DT additional/JJ benefit/NN ,/, performing/VBG inference/NN in/IN the/DT approximate/JJ model/NN is/VBZ very/RB simple/JJ and/CC intuitive/JJ and/CC can/MD provide/VB additional/JJ insights/NNS ./.
The/DT computational/JJ complexity/NN for/IN the/DT calculation/NN of/IN the/DT mixture/NN approximations/NNS criticaly/RB depends/VBZ on/IN the/DT measure/NN which/WDT defines/VBZ the/DT distance/NN between/IN the/DT probability/NN distribution/NN represented/VBN by/IN the/DT Bayesian/JJ network/NN and/CC the/DT approximate/JJ distribution/NN ./.
Both/CC the/DT KL/NN -/HYPH divergence/NN and/CC the/DT backward/JJ KL/NN -/HYPH divergence/NN lead/NN to/IN inefficient/JJ algorithms/NNS ./.
Incidentally/RB ,/, the/DT latter/JJ is/VBZ used/VBN in/IN recent/JJ work/NN on/IN mixtures/NNS of/IN mean/JJ field/NN solutions/NNS to/TO which/WDT the/DT work/NN presented/VBN here/RB is/VBZ closely/RB related/JJ ./.
We/PRP show/VBP ,/, however/RB ,/, that/IN using/VBG a/DT mean/JJ squared/JJ error/NN cost/NN function/NN leads/VBZ to/IN update/NN equations/NNS which/WDT can/MD be/VB solved/VBN using/VBG the/DT junction/NN tree/NN algorithm/NN ./.
We/PRP conclude/VBP that/IN the/DT mean/JJ squared/JJ error/NN cost/NN function/NN can/MD be/VB used/VBN for/IN Bayesian/JJ networks/NNS in/IN which/WDT inference/NN based/VBN on/IN the/DT junction/NN tree/NN is/VBZ tractable/JJ ./.
For/IN large/JJ networks/NNS ,/, however/RB ,/, one/CD may/MD have/VB to/TO rely/VB on/IN mean/JJ field/NN approximations/NNS ./.
