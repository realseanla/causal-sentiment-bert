In/IN this/DT paper/NN ,/, we/PRP combine/VBP task/NN -/HYPH dependent/JJ reward/NN shaping/NN and/CC task/NN -/HYPH independent/JJ proto/NN -/HYPH value/NN functions/VBZ to/TO obtain/VB reward/NN dependent/JJ proto/NN -/HYPH value/NN functions/NNS (/-LRB- RPVFs/NNS )/-RRB- ./.
In/IN constructing/VBG the/DT RPVFs/NNPS we/PRP are/VBP making/VBG use/NN of/IN the/DT immediate/JJ rewards/NNS which/WDT are/VBP available/JJ during/IN the/DT sampling/NN phase/NN but/CC are/VBP not/RB used/VBN in/IN the/DT PVF/NN construction/NN ./.
We/PRP show/VBP via/IN experiments/NNS that/WDT learning/VBG with/IN an/DT RPVF/NN based/VBN representation/NN is/VBZ better/JJR than/IN learning/VBG with/IN just/RB reward/VB shaping/VBG or/CC PVFs/NNS ./.
In/IN particular/JJ ,/, when/WRB the/DT state/NN space/NN is/VBZ symmetrical/JJ and/CC the/DT rewards/NNS are/VBP asymmetrical/JJ ,/, the/DT RPVF/NN capture/NN the/DT asymmetry/NN better/JJR than/IN the/DT PVFs/NNS ./.
