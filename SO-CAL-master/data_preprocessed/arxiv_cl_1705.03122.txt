The/DT prevalent/JJ approach/NN to/IN sequence/NN to/IN sequence/NN learning/NN maps/VBZ an/DT input/NN sequence/NN to/IN a/DT variable/JJ length/NN output/NN sequence/NN via/IN recurrent/JJ neural/JJ networks/NNS ./.
We/PRP introduce/VBP an/DT architecture/NN based/VBN entirely/RB on/IN convolutional/JJ neural/JJ networks/NNS ./.
Compared/VBN to/IN recurrent/JJ models/NNS ,/, computations/NNS over/IN all/DT elements/NNS can/MD be/VB fully/RB parallelized/VBN during/IN training/NN and/CC optimization/NN is/VBZ easier/JJR since/IN the/DT number/NN of/IN non-linearities/NNS is/VBZ fixed/VBN and/CC independent/JJ of/IN the/DT input/NN length/NN ./.
Our/PRP$ use/NN of/IN gated/VBN linear/JJ units/NNS eases/VBZ gradient/NN propagation/NN and/CC we/PRP equip/VBP each/DT decoder/NN layer/NN with/IN a/DT separate/JJ attention/NN module/NN ./.
We/PRP outperform/VBP the/DT accuracy/NN of/IN the/DT deep/JJ LSTM/NN setup/NN of/IN Wu/NNP et/FW al./FW (/-LRB- 2016/CD )/-RRB- on/IN both/DT WMT/NNP '14/CD English/NNP -/HYPH German/NNP and/CC WMT/NNP '14/CD English/NNP -/HYPH French/NNP translation/NN at/IN an/DT order/NN of/IN magnitude/NN faster/RBR speed/NN ,/, both/CC on/IN GPU/NNP and/CC CPU/NN ./.
