We/PRP introduce/VBP BinaryNet/NNP ,/, a/DT method/NN which/WDT trains/VBZ DNNs/NNS with/IN binary/JJ weights/NNS and/CC activations/NNS when/WRB computing/VBG parameters/NNS '/POS gradient/NN ./.
We/PRP show/VBP that/IN it/PRP is/VBZ possible/JJ to/TO train/VB a/DT Multi/NNP Layer/NNP Perceptron/NNP (/-LRB- MLP/NNP )/-RRB- on/IN MNIST/NN and/CC ConvNets/NNP on/IN CIFAR/NNP -/HYPH 10/CD and/CC SVHN/NN with/IN BinaryNet/NNP and/CC achieve/VB nearly/RB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS ./.
At/IN run/NN -/HYPH time/NN ,/, BinaryNet/NNP drastically/RB reduces/VBZ memory/NN usage/NN and/CC replaces/VBZ most/RBS multiplications/NNS by/IN 1/CD -/HYPH bit/NN exclusive/JJ -/HYPH not/RB -/HYPH or/CC (/-LRB- XNOR/NNP )/-RRB- operations/NNS ,/, which/WDT might/MD have/VB a/DT big/JJ impact/NN on/IN both/DT general/JJ -/HYPH purpose/NN and/CC dedicated/JJ Deep/JJ Learning/NN hardware/NN ./.
We/PRP wrote/VBD a/DT binary/JJ matrix/NN multiplication/NN GPU/NNP kernel/NN with/IN which/WDT it/PRP is/VBZ possible/JJ to/TO run/VB our/PRP$ MNIST/NN MLP/NN 7/CD times/NNS faster/JJR than/IN with/IN an/DT unoptimized/JJ GPU/NNP kernel/NN ,/, without/IN suffering/VBG any/DT loss/NN in/IN classification/NN accuracy/NN ./.
The/DT code/NN for/IN BinaryNet/NNP is/VBZ available/JJ ./.
