We/PRP describe/VBP computational/JJ tasks/NNS -/HYPH especially/RB in/IN vision/NN -/HYPH that/WDT correspond/VBP to/IN compositional/JJ //HYPH hierarchical/JJ functions/NNS ./.
While/IN the/DT universal/JJ approximation/NN property/NN holds/VBZ both/CC for/IN hierarchical/JJ and/CC shallow/JJ networks/NNS ,/, we/PRP prove/VBP that/IN deep/JJ (/-LRB- hierarchical/JJ )/-RRB- networks/NNS can/MD approximate/VB the/DT class/NN of/IN compositional/JJ functions/NNS with/IN the/DT same/JJ accuracy/NN as/IN shallow/JJ networks/NNS but/CC with/IN exponentially/RB lower/JJR VC/NNP -/HYPH dimension/NN as/RB well/RB as/IN the/DT number/NN of/IN training/NN parameters/NNS ./.
This/DT leads/VBZ to/IN the/DT question/NN of/IN approximation/NN by/IN sparse/JJ polynomials/NNS (/-LRB- in/IN the/DT number/NN of/IN independent/JJ parameters/NNS )/-RRB- and/CC ,/, as/IN a/DT consequence/NN ,/, by/IN deep/JJ networks/NNS ./.
We/PRP also/RB discuss/VBP connections/NNS between/IN our/PRP$ results/NNS and/CC learnability/NN of/IN sparse/JJ Boolean/JJ functions/NNS ,/, settling/VBG an/DT old/JJ conjecture/NN by/IN Bengio/NNP ./.
