With/IN the/DT recent/JJ proliferation/NN of/IN large/JJ -/HYPH scale/NN learning/NN problems/NNS ,/, there/EX have/VBP been/VBN a/DT lot/NN of/IN interest/NN on/IN distributed/VBN machine/NN learning/NN algorithms/NNS ,/, particularly/RB those/DT that/WDT are/VBP based/VBN on/IN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- and/CC its/PRP$ variants/NNS ./.
However/RB ,/, existing/VBG algorithms/NNS either/CC suffer/VB from/IN slow/JJ convergence/NN due/IN to/IN the/DT inherent/JJ variance/NN of/IN stochastic/JJ gradients/NNS ,/, or/CC have/VBP a/DT fast/JJ linear/JJ convergence/NN rate/NN but/CC at/IN the/DT expense/NN of/IN poorer/JJR solution/NN quality/NN ./.
In/IN this/DT paper/NN ,/, we/PRP combine/VBP their/PRP$ merits/NNS together/RB by/IN proposing/VBG a/DT distributed/VBN asynchronous/JJ SGD/NNP -/HYPH based/VBN algorithm/NN with/IN variance/NN reduction/NN ./.
A/DT constant/JJ learning/NN rate/NN can/MD be/VB used/VBN ,/, and/CC it/PRP is/VBZ also/RB guaranteed/VBN to/TO converge/VB linearly/RB to/IN the/DT optimal/JJ solution/NN ./.
Experiments/NNS on/IN the/DT Google/NNP Cloud/NNP Computing/NNP Platform/NN demonstrate/VBP that/IN the/DT proposed/VBN algorithm/NN outperforms/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN distributed/VBN asynchronous/JJ algorithms/NNS in/IN terms/NNS of/IN both/DT wall/NN clock/NN time/NN and/CC solution/NN quality/NN ./.
