Language/NNP models/NNS (/-LRB- LMs/NNS )/-RRB- are/VBP statistical/JJ models/NNS that/WDT calculate/VBP probabilities/NNS over/IN sequences/NNS of/IN words/NNS or/CC other/JJ discrete/JJ symbols/NNS ./.
Currently/RB two/CD major/JJ paradigms/NNS for/IN language/NN modeling/NN exist/VBP :/: count/VB -/HYPH based/VBN n/NN -/HYPH gram/NN models/NNS ,/, which/WDT have/VBP advantages/NNS of/IN scalability/NN and/CC test/NN -/HYPH time/NN speed/NN ,/, and/CC neural/JJ LMs/NNS ,/, which/WDT often/RB achieve/VBP superior/JJ modeling/NN performance/NN ./.
We/PRP demonstrate/VBP how/WRB both/DT varieties/NNS of/IN models/NNS can/MD be/VB unified/VBN in/IN a/DT single/JJ modeling/NN framework/NN that/WDT defines/VBZ a/DT set/NN of/IN probability/NN distributions/NNS over/IN the/DT vocabulary/NN of/IN words/NNS ,/, and/CC then/RB dynamically/RB calculates/VBZ mixture/NN weights/NNS over/IN these/DT distributions/NNS ./.
This/DT formulation/NN allows/VBZ us/PRP to/TO create/VB novel/JJ hybrid/NN models/NNS that/WDT combine/VBP the/DT desirable/JJ features/NNS of/IN count/NN -/HYPH based/VBN and/CC neural/JJ LMs/NNS ,/, and/CC experiments/NNS demonstrate/VBP the/DT advantages/NNS of/IN these/DT approaches/NNS ./.
