We/PRP investigate/VBP the/DT use/NN of/IN pivot/NN languages/NNS for/IN phrase/NN -/HYPH based/VBN statistical/JJ machine/NN translation/NN (/-LRB- PB/NNP -/HYPH SMT/NNP )/-RRB- between/IN related/JJ languages/NNS with/IN limited/JJ parallel/JJ corpora/NNS ./.
We/PRP show/VBP that/IN subword/NN -/HYPH level/NN pivot/NN translation/NN via/IN a/DT related/JJ pivot/NN language/NN is/VBZ :/: (/-LRB- i/LS )/-RRB- highly/RB competitive/JJ with/IN the/DT best/JJS direct/JJ translation/NN model/NN and/CC (/-LRB- ii/LS )/-RRB- better/JJR than/IN a/DT pivot/NN model/NN which/WDT uses/VBZ an/DT unrelated/JJ pivot/NN language/NN ,/, but/CC has/VBZ at/IN its/PRP$ disposal/NN large/JJ parallel/JJ corpora/NNS to/TO build/VB the/DT source/NN -/HYPH pivot/NN (/-LRB- S/NN -/HYPH P/NN )/-RRB- and/CC pivot/NN -/HYPH target/NN (/-LRB- P/NN -/HYPH T/NN )/-RRB- translation/NN models/NNS ./.
In/IN contrast/NN ,/, pivot/NN models/NNS trained/VBN at/IN word/NN and/CC morpheme/NN level/NN are/VBP far/RB inferior/JJ to/IN their/PRP$ direct/JJ counterparts/NNS ./.
We/PRP also/RB show/VBP that/IN using/VBG multiple/JJ related/JJ pivot/NN languages/NNS can/MD outperform/VB a/DT direct/JJ translation/NN model/NN ./.
Thus/RB ,/, the/DT use/NN of/IN subwords/NNS as/IN translation/NN units/NNS coupled/VBN with/IN the/DT use/NN of/IN multiple/JJ related/JJ pivot/NN languages/NNS can/MD compensate/VB for/IN the/DT lack/NN of/IN a/DT direct/JJ parallel/JJ corpus/NN ./.
Subword/NNP units/NNS make/VBP pivot/NN models/NNS competitive/JJ by/IN (/-LRB- i/LS )/-RRB- utilizing/VBG lexical/JJ similarity/NN to/TO improve/VB the/DT underlying/VBG S/NN -/HYPH P/NN and/CC P/NN -/HYPH T/NN translation/NN models/NNS ,/, and/CC (/-LRB- ii/LS )/-RRB- reducing/VBG loss/NN of/IN translation/NN candidates/NNS during/IN pivoting/VBG ./.
