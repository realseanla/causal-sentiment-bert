Deep/JJ learning/NN (/-LRB- DL/NN )/-RRB- has/VBZ achieved/VBN notable/JJ successes/NNS in/IN many/JJ machine/NN learning/NN tasks/NNS ./.
A/DT number/NN of/IN frameworks/NNS have/VBP been/VBN developed/VBN to/TO expedite/VB the/DT process/NN of/IN designing/VBG and/CC training/VBG deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- ,/, such/JJ as/IN Caffe/NNP ,/, Torch/NNP and/CC Theano/NNP ./.
Currently/RB they/PRP can/MD harness/NN multiple/JJ GPUs/NNS on/IN a/DT single/JJ machine/NN ,/, but/CC are/VBP unable/JJ to/TO use/VB GPUs/NNS that/WDT are/VBP distributed/VBN across/IN multiple/JJ machines/NNS ;/: as/IN even/RB average/JJ -/HYPH sized/JJ DNNs/NNS can/MD take/VB days/NNS to/TO train/VB on/IN a/DT single/JJ GPU/NNP with/IN 100s/NNPS of/IN GBs/NNS to/IN TBs/NNS of/IN data/NNS ,/, distributed/VBN GPUs/NNS present/VBP a/DT prime/JJ opportunity/NN for/IN scaling/VBG up/RP DL/NN ./.
However/RB ,/, the/DT limited/JJ bandwidth/NN available/JJ on/IN commodity/NN Ethernet/NN networks/NNS presents/VBZ a/DT bottleneck/NN to/IN distributed/VBN GPU/NN training/NN ,/, and/CC prevents/VBZ its/PRP$ trivial/JJ realization/NN ./.
