We/PRP consider/VBP the/DT problem/NN of/IN learning/NN preferences/NNS over/IN trajectories/NNS for/IN mobile/JJ manipulators/NNS such/JJ as/IN personal/JJ robots/NNS and/CC assembly/NN line/NN robots/NNS ./.
The/DT preferences/NNS we/PRP learn/VBP are/VBP more/RBR intricate/JJ than/IN simple/JJ geometric/JJ constraints/NNS on/IN trajectories/NNS ;/: they/PRP are/VBP rather/RB governed/VBN by/IN the/DT surrounding/VBG context/NN of/IN various/JJ objects/NNS and/CC human/JJ interactions/NNS in/IN the/DT environment/NN ./.
We/PRP propose/VBP a/DT coactive/JJ online/JJ learning/NN framework/NN for/IN teaching/NN preferences/NNS in/IN contextually/RB rich/JJ environments/NNS ./.
The/DT key/JJ novelty/NN of/IN our/PRP$ approach/NN lies/VBZ in/IN the/DT type/NN of/IN feedback/NN expected/VBN from/IN the/DT user/NN :/: the/DT human/JJ user/NN does/VBZ not/RB need/VB to/TO demonstrate/VB optimal/JJ trajectories/NNS as/IN training/NN data/NNS ,/, but/CC merely/RB needs/VBZ to/TO iteratively/RB provide/VB trajectories/NNS that/WDT slightly/RB improve/VBP over/IN the/DT trajectory/NN currently/RB proposed/VBN by/IN the/DT system/NN ./.
We/PRP argue/VBP that/IN this/DT coactive/JJ preference/NN feedback/NN can/MD be/VB more/RBR easily/RB elicited/VBN than/IN demonstrations/NNS of/IN optimal/JJ trajectories/NNS ./.
Nevertheless/RB ,/, theoretical/JJ regret/NN bounds/NNS of/IN our/PRP$ algorithm/NN match/NN the/DT asymptotic/JJ rates/NNS of/IN optimal/JJ trajectory/NN algorithms/NNS ./.
