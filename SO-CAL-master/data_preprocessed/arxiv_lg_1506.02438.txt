This/DT paper/NN is/VBZ concerned/VBN with/IN developing/VBG policy/NN gradient/NN methods/NNS that/WDT gracefully/RB scale/VBP up/RP to/IN challenging/JJ problems/NNS with/IN high/JJ -/HYPH dimensional/JJ state/NN and/CC action/NN spaces/NNS ./.
Towards/IN this/DT end/NN ,/, we/PRP develop/VBP a/DT scheme/NN that/WDT uses/VBZ value/NN functions/VBZ to/IN substantially/RB reduce/VB the/DT variance/NN of/IN policy/NN gradient/NN estimates/NNS ,/, while/IN introducing/VBG a/DT tolerable/JJ amount/NN of/IN bias/NN ./.
This/DT scheme/NN ,/, which/WDT we/PRP call/VBP generalized/VBN advantage/NN estimation/NN (/-LRB- GAE/NN )/-RRB- ,/, involves/VBZ using/VBG a/DT discounted/VBN sum/NN of/IN temporal/JJ difference/NN residuals/NNS as/IN an/DT estimate/NN of/IN the/DT advantage/NN function/NN ,/, and/CC can/MD be/VB interpreted/VBN as/IN a/DT type/NN of/IN automated/VBN cost/NN shaping/NN ./.
It/PRP is/VBZ simple/JJ to/TO implement/VB and/CC can/MD be/VB used/VBN with/IN a/DT variety/NN of/IN policy/NN gradient/NN methods/NNS and/CC value/NN function/NN approximators/NNS ./.
Along/IN with/IN this/DT variance/NN -/HYPH reduction/NN scheme/NN ,/, we/PRP use/VBP trust/NN region/NN algorithms/NNS to/TO optimize/VB the/DT policy/NN and/CC value/NN function/NN ,/, both/CC represented/VBD as/IN neural/JJ networks/NNS ./.
We/PRP present/VBP experimental/JJ results/NNS on/IN a/DT number/NN of/IN highly/RB challenging/JJ 3D/NN loco/NN -/HYPH motion/NN tasks/NNS ,/, where/WRB our/PRP$ approach/NN learns/VBZ complex/JJ gaits/NNS for/IN bipedal/JJ and/CC quadrupedal/JJ simulated/JJ robots/NNS ./.
We/PRP also/RB learn/VBP controllers/NNS for/IN the/DT biped/NN getting/VBG up/RP off/IN the/DT ground/NN ./.
In/IN contrast/NN to/IN prior/JJ work/NN that/WDT uses/VBZ hand/NN -/HYPH crafted/VBN low/JJ -/HYPH dimensional/JJ policy/NN representations/NNS ,/, our/PRP$ neural/JJ network/NN policies/NNS map/VBP directly/RB from/IN raw/JJ kinematics/NNS to/IN joint/JJ torques/NNS ./.
