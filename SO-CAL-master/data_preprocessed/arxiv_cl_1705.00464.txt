This/DT paper/NN introduces/VBZ the/DT task/NN of/IN speech/NN -/HYPH based/VBN visual/JJ question/NN answering/VBG (/-LRB- VQA/NN )/-RRB- ,/, that/WDT is/VBZ ,/, to/TO generate/VB an/DT answer/NN given/VBN an/DT image/NN and/CC an/DT associated/VBN spoken/VBN question/NN ./.
Our/PRP$ work/NN is/VBZ the/DT first/JJ study/NN of/IN speech/NN -/HYPH based/VBN VQA/NN with/IN the/DT intention/NN of/IN providing/VBG insights/NNS for/IN applications/NNS such/JJ as/IN speech/NN -/HYPH based/VBN virtual/JJ assistants/NNS ./.
Two/CD methods/NNS are/VBP studied/VBN :/: an/DT end/NN to/IN end/NN ,/, deep/JJ neural/JJ network/NN that/WDT directly/RB uses/VBZ audio/JJ waveforms/NNS as/IN input/NN versus/IN a/DT pipelined/JJ approach/NN that/WDT performs/VBZ ASR/NNP (/-LRB- Automatic/NNP Speech/NNP Recognition/NNP )/-RRB- on/IN the/DT question/NN ,/, followed/VBN by/IN text/NN -/HYPH based/VBN visual/JJ question/NN answering/VBG ./.
Our/PRP$ main/JJ findings/NNS are/VBP 1/CD )/-RRB- speech/NN -/HYPH based/VBN VQA/NNP achieves/VBZ slightly/RB worse/JJR results/NNS than/IN the/DT extensively/RB -/HYPH studied/VBN VQA/NN with/IN noise/NN -/HYPH free/JJ text/NN and/CC 2/CD )/-RRB- the/DT end/NN -/HYPH to/IN -/HYPH end/NN model/NN is/VBZ competitive/JJ even/RB though/IN it/PRP has/VBZ a/DT simple/JJ architecture/NN ./.
Furthermore/RB ,/, we/PRP investigate/VBP the/DT robustness/NN of/IN both/DT methods/NNS by/IN injecting/VBG various/JJ levels/NNS of/IN noise/NN into/IN the/DT spoken/VBN question/NN and/CC find/VB speech/NN -/HYPH based/VBN VQA/NNP to/TO be/VB tolerant/JJ of/IN noise/NN at/IN reasonable/JJ levels/NNS ./.
The/DT speech/NN dataset/NN ,/, code/NN ,/, and/CC supplementary/JJ material/NN will/MD be/VB released/VBN to/IN the/DT public/NN ./.
