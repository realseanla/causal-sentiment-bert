In/IN this/DT paper/NN ,/, we/PRP analyze/VBP the/DT numerics/NNS of/IN common/JJ algorithms/NNS for/IN training/NN Generative/JJ Adversarial/JJ Networks/NNS (/-LRB- GANs/NNS )/-RRB- ./.
Using/VBG the/DT formalism/NN of/IN smooth/JJ two/CD -/HYPH player/NN games/NNS we/PRP analyze/VBP the/DT associated/VBN gradient/NN vector/NN field/NN of/IN GAN/NNP training/NN objectives/NNS ./.
Our/PRP$ findings/NNS suggest/VBP that/IN the/DT convergence/NN of/IN current/JJ algorithms/NNS suffers/VBZ due/IN to/IN two/CD factors/NNS :/: i/LS )/-RRB- presence/NN of/IN eigenvalues/NNS of/IN the/DT Jacobian/NNP of/IN the/DT gradient/NN vector/NN field/NN with/IN zero/CD real/JJ -/HYPH part/NN ,/, and/CC ii/LS )/-RRB- eigenvalues/NNS with/IN big/JJ imaginary/JJ part/NN ./.
Using/VBG these/DT findings/NNS ,/, we/PRP design/VBP a/DT new/JJ algorithm/NN that/WDT overcomes/VBZ some/DT of/IN these/DT limitations/NNS and/CC has/VBZ better/JJR convergence/NN properties/NNS ./.
Experimentally/RB ,/, we/PRP demonstrate/VBP its/PRP$ superiority/NN on/IN training/NN common/JJ GAN/NNP architectures/NNS and/CC show/VBP convergence/NN on/IN GAN/NNP architectures/NNS that/WDT are/VBP known/VBN to/TO be/VB notoriously/RB hard/JJ to/TO train/VB ./.
