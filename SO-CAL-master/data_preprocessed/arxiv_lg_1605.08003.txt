We/PRP provide/VBP tight/JJ upper/JJ and/CC lower/JJR bounds/NNS on/IN the/DT complexity/NN of/IN minimizing/VBG the/DT average/NN of/IN m/NN convex/NN functions/NNS using/VBG gradient/NN and/CC prox/NN oracles/NNS of/IN the/DT component/NN functions/VBZ ./.
We/PRP show/VBP a/DT significant/JJ gap/NN between/IN the/DT complexity/NN of/IN deterministic/JJ vs/IN randomized/JJ optimization/NN ./.
For/IN smooth/JJ functions/NNS ,/, we/PRP show/VBP that/IN accelerated/VBN gradient/NN descent/NN (/-LRB- AGD/NN )/-RRB- and/CC Katyusha/NNP are/VBP optimal/JJ in/IN the/DT deterministic/JJ and/CC randomized/JJ settings/NNS respectively/RB ,/, and/CC that/IN a/DT gradient/NN oracle/NN is/VBZ sufficient/JJ for/IN the/DT optimal/JJ rate/NN ./.
For/IN non-smooth/JJ functions/NNS ,/, having/VBG access/NN to/IN prox/NN oracles/NNS reduces/VBZ the/DT complexity/NN and/CC we/PRP present/VBP optimal/JJ methods/NNS based/VBN on/IN smoothing/VBG AGD/NN that/WDT improve/VBP over/IN methods/NNS using/VBG just/RB gradient/NN accesses/VBZ ./.
