The/DT problem/NN of/IN selecting/VBG the/DT right/JJ state/NN -/HYPH representation/NN in/IN a/DT reinforcement/NN learning/VBG problem/NN is/VBZ considered/VBN ./.
Several/JJ models/NNS (/-LRB- functions/NNS mapping/VBG past/JJ observations/NNS to/IN a/DT finite/NN set/NN )/-RRB- of/IN the/DT observations/NNS are/VBP given/VBN ,/, and/CC it/PRP is/VBZ known/VBN that/IN for/IN at/RB least/RBS one/CD of/IN these/DT models/NNS the/DT resulting/VBG state/NN dynamics/NNS are/VBP indeed/RB Markovian/JJ ./.
Without/IN knowing/VBG neither/DT which/WDT of/IN the/DT models/NNS is/VBZ the/DT correct/JJ one/CD ,/, nor/CC what/WP are/VBP the/DT probabilistic/JJ characteristics/NNS of/IN the/DT resulting/VBG MDP/NNP ,/, it/PRP is/VBZ required/VBN to/TO obtain/VB as/RB much/JJ reward/NN as/IN the/DT optimal/JJ policy/NN for/IN the/DT correct/JJ model/NN (/-LRB- or/CC for/IN the/DT best/JJS of/IN the/DT correct/JJ models/NNS ,/, if/IN there/EX are/VBP several/JJ )/-RRB- ./.
We/PRP propose/VBP an/DT algorithm/NN that/WDT achieves/VBZ that/IN ,/, with/IN a/DT regret/NN of/IN order/NN T/NN ^/SYM {/-LRB- 2/3/CD }/-RRB- where/WRB T/NN is/VBZ the/DT horizon/NN time/NN ./.
