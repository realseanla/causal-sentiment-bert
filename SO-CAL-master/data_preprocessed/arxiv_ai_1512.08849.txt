Natural/JJ language/NN inference/NN (/-LRB- NLI/NN )/-RRB- is/VBZ a/DT fundamentally/RB important/JJ task/NN in/IN natural/JJ language/NN processing/NN that/WDT has/VBZ many/JJ applications/NNS ./.
The/DT recently/RB released/VBN Stanford/NNP Natural/NNP Language/NNP Inference/NN (/-LRB- SNLI/NN )/-RRB- corpus/NN has/VBZ made/VBN it/PRP possible/JJ to/TO develop/VB and/CC evaluate/VB learning/NN -/HYPH centered/VBN methods/NNS such/JJ as/IN deep/JJ neural/JJ networks/NNS for/IN the/DT NLI/NNP task/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT special/JJ long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- architecture/NN for/IN NLI/NNP ./.
Our/PRP$ model/NN builds/VBZ on/IN top/NN of/IN a/DT recently/RB proposed/VBN neutral/JJ attention/NN model/NN for/IN NLI/NNP but/CC is/VBZ based/VBN on/IN a/DT significantly/RB different/JJ idea/NN ./.
Instead/RB of/IN deriving/VBG sentence/NN embeddings/NNS for/IN the/DT premise/NN and/CC the/DT hypothesis/NN to/TO be/VB used/VBN for/IN classification/NN ,/, our/PRP$ solution/NN uses/VBZ a/DT matching/NN -/HYPH LSTM/NN that/WDT performs/VBZ word/NN -/HYPH by/IN -/HYPH word/NN matching/NN of/IN the/DT hypothesis/NN with/IN the/DT premise/NN ./.
This/DT LSTM/NN is/VBZ able/JJ to/TO place/VB more/JJR emphasis/NN on/IN important/JJ word/NN -/HYPH level/NN matching/NN results/NNS ./.
In/IN particular/JJ ,/, we/PRP observe/VBP that/IN this/DT LSTM/NNP remembers/VBZ important/JJ mismatches/NNS that/WDT are/VBP critical/JJ for/IN predicting/VBG the/DT contradiction/NN or/CC the/DT neutral/JJ relationship/NN label/NN ./.
Our/PRP$ experiments/NNS on/IN the/DT SNLI/NN corpus/NN show/VBP that/IN our/PRP$ model/NN outperforms/VBZ the/DT state/NN of/IN the/DT art/NN ,/, achieving/VBG an/DT accuracy/NN of/IN 86.1/CD percent/NN on/IN the/DT test/NN data/NNS ./.
