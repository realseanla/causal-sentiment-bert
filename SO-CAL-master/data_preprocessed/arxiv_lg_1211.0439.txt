We/PRP study/VBP the/DT average/JJ case/NN performance/NN of/IN multi-task/VB Gaussian/JJ process/NN (/-LRB- GP/NNP )/-RRB- regression/NN as/IN captured/VBN in/IN the/DT learning/NN curve/NN ,/, i.e./FW the/DT average/JJ Bayes/NNP error/NN for/IN a/DT chosen/VBN task/NN versus/IN the/DT total/JJ number/NN of/IN examples/NNS $/$ n/NN $/$ for/IN all/DT tasks/NNS ./.
For/IN GP/NNP covariances/NNS that/WDT are/VBP the/DT product/NN of/IN an/DT input/NN -/HYPH dependent/JJ covariance/NN function/NN and/CC a/DT free/JJ -/HYPH form/NN inter-task/NN covariance/NN matrix/NN ,/, we/PRP show/VBP that/IN accurate/JJ approximations/NNS for/IN the/DT learning/NN curve/NN can/MD be/VB obtained/VBN for/IN an/DT arbitrary/JJ number/NN of/IN tasks/NNS $/$ T$/CD ./.
We/PRP use/VBP these/DT to/TO study/VB the/DT asymptotic/JJ learning/NN behaviour/NN for/IN large/JJ $/NN n/NN $/$ ./.
Surprisingly/RB ,/, multi-task/VB learning/NN can/MD be/VB asymptotically/RB essentially/RB useless/JJ ,/, in/IN the/DT sense/NN that/IN examples/NNS from/IN other/JJ tasks/NNS help/VBP only/RB when/WRB the/DT degree/NN of/IN inter-task/NN correlation/NN ,/, $/$ \/CD rho/CD $/$ ,/, is/VBZ near/IN its/PRP$ maximal/JJ value/NN $/$ \/SYM rho/NN =/SYM 1/CD $/$ ./.
This/DT effect/NN is/VBZ most/RBS extreme/JJ for/IN learning/NN of/IN smooth/JJ target/NN functions/VBZ as/IN described/VBN by/IN e.g./FW squared/VBD exponential/JJ kernels/NNS ./.
We/PRP also/RB demonstrate/VBP that/IN when/WRB learning/VBG many/JJ tasks/NNS ,/, the/DT learning/NN curves/NNS separate/VBP into/IN an/DT initial/JJ phase/NN ,/, where/WRB the/DT Bayes/NNP error/NN on/IN each/DT task/NN is/VBZ reduced/VBN down/RP to/IN a/DT plateau/NN value/NN by/IN "/`` collective/JJ learning/NN "/'' even/RB though/IN most/JJS tasks/NNS have/VBP not/RB seen/VBN examples/NNS ,/, and/CC a/DT final/JJ decay/NN that/WDT occurs/VBZ once/IN the/DT number/NN of/IN examples/NNS is/VBZ proportional/JJ to/IN the/DT number/NN of/IN tasks/NNS ./.
