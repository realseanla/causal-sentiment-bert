We/PRP study/VBP the/DT learnability/NN of/IN linear/JJ separators/NNS in/IN $/$ \/CD Re/IN ^/SYM d/NN $/$ in/IN the/DT presence/NN of/IN bounded/VBN (/-LRB- a.k.a/IN Massart/NNP )/-RRB- noise/NN ./.
This/DT is/VBZ a/DT realistic/JJ generalization/NN of/IN the/DT random/JJ classification/NN noise/NN model/NN ,/, where/WRB the/DT adversary/NN can/MD flip/VB each/DT example/NN $/$ x/SYM $/$ with/IN probability/NN $/$ \/CD eta/NN (/-LRB- x/NN )/-RRB- \/SYM leq/NN \/SYM eta/NN $/$ ./.
We/PRP provide/VBP the/DT first/JJ polynomial/JJ time/NN algorithm/NN that/WDT can/MD learn/VB linear/JJ separators/NNS to/IN arbitrarily/RB small/JJ excess/JJ error/NN in/IN this/DT noise/NN model/NN under/IN the/DT uniform/JJ distribution/NN over/IN the/DT unit/NN ball/NN in/IN $/$ \/CD Re/IN ^/SYM d/NN $/$ ,/, for/IN some/DT constant/JJ value/NN of/IN $/$ \/CD eta/NN $/$ ./.
While/IN widely/RB studied/VBN in/IN the/DT statistical/JJ learning/NN theory/NN community/NN in/IN the/DT context/NN of/IN getting/VBG faster/RBR convergence/NN rates/NNS ,/, computationally/RB efficient/JJ algorithms/NNS in/IN this/DT model/NN had/VBD remained/VBN elusive/JJ ./.
Our/PRP$ work/NN provides/VBZ the/DT first/JJ evidence/NN that/IN one/PRP can/MD indeed/RB design/VB algorithms/NNS achieving/VBG arbitrarily/RB small/JJ excess/JJ error/NN in/IN polynomial/JJ time/NN under/IN this/DT realistic/JJ noise/NN model/NN and/CC thus/RB opens/VBZ up/RP a/DT new/JJ and/CC exciting/JJ line/NN of/IN research/NN ./.
