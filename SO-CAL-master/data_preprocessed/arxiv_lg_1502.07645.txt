We/PRP consider/VBP the/DT problem/NN of/IN Bayesian/JJ learning/NN on/IN sensitive/JJ datasets/NNS and/CC present/JJ two/CD simple/JJ but/CC somewhat/RB surprising/JJ results/NNS that/WDT connect/VBP Bayesian/JJ learning/NN to/IN "/`` differential/JJ privacy/NN :/: ,/, a/DT cryptographic/JJ approach/NN to/TO protect/VB individual/JJ -/HYPH level/NN privacy/NN while/IN permiting/VBG database/NN -/HYPH level/NN utility/NN ./.
Specifically/RB ,/, we/PRP show/VBP that/IN that/DT under/IN standard/JJ assumptions/NNS ,/, getting/VBG one/CD single/JJ sample/NN from/IN a/DT posterior/JJ distribution/NN is/VBZ differentially/RB private/JJ "/'' for/IN free/JJ "/'' ./.
We/PRP will/MD see/VB that/DT estimator/NN is/VBZ statistically/RB consistent/JJ ,/, near/IN optimal/JJ and/CC computationally/RB tractable/JJ whenever/WRB the/DT Bayesian/JJ model/NN of/IN interest/NN is/VBZ consistent/JJ ,/, optimal/JJ and/CC tractable/JJ ./.
Similarly/RB but/CC separately/RB ,/, we/PRP show/VBP that/IN a/DT recent/JJ line/NN of/IN works/NNS that/WDT use/VBP stochastic/JJ gradient/NN for/IN Hybrid/NNP Monte/NNP Carlo/NNP (/-LRB- HMC/NNP )/-RRB- sampling/NN also/RB preserve/VB differentially/RB privacy/NN with/IN minor/JJ or/CC no/DT modifications/NNS of/IN the/DT algorithmic/JJ procedure/NN at/IN all/RB ,/, these/DT observations/NNS lead/VBP to/IN an/DT "/`` anytime/NN "/'' algorithm/NN for/IN Bayesian/JJ learning/NN under/IN privacy/NN constraint/NN ./.
We/PRP demonstrate/VBP that/IN it/PRP performs/VBZ much/RB better/JJR than/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN differential/JJ private/JJ methods/NNS on/IN synthetic/JJ and/CC real/JJ datasets/NNS ./.
