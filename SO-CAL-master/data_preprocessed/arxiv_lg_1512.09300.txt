We/PRP present/VBP an/DT autoencoder/NN that/WDT leverages/VBZ the/DT power/NN of/IN learned/VBN representations/NNS to/TO better/RBR measure/VB similarities/NNS in/IN data/NNS space/NN ./.
By/IN combining/VBG a/DT variational/JJ autoencoder/NN (/-LRB- VAE/NN )/-RRB- with/IN a/DT generative/JJ adversarial/JJ network/NN (/-LRB- GAN/NN )/-RRB- we/PRP can/MD use/VB learned/VBN feature/NN representations/NNS in/IN the/DT GAN/NNP discriminator/NN as/IN basis/NN for/IN the/DT VAE/NNP reconstruction/NN objective/NN ./.
Thereby/RB ,/, we/PRP replace/VBP element-wise/JJ errors/NNS with/IN feature-wise/JJ errors/NNS that/WDT better/JJR capture/NN the/DT data/NNS distribution/NN while/IN offering/VBG invariance/NN towards/IN e.g./FW translation/NN ./.
We/PRP apply/VBP our/PRP$ method/NN to/TO images/NNS of/IN faces/NNS and/CC show/VBP that/IN our/PRP$ method/NN outperforms/VBZ VAEs/NNS with/IN element-wise/JJ similarity/NN measures/NNS in/IN terms/NNS of/IN visual/JJ fidelity/NN ./.
Moreover/RB ,/, we/PRP show/VBP that/IN our/PRP$ method/NN learns/VBZ an/DT embedding/NN in/IN which/WDT high/JJ -/HYPH level/NN abstract/JJ visual/JJ features/NNS (/-LRB- e.g./FW wearing/VBG glasses/NNS )/-RRB- can/MD be/VB modified/VBN using/VBG simple/JJ arithmetic/NN ./.
