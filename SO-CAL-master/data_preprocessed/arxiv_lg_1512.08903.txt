In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT context/NN -/HYPH aware/JJ keyword/NN spotting/NN model/NN employing/VBG a/DT character/NN -/HYPH level/NN recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- for/IN spoken/VBN term/NN detection/NN in/IN continuous/JJ speech/NN ./.
The/DT RNN/NNP is/VBZ end/NN -/HYPH to/IN -/HYPH end/NN trained/VBN with/IN connectionist/JJ temporal/JJ classification/NN (/-LRB- CTC/NN )/-RRB- to/TO generate/VB the/DT probabilities/NNS of/IN character/NN and/CC word/NN -/HYPH boundary/NN labels/NNS ./.
There/EX is/VBZ no/DT need/NN for/IN the/DT phonetic/JJ transcription/NN ,/, senone/NN modeling/NN ,/, or/CC system/NN dictionary/NN in/RB training/NN and/CC testing/NN ./.
Also/RB ,/, keywords/NNS can/MD easily/RB be/VB added/VBN and/CC modified/VBN by/IN editing/VBG the/DT text/NN based/VBN keyword/NN list/NN without/IN retraining/VBG the/DT RNN/NN ./.
Moreover/RB ,/, the/DT unidirectional/JJ RNN/NN processes/VBZ an/DT infinitely/RB long/JJ input/NN audio/NN streams/NNS without/IN pre-segmentation/NN and/CC keywords/NNS are/VBP detected/VBN with/IN low/JJ -/HYPH latency/NN before/IN the/DT utterance/NN is/VBZ finished/VBN ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT proposed/VBN keyword/NN spotter/NN significantly/RB outperforms/VBZ the/DT deep/JJ neural/JJ network/NN (/-LRB- DNN/NN )/-RRB- and/CC hidden/VBN Markov/NNP model/NN (/-LRB- HMM/NN )/-RRB- based/VBN keyword/NN -/HYPH filler/NN model/NN even/RB with/IN less/JJR computations/NNS ./.
