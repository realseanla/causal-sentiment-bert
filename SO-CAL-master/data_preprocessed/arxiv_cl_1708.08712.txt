In/IN this/DT paper/NN ,/, we/PRP explore/VBP alternative/JJ ways/NNS to/TO train/VB a/DT neural/JJ machine/NN translation/NN system/NN in/IN a/DT multi-domain/JJ scenario/NN ./.
We/PRP investigate/VBP data/NNS concatenation/NN (/-LRB- with/IN fine/JJ tuning/NN )/-RRB- ,/, model/NN stacking/VBG (/-LRB- multi-level/JJ fine/JJ tuning/NN )/-RRB- ,/, data/NNS selection/NN and/CC weighted/JJ ensemble/NN ./.
We/PRP evaluate/VBP these/DT methods/NNS based/VBN on/IN three/CD criteria/NNS :/: i/LS )/-RRB- translation/NN quality/NN ,/, ii/CD )/-RRB- training/NN time/NN ,/, and/CC iii/LS )/-RRB- robustness/NN towards/IN out/IN -/HYPH of/IN -/HYPH domain/NN tests/NNS ./.
Our/PRP$ findings/NNS on/IN Arabic/JJ -/HYPH English/JJ and/CC German/JJ -/HYPH English/JJ language/NN pairs/NNS show/VBP that/IN the/DT best/JJS translation/NN quality/NN can/MD be/VB achieved/VBN by/IN building/VBG an/DT initial/JJ system/NN on/IN a/DT concatenation/NN of/IN available/JJ out/RB -/HYPH of/IN -/HYPH domain/NN data/NNS and/CC then/RB fine/JJ -/HYPH tuning/NN it/PRP on/IN in/IN -/HYPH domain/NN data/NNS ./.
Model/NN stacking/VBG works/NNS best/RBS when/WRB training/NN begins/VBZ with/IN the/DT furthest/JJS out/IN -/HYPH of/IN -/HYPH domain/NN data/NNS and/CC the/DT model/NN is/VBZ incrementally/RB fine/JJ -/HYPH tuned/VBN with/IN the/DT next/JJ furthest/JJS domain/NN and/CC so/RB on/RB ./.
Data/NNS selection/NN did/VBD not/RB give/VB the/DT best/JJS results/NNS ,/, but/CC can/MD be/VB considered/VBN as/IN a/DT decent/JJ compromise/NN between/IN training/NN time/NN and/CC translation/NN quality/NN ./.
A/DT weighted/JJ ensemble/NN of/IN different/JJ individual/JJ models/NNS performed/VBN better/RBR than/IN data/NNS selection/NN ./.
It/PRP is/VBZ beneficial/JJ in/IN a/DT scenario/NN when/WRB there/EX is/VBZ no/DT time/NN for/IN fine/JJ -/HYPH tuning/NN ./.
