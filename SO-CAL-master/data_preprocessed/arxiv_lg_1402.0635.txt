We/PRP consider/VBP the/DT problem/NN of/IN reinforcement/NN learning/VBG with/IN an/DT orientation/NN toward/IN contexts/NNS in/IN which/WDT an/DT agent/NN must/MD generalize/VB from/IN past/JJ experience/NN and/CC explore/VB to/TO reduce/VB uncertainty/NN ./.
We/PRP propose/VBP an/DT approach/NN to/IN exploration/NN based/VBN on/IN randomized/JJ value/NN functions/NNS and/CC an/DT algorithm/NN --/: randomized/VBN least/RBS -/HYPH squares/NNS value/NN iteration/NN (/-LRB- RLSVI/NN )/-RRB- --/: that/WDT embodies/VBZ this/DT approach/NN ./.
We/PRP explain/VBP why/WRB versions/NNS of/IN least/JJS -/HYPH squares/NNS value/NN iteration/NN that/WDT use/VBP Boltzmann/NNP or/CC epsilon/NNP -/HYPH greedy/JJ exploration/NN can/MD be/VB highly/RB inefficient/JJ and/CC present/JJ computational/JJ results/NNS that/WDT demonstrate/VBP dramatic/JJ efficiency/NN gains/NNS enjoyed/VBN by/IN RLSVI/NN ./.
Our/PRP$ experiments/NNS focus/VBP on/IN learning/VBG over/IN episodes/NNS of/IN a/DT finite/NN -/HYPH horizon/NN Markov/NNP decision/NN process/NN and/CC use/VB a/DT version/NN of/IN RLSVI/NN designed/VBN for/IN that/DT task/NN ,/, but/CC we/PRP also/RB propose/VBP a/DT version/NN of/IN RLSVI/NN that/WDT addresses/VBZ continual/JJ learning/NN in/IN an/DT infinite/JJ -/HYPH horizon/NN discounted/VBN Markov/NNP decision/NN process/NN ./.
