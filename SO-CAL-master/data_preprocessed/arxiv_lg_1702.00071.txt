It/PRP is/VBZ well/RB known/VBN that/IN it/PRP is/VBZ challenging/VBG to/TO train/VB deep/JJ neural/JJ networks/NNS and/CC recurrent/JJ neural/JJ networks/NNS for/IN tasks/NNS that/WDT exhibit/VBP long/JJ term/NN dependencies/NNS ./.
The/DT vanishing/VBG or/CC exploding/VBG gradient/NN problem/NN is/VBZ a/DT well/RB known/JJ issue/NN associated/VBN with/IN these/DT challenges/NNS ./.
One/CD approach/NN to/IN addressing/VBG vanishing/VBG and/CC exploding/VBG gradients/NNS is/VBZ to/TO use/VB either/CC soft/JJ or/CC hard/JJ constraints/NNS on/IN weight/NN matrices/NNS so/RB as/IN to/TO encourage/VB or/CC enforce/VB orthogonality/NN ./.
Orthogonal/JJ matrices/NNS preserve/VB gradient/NN norm/NN during/IN backpropagation/NN and/CC can/MD therefore/RB be/VB a/DT desirable/JJ property/NN ;/: however/RB ,/, we/PRP find/VBP that/IN hard/JJ constraints/NNS on/IN orthogonality/NN can/MD negatively/RB affect/VB the/DT speed/NN of/IN convergence/NN and/CC model/NN performance/NN ./.
This/DT paper/NN explores/VBZ the/DT issues/NNS of/IN optimization/NN convergence/NN ,/, speed/NN and/CC gradient/NN stability/NN using/VBG a/DT variety/NN of/IN different/JJ methods/NNS for/IN encouraging/VBG or/CC enforcing/VBG orthogonality/NN ./.
In/IN particular/JJ we/PRP propose/VBP a/DT weight/NN matrix/NN factorization/NN and/CC parameterization/NN strategy/NN through/IN which/WDT we/PRP can/MD bound/VBN matrix/NN norms/NNS and/CC therein/RB control/VB the/DT degree/NN of/IN expansivity/NN induced/VBN during/IN backpropagation/NN ./.
