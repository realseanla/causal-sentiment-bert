This/DT paper/NN presents/VBZ an/DT algorithm/NN ,/, Voted/VBD Kernel/NNP Regularization/NNP ,/, that/WDT provides/VBZ the/DT flexibility/NN of/IN using/VBG potentially/RB very/RB complex/JJ kernel/NN functions/NNS such/JJ as/IN predictors/NNS based/VBN on/IN much/RB higher/JJR -/HYPH degree/NN polynomial/JJ kernels/NNS ,/, while/IN benefitting/VBG from/IN strong/JJ learning/NN guarantees/NNS ./.
The/DT success/NN of/IN our/PRP$ algorithm/NN arises/VBZ from/IN derived/VBN bounds/NNS that/WDT suggest/VBP a/DT new/JJ regularization/NN penalty/NN in/IN terms/NNS of/IN the/DT Rademacher/NNP complexities/NNS of/IN the/DT corresponding/VBG families/NNS of/IN kernel/NN maps/NNS ./.
In/IN a/DT series/NN of/IN experiments/NNS we/PRP demonstrate/VBP the/DT improved/VBN performance/NN of/IN our/PRP$ algorithm/NN as/IN compared/VBN to/IN baselines/NNS ./.
Furthermore/RB ,/, the/DT algorithm/NN enjoys/VBZ several/JJ favorable/JJ properties/NNS ./.
The/DT optimization/NN problem/NN is/VBZ convex/NN ,/, it/PRP allows/VBZ for/IN learning/VBG with/IN non-PDS/JJ kernels/NNS ,/, and/CC the/DT solutions/NNS are/VBP highly/RB sparse/JJ ,/, resulting/VBG in/IN improved/VBN classification/NN speed/NN and/CC memory/NN requirements/NNS ./.
