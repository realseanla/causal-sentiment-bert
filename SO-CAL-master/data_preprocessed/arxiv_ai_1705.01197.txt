We/PRP analyze/VBP how/WRB the/DT knowledge/NN to/TO autonomously/RB handle/VB one/CD type/NN of/IN intersection/NN ,/, represented/VBN as/IN a/DT Deep/JJ Q/NN -/HYPH Network/NN ,/, translates/VBZ to/IN other/JJ types/NNS of/IN intersections/NNS (/-LRB- tasks/NNS )/-RRB- ./.
We/PRP view/VBP intersection/NN handling/NN as/IN a/DT deep/JJ reinforcement/NN learning/VBG problem/NN ,/, which/WDT approximates/VBZ the/DT state/NN action/NN Q/NN function/NN as/IN a/DT deep/JJ neural/JJ network/NN ./.
Using/VBG a/DT traffic/NN simulator/NN ,/, we/PRP show/VBP that/IN directly/RB copying/VBG a/DT network/NN trained/VBN for/IN one/CD type/NN of/IN intersection/NN to/IN another/DT type/NN of/IN intersection/NN decreases/VBZ the/DT success/NN rate/NN ./.
We/PRP also/RB show/VBP that/IN when/WRB a/DT network/NN that/WDT is/VBZ pre-trained/VBN on/IN Task/NNP A/NNP and/CC then/RB is/VBZ fine/JJ -/HYPH tuned/VBN on/IN a/DT Task/NNP B/NNP ,/, the/DT resulting/VBG network/NN not/RB only/RB performs/VBZ better/JJR on/IN the/DT Task/NNP B/NNP than/IN an/DT network/NN exclusively/RB trained/VBN on/IN Task/NNP A/NNP ,/, but/CC also/RB retained/VBD knowledge/NN on/IN the/DT Task/NNP A./NNP Finally/RB ,/, we/PRP examine/VBP a/DT lifelong/JJ learning/NN setting/NN ,/, where/WRB we/PRP train/VBP a/DT single/JJ network/NN on/IN five/CD different/JJ types/NNS of/IN intersections/NNS sequentially/RB and/CC show/VB that/IN the/DT resulting/VBG network/NN exhibited/VBD catastrophic/JJ forgetting/VBG of/IN knowledge/NN on/IN previous/JJ tasks/NNS ./.
This/DT result/NN suggests/VBZ a/DT need/NN for/IN a/DT long/JJ -/HYPH term/NN memory/NN component/NN to/TO preserve/VB knowledge/NN ./.
