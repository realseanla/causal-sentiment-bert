Recently/RB ,/, recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- as/IN powerful/JJ sequence/NN models/NNS have/VBP re-emerged/VBN as/IN a/DT potential/JJ acoustic/JJ model/NN for/IN statistical/JJ parametric/JJ speech/NN synthesis/NN (/-LRB- SPSS/NNP )/-RRB- ./.
The/DT long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- architecture/NN is/VBZ particularly/RB attractive/JJ because/IN it/PRP addresses/VBZ the/DT vanishing/VBG gradient/NN problem/NN in/IN standard/JJ RNNs/NNS ,/, making/VBG them/PRP easier/JJR to/TO train/VB ./.
Although/IN recent/JJ studies/NNS have/VBP demonstrated/VBN that/IN LSTMs/NNPS can/MD achieve/VB significantly/RB better/JJR performance/NN on/IN SPSS/NNP than/IN deep/JJ feed/NN -/HYPH forward/JJ neural/JJ networks/NNS ,/, little/JJ is/VBZ known/VBN about/IN why/WRB ./.
Here/RB we/PRP attempt/VBP to/TO answer/VB two/CD questions/NNS :/: a/LS )/-RRB- why/WRB do/VBP LSTMs/NNPS work/VB well/RB as/IN a/DT sequence/NN model/NN for/IN SPSS/NNP ;/: b/LS )/-RRB- which/WDT component/NN (/-LRB- e.g./FW ,/, input/NN gate/NN ,/, output/NN gate/NN ,/, forget/VB gate/NN )/-RRB- is/VBZ most/RBS important/JJ ./.
We/PRP present/VBP a/DT visual/JJ analysis/NN alongside/IN a/DT series/NN of/IN experiments/NNS ,/, resulting/VBG in/IN a/DT proposal/NN for/IN a/DT simplified/JJ architecture/NN ./.
The/DT simplified/JJ architecture/NN has/VBZ significantly/RB fewer/JJR parameters/NNS than/IN an/DT LSTM/NNP ,/, thus/RB reducing/VBG generation/NN complexity/NN considerably/RB without/IN degrading/JJ quality/NN ./.
