There/EX are/VBP many/JJ applications/NNS in/IN which/WDT it/PRP is/VBZ desirable/JJ to/TO order/VB rather/RB than/IN classify/VB instances/NNS ./.
Here/RB we/PRP consider/VBP the/DT problem/NN of/IN learning/VBG how/WRB to/TO order/VB instances/NNS given/VBN feedback/NN in/IN the/DT form/NN of/IN preference/NN judgments/NNS ,/, i.e./FW ,/, statements/NNS to/IN the/DT effect/NN that/WDT one/CD instance/NN should/MD be/VB ranked/VBN ahead/RB of/IN another/DT ./.
We/PRP outline/VBP a/DT two/CD -/HYPH stage/NN approach/NN in/IN which/WDT one/NN first/RB learns/VBZ by/IN conventional/JJ means/NNS a/DT binary/JJ preference/NN function/NN indicating/VBG whether/IN it/PRP is/VBZ advisable/JJ to/IN rank/NN one/CD instance/NN before/IN another/DT ./.
Here/RB we/PRP consider/VBP an/DT on/IN -/HYPH line/NN algorithm/NN for/IN learning/VBG preference/NN functions/NNS that/WDT is/VBZ based/VBN on/IN Freund/NNP and/CC Schapire/NNP 's/POS '/`` Hedge/NNP '/'' algorithm/NN ./.
In/IN the/DT second/JJ stage/NN ,/, new/JJ instances/NNS are/VBP ordered/VBN so/RB as/IN to/TO maximize/VB agreement/NN with/IN the/DT learned/VBN preference/NN function/NN ./.
We/PRP show/VBP that/IN the/DT problem/NN of/IN finding/VBG the/DT ordering/NN that/WDT agrees/VBZ best/JJS with/IN a/DT learned/VBN preference/NN function/NN is/VBZ NP/NNP -/HYPH complete/JJ ./.
Nevertheless/RB ,/, we/PRP describe/VBP simple/JJ greedy/JJ algorithms/NNS that/WDT are/VBP guaranteed/VBN to/TO find/VB a/DT good/JJ approximation/NN ./.
Finally/RB ,/, we/PRP show/VBP how/WRB metasearch/NN can/MD be/VB formulated/VBN as/IN an/DT ordering/NN problem/NN ,/, and/CC present/JJ experimental/JJ results/NNS on/IN learning/VBG a/DT combination/NN of/IN '/`` search/NN experts/NNS '/'' ,/, each/DT of/IN which/WDT is/VBZ a/DT domain/NN -/HYPH specific/JJ query/NN expansion/NN strategy/NN for/IN a/DT web/NN search/NN engine/NN ./.
