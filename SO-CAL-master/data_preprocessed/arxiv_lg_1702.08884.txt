The/DT success/NN of/IN semi-supervised/VBN learning/NN crucially/RB relies/VBZ on/IN the/DT scalability/NN to/IN a/DT huge/JJ amount/NN of/IN unlabelled/JJ data/NNS that/WDT are/VBP needed/VBN to/TO capture/VB the/DT underlying/VBG manifold/JJ structure/NN for/IN better/JJR classification/NN ./.
Since/IN computing/VBG the/DT pairwise/JJ similarity/NN between/IN the/DT training/NN data/NNS is/VBZ prohibitively/RB expensive/JJ in/IN most/JJS kinds/NNS of/IN input/NN data/NNS ,/, currently/RB ,/, there/EX is/VBZ no/DT general/JJ ready/JJ -/HYPH to/IN -/HYPH use/NN semi-supervised/VBN learning/NN method/NN //HYPH tool/NN available/JJ for/IN learning/VBG with/IN tens/NNS of/IN millions/NNS or/CC more/JJR data/NNS points/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP adopted/VBD the/DT idea/NN of/IN two/CD low/JJ -/HYPH rank/NN label/NN propagation/NN algorithms/NNS ,/, GLNP/NN (/-LRB- Global/NNP Linear/NNP Neighborhood/NNP Propagation/NNP )/-RRB- and/CC Kernel/NNP Nystr/NNP \/SYM "/`` om/FW Approximation/NN ,/, and/CC implemented/VBD the/DT parallelized/JJ version/NN of/IN the/DT two/CD algorithms/NNS accelerated/VBN with/IN Nesterov/NNP 's/POS accelerated/VBN projected/VBN gradient/NN descent/NN for/IN Big/JJ -/HYPH data/NNS Label/NN Propagation/NN (/-LRB- BigLP/NN )/-RRB- ./.
