Recent/JJ advances/NNS in/IN neural/JJ variational/JJ inference/NN have/VBP spawned/VBN a/DT renaissance/NN in/IN deep/JJ latent/JJ variable/JJ models/NNS ./.
In/IN this/DT paper/NN we/PRP introduce/VBP a/DT generic/JJ variational/JJ inference/NN framework/NN for/IN generative/JJ and/CC conditional/JJ models/NNS of/IN text/NN ./.
While/IN traditional/JJ variational/JJ methods/NNS derive/VBP an/DT analytic/JJ approximation/NN for/IN the/DT intractable/JJ distributions/NNS over/IN latent/JJ variables/NNS ,/, here/RB we/PRP construct/VB an/DT inference/NN network/NN conditioned/VBN on/IN the/DT discrete/JJ text/NN input/NN to/TO provide/VB the/DT variational/JJ distribution/NN ./.
We/PRP validate/VBP this/DT framework/NN on/IN two/CD very/RB different/JJ text/NN modelling/NN applications/NNS ,/, generative/JJ document/NN modelling/NN and/CC supervised/VBD question/NN answering/VBG ./.
Our/PRP$ neural/JJ variational/JJ document/NN model/NN combines/VBZ a/DT continuous/JJ stochastic/JJ document/NN representation/NN with/IN a/DT bag/NN -/HYPH of/IN -/HYPH words/NNS generative/JJ model/NN and/CC achieves/VBZ the/DT lowest/JJS reported/VBN perplexities/NNS on/IN two/CD standard/JJ test/NN corpora/NNS ./.
The/DT neural/JJ answer/NN selection/NN model/NN employs/VBZ a/DT stochastic/JJ representation/NN layer/NN within/IN an/DT attention/NN mechanism/NN to/TO extract/VB the/DT semantics/NNS between/IN a/DT question/NN and/CC answer/NN pair/NN ./.
On/IN two/CD question/NN answering/VBG benchmarks/NNS this/DT model/NN exceeds/VBZ all/DT previous/JJ published/VBN benchmarks/NNS ./.
