Visual/JJ question/NN answering/VBG (/-LRB- VQA/NN )/-RRB- has/VBZ witnessed/VBN great/JJ progress/NN since/IN May/NNP ,/, 2015/CD as/IN a/DT classic/JJ problem/NN unifying/JJ visual/JJ and/CC textual/JJ data/NNS into/IN a/DT system/NN ./.
Many/JJ enlightening/JJ VQA/NN works/VBZ explore/VB deep/RB into/IN the/DT image/NN and/CC question/NN encodings/NNS and/CC fusing/VBG methods/NNS ,/, of/IN which/WDT attention/NN is/VBZ the/DT most/RBS effective/JJ and/CC infusive/JJ mechanism/NN ./.
Current/JJ attention/NN based/VBN methods/NNS focus/VBP on/IN adequate/JJ fusion/NN of/IN visual/JJ and/CC textual/JJ features/NNS ,/, but/CC lack/VBP the/DT attention/NN to/IN where/WRB people/NNS focus/VBP to/TO ask/VB questions/NNS about/IN the/DT image/NN ./.
Traditional/JJ attention/NN based/VBN methods/NNS attach/VBP a/DT single/JJ value/NN to/IN the/DT feature/NN at/IN each/DT spatial/JJ location/NN ,/, which/WDT losses/NNS many/JJ useful/JJ information/NN ./.
To/TO remedy/VB these/DT problems/NNS ,/, we/PRP propose/VBP a/DT general/JJ method/NN to/TO perform/VB saliency/NN -/HYPH like/JJ pre-selection/NN on/IN overlapped/VBN region/NN features/NNS by/IN the/DT interrelation/NN of/IN bidirectional/JJ LSTM/NN (/-LRB- BiLSTM/NN )/-RRB- ,/, and/CC use/VB a/DT novel/JJ element-wise/JJ multiplication/NN based/VBN attention/NN method/NN to/TO capture/VB more/JJR competent/JJ correlation/NN information/NN between/IN visual/JJ and/CC textual/JJ features/NNS ./.
We/PRP conduct/VBP experiments/NNS on/IN the/DT large/JJ -/HYPH scale/NN COCO/NN -/HYPH VQA/NN dataset/NN and/CC analyze/VB the/DT effectiveness/NN of/IN our/PRP$ model/NN demonstrated/VBN by/IN strong/JJ empirical/JJ results/NNS ./.
