We/PRP introduce/VBP a/DT novel/JJ schema/NN for/IN sequence/NN to/IN sequence/NN learning/NN with/IN a/DT Deep/JJ Q/NN -/HYPH Network/NN (/-LRB- DQN/NN )/-RRB- ,/, which/WDT decodes/VBZ the/DT output/NN sequence/NN iteratively/RB ./.
The/DT aim/NN here/RB is/VBZ to/TO enable/VB the/DT decoder/NN to/TO first/RB tackle/VB easier/JJR portions/NNS of/IN the/DT sequences/NNS ,/, and/CC then/RB turn/VB to/TO cope/VB with/IN difficult/JJ parts/NNS ./.
Specifically/RB ,/, in/IN each/DT iteration/NN ,/, an/DT encoder/NN -/HYPH decoder/NN Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- network/NN is/VBZ employed/VBN to/IN ,/, from/IN the/DT input/NN sequence/NN ,/, automatically/RB create/VB features/NNS to/TO represent/VB the/DT internal/JJ states/NNS of/IN and/CC formulate/VB a/DT list/NN of/IN potential/JJ actions/NNS for/IN the/DT DQN/NNP ./.
Take/VB rephrasing/VBG a/DT natural/JJ sentence/NN as/IN an/DT example/NN ./.
This/DT list/NN can/MD contain/VB ranked/VBN potential/JJ words/NNS ./.
Next/RB ,/, the/DT DQN/NNP learns/VBZ to/TO make/VB decision/NN on/IN which/WDT action/NN (/-LRB- e.g./FW ,/, word/NN )/-RRB- will/MD be/VB selected/VBN from/IN the/DT list/NN to/TO modify/VB the/DT current/JJ decoded/VBN sequence/NN ./.
The/DT newly/RB modified/VBN output/NN sequence/NN is/VBZ subsequently/RB used/VBN as/IN the/DT input/NN to/IN the/DT DQN/NNP for/IN the/DT next/JJ decoding/NN iteration/NN ./.
In/IN each/DT iteration/NN ,/, we/PRP also/RB bias/VBP the/DT reinforcement/NN learning/NN 's/POS attention/NN to/TO explore/VB sequence/NN portions/NNS which/WDT are/VBP previously/RB difficult/JJ to/TO be/VB decoded/VBN ./.
For/IN evaluation/NN ,/, the/DT proposed/VBN strategy/NN was/VBD trained/VBN to/TO decode/VB ten/CD thousands/NNS natural/JJ sentences/NNS ./.
Our/PRP$ experiments/NNS indicate/VBP that/IN ,/, when/WRB compared/VBN to/IN a/DT left/JJ -/HYPH to/TO -/HYPH right/JJ greedy/JJ beam/NN search/NN LSTM/NN decoder/NN ,/, the/DT proposed/JJ method/NN performed/VBN competitively/RB well/RB when/WRB decoding/VBG sentences/NNS from/IN the/DT training/NN set/NN ,/, but/CC significantly/RB outperformed/VBD the/DT baseline/NN when/WRB decoding/VBG unseen/JJ sentences/NNS ,/, in/IN terms/NNS of/IN BLEU/NN score/NN obtained/VBN ./.
