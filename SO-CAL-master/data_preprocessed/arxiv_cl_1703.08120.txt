We/PRP propose/VBP a/DT series/NN of/IN recurrent/JJ and/CC contextual/JJ neural/JJ network/NN models/NNS for/IN multiple/JJ choice/NN visual/JJ question/NN answering/VBG on/IN the/DT Visual7W/NN dataset/NN ./.
Motivated/VBN by/IN divergent/JJ trends/NNS in/IN model/NN complexities/NNS in/IN the/DT literature/NN ,/, we/PRP explore/VBP the/DT balance/NN between/IN model/NN expressiveness/NN and/CC simplicity/NN by/IN studying/VBG incrementally/RB more/RBR complex/JJ architectures/NNS ./.
We/PRP start/VBP with/IN LSTM/NN -/HYPH encoding/NN of/IN input/NN questions/NNS and/CC answers/NNS ;/: build/VB on/IN this/DT with/IN context/NN generation/NN by/IN LSTM/NNP -/HYPH encodings/NNS of/IN neural/JJ image/NN and/CC question/NN representations/NNS and/CC attention/NN over/IN images/NNS ;/: and/CC evaluate/VB the/DT diversity/NN and/CC predictive/JJ power/NN of/IN our/PRP$ models/NNS and/CC the/DT ensemble/NN thereof/RB ./.
All/DT models/NNS are/VBP evaluated/VBN against/IN a/DT simple/JJ baseline/NN inspired/VBN by/IN the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ,/, consisting/VBG of/IN involving/VBG simple/JJ concatenation/NN of/IN bag/NN -/HYPH of/IN -/HYPH words/NNS and/CC CNN/NNP representations/NNS for/IN the/DT text/NN and/CC images/NNS ,/, respectively/RB ./.
Generally/RB ,/, we/PRP observe/VBP marked/JJ variation/NN in/IN image/NN -/HYPH reasoning/NN performance/NN between/IN our/PRP$ models/NNS not/RB obvious/JJ from/IN their/PRP$ overall/JJ performance/NN ,/, as/RB well/RB as/IN evidence/NN of/IN dataset/NN bias/NN ./.
Our/PRP$ standalone/JJ models/NNS achieve/VBP accuracies/NNS up/IN to/IN $/$ 64.6/CD \/SYM percent/NN $/$ ,/, while/IN the/DT ensemble/NN of/IN all/DT models/NNS achieves/VBZ the/DT best/JJS accuracy/NN of/IN $/$ 66.67/CD \/SYM percent/NN $/$ ,/, within/IN $/$ 0.5/CD \/SYM percent/NN $/$ of/IN the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN for/IN Visual7W/NN ./.
