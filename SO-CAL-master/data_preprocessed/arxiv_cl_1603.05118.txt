This/DT paper/NN presents/VBZ a/DT novel/JJ approach/NN to/IN recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- regularization/NN ./.
Differently/RB from/IN the/DT widely/RB adopted/VBN dropout/NN method/NN ,/, which/WDT is/VBZ applied/VBN to/TO forward/VB connections/NNS of/IN feed/NN -/HYPH forward/JJ architectures/NNS or/CC RNNs/NNS ,/, we/PRP propose/VBP to/TO drop/VB neurons/NNS directly/RB in/IN recurrent/JJ connections/NNS in/IN a/DT way/NN that/WDT does/VBZ not/RB cause/VB loss/NN of/IN long/JJ -/HYPH term/NN memory/NN ./.
Our/PRP$ approach/NN is/VBZ as/IN easy/JJ to/TO implement/VB and/CC apply/VB as/IN the/DT regular/JJ feed/NN -/HYPH forward/JJ dropout/NN and/CC we/PRP demonstrate/VBP its/PRP$ effectiveness/NN for/IN the/DT most/RBS popular/JJ recurrent/JJ networks/NNS :/: vanilla/NN RNNs/NNS ,/, Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- and/CC Gated/VBN Recurrent/JJ Unit/NN (/-LRB- GRU/NN )/-RRB- networks/NNS ./.
Our/PRP$ experiments/NNS on/IN three/CD NLP/NN benchmarks/NNS show/VBP consistent/JJ improvements/NNS even/RB when/WRB combined/VBN with/IN conventional/JJ feed/NN -/HYPH forward/JJ dropout/NN ./.
