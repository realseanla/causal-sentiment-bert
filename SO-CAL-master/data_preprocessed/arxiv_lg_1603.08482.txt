Mixture/NN modeling/NN is/VBZ a/DT general/JJ technique/NN for/IN making/VBG any/DT simple/JJ model/NN more/RBR expressive/JJ through/IN weighted/JJ combination/NN ./.
This/DT generality/NN and/CC simplicity/NN in/IN part/NN explains/VBZ the/DT success/NN of/IN the/DT Expectation/NN Maximization/NN (/-LRB- EM/NN )/-RRB- algorithm/NN ,/, in/IN which/WDT updates/NNS are/VBP easy/JJ to/TO derive/VB for/IN a/DT wide/JJ class/NN of/IN mixture/NN models/NNS ./.
However/RB ,/, the/DT likelihood/NN of/IN a/DT mixture/NN model/NN is/VBZ non-convex/JJ ,/, so/RB EM/NNP has/VBZ no/DT known/VBN global/JJ convergence/NN guarantees/NNS ./.
Recently/RB ,/, method/NN of/IN moments/NNS approaches/VBZ offer/NN global/JJ guarantees/NNS for/IN some/DT mixture/NN models/NNS ,/, but/CC they/PRP do/VBP not/RB extend/VB easily/RB to/IN the/DT range/NN of/IN mixture/NN models/NNS that/WDT exist/VBP ./.
In/IN this/DT work/NN ,/, we/PRP present/VBP Polymom/NNP ,/, an/DT unifying/JJ framework/NN based/VBN on/IN method/NN of/IN moments/NNS in/IN which/WDT estimation/NN procedures/NNS are/VBP easily/RB derivable/JJ ,/, just/RB as/IN in/IN EM/NNP ./.
Polymom/NNP is/VBZ applicable/JJ when/WRB the/DT moments/NNS of/IN a/DT single/JJ mixture/NN component/NN are/VBP polynomials/NNS of/IN the/DT parameters/NNS ./.
Our/PRP$ key/JJ observation/NN is/VBZ that/IN the/DT moments/NNS of/IN the/DT mixture/NN model/NN are/VBP a/DT mixture/NN of/IN these/DT polynomials/NNS ,/, which/WDT allows/VBZ us/PRP to/TO cast/VB estimation/NN as/IN a/DT Generalized/VBN Moment/NN Problem/NNP ./.
We/PRP solve/VB its/PRP$ relaxations/NNS using/VBG semidefinite/NN optimization/NN ,/, and/CC then/RB extract/VB parameters/NNS using/VBG ideas/NNS from/IN computer/NN algebra/NN ./.
This/DT framework/NN allows/VBZ us/PRP to/TO draw/VB insights/NNS and/CC apply/VB tools/NNS from/IN convex/NN optimization/NN ,/, computer/NN algebra/NN and/CC the/DT theory/NN of/IN moments/NNS to/TO study/VB problems/NNS in/IN statistical/JJ estimation/NN ./.
