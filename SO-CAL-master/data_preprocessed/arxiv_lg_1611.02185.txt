We/PRP present/VBP a/DT novel/JJ layerwise/NN optimization/NN algorithm/NN for/IN the/DT learning/NN objective/NN of/IN a/DT large/JJ class/NN of/IN convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- ./.
Specifically/RB ,/, we/PRP consider/VBP CNNs/NNS that/WDT employ/VBP piecewise/RB linear/JJ non-linearities/NNS such/JJ as/IN the/DT commonly/RB used/VBN ReLU/NN and/CC max/NN -/HYPH pool/NN ,/, and/CC an/DT SVM/NN classifier/NN as/IN the/DT final/JJ layer/NN ./.
The/DT key/JJ observation/NN of/IN our/PRP$ approach/NN is/VBZ that/IN the/DT problem/NN corresponding/VBG to/IN the/DT parameter/NN estimation/NN of/IN a/DT layer/NN can/MD be/VB formulated/VBN as/IN a/DT difference/NN -/HYPH of/IN -/HYPH convex/NN (/-LRB- DC/NNP )/-RRB- program/NN ,/, which/WDT happens/VBZ to/TO be/VB a/DT latent/NN structured/VBN SVM/NNP ./.
We/PRP optimize/VBP the/DT DC/NNP program/NN using/VBG the/DT concave/NN -/HYPH convex/NN procedure/NN ,/, which/WDT requires/VBZ us/PRP to/TO iteratively/RB solve/VB a/DT structured/JJ SVM/NN problem/NN ./.
To/IN this/DT end/NN ,/, we/PRP extend/VBP the/DT block/NN -/HYPH coordinate/NN Frank/NNP -/HYPH Wolfe/NNP (/-LRB- BCFW/NNP )/-RRB- algorithm/NN in/IN three/CD important/JJ ways/NNS :/: (/-LRB- i/LS )/-RRB- we/PRP include/VBP a/DT trust/NN -/HYPH region/NN for/IN the/DT parameters/NNS ,/, which/WDT allows/VBZ us/PRP to/TO use/VB the/DT previous/JJ parameters/NNS as/IN an/DT initialization/NN ;/: (/-LRB- ii/LS )/-RRB- we/PRP reduce/VBP the/DT memory/NN requirement/NN of/IN BCFW/NNP by/IN potentially/RB several/JJ orders/NNS of/IN magnitude/NN for/IN the/DT dense/JJ layers/NNS ,/, which/WDT enables/VBZ us/PRP to/TO learn/VB a/DT large/JJ set/NN of/IN parameters/NNS ;/: and/CC (/-LRB- iii/LS )/-RRB- we/PRP observe/VBP that/IN ,/, empirically/RB ,/, the/DT optimal/JJ solution/NN of/IN the/DT structured/JJ SVM/NN problem/NN can/MD be/VB obtained/VBN efficiently/RB by/IN solving/VBG a/DT related/VBN ,/, but/CC significantly/RB easier/JJR ,/, multi-class/NN SVM/NN problem/NN ./.
Using/VBG publicly/RB available/JJ data/NNS sets/NNS ,/, we/PRP show/VBP that/IN our/PRP$ approach/NN outperforms/VBZ the/DT state/NN of/IN the/DT art/NN variants/NNS of/IN backpropagation/NN ,/, and/CC is/VBZ also/RB more/RBR robust/JJ to/IN the/DT hyperparameters/NNS of/IN the/DT learning/NN objective/NN ./.
