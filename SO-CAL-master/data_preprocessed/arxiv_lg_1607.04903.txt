A/DT major/JJ challenge/NN in/IN the/DT training/NN of/IN recurrent/JJ neural/JJ networks/NNS is/VBZ the/DT so/RB -/HYPH called/VBN vanishing/VBG or/CC exploding/VBG gradient/NN problem/NN ./.
The/DT use/NN of/IN a/DT norm/NN -/HYPH preserving/VBG transition/NN operator/NN can/MD address/VB this/DT issue/NN ,/, but/CC parametrization/NN is/VBZ challenging/JJ ./.
In/IN this/DT work/NN we/PRP focus/VBP on/IN unitary/JJ operators/NNS and/CC describe/VB a/DT parametrization/NN using/VBG the/DT Lie/NN algebra/NN u/NN (/-LRB- n/NN )/-RRB- associated/VBN with/IN the/DT Lie/NN group/NN U/NN (/-LRB- n/NN )/-RRB- of/IN n/NN x/SYM n/NN unitary/JJ matrices/NNS ./.
The/DT exponential/JJ map/NN provides/VBZ a/DT correspondence/NN between/IN these/DT spaces/NNS ,/, and/CC allows/VBZ us/PRP to/TO define/VB a/DT unitary/JJ matrix/NN using/VBG n/NN ^/SYM 2/CD real/JJ coefficients/NNS relative/JJ to/IN a/DT basis/NN of/IN the/DT Lie/NN algebra/NN ./.
The/DT parametrization/NN is/VBZ closed/VBN under/IN additive/JJ updates/NNS of/IN these/DT coefficients/NNS ,/, and/CC thus/RB provides/VBZ a/DT simple/JJ space/NN in/IN which/WDT to/TO do/VB gradient/NN descent/NN ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN of/IN this/DT parametrization/NN on/IN the/DT problem/NN of/IN learning/VBG arbitrary/JJ unitary/JJ operators/NNS ,/, comparing/VBG to/IN several/JJ baselines/NNS and/CC outperforming/VBG a/DT recently/RB -/HYPH proposed/VBN lower/JJR -/HYPH dimensional/JJ parametrization/NN ./.
