Boosting/VBG is/VBZ a/DT popular/JJ ensemble/NN algorithm/NN that/WDT generates/VBZ more/RBR powerful/JJ learners/NNS by/IN linearly/RB combining/VBG base/NN models/NNS from/IN a/DT simpler/JJR hypothesis/NN class/NN ./.
In/IN this/DT work/NN ,/, we/PRP investigate/VBP the/DT problem/NN of/IN adapting/VBG batch/NN gradient/NN boosting/VBG for/IN minimizing/VBG convex/NN loss/NN functions/VBZ to/IN online/JJ setting/NN where/WRB the/DT loss/NN at/IN each/DT iteration/NN is/VBZ i.i.d/JJ sampled/VBN from/IN an/DT unknown/JJ distribution/NN ./.
To/TO generalize/VB from/IN batch/NN to/IN online/RB ,/, we/PRP first/RB introduce/VB the/DT definition/NN of/IN online/JJ weak/JJ learning/NN edge/NN with/IN which/WDT for/IN strongly/RB convex/JJ and/CC smooth/JJ loss/NN functions/NNS ,/, we/PRP present/VBP an/DT algorithm/NN ,/, Streaming/VBG Gradient/NN Boosting/VBG (/-LRB- SGB/NN )/-RRB- with/IN exponential/JJ shrinkage/NN guarantees/NNS in/IN the/DT number/NN of/IN weak/JJ learners/NNS ./.
We/PRP further/RB present/VBP an/DT adaptation/NN of/IN SGB/NNP to/TO optimize/VB non-smooth/JJ loss/NN functions/NNS ,/, for/IN which/WDT we/PRP derive/VBP a/DT O/NN (/-LRB- ln/NN N/NN //HYPH N/NN )/-RRB- convergence/NN rate/NN ./.
We/PRP also/RB show/VBP that/IN our/PRP$ analysis/NN can/MD extend/VB to/IN adversarial/JJ online/JJ learning/NN setting/VBG under/IN a/DT stronger/JJR assumption/NN that/IN the/DT online/JJ weak/JJ learning/NN edge/NN will/MD hold/VB in/IN adversarial/JJ setting/NN ./.
We/PRP finally/RB demonstrate/VBP experimental/JJ results/NNS showing/VBG that/IN in/IN practice/NN our/PRP$ algorithms/NNS can/MD achieve/VB competitive/JJ results/NNS as/IN classic/JJ gradient/NN boosting/VBG while/IN using/VBG less/JJR computation/NN ./.
