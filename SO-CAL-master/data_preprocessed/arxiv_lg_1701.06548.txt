We/PRP systematically/RB explore/VB regularizing/VBG neural/JJ networks/NNS by/IN penalizing/VBG low/JJ entropy/NN output/NN distributions/NNS ./.
We/PRP show/VBP that/IN penalizing/VBG low/JJ entropy/NN output/NN distributions/NNS ,/, which/WDT has/VBZ been/VBN shown/VBN to/TO improve/VB exploration/NN in/IN reinforcement/NN learning/NN ,/, acts/VBZ as/IN a/DT strong/JJ regularizer/NN in/IN supervised/JJ learning/NN ./.
Furthermore/RB ,/, we/PRP connect/VBP a/DT maximum/JJ entropy/NN based/VBN confidence/NN penalty/NN to/IN label/NN smoothing/VBG through/IN the/DT direction/NN of/IN the/DT KL/NN divergence/NN ./.
We/PRP exhaustively/RB evaluate/VB the/DT proposed/VBN confidence/NN penalty/NN and/CC label/NN smoothing/VBG on/IN 6/CD common/JJ benchmarks/NNS :/: image/NN classification/NN (/-LRB- MNIST/NN and/CC Cifar/NN -/HYPH 10/CD )/-RRB- ,/, language/NN modeling/NN (/-LRB- Penn/NNP Treebank/NNP )/-RRB- ,/, machine/NN translation/NN (/-LRB- WMT/NN '14/CD English/NNP -/HYPH to/IN -/HYPH German/JJ )/-RRB- ,/, and/CC speech/NN recognition/NN (/-LRB- TIMIT/NN and/CC WSJ/NNP )/-RRB- ./.
We/PRP find/VBP that/IN both/DT label/NN smoothing/NN and/CC the/DT confidence/NN penalty/NN improve/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN models/NNS across/IN benchmarks/NNS without/IN modifying/VBG existing/VBG hyperparameters/NNS ,/, suggesting/VBG the/DT wide/JJ applicability/NN of/IN these/DT regularizers/NNS ./.
