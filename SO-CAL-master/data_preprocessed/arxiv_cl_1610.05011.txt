Conventional/JJ attention/NN -/HYPH based/VBN Neural/JJ Machine/NN Translation/NN (/-LRB- NMT/NN )/-RRB- conducts/VBZ dynamic/JJ alignment/NN in/IN generating/VBG the/DT target/NN sentence/NN ./.
By/IN repeatedly/RB reading/VBG the/DT representation/NN of/IN source/NN sentence/NN ,/, which/WDT keeps/VBZ fixed/VBN after/IN generated/VBN by/IN the/DT encoder/NN (/-LRB- Bahdanau/NNP et/FW al./FW ,/, 2015/CD )/-RRB- ,/, the/DT attention/NN mechanism/NN has/VBZ greatly/RB enhanced/VBN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN NMT/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ attention/NN mechanism/NN ,/, called/VBN INTERACTIVE/JJ ATTENTION/NN ,/, which/WDT models/NNS the/DT interaction/NN between/IN the/DT decoder/NN and/CC the/DT representation/NN of/IN source/NN sentence/NN during/IN translation/NN by/IN both/CC reading/VBG and/CC writing/VBG operations/NNS ./.
INTERACTIVE/JJ ATTENTION/NN can/MD keep/VB track/NN of/IN the/DT interaction/NN history/NN and/CC therefore/RB improve/VB the/DT translation/NN performance/NN ./.
Experiments/NNS on/IN NIST/NNP Chinese/NNP -/HYPH English/NNP translation/NN task/NN show/VBP that/IN INTERACTIVE/JJ ATTENTION/NN can/MD achieve/VB significant/JJ improvements/NNS over/IN both/CC the/DT previous/JJ attention/NN -/HYPH based/VBN NMT/NN baseline/NN and/CC some/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN variants/NNS of/IN attention/NN -/HYPH based/VBN NMT/NNP (/-LRB- i.e./FW ,/, coverage/NN models/NNS (/-LRB- Tu/NNP et/FW al./FW ,/, 2016/CD )/-RRB- )/-RRB- ./.
And/CC neural/JJ machine/NN translator/NN with/IN our/PRP$ INTERACTIVE/JJ ATTENTION/NN can/MD outperform/VB the/DT open/JJ source/NN attention/NN -/HYPH based/VBN NMT/NN system/NN Groundhog/NNP by/IN 4.22/CD BLEU/NN points/NNS and/CC the/DT open/JJ source/NN phrase/NN -/HYPH based/VBN system/NN Moses/NNP by/IN 3.94/CD BLEU/NN points/NNS averagely/RB on/IN multiple/JJ test/NN sets/NNS ./.
