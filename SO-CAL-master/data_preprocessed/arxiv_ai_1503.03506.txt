High/JJ computational/JJ costs/NNS of/IN manifold/JJ learning/NN prohibit/VBP its/PRP$ application/NN for/IN large/JJ point/NN sets/NNS ./.
A/DT common/JJ strategy/NN to/TO overcome/VB this/DT problem/NN is/VBZ to/TO perform/VB dimensionality/NN reduction/NN on/IN selected/VBN landmarks/NNS and/CC to/TO successively/RB embed/VB the/DT entire/JJ dataset/NN with/IN the/DT Nystr/NNP \/SYM "/`` om/NN method/NN ./.
The/DT two/CD main/JJ challenges/NNS that/WDT arise/VBP are/VBP :/: (/-LRB- i/LS )/-RRB- the/DT landmarks/NNS selected/VBN in/IN non-Euclidean/JJ geometries/NNS must/MD result/VB in/IN a/DT low/JJ reconstruction/NN error/NN ,/, (/-LRB- ii/LS )/-RRB- the/DT graph/NN constructed/VBN from/IN sparsely/RB sampled/VBN landmarks/NNS must/MD approximate/VB the/DT manifold/NN well/RB ./.
We/PRP propose/VBP the/DT sampling/NN of/IN landmarks/NNS from/IN determinantal/JJ distributions/NNS on/IN non-Euclidean/JJ spaces/NNS ./.
Since/IN current/JJ determinantal/JJ sampling/NN algorithms/NNS have/VBP the/DT same/JJ complexity/NN as/IN those/DT for/IN manifold/JJ learning/NN ,/, we/PRP present/VBP an/DT efficient/JJ approximation/NN running/VBG in/IN linear/JJ time/NN ./.
Further/RB ,/, we/PRP recover/VBP the/DT local/JJ geometry/NN after/IN the/DT sparsification/NN by/IN assigning/VBG each/DT landmark/NN a/DT local/JJ covariance/NN matrix/NN ,/, estimated/VBN from/IN the/DT original/JJ point/NN set/NN ./.
The/DT resulting/VBG neighborhood/NN selection/NN based/VBN on/IN the/DT Bhattacharyya/NNP distance/NN improves/VBZ the/DT embedding/NN of/IN sparsely/RB sampled/VBN manifolds/NNS ./.
Our/PRP$ experiments/NNS show/VBP a/DT significant/JJ performance/NN improvement/NN compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN landmark/NN selection/NN techniques/NNS ./.
