How/WRB to/TO develop/VB slim/JJ and/CC accurate/JJ deep/JJ neural/JJ networks/NNS has/VBZ become/VBN crucial/JJ for/IN real/JJ -/HYPH world/NN applications/NNS ,/, especially/RB for/IN those/DT employed/VBN in/IN embedded/VBN systems/NNS ./.
Though/IN previous/JJ work/NN along/IN this/DT research/NN line/NN has/VBZ shown/VBN some/DT promising/JJ results/NNS ,/, most/JJS existing/VBG methods/NNS either/CC fail/VBP to/TO significantly/RB compress/VB a/DT well/RB -/HYPH trained/VBN deep/JJ network/NN or/CC require/VB a/DT heavy/JJ retraining/VBG process/NN for/IN the/DT pruned/VBN deep/JJ network/NN to/TO re-boost/VB its/PRP$ prediction/NN performance/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ layer-wise/JJ pruning/NN method/NN for/IN deep/JJ neural/JJ networks/NNS ./.
In/IN our/PRP$ proposed/JJ method/NN ,/, parameters/NNS of/IN each/DT individual/JJ layer/NN are/VBP pruned/VBN independently/RB based/VBN on/IN second/JJ order/NN derivatives/NNS of/IN a/DT layer-wise/JJ error/NN function/NN with/IN respect/NN to/IN the/DT corresponding/VBG parameters/NNS ./.
We/PRP prove/VBP that/IN the/DT final/JJ prediction/NN performance/NN drop/NN after/IN pruning/NN is/VBZ bounded/VBN by/IN a/DT linear/JJ combination/NN of/IN the/DT reconstructed/JJ errors/NNS caused/VBN at/IN each/DT layer/NN ./.
Therefore/RB ,/, there/EX is/VBZ a/DT guarantee/NN that/IN one/CD only/RB needs/VBZ to/TO perform/VB a/DT light/NN retraining/VBG process/NN on/IN the/DT pruned/VBN network/NN to/TO resume/VB its/PRP$ original/JJ prediction/NN performance/NN ./.
We/PRP conduct/VBP extensive/JJ experiments/NNS on/IN benchmark/NN datasets/NNS to/TO demonstrate/VB the/DT effectiveness/NN of/IN our/PRP$ pruning/NN method/NN compared/VBN with/IN several/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baseline/NN methods/NNS ./.
