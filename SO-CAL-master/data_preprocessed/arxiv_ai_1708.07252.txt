An/DT exhaustive/JJ study/NN on/IN neural/JJ network/NN language/NN modeling/NN (/-LRB- NNLM/NN )/-RRB- is/VBZ performed/VBN in/IN this/DT paper/NN ./.
Different/JJ architectures/NNS of/IN basic/JJ neural/JJ network/NN language/NN models/NNS are/VBP described/VBN and/CC examined/VBN ./.
A/DT number/NN of/IN different/JJ improvements/NNS over/IN basic/JJ neural/JJ network/NN language/NN models/NNS ,/, including/VBG importance/NN sampling/NN ,/, word/NN classes/NNS ,/, caching/NN and/CC bidirectional/JJ recurrent/JJ neural/JJ network/NN (/-LRB- BiRNN/NN )/-RRB- ,/, are/VBP studied/VBN separately/RB ,/, and/CC the/DT advantages/NNS and/CC disadvantages/NNS of/IN every/DT technique/NN are/VBP evaluated/VBN ./.
Then/RB ,/, the/DT limits/NNS of/IN neural/JJ network/NN language/NN modeling/NN are/VBP explored/VBN from/IN the/DT aspects/NNS of/IN model/NN architecture/NN and/CC knowledge/NN representation/NN ./.
Part/NN of/IN the/DT statistical/JJ information/NN from/IN a/DT word/NN sequence/NN will/MD loss/NN when/WRB it/PRP is/VBZ processed/VBN word/NN by/IN word/NN in/IN a/DT certain/JJ order/NN ,/, and/CC the/DT mechanism/NN of/IN training/NN neural/JJ network/NN by/IN updating/VBG weight/NN matrixes/NNS and/CC vectors/NNS imposes/VBZ severe/JJ restrictions/NNS on/IN any/DT significant/JJ enhancement/NN of/IN NNLM/NNP ./.
For/IN knowledge/NN representation/NN ,/, the/DT knowledge/NN represented/VBN by/IN neural/JJ network/NN language/NN models/NNS is/VBZ the/DT approximate/JJ probabilistic/JJ distribution/NN of/IN word/NN sequences/NNS from/IN a/DT certain/JJ training/NN data/NNS set/VBN rather/RB than/IN the/DT knowledge/NN of/IN a/DT language/NN itself/PRP or/CC the/DT information/NN conveyed/VBN by/IN word/NN sequences/NNS in/IN a/DT natural/JJ language/NN ./.
Finally/RB ,/, some/DT directions/NNS for/IN improving/VBG neural/JJ network/NN language/NN modeling/VBG further/RB is/VBZ discussed/VBN ./.
