We/PRP study/VBP pool/NN -/HYPH based/VBN active/JJ learning/NN of/IN halfspaces/NNS ,/, in/IN which/WDT a/DT learner/NN receives/VBZ a/DT pool/NN of/IN unlabeled/JJ examples/NNS ,/, and/CC iteratively/RB queries/VBZ a/DT teacher/NN for/IN the/DT labels/NNS of/IN examples/NNS from/IN the/DT pool/NN ,/, in/IN order/NN to/TO identify/VB all/PDT the/DT labels/NNS of/IN pool/NN examples/NNS ./.
We/PRP revisit/VBP the/DT idea/NN of/IN greedily/RB selecting/VBG examples/NNS to/IN label/NN ,/, and/CC use/VB it/PRP to/TO derive/VB an/DT efficient/JJ algorithm/NN ,/, called/VBN ALuMA/NNP ,/, that/DT approximates/VBZ the/DT optimal/JJ label/NN complexity/NN for/IN a/DT given/VBN pool/NN in/IN $/$ \/CD reals/NNS ^/SYM d/NN $/$ ./.
We/PRP show/VBP that/IN ALuMA/NNP obtains/VBZ an/DT $/$ O/UH (/-LRB- d/NN ^/SYM 2/CD \/SYM log/NN (/-LRB- d/NN )/-RRB- )/-RRB- $/$ approximation/NN factor/NN if/IN the/DT examples/NNS in/IN the/DT pool/NN are/VBP numbers/NNS with/IN a/DT finite/JJ accuracy/NN ./.
We/PRP further/RB prove/VB a/DT result/NN for/IN general/JJ hypothesis/NN classes/NNS ,/, showing/VBG that/IN a/DT slight/JJ change/NN to/IN the/DT greedy/JJ approach/NN leads/VBZ to/IN an/DT improved/JJ target/NN -/HYPH dependent/JJ guarantee/NN on/IN the/DT label/NN complexity/NN ./.
In/IN particular/JJ ,/, we/PRP conclude/VBP a/DT better/JJR guarantee/NN for/IN ALuMA/NNP if/IN the/DT target/NN hypothesis/NN has/VBZ a/DT large/JJ margin/NN ./.
We/PRP further/RB compare/VBP our/PRP$ approach/NN to/IN other/JJ common/JJ active/JJ learning/NN strategies/NNS ,/, and/CC provide/VB a/DT theoretical/JJ and/CC empirical/JJ evaluation/NN of/IN the/DT advantages/NNS and/CC disadvantages/NNS of/IN the/DT approach/NN ./.
