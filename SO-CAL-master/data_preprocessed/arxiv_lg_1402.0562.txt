In/IN this/DT paper/NN we/PRP consider/VBP the/DT problem/NN of/IN online/JJ stochastic/JJ optimization/NN of/IN a/DT locally/RB smooth/JJ function/NN under/IN bandit/NN feedback/NN ./.
We/PRP introduce/VBP the/DT high/JJ confidence/NN tree/NN (/-LRB- HCT/NN )/-RRB- algorithm/NN ,/, a/DT novel/NN any/DT -/HYPH time/NN $/$ \/CD mathcal/JJ X$/NN -/HYPH armed/JJ bandit/NN algorithm/NN ,/, and/CC derive/VBP regret/NN bounds/NNS matching/VBG the/DT performance/NN of/IN existing/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN in/IN terms/NNS of/IN dependency/NN on/IN number/NN of/IN steps/NNS $/$ n/NN $/$ and/CC near/IN -/HYPH optimality/NN dimensions/NNS $/$ d/LS $/$ ./.
The/DT main/JJ advantage/NN of/IN HCT/NNP is/VBZ that/IN it/PRP handles/VBZ the/DT challenging/JJ case/NN of/IN correlated/VBN arms/NNS ,/, whereas/IN existing/VBG methods/NNS require/VBP that/IN rewards/NNS to/TO be/VB conditionally/RB independent/JJ of/IN each/DT others/NNS ./.
HCT/NNP also/RB improves/VBZ on/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN in/IN terms/NNS of/IN space/NN complexity/NN as/RB well/RB as/IN requiring/VBG a/DT weaker/JJR smoothness/NN assumption/NN on/IN the/DT mean/NN -/HYPH reward/NN function/NN in/IN compare/VB to/IN the/DT previous/JJ any/DT time/NN algorithms/NNS ./.
Finally/RB ,/, we/PRP discuss/VBP how/WRB HCT/NNP can/MD be/VB applied/VBN to/IN the/DT problem/NN of/IN policy/NN search/NN in/IN reinforcement/NN learning/NN and/CC we/PRP report/VBP preliminary/JJ empirical/JJ results/NNS ./.
