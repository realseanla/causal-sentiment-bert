A/DT long/JJ -/HYPH lived/JJ autonomous/JJ agent/NN should/MD be/VB able/JJ to/TO respond/VB online/RB to/IN novel/JJ instances/NNS of/IN tasks/NNS from/IN a/DT familiar/JJ domain/NN ./.
Acting/VBG online/RB requires/VBZ '/`` fast/JJ '/'' responses/NNS ,/, in/IN terms/NNS of/IN rapid/JJ convergence/NN ,/, especially/RB when/WRB the/DT task/NN instance/NN has/VBZ a/DT short/JJ duration/NN ,/, such/JJ as/IN in/IN applications/NNS involving/VBG interactions/NNS with/IN humans/NNS ./.
These/DT requirements/NNS can/MD be/VB problematic/JJ for/IN many/JJ established/VBN methods/NNS for/IN learning/VBG to/TO act/VB ./.
In/IN domains/NNS where/WRB the/DT agent/NN knows/VBZ that/IN the/DT task/NN instance/NN is/VBZ drawn/VBN from/IN a/DT family/NN of/IN related/JJ tasks/NNS ,/, albeit/IN without/IN access/NN to/IN the/DT label/NN of/IN any/DT given/VBN instance/NN ,/, it/PRP can/MD choose/VB to/TO act/VB through/IN a/DT process/NN of/IN policy/NN reuse/VBP from/IN a/DT library/NN ,/, rather/RB than/IN policy/NN learning/NN from/IN scratch/NN ./.
In/IN policy/NN reuse/VBP ,/, the/DT agent/NN has/VBZ prior/JJ knowledge/NN of/IN the/DT class/NN of/IN tasks/NNS in/IN the/DT form/NN of/IN a/DT library/NN of/IN policies/NNS that/WDT were/VBD learnt/VBN from/IN sample/NN task/NN instances/NNS during/IN an/DT offline/RB training/NN phase/NN ./.
We/PRP formalise/VBP the/DT problem/NN of/IN policy/NN reuse/VBP ,/, and/CC present/VB an/DT algorithm/NN for/IN efficiently/RB responding/VBG to/IN a/DT novel/JJ task/NN instance/NN by/IN reusing/VBG a/DT policy/NN from/IN the/DT library/NN of/IN existing/VBG policies/NNS ,/, where/WRB the/DT choice/NN is/VBZ based/VBN on/IN observed/VBN '/`` signals/NNS '/'' which/WDT correlate/VBP to/IN policy/NN performance/NN ./.
We/PRP achieve/VBP this/DT by/IN posing/VBG the/DT problem/NN as/IN a/DT Bayesian/JJ choice/NN problem/NN with/IN a/DT corresponding/VBG notion/NN of/IN an/DT optimal/JJ response/NN ,/, but/CC the/DT computation/NN of/IN that/DT response/NN is/VBZ in/IN many/JJ cases/NNS intractable/JJ ./.
Therefore/RB ,/, to/TO reduce/VB the/DT computation/NN cost/NN of/IN the/DT posterior/JJ ,/, we/PRP follow/VBP a/DT Bayesian/JJ optimisation/NN approach/NN and/CC define/VB a/DT set/NN of/IN policy/NN selection/NN functions/NNS ,/, which/WDT balance/NN exploration/NN in/IN the/DT policy/NN library/NN against/IN exploitation/NN of/IN previously/RB tried/VBN policies/NNS ,/, together/RB with/IN a/DT model/NN of/IN expected/VBN performance/NN of/IN the/DT policy/NN library/NN on/IN their/PRP$ corresponding/VBG task/NN instances/NNS ./.
We/PRP validate/VBP our/PRP$ method/NN in/IN several/JJ simulated/JJ domains/NNS of/IN interactive/JJ ,/, short/JJ -/HYPH duration/NN episodic/JJ tasks/NNS ,/, showing/VBG rapid/JJ convergence/NN in/IN unknown/JJ task/NN variations/NNS ./.
