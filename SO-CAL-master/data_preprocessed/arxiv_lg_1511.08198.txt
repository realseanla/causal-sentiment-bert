In/IN this/DT paper/NN ,/, we/PRP show/VBP how/WRB to/TO create/VB paraphrastic/JJ sentence/NN embeddings/NNS using/VBG the/DT Paraphrase/NN Database/NN (/-LRB- Ganitkevitch/NNP et/FW al./FW ,/, 2013/CD )/-RRB- ,/, an/DT extensive/JJ semantic/JJ resource/NN with/IN millions/NNS of/IN phrase/NN pairs/NNS ./.
We/PRP consider/VBP several/JJ compositional/JJ architectures/NNS and/CC evaluate/VB them/PRP on/IN 24/CD textual/JJ similarity/NN datasets/NNS encompassing/VBG domains/NNS such/JJ as/IN news/NN ,/, tweets/NNS ,/, web/NN forums/NNS ,/, news/NN headlines/NNS ,/, machine/NN translation/NN output/NN ,/, glosses/VBZ ,/, and/CC image/NN and/CC video/NN captions/NNS ./.
We/PRP present/VBP the/DT interesting/JJ result/NN that/WDT simple/JJ compositional/JJ architectures/NNS based/VBN on/IN updated/VBN vector/NN averaging/VBG vastly/RB outperform/VBP long/RB short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- recurrent/JJ neural/JJ networks/NNS and/CC that/IN these/DT simpler/JJR architectures/NNS allow/VBP us/PRP to/TO learn/VB models/NNS with/IN superior/JJ generalization/NN ./.
Our/PRP$ models/NNS are/VBP efficient/JJ ,/, very/RB easy/JJ to/TO use/VB ,/, and/CC competitive/JJ with/IN task/NN -/HYPH tuned/VBN systems/NNS ./.
We/PRP make/VBP them/PRP available/JJ to/IN the/DT research/NN community/NN with/IN the/DT hope/NN that/IN they/PRP can/MD serve/VB as/IN the/DT new/JJ baseline/NN for/IN further/JJ work/NN on/IN universal/JJ paraphrastic/JJ sentence/NN embeddings/NNS ./.
