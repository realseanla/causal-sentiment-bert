Recurrent/JJ neural/JJ network/NN models/NNS with/IN an/DT attention/NN mechanism/NN have/VBP proven/VBN to/TO be/VB extremely/RB effective/JJ on/IN a/DT wide/JJ variety/NN of/IN sequence/NN -/HYPH to/IN -/HYPH sequence/NN problems/NNS ./.
However/RB ,/, the/DT fact/NN that/IN soft/JJ attention/NN mechanisms/NNS perform/VBP a/DT pass/NN over/IN the/DT entire/JJ input/NN sequence/NN when/WRB producing/VBG each/DT element/NN in/IN the/DT output/NN sequence/NN precludes/VBZ their/PRP$ use/NN in/IN online/JJ settings/NNS and/CC results/NNS in/IN a/DT quadratic/JJ time/NN complexity/NN ./.
Based/VBN on/IN the/DT insight/NN that/IN the/DT alignment/NN between/IN input/NN and/CC output/NN sequence/NN elements/NNS is/VBZ monotonic/JJ in/IN many/JJ problems/NNS of/IN interest/NN ,/, we/PRP propose/VBP an/DT end/NN -/HYPH to/IN -/HYPH end/NN differentiable/JJ method/NN for/IN learning/VBG monotonic/JJ alignments/NNS which/WDT ,/, at/IN test/NN time/NN ,/, enables/VBZ computing/VBG attention/NN online/RB and/CC in/IN linear/JJ time/NN ./.
We/PRP validate/VBP our/PRP$ approach/NN on/IN sentence/NN summarization/NN ,/, machine/NN translation/NN ,/, and/CC online/JJ speech/NN recognition/NN problems/NNS and/CC achieve/VB results/NNS competitive/JJ with/IN existing/VBG sequence/NN -/HYPH to/IN -/HYPH sequence/NN models/NNS ./.
