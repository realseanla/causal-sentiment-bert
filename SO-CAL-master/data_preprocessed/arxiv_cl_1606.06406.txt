Recently/RB ,/, neural/JJ network/NN approaches/VBZ for/IN parsing/VBG have/VBP largely/RB automated/VBN the/DT combination/NN of/IN individual/JJ features/NNS ,/, but/CC still/RB rely/VBP on/IN (/-LRB- often/RB a/DT larger/JJR number/NN of/IN )/-RRB- atomic/JJ features/NNS created/VBN from/IN human/JJ linguistic/JJ intuition/NN ,/, and/CC potentially/RB omitting/VBG important/JJ global/JJ context/NN ./.
To/TO further/RB reduce/VB feature/NN engineering/NN to/IN the/DT bare/JJ minimum/NN ,/, we/PRP use/VBP bi-directional/JJ LSTM/NN sentence/NN representations/NNS to/TO model/VB a/DT parser/NN state/NN with/IN only/RB three/CD sentence/NN positions/NNS ,/, which/WDT automatically/RB identifies/VBZ important/JJ aspects/NNS of/IN the/DT entire/JJ sentence/NN ./.
This/DT model/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS among/IN greedy/JJ dependency/NN parsers/NNS for/IN English/NNP ./.
We/PRP also/RB introduce/VBP a/DT novel/JJ transition/NN system/NN for/IN constituency/NN parsing/VBG which/WDT does/VBZ not/RB require/VB binarization/NN ,/, and/CC together/RB with/IN the/DT above/JJ architecture/NN ,/, achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS among/IN greedy/JJ parsers/NNS for/IN both/DT English/NNP and/CC Chinese/NNP ./.
