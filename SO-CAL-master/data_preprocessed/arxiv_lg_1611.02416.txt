Nowadays/RB deep/JJ learning/NN is/VBZ dominating/VBG the/DT field/NN of/IN machine/NN learning/NN with/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN in/IN various/JJ application/NN areas/NNS ./.
Recently/RB ,/, spiking/VBG neural/JJ networks/NNS (/-LRB- SNNs/NNS )/-RRB- have/VBP been/VBN attracting/VBG a/DT great/JJ deal/NN of/IN attention/NN ,/, notably/RB owning/VBG to/IN their/PRP$ power/NN efficiency/NN ,/, which/WDT can/MD potentially/RB allow/VB us/PRP to/TO implement/VB a/DT low/JJ -/HYPH power/NN deep/JJ learning/NN engine/NN suitable/JJ for/IN real/JJ -/HYPH time/NN //HYPH mobile/NN applications/NNS ./.
However/RB ,/, implementing/VBG SNN/NNP -/HYPH based/VBN deep/JJ learning/NN remains/VBZ challenging/JJ ,/, especially/RB gradient/NN -/HYPH based/VBN training/NN of/IN SNNs/NNS by/IN error/NN backpropagation/NN ./.
We/PRP can/MD not/RB simply/RB propagate/VB errors/NNS through/IN SNNs/NNS in/IN conventional/JJ way/NN because/IN of/IN the/DT property/NN of/IN SNNs/NNS that/WDT process/VBP discrete/JJ data/NNS in/IN the/DT form/NN of/IN a/DT series/NN ./.
Consequently/RB ,/, most/JJS of/IN the/DT previous/JJ studies/NNS employ/VBP a/DT workaround/NN technique/NN ,/, which/WDT first/RB trains/VBZ a/DT conventional/JJ weighted/JJ -/HYPH sum/NN deep/JJ neural/JJ network/NN and/CC then/RB maps/VBZ the/DT learning/NN weights/NNS to/IN the/DT SNN/NNP under/IN training/NN ,/, instead/RB of/IN training/NN SNN/NN parameters/NNS directly/RB ./.
In/IN order/NN to/TO eliminate/VB this/DT workaround/NN ,/, recently/RB proposed/VBN is/VBZ a/DT new/JJ class/NN of/IN SNN/NNP named/VBN deep/RB spiking/VBG networks/NNS (/-LRB- DSNs/NNS )/-RRB- ,/, which/WDT can/MD be/VB trained/VBN directly/RB (/-LRB- without/IN a/DT mapping/NN from/IN conventional/JJ deep/JJ networks/NNS )/-RRB- by/IN error/NN backpropagation/NN with/IN stochastic/JJ gradient/NN descent/NN ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN the/DT initialization/NN of/IN the/DT membrane/NN potential/NN on/IN the/DT backward/JJ path/NN is/VBZ an/DT important/JJ step/NN in/IN DSN/NN training/NN ,/, through/IN diverse/JJ experiments/NNS performed/VBN under/IN various/JJ conditions/NNS ./.
Furthermore/RB ,/, we/PRP propose/VBP a/DT simple/JJ and/CC efficient/JJ method/NN that/WDT can/MD improve/VB DSN/NN training/NN by/IN controlling/VBG the/DT initial/JJ membrane/NN potential/NN on/IN the/DT backward/JJ path/NN ./.
In/IN our/PRP$ experiments/NNS ,/, adopting/VBG the/DT proposed/VBN approach/NN allowed/VBD us/PRP to/TO boost/VB the/DT performance/NN of/IN DSN/NN training/NN in/IN terms/NNS of/IN converging/VBG time/NN and/CC accuracy/NN ./.
