We/PRP propose/VBP and/CC study/VBP a/DT new/JJ tractable/JJ model/NN for/IN reinforcement/NN learning/VBG with/IN high/JJ -/HYPH dimensional/JJ observation/NN called/VBN Contextual/NNP -/HYPH MDPs/NNP ,/, generalizing/VBG contextual/JJ bandits/NNS to/IN a/DT sequential/JJ decision/NN making/VBG setting/NN ./.
These/DT models/NNS require/VBP an/DT agent/NN to/TO take/VB actions/NNS based/VBN on/IN high/JJ -/HYPH dimensional/JJ observations/NNS (/-LRB- features/NNS )/-RRB- with/IN the/DT goal/NN of/IN achieving/VBG long/JJ -/HYPH term/NN performance/NN competitive/JJ with/IN a/DT large/JJ set/NN of/IN policies/NNS ./.
Since/IN the/DT size/NN of/IN the/DT observation/NN space/NN is/VBZ a/DT primary/JJ obstacle/NN to/IN sample/NN -/HYPH efficient/JJ learning/NN ,/, Contextual/JJ -/HYPH MDPs/NNS are/VBP assumed/VBN to/TO be/VB summarizable/JJ by/IN a/DT small/JJ number/NN of/IN hidden/JJ states/NNS ./.
In/IN this/DT setting/NN ,/, we/PRP design/VBP a/DT new/JJ reinforcement/NN learning/VBG algorithm/NN that/WDT engages/VBZ in/IN global/JJ exploration/NN while/IN using/VBG a/DT function/NN class/NN to/TO approximate/VB future/JJ performance/NN ./.
We/PRP also/RB establish/VBP a/DT sample/NN complexity/NN guarantee/NN for/IN this/DT algorithm/NN ,/, proving/VBG that/IN it/PRP learns/VBZ near/IN optimal/JJ behavior/NN after/IN a/DT number/NN of/IN episodes/NNS that/WDT is/VBZ polynomial/JJ in/IN all/DT relevant/JJ parameters/NNS ,/, logarithmic/JJ in/IN the/DT number/NN of/IN policies/NNS ,/, and/CC independent/JJ of/IN the/DT size/NN of/IN the/DT observation/NN space/NN ./.
This/DT represents/VBZ an/DT exponential/JJ improvement/NN on/IN the/DT sample/NN complexity/NN of/IN all/DT existing/VBG alternative/JJ approaches/NNS and/CC provides/VBZ theoretical/JJ justification/NN for/IN reinforcement/NN learning/VBG with/IN function/NN approximation/NN ./.
