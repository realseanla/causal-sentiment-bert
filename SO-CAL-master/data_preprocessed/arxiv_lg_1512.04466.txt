In/IN this/DT paper/NN ,/, we/PRP investigate/VBP the/DT usage/NN of/IN autoencoders/NNS in/IN modeling/VBG textual/JJ data/NNS ./.
Traditional/JJ autoencoders/NNS suffer/VBP from/IN at/IN least/RBS two/CD aspects/NNS :/: scalability/NN with/IN the/DT high/JJ dimensionality/NN of/IN vocabulary/NN size/NN and/CC dealing/VBG with/IN task/NN -/HYPH irrelevant/JJ words/NNS ./.
We/PRP address/VBP this/DT problem/NN by/IN introducing/VBG supervision/NN via/IN the/DT loss/NN function/NN of/IN autoencoders/NNS ./.
In/IN particular/JJ ,/, we/PRP first/RB train/VB a/DT linear/JJ classifier/NN on/IN the/DT labeled/VBN data/NNS ,/, then/RB define/VB a/DT loss/NN for/IN the/DT autoencoder/NN with/IN the/DT weights/NNS learned/VBD from/IN the/DT linear/JJ classifier/NN ./.
To/TO reduce/VB the/DT bias/NN brought/VBN by/IN one/CD single/JJ classifier/NN ,/, we/PRP define/VBP a/DT posterior/JJ probability/NN distribution/NN on/IN the/DT weights/NNS of/IN the/DT classifier/NN ,/, and/CC derive/VBP the/DT marginalized/VBN loss/NN of/IN the/DT autoencoder/NN with/IN Laplace/NNP approximation/NN ./.
We/PRP show/VBP that/IN our/PRP$ choice/NN of/IN loss/NN function/NN can/MD be/VB rationalized/VBN from/IN the/DT perspective/NN of/IN Bregman/NNP Divergence/NNP ,/, which/WDT justifies/VBZ the/DT soundness/NN of/IN our/PRP$ model/NN ./.
We/PRP evaluate/VBP the/DT effectiveness/NN of/IN our/PRP$ model/NN on/IN six/CD sentiment/NN analysis/NN datasets/NNS ,/, and/CC show/VBP that/IN our/PRP$ model/NN significantly/RB outperforms/VBZ all/PDT the/DT competing/VBG methods/NNS with/IN respect/NN to/IN classification/NN accuracy/NN ./.
We/PRP also/RB show/VBP that/IN our/PRP$ model/NN is/VBZ able/JJ to/TO take/VB advantage/NN of/IN unlabeled/JJ dataset/NN and/CC get/VB improved/VBN performance/NN ./.
We/PRP further/RB show/VBP that/IN our/PRP$ model/NN successfully/RB learns/VBZ highly/RB discriminative/JJ feature/NN maps/NNS ,/, which/WDT explains/VBZ its/PRP$ superior/JJ performance/NN ./.
