Recurrent/JJ Neural/JJ Networks/NNS are/VBP showing/VBG much/JJ promise/NN in/IN many/JJ sub-areas/NNS of/IN natural/JJ language/NN processing/NN ,/, ranging/VBG from/IN document/NN classification/NN to/IN machine/NN translation/NN to/IN automatic/JJ question/NN answering/VBG ./.
Despite/IN their/PRP$ promise/NN ,/, many/JJ recurrent/JJ models/NNS have/VBP to/TO read/VB the/DT whole/JJ text/NN word/NN by/IN word/NN ,/, making/VBG it/PRP slow/JJ to/TO handle/VB long/JJ documents/NNS ./.
For/IN example/NN ,/, it/PRP is/VBZ difficult/JJ to/TO use/VB a/DT recurrent/JJ network/NN to/TO read/VB a/DT book/NN and/CC answer/NN questions/NNS about/IN it/PRP ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP an/DT approach/NN of/IN reading/VBG text/NN while/IN skipping/VBG irrelevant/JJ information/NN if/IN needed/VBN ./.
The/DT underlying/JJ model/NN is/VBZ a/DT recurrent/JJ network/NN that/WDT learns/VBZ how/WRB far/RB to/TO jump/VB after/IN reading/VBG a/DT few/JJ words/NNS of/IN the/DT input/NN text/NN ./.
We/PRP employ/VBP a/DT standard/JJ policy/NN gradient/NN method/NN to/TO train/VB the/DT model/NN to/TO make/VB discrete/JJ jumping/NN decisions/NNS ./.
In/IN our/PRP$ benchmarks/NNS on/IN four/CD different/JJ tasks/NNS ,/, including/VBG number/NN prediction/NN ,/, sentiment/NN analysis/NN ,/, news/NN article/NN classification/NN and/CC automatic/JJ Q/NN \/NN &amp;/CC A/NN ,/, our/PRP$ proposed/VBN model/NN ,/, a/DT modified/VBN LSTM/NN with/IN jumping/NN ,/, is/VBZ up/RB to/IN 6/CD times/NNS faster/JJR than/IN the/DT standard/JJ sequential/JJ LSTM/NN ,/, while/IN maintaining/VBG the/DT same/JJ or/CC even/RB better/JJR accuracy/NN ./.
