Type/NN -/HYPH level/NN word/NN embeddings/NNS use/VBP the/DT same/JJ set/NN of/IN parameters/NNS to/TO represent/VB all/DT instances/NNS of/IN a/DT word/NN regardless/RB of/IN its/PRP$ context/NN ,/, ignoring/VBG the/DT inherent/JJ lexical/JJ ambiguity/NN in/IN language/NN ./.
Instead/RB ,/, we/PRP embed/VBP semantic/JJ concepts/NNS (/-LRB- or/CC synsets/NNS )/-RRB- as/IN defined/VBN in/IN WordNet/NNP and/CC represent/VB a/DT word/NN token/NN in/IN a/DT particular/JJ context/NN by/IN estimating/VBG a/DT distribution/NN over/IN relevant/JJ semantic/JJ concepts/NNS ./.
We/PRP use/VBP the/DT new/JJ ,/, context/NN -/HYPH sensitive/JJ embeddings/NNS in/IN a/DT model/NN for/IN predicting/VBG prepositional/JJ phrase/NN (/-LRB- PP/NN )/-RRB- attachments/NNS and/CC jointly/RB learn/VB the/DT concept/NN embeddings/NNS and/CC model/NN parameters/NNS ./.
We/PRP show/VBP that/IN using/VBG context/NN -/HYPH sensitive/JJ embeddings/NNS improves/VBZ the/DT accuracy/NN of/IN the/DT PP/NN attachment/NN model/NN by/IN 5.4/CD percent/NN absolute/JJ points/NNS ,/, which/WDT amounts/VBZ to/IN a/DT 34.4/CD percent/NN relative/JJ reduction/NN in/IN errors/NNS ./.
