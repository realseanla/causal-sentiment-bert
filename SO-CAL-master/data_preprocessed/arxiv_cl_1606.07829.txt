We/PRP present/VBP a/DT token/NN -/HYPH level/NN decision/NN summarization/NN framework/NN that/WDT utilizes/VBZ the/DT latent/NN topic/NN structures/NNS of/IN utterances/NNS to/TO identify/VB "/`` summary/NN -/HYPH worthy/JJ "/`` words/NNS ./.
Concretely/RB ,/, a/DT series/NN of/IN unsupervised/JJ topic/NN models/NNS is/VBZ explored/VBN and/CC experimental/JJ results/NNS show/VBP that/IN fine/JJ -/HYPH grained/JJ topic/NN models/NNS ,/, which/WDT discover/VBP topics/NNS at/IN the/DT utterance/NN -/HYPH level/NN rather/RB than/IN the/DT document/NN -/HYPH level/NN ,/, can/MD better/RBR identify/VB the/DT gist/NN of/IN the/DT decision/NN -/HYPH making/VBG process/NN ./.
Moreover/RB ,/, our/PRP$ proposed/VBN token/NN -/HYPH level/NN summarization/NN approach/NN ,/, which/WDT is/VBZ able/JJ to/TO remove/VB redundancies/NNS within/IN utterances/NNS ,/, outperforms/VBZ existing/VBG utterance/NN ranking/VBG based/VBN summarization/NN methods/NNS ./.
Finally/RB ,/, context/NN information/NN is/VBZ also/RB investigated/VBN to/TO add/VB additional/JJ relevant/JJ information/NN to/IN the/DT summary/NN ./.
