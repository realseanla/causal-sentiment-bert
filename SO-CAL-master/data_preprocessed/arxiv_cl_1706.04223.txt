Generative/JJ adversarial/JJ networks/NNS are/VBP an/DT effective/JJ approach/NN for/IN learning/VBG rich/JJ latent/JJ representations/NNS of/IN continuous/JJ data/NNS ,/, but/CC have/VBP proven/VBN difficult/JJ to/TO apply/VB directly/RB to/IN discrete/JJ structured/JJ data/NNS ,/, such/JJ as/IN text/NN sequences/NNS or/CC discretized/JJ images/NNS ./.
Ideally/RB we/PRP could/MD encode/VB discrete/JJ structures/NNS in/IN a/DT continuous/JJ code/NN space/NN to/TO avoid/VB this/DT problem/NN ,/, but/CC it/PRP is/VBZ difficult/JJ to/TO learn/VB an/DT appropriate/JJ general/JJ -/HYPH purpose/NN encoder/NN ./.
In/IN this/DT work/NN ,/, we/PRP consider/VBP a/DT simple/JJ approach/NN for/IN handling/VBG these/DT two/CD challenges/NNS jointly/RB ,/, employing/VBG a/DT discrete/JJ structure/NN autoencoder/NN with/IN a/DT code/NN space/NN regularized/VBN by/IN generative/JJ adversarial/JJ training/NN ./.
The/DT model/NN learns/VBZ a/DT smooth/JJ regularized/VBN code/NN space/NN while/IN still/RB being/VBG able/JJ to/TO model/VB the/DT underlying/VBG data/NNS ,/, and/CC can/MD be/VB used/VBN as/IN a/DT discrete/JJ GAN/NNP with/IN the/DT ability/NN to/TO generate/VB coherent/JJ discrete/JJ outputs/NNS from/IN continuous/JJ samples/NNS ./.
We/PRP demonstrate/VBP empirically/RB how/WRB key/JJ properties/NNS of/IN the/DT data/NNS are/VBP captured/VBN in/IN the/DT model/NN 's/POS latent/JJ space/NN ,/, and/CC evaluate/VB the/DT model/NN itself/PRP on/IN the/DT tasks/NNS of/IN discrete/JJ image/NN generation/NN ,/, text/NN generation/NN ,/, and/CC semi-supervised/VBN learning/NN ./.
