We/PRP introduce/VBP a/DT generalization/NN of/IN temporal/JJ -/HYPH difference/NN (/-LRB- TD/NN )/-RRB- learning/NN to/IN networks/NNS of/IN interrelated/JJ predictions/NNS ./.
Rather/RB than/IN relating/VBG a/DT single/JJ prediction/NN to/IN itself/PRP at/IN a/DT later/JJ time/NN ,/, as/IN in/IN conventional/JJ TD/NN methods/NNS ,/, a/DT TD/NN network/NN relates/VBZ each/DT prediction/NN in/IN a/DT set/NN of/IN predictions/NNS to/IN other/JJ predictions/NNS in/IN the/DT set/NN at/IN a/DT later/JJ time/NN ./.
TD/NNP networks/NNS can/MD represent/VB and/CC apply/VB TD/NN learning/NN to/IN a/DT much/JJ wider/JJR class/NN of/IN predictions/NNS than/IN has/VBZ previously/RB been/VBN possible/JJ ./.
Using/VBG a/DT random/JJ -/HYPH walk/NN example/NN ,/, we/PRP show/VBP that/IN these/DT networks/NNS can/MD be/VB used/VBN to/TO learn/VB to/TO predict/VB by/IN a/DT fixed/VBN interval/NN ,/, which/WDT is/VBZ not/RB possible/JJ with/IN conventional/JJ TD/NN methods/NNS ./.
Secondly/RB ,/, we/PRP show/VBP that/IN if/IN the/DT inter-predictive/JJ relationships/NNS are/VBP made/VBN conditional/JJ on/IN action/NN ,/, then/RB the/DT usual/JJ learning/NN -/HYPH efficiency/NN advantage/NN of/IN TD/NN methods/NNS over/IN Monte/NNP Carlo/NNP (/-LRB- supervised/JJ learning/NN )/-RRB- methods/NNS becomes/VBZ particularly/RB pronounced/VBN ./.
Thirdly/RB ,/, we/PRP demonstrate/VBP that/IN TD/NNP networks/NNS can/MD learn/VB predictive/JJ state/NN representations/NNS that/WDT enable/VBP exact/JJ solution/NN of/IN a/DT non-Markov/JJ problem/NN ./.
A/DT very/RB broad/JJ range/NN of/IN inter-predictive/JJ temporal/JJ relationships/NNS can/MD be/VB expressed/VBN in/IN these/DT networks/NNS ./.
Overall/RB we/PRP argue/VBP that/IN TD/NNP networks/NNS represent/VBP a/DT substantial/JJ extension/NN of/IN the/DT abilities/NNS of/IN TD/NN methods/NNS and/CC bring/VB us/PRP closer/JJR to/IN the/DT goal/NN of/IN representing/VBG world/NN knowledge/NN in/IN entirely/RB predictive/JJ ,/, grounded/VBN terms/NNS ./.
