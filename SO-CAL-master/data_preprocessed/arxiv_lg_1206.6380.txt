In/IN this/DT paper/NN we/PRP address/VBP the/DT following/VBG question/NN :/: Can/MD we/PRP approximately/RB sample/NN from/IN a/DT Bayesian/JJ posterior/JJ distribution/NN if/IN we/PRP are/VBP only/RB allowed/VBN to/TO touch/VB a/DT small/JJ mini-batch/NN of/IN data/NN -/HYPH items/NNS for/IN every/DT sample/NN we/PRP generate/VBP ?/.
./.
An/DT algorithm/NN based/VBN on/IN the/DT Langevin/NNP equation/NN with/IN stochastic/JJ gradients/NNS (/-LRB- SGLD/NN )/-RRB- was/VBD previously/RB proposed/VBN to/TO solve/VB this/DT ,/, but/CC its/PRP$ mixing/VBG rate/NN was/VBD slow/JJ ./.
By/IN leveraging/VBG the/DT Bayesian/JJ Central/NNP Limit/NNP Theorem/NNP ,/, we/PRP extend/VBP the/DT SGLD/NN algorithm/NN so/IN that/IN at/IN high/JJ mixing/NN rates/NNS it/PRP will/MD sample/VB from/IN a/DT normal/JJ approximation/NN of/IN the/DT posterior/JJ ,/, while/IN for/IN slow/JJ mixing/NN rates/NNS it/PRP will/MD mimic/VB the/DT behavior/NN of/IN SGLD/NN with/IN a/DT pre-conditioner/NN matrix/NN ./.
As/IN a/DT bonus/NN ,/, the/DT proposed/VBN algorithm/NN is/VBZ reminiscent/JJ of/IN Fisher/NNP scoring/VBG (/-LRB- with/IN stochastic/JJ gradients/NNS )/-RRB- and/CC as/IN such/PDT an/DT efficient/JJ optimizer/NN during/IN burn/VB -/HYPH in/RP ./.
