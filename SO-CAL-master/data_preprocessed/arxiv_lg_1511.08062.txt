We/PRP propose/VBP a/DT new/JJ majorization/NN -/HYPH minimization/NN (/-LRB- MM/NNP )/-RRB- method/NN for/IN non-smooth/JJ and/CC non-convex/JJ programs/NNS ,/, which/WDT is/VBZ general/JJ enough/RB to/TO include/VB the/DT existing/VBG MM/NNP methods/NNS ./.
Besides/IN the/DT local/JJ majorization/NN condition/NN ,/, we/PRP only/RB require/VBP that/IN the/DT difference/NN between/IN the/DT directional/JJ derivatives/NNS of/IN the/DT objective/JJ function/NN and/CC its/PRP$ surrogate/JJ function/NN vanishes/VBZ when/WRB the/DT number/NN of/IN iterations/NNS approaches/VBZ infinity/NN ,/, which/WDT is/VBZ a/DT very/RB weak/JJ condition/NN ./.
So/RB our/PRP$ method/NN can/MD use/VB a/DT surrogate/JJ function/NN that/WDT directly/RB approximates/VBZ the/DT non-smooth/JJ objective/NN function/NN ./.
In/IN comparison/NN ,/, all/PDT the/DT existing/VBG MM/NNP methods/NNS construct/VB the/DT surrogate/JJ function/NN by/IN approximating/VBG the/DT smooth/JJ component/NN of/IN the/DT objective/JJ function/NN ./.
We/PRP apply/VBP our/PRP$ relaxed/JJ MM/NNP methods/NNS to/IN the/DT robust/JJ matrix/NN factorization/NN (/-LRB- RMF/NN )/-RRB- problem/NN with/IN different/JJ regularizations/NNS ,/, where/WRB our/PRP$ locally/RB majorant/JJ algorithm/NN shows/VBZ advantages/NNS over/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN approaches/NNS for/IN RMF/NNP ./.
This/DT is/VBZ the/DT first/JJ algorithm/NN for/IN RMF/NNP ensuring/VBG ,/, without/IN extra/JJ assumptions/NNS ,/, that/IN any/DT limit/NN point/NN of/IN the/DT iterates/NNS is/VBZ a/DT stationary/JJ point/NN ./.
