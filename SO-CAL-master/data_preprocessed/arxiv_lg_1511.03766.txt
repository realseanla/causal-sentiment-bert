In/IN this/DT paper/NN ,/, we/PRP develop/VBP a/DT randomized/JJ algorithm/NN and/CC theory/NN for/IN learning/VBG a/DT sparse/JJ model/NN from/IN large/JJ -/HYPH scale/NN and/CC high/JJ -/HYPH dimensional/JJ data/NNS ,/, which/WDT is/VBZ usually/RB formulated/VBN as/IN an/DT empirical/JJ risk/NN minimization/NN problem/NN with/IN a/DT sparsity/NN -/HYPH inducing/VBG regularizer/NN ./.
Under/IN the/DT assumption/NN that/IN there/EX exists/VBZ a/DT (/-LRB- approximately/RB )/-RRB- sparse/JJ solution/NN with/IN high/JJ classification/NN accuracy/NN ,/, we/PRP argue/VBP that/IN the/DT dual/JJ solution/NN is/VBZ also/RB sparse/JJ or/CC approximately/RB sparse/JJ ./.
The/DT fact/NN that/IN both/DT primal/JJ and/CC dual/JJ solutions/NNS are/VBP sparse/JJ motivates/VBZ us/PRP to/TO develop/VB a/DT randomized/JJ approach/NN for/IN a/DT general/JJ convex/NN -/HYPH concave/NN optimization/NN problem/NN ./.
Specifically/RB ,/, the/DT proposed/VBN approach/NN combines/VBZ the/DT strength/NN of/IN random/JJ projection/NN with/IN that/DT of/IN sparse/JJ learning/NN :/: it/PRP utilizes/VBZ random/JJ projection/NN to/TO reduce/VB the/DT dimensionality/NN ,/, and/CC introduces/VBZ $/$ \/CD ell_1/CD $/$ -/HYPH norm/NN regularization/NN to/TO alleviate/VB the/DT approximation/NN error/NN caused/VBN by/IN random/JJ projection/NN ./.
Theoretical/JJ analysis/NN shows/VBZ that/IN under/IN favored/VBN conditions/NNS ,/, the/DT randomized/JJ algorithm/NN can/MD accurately/RB recover/VB the/DT optimal/JJ solutions/NNS to/IN the/DT convex/NN -/HYPH concave/NN optimization/NN problem/NN (/-LRB- i.e./FW ,/, recover/VBP both/CC the/DT primal/JJ and/CC dual/JJ solutions/NNS )/-RRB- ./.
Furthermore/RB ,/, the/DT solutions/NNS returned/VBN by/IN our/PRP$ algorithm/NN are/VBP guaranteed/VBN to/TO be/VB approximately/RB sparse/JJ ./.
