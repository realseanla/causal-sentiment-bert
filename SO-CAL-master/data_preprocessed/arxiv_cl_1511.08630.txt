Neural/JJ network/NN models/NNS have/VBP been/VBN demon/NN -/HYPH strated/VBN to/TO be/VB capable/JJ of/IN achieving/VBG remarkable/JJ performance/NN in/IN sentence/NN and/CC document/NN mod/JJ -/HYPH eling/NN ./.
Convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- and/CC recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- are/VBP two/CD mainstream/JJ architectures/NNS for/IN such/JJ modeling/NN tasks/NNS ,/, which/WDT adopt/VBP totally/RB different/JJ ways/NNS of/IN understanding/NN natural/JJ languages/NNS ./.
In/IN this/DT work/NN ,/, we/PRP combine/VBP the/DT strengths/NNS of/IN both/DT architectures/NNS and/CC propose/VB a/DT novel/NN and/CC unified/VBN model/NN called/VBN C/NN -/HYPH LSTM/NN for/IN sentence/NN representation/NN and/CC text/NN classification/NN ./.
C/NN -/HYPH LSTM/NN utilizes/VBZ CNN/NNP to/IN ex/NN -/HYPH tract/NN a/DT sequence/NN of/IN higher/JJR -/HYPH level/NN phrase/NN repre/NN -/HYPH sentations/NNS ,/, and/CC are/VBP fed/VBN into/IN a/DT long/JJ short/JJ -/HYPH term/NN memory/NN recurrent/JJ neural/JJ network/NN (/-LRB- LSTM/NN )/-RRB- to/TO obtain/VB the/DT sentence/NN representation/NN ./.
C/NN -/HYPH LSTM/NN is/VBZ able/JJ to/TO capture/VB both/DT local/JJ features/NNS of/IN phrases/NNS as/RB well/RB as/IN global/JJ and/CC temporal/JJ sentence/NN se/FW -/HYPH mantics/FW ./.
We/PRP evaluate/VBP the/DT proposed/VBN archi/NN -/HYPH tecture/NN on/IN sentiment/NN classification/NN and/CC ques/NN -/HYPH tion/NN classification/NN tasks/NNS ./.
The/DT experimental/JJ re/IN -/HYPH sults/NNS show/VBP that/IN the/DT C/NN -/HYPH LSTM/NN outperforms/VBZ both/CC CNN/NNP and/CC LSTM/NNP and/CC can/MD achieve/VB excellent/JJ performance/NN on/IN these/DT tasks/NNS ./.
