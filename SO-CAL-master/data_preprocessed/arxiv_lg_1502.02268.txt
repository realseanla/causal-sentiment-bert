We/PRP propose/VBP a/DT new/JJ algorithm/NN for/IN minimizing/VBG regularized/VBN empirical/JJ loss/NN :/: Stochastic/JJ Dual/JJ Newton/NNP Ascent/NN (/-LRB- SDNA/NN )/-RRB- ./.
Our/PRP$ method/NN is/VBZ dual/JJ in/IN nature/NN :/: in/IN each/DT iteration/NN we/PRP update/VBP a/DT random/JJ subset/NN of/IN the/DT dual/JJ variables/NNS ./.
However/RB ,/, unlike/IN existing/VBG methods/NNS such/JJ as/IN stochastic/JJ dual/JJ coordinate/NN ascent/NN ,/, SDNA/NNP is/VBZ capable/JJ of/IN utilizing/VBG all/DT curvature/NN information/NN contained/VBN in/IN the/DT examples/NNS ,/, which/WDT leads/VBZ to/IN striking/JJ improvements/NNS in/IN both/DT theory/NN and/CC practice/NN -/HYPH sometimes/RB by/IN orders/NNS of/IN magnitude/NN ./.
In/IN the/DT special/JJ case/NN when/WRB an/DT L2/NN -/HYPH regularizer/NN is/VBZ used/VBN in/IN the/DT primal/JJ ,/, the/DT dual/JJ problem/NN is/VBZ a/DT concave/JJ quadratic/JJ maximization/NN problem/NN plus/CC a/DT separable/JJ term/NN ./.
In/IN this/DT regime/NN ,/, SDNA/NNP in/IN each/DT step/NN solves/VBZ a/DT proximal/JJ subproblem/NN involving/VBG a/DT random/JJ principal/NN submatrix/NN of/IN the/DT Hessian/NNP of/IN the/DT quadratic/JJ function/NN ;/: whence/IN the/DT name/NN of/IN the/DT method/NN ./.
If/IN ,/, in/IN addition/NN ,/, the/DT loss/NN functions/NNS are/VBP quadratic/JJ ,/, our/PRP$ method/NN can/MD be/VB interpreted/VBN as/IN a/DT novel/JJ variant/NN of/IN the/DT recently/RB introduced/VBN Iterative/JJ Hessian/JJ Sketch/VB ./.
