The/DT attention/NN mechanisim/NN is/VBZ appealing/VBG for/IN neural/JJ machine/NN translation/NN ,/, since/IN it/PRP is/VBZ able/JJ to/TO dynam/VB -/: ically/RB encode/VBP a/DT source/NN sentence/NN by/IN generating/VBG a/DT alignment/NN between/IN a/DT target/NN word/NN and/CC source/NN words/NNS ./.
Unfortunately/RB ,/, it/PRP has/VBZ been/VBN proved/VBN to/TO be/VB worse/JJR than/IN conventional/JJ alignment/NN models/NNS in/IN aligment/NN accuracy/NN ./.
In/IN this/DT paper/NN ,/, we/PRP analyze/VBP and/CC explain/VBP this/DT issue/NN from/IN the/DT point/NN view/NN of/IN re/IN -/HYPH ordering/NN ,/, and/CC propose/VB a/DT supervised/JJ attention/NN which/WDT is/VBZ learned/VBN with/IN guidance/NN from/IN conventional/JJ alignment/NN models/NNS ./.
Experiments/NNS on/IN two/CD Chinese/JJ -/HYPH to/IN -/HYPH English/NNP translation/NN tasks/NNS show/VBP that/IN the/DT super/JJ -/HYPH vised/VBN attention/NN mechanism/NN yields/VBZ better/JJR alignments/NNS leading/VBG to/IN substantial/JJ gains/NNS over/IN the/DT standard/JJ attention/NN based/VBN NMT/NNP ./.
