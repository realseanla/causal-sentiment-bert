We/PRP envision/VBP a/DT machine/NN learning/NN service/NN provider/NN facing/VBG a/DT continuous/JJ stream/NN of/IN problems/NNS with/IN the/DT same/JJ input/NN domain/NN ,/, but/CC with/IN output/NN domains/NNS that/WDT may/MD differ/VB ./.
Clients/NNS present/VBP the/DT provider/NN with/IN problems/NNS implicitly/RB ,/, by/IN labeling/VBG a/DT few/JJ example/NN inputs/NNS ,/, and/CC then/RB ask/VB the/DT provider/NN to/TO train/VB models/NNS which/WDT reasonably/RB extend/VBP their/PRP$ labelings/NNS to/IN novel/JJ inputs/NNS ./.
The/DT provider/NN wants/VBZ to/TO avoid/VB constraining/VBG its/PRP$ users/NNS to/IN a/DT set/NN of/IN common/JJ labels/NNS ,/, so/IN it/PRP does/VBZ not/RB assume/VB any/DT particular/JJ correspondence/NN between/IN labels/NNS for/IN a/DT new/JJ task/NN and/CC labels/NNS for/IN previously/RB encountered/VBN tasks/NNS ./.
To/TO perform/VB well/RB in/IN this/DT setting/NN ,/, the/DT provider/NN needs/VBZ a/DT representation/NN of/IN the/DT input/NN domain/NN which/WDT ,/, in/IN expectation/NN ,/, permits/VBZ effective/JJ models/NNS for/IN new/JJ problems/NNS to/TO be/VB learned/VBN efficiently/RB from/IN a/DT small/JJ number/NN of/IN examples/NNS ./.
While/IN this/DT bears/VBZ a/DT resemblance/NN to/IN settings/NNS considered/VBN in/IN previous/JJ work/NN on/IN multitask/JJ and/CC lifelong/JJ learning/NN ,/, our/PRP$ non-assumption/NN of/IN inter-task/JJ label/NN correspondence/NN leads/VBZ to/IN a/DT novel/JJ algorithm/NN :/: Lifelong/JJ Learner/NN of/IN Discriminative/JJ Representations/NNS (/-LRB- LLDR/NN )/-RRB- ,/, which/WDT explicitly/RB minimizes/VBZ a/DT proxy/NN for/IN the/DT intra-task/JJ small/JJ -/HYPH sample/NN generalization/NN error/NN ./.
We/PRP examine/VBP the/DT relative/JJ benefits/NNS of/IN our/PRP$ approach/NN on/IN a/DT diverse/JJ set/NN of/IN real/JJ -/HYPH world/NN datasets/NNS in/IN three/CD significant/JJ scenarios/NNS :/: representation/NN learning/NN ,/, multitask/JJ learning/NN and/CC lifelong/JJ learning/NN ./.
