The/DT problem/NN of/IN quantizing/VBG the/DT activations/NNS of/IN a/DT deep/JJ neural/JJ network/NN is/VBZ considered/VBN ./.
An/DT examination/NN of/IN the/DT popular/JJ binary/JJ quantization/NN approach/NN shows/VBZ that/IN this/DT consists/VBZ of/IN approximating/VBG a/DT classical/JJ non-linearity/NN ,/, the/DT hyperbolic/JJ tangent/NN ,/, by/IN two/CD functions/NNS :/: a/DT piecewise/RB constant/JJ sign/NN function/NN ,/, which/WDT is/VBZ used/VBN in/IN feedforward/JJ network/NN computations/NNS ,/, and/CC a/DT piecewise/JJ linear/JJ hard/JJ tanh/NN function/NN ,/, used/VBN in/IN the/DT backpropagation/NN step/NN during/IN network/NN learning/NN ./.
The/DT problem/NN of/IN approximating/VBG the/DT ReLU/NNP non-linearity/NN ,/, widely/RB used/VBN in/IN the/DT recent/JJ deep/JJ learning/NN literature/NN ,/, is/VBZ then/RB considered/VBN ./.
An/DT half/NN -/HYPH wave/NN Gaussian/JJ quantizer/NN (/-LRB- HWGQ/NN )/-RRB- is/VBZ proposed/VBN for/IN forward/JJ approximation/NN and/CC shown/VBN to/TO have/VB efficient/JJ implementation/NN ,/, by/IN exploiting/VBG the/DT statistics/NNS of/IN of/IN network/NN activations/NNS and/CC batch/NN normalization/NN operations/NNS commonly/RB used/VBN in/IN the/DT literature/NN ./.
To/TO overcome/VB the/DT problem/NN of/IN gradient/NN mismatch/NN ,/, due/IN to/IN the/DT use/NN of/IN different/JJ forward/JJ and/CC backward/JJ approximations/NNS ,/, several/JJ piece-wise/JJ backward/JJ approximators/NNS are/VBP then/RB investigated/VBN ./.
The/DT implementation/NN of/IN the/DT resulting/VBG quantized/VBN network/NN ,/, denoted/VBN as/IN HWGQ/NN -/HYPH Net/NN ,/, is/VBZ shown/VBN to/TO achieve/VB much/RB closer/JJR performance/NN to/IN full/JJ precision/NN networks/NNS ,/, such/JJ as/IN AlexNet/NNP ,/, ResNet/NNP ,/, GoogLeNet/NNP and/CC VGG/NNP -/HYPH Net/NNP ,/, than/IN previously/RB available/JJ low/JJ -/HYPH precision/NN networks/NNS ,/, with/IN 1/CD -/HYPH bit/NN binary/JJ weights/NNS and/CC 2/CD -/HYPH bit/NN quantized/JJ activations/NNS ./.
