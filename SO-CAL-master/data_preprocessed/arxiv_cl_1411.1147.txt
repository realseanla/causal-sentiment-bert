We/PRP introduce/VBP a/DT framework/NN for/IN unsupervised/JJ learning/NN of/IN structured/JJ predictors/NNS with/IN overlapping/VBG ,/, global/JJ features/NNS ./.
Each/DT input/NN 's/POS latent/JJ representation/NN is/VBZ predicted/VBN conditional/JJ on/IN the/DT observable/JJ data/NNS using/VBG a/DT feature/NN -/HYPH rich/JJ conditional/JJ random/JJ field/NN ./.
Then/RB a/DT reconstruction/NN of/IN the/DT input/NN is/VBZ (/-LRB- re/IN )/-RRB- generated/VBN ,/, conditional/JJ on/IN the/DT latent/JJ structure/NN ,/, using/VBG models/NNS for/IN which/WDT maximum/JJ likelihood/NN estimation/NN has/VBZ a/DT closed/JJ -/HYPH form/NN ./.
Our/PRP$ autoencoder/NN formulation/NN enables/VBZ efficient/JJ learning/NN without/IN making/VBG unrealistic/JJ independence/NN assumptions/NNS or/CC restricting/VBG the/DT kinds/NNS of/IN features/NNS that/WDT can/MD be/VB used/VBN ./.
We/PRP illustrate/VBP insightful/JJ connections/NNS to/IN traditional/JJ autoencoders/NNS ,/, posterior/JJ regularization/NN and/CC multi-view/NN learning/NN ./.
We/PRP show/VBP competitive/JJ results/NNS with/IN instantiations/NNS of/IN the/DT model/NN for/IN two/CD canonical/JJ NLP/NN tasks/NNS :/: part/NN -/HYPH of/IN -/HYPH speech/NN induction/NN and/CC bitext/NN word/NN alignment/NN ,/, and/CC show/VBP that/IN training/VBG our/PRP$ model/NN can/MD be/VB substantially/RB more/RBR efficient/JJ than/IN comparable/JJ feature/NN -/HYPH rich/JJ baselines/NNS ./.
