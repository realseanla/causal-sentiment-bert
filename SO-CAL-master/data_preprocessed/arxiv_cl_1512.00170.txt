Pivot/NN language/NN is/VBZ employed/VBN as/IN a/DT way/NN to/TO solve/VB the/DT data/NNS sparseness/NN problem/NN in/IN machine/NN translation/NN ,/, especially/RB when/WRB the/DT data/NNS for/IN a/DT particular/JJ language/NN pair/NN does/VBZ not/RB exist/VB ./.
The/DT combination/NN of/IN source/NN -/HYPH to/IN -/HYPH pivot/NN and/CC pivot/NN -/HYPH to/IN -/HYPH target/NN translation/NN models/NNS can/MD induce/VB a/DT new/JJ translation/NN model/NN through/IN the/DT pivot/NN language/NN ./.
However/RB ,/, the/DT errors/NNS in/IN two/CD models/NNS may/MD compound/VB as/IN noise/NN ,/, and/CC still/RB ,/, the/DT combined/VBN model/NN may/MD suffer/VB from/IN a/DT serious/JJ phrase/NN sparsity/NN problem/NN ./.
In/IN this/DT paper/NN ,/, we/PRP directly/RB employ/VBP the/DT word/NN lexical/JJ model/NN in/IN IBM/NNP models/NNS as/IN an/DT additional/JJ resource/NN to/TO augment/VB pivot/NN phrase/NN table/NN ./.
In/IN addition/NN ,/, we/PRP also/RB propose/VBP a/DT phrase/NN table/NN pruning/NN method/NN which/WDT takes/VBZ into/IN account/NN both/DT of/IN the/DT source/NN and/CC target/NN phrasal/JJ coverage/NN ./.
Experimental/JJ result/NN shows/VBZ that/IN our/PRP$ pruning/NN method/NN significantly/RB outperforms/VBZ the/DT conventional/JJ one/CD ,/, which/WDT only/RB considers/VBZ source/NN side/NN phrasal/JJ coverage/NN ./.
Furthermore/RB ,/, by/IN including/VBG the/DT entries/NNS in/IN the/DT lexicon/NN model/NN ,/, the/DT phrase/NN coverage/NN increased/VBD ,/, and/CC we/PRP achieved/VBD improved/VBN results/NNS in/IN Chinese/JJ -/HYPH to/IN -/HYPH Japanese/JJ translation/NN using/VBG English/NNP as/IN pivot/NN language/NN ./.
