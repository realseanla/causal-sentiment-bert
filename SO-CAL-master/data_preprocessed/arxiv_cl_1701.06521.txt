We/PRP introduce/VBP multi-modal/JJ ,/, attention/NN -/HYPH based/VBN neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- models/NNS which/WDT incorporate/VBP visual/JJ features/NNS into/IN different/JJ parts/NNS of/IN both/CC the/DT encoder/NN and/CC the/DT decoder/NN ./.
We/PRP utilise/VBP global/JJ image/NN features/VBZ extracted/VBN using/VBG a/DT pre-trained/JJ convolutional/JJ neural/JJ network/NN and/CC incorporate/VB them/PRP (/-LRB- i/LS )/-RRB- as/IN words/NNS in/IN the/DT source/NN sentence/NN ,/, (/-LRB- ii/LS )/-RRB- to/TO initialise/VB the/DT encoder/NN hidden/VBN state/NN ,/, and/CC (/-LRB- iii/LS )/-RRB- as/IN additional/JJ data/NNS to/TO initialise/VB the/DT decoder/NN hidden/VBN state/NN ./.
In/IN our/PRP$ experiments/NNS ,/, we/PRP evaluate/VBP how/WRB these/DT different/JJ strategies/NNS to/TO incorporate/VB global/JJ image/NN features/NNS compare/VBP and/CC which/WDT ones/NNS perform/VBP best/JJS ./.
We/PRP also/RB study/VB the/DT impact/NN that/WDT adding/VBG synthetic/JJ multi-modal/JJ ,/, multilingual/JJ data/NNS brings/VBZ and/CC find/VB that/IN the/DT additional/JJ data/NNS have/VBP a/DT positive/JJ impact/NN on/IN multi-modal/JJ models/NNS ./.
We/PRP report/VBP new/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS and/CC our/PRP$ best/JJS models/NNS also/RB significantly/RB improve/VB on/IN a/DT comparable/JJ phrase/NN -/HYPH based/VBN Statistical/JJ MT/NN (/-LRB- PBSMT/NN )/-RRB- model/NN trained/VBN on/IN the/DT Multi30k/NN data/NNS set/VBN according/VBG to/IN all/DT metrics/NNS evaluated/VBN ./.
To/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, it/PRP is/VBZ the/DT first/JJ time/NN a/DT purely/RB neural/JJ model/NN significantly/RB improves/VBZ over/IN a/DT PBSMT/NN model/NN on/IN all/DT metrics/NNS evaluated/VBN on/IN this/DT data/NN set/NN ./.
