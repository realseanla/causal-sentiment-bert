This/DT paper/NN shows/VBZ that/IN a/DT perturbed/VBN form/NN of/IN gradient/NN descent/NN converges/VBZ to/IN a/DT second/JJ -/HYPH order/NN stationary/JJ point/NN in/IN a/DT number/NN iterations/NNS which/WDT depends/VBZ only/RB poly/JJ -/HYPH logarithmically/RB on/IN dimension/NN (/-LRB- i.e./FW ,/, it/PRP is/VBZ almost/RB "/`` dimension/NN -/HYPH free/JJ "/'' )/-RRB- ./.
The/DT convergence/NN rate/NN of/IN this/DT procedure/NN matches/VBZ the/DT well/NN -/HYPH known/VBN convergence/NN rate/NN of/IN gradient/NN descent/NN to/IN first/JJ -/HYPH order/NN stationary/JJ points/NNS ,/, up/IN to/IN log/NN factors/NNS ./.
When/WRB all/DT saddle/NN points/NNS are/VBP non-degenerate/JJ ,/, all/DT second/JJ -/HYPH order/NN stationary/JJ points/NNS are/VBP local/JJ minima/NN ,/, and/CC our/PRP$ result/NN thus/RB shows/VBZ that/IN perturbed/VBN gradient/NN descent/NN can/MD escape/VB saddle/NN points/NNS almost/RB for/IN free/JJ ./.
Our/PRP$ results/NNS can/MD be/VB directly/RB applied/VBN to/IN many/JJ machine/NN learning/NN applications/NNS ,/, including/VBG deep/JJ learning/NN ./.
As/IN a/DT particular/JJ concrete/JJ example/NN of/IN such/PDT an/DT application/NN ,/, we/PRP show/VBP that/IN our/PRP$ results/NNS can/MD be/VB used/VBN directly/RB to/TO establish/VB sharp/JJ global/JJ convergence/NN rates/NNS for/IN matrix/NN factorization/NN ./.
Our/PRP$ results/NNS rely/VBP on/IN a/DT novel/JJ characterization/NN of/IN the/DT geometry/NN around/IN saddle/NN points/NNS ,/, which/WDT may/MD be/VB of/IN independent/JJ interest/NN to/IN the/DT non-convex/JJ optimization/NN community/NN ./.
