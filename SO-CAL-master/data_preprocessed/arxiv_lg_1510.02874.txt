In/IN model/NN -/HYPH based/VBN solution/NN approaches/VBZ to/IN the/DT problem/NN of/IN learning/NN in/IN an/DT unknown/JJ environment/NN ,/, exploring/VBG to/TO learn/VB the/DT model/NN parameters/NNS takes/VBZ a/DT toll/NN on/IN the/DT regret/NN ./.
The/DT optimal/JJ performance/NN with/IN respect/NN to/IN regret/NN or/CC PAC/NN bounds/NNS is/VBZ achievable/JJ ,/, if/IN the/DT algorithm/NN exploits/NNS with/IN respect/NN to/IN reward/NN or/CC explores/VBZ with/IN respect/NN to/IN the/DT model/NN parameters/NNS ,/, respectively/RB ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP TSEB/NNP ,/, a/DT Thompson/NNP Sampling/NNP based/VBN algorithm/NN with/IN adaptive/JJ exploration/NN bonus/NN that/WDT aims/VBZ to/TO solve/VB the/DT problem/NN with/IN tighter/JJR PAC/NN guarantees/NNS ,/, while/IN being/VBG cautious/JJ on/IN the/DT regret/NN as/RB well/RB ./.
The/DT proposed/VBN approach/NN maintains/VBZ distributions/NNS over/IN the/DT model/NN parameters/NNS which/WDT are/VBP successively/RB refined/VBN with/IN more/JJR experience/NN ./.
At/IN any/DT given/VBN time/NN ,/, the/DT agent/NN solves/VBZ a/DT model/NN sampled/VBN from/IN this/DT distribution/NN ,/, and/CC the/DT sampled/VBN reward/NN distribution/NN is/VBZ skewed/VBN by/IN an/DT exploration/NN bonus/NN in/IN order/NN to/TO generate/VB more/JJR informative/JJ exploration/NN ./.
The/DT policy/NN by/IN solving/VBG is/VBZ then/RB used/VBN for/IN generating/VBG more/JJR experience/NN that/WDT helps/VBZ in/IN updating/VBG the/DT posterior/JJ over/IN the/DT model/NN parameters/NNS ./.
We/PRP provide/VBP a/DT detailed/JJ analysis/NN of/IN the/DT PAC/NN guarantees/NNS ,/, and/CC convergence/NN of/IN the/DT proposed/VBN approach/NN ./.
We/PRP show/VBP that/IN our/PRP$ adaptive/JJ exploration/NN bonus/NN encourages/VBZ the/DT additional/JJ exploration/NN required/VBN for/IN better/JJR PAC/NN bounds/NNS on/IN the/DT algorithm/NN ./.
We/PRP provide/VBP empirical/JJ analysis/NN on/IN two/CD different/JJ simulated/JJ domains/NNS ./.
