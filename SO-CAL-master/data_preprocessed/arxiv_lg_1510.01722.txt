We/PRP consider/VBP the/DT task/NN of/IN building/VBG compact/JJ deep/JJ learning/NN pipelines/NNS suitable/JJ for/IN deployment/NN on/IN storage/NN and/CC power/NN constrained/VBN mobile/JJ devices/NNS ./.
We/PRP propose/VBP a/DT unified/VBN framework/NN to/TO learn/VB a/DT broad/JJ family/NN of/IN structured/JJ parameter/NN matrices/NNS that/WDT are/VBP characterized/VBN by/IN the/DT notion/NN of/IN low/JJ displacement/NN rank/NN ./.
Our/PRP$ structured/VBN transforms/VBZ admit/VBP fast/JJ function/NN and/CC gradient/NN evaluation/NN ,/, and/CC span/VBP a/DT rich/JJ range/NN of/IN parameter/NN sharing/NN configurations/NNS whose/WP$ statistical/JJ modeling/NN capacity/NN can/MD be/VB explicitly/RB tuned/VBN along/IN a/DT continuum/NN from/IN structured/VBN to/TO unstructured/JJ ./.
Experimental/JJ results/NNS show/VBP that/IN these/DT transforms/VBZ can/MD significantly/RB accelerate/VB inference/NN and/CC forward/NN //HYPH backward/JJ passes/NNS during/IN training/NN ,/, and/CC offer/VB superior/JJ accuracy/NN -/HYPH compactness/NN -/HYPH speed/NN tradeoffs/NNS in/IN comparison/NN to/IN a/DT number/NN of/IN existing/VBG techniques/NNS ./.
In/IN keyword/NN spotting/NN applications/NNS in/IN mobile/JJ speech/NN recognition/NN ,/, our/PRP$ methods/NNS are/VBP much/RB more/RBR effective/JJ than/IN standard/JJ linear/JJ low/JJ -/HYPH rank/NN bottleneck/NN layers/NNS and/CC nearly/RB retain/VB the/DT performance/NN of/IN state/NN of/IN the/DT art/NN models/NNS ,/, while/IN providing/VBG more/JJR than/IN 3.5-fold/RB compression/NN ./.
