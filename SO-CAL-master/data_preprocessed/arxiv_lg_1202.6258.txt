We/PRP propose/VBP a/DT new/JJ stochastic/JJ gradient/NN method/NN for/IN optimizing/VBG the/DT sum/NN of/IN a/DT finite/NN set/NN of/IN smooth/JJ functions/NNS ,/, where/WRB the/DT sum/NN is/VBZ strongly/RB convex/JJ ./.
While/IN standard/JJ stochastic/JJ gradient/NN methods/NNS converge/VBP at/IN sublinear/JJ rates/NNS for/IN this/DT problem/NN ,/, the/DT proposed/JJ method/NN incorporates/VBZ a/DT memory/NN of/IN previous/JJ gradient/NN values/NNS in/IN order/NN to/TO achieve/VB a/DT linear/JJ convergence/NN rate/NN ./.
Numerical/NNP experiments/NNS indicate/VBP that/IN the/DT new/JJ algorithm/NN can/MD dramatically/RB outperform/VB standard/JJ algorithms/NNS ./.
