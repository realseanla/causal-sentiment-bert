In/IN this/DT paper/NN ,/, we/PRP propose/VBP an/DT extremely/RB simple/JJ deep/JJ model/NN for/IN the/DT unsupervised/JJ nonlinear/JJ dimensionality/NN reduction/NN --/: deep/JJ distributed/VBN random/JJ samplings/NNS ,/, which/WDT performs/VBZ like/IN a/DT stack/NN of/IN unsupervised/JJ bootstrap/NN aggregating/NN ./.
First/RB ,/, its/PRP$ network/NN structure/NN is/VBZ novel/JJ :/: each/DT layer/NN of/IN the/DT network/NN is/VBZ a/DT group/NN of/IN mutually/RB independent/JJ $/$ k/CD $/$ -/HYPH centers/NNS clusterings/NNS ./.
Second/RB ,/, its/PRP$ learning/NN method/NN is/VBZ extremely/RB simple/JJ :/: the/DT $/$ k/CD $/$ centers/NNS of/IN each/DT clustering/NN are/VBP only/RB $/$ k/CD $/$ randomly/RB selected/VBN examples/NNS from/IN the/DT training/NN data/NNS ;/: for/IN small/JJ -/HYPH scale/NN data/NN sets/NNS ,/, the/DT $/$ k/CD $/$ centers/NNS are/VBP further/JJ randomly/RB reconstructed/VBN by/IN a/DT simple/JJ cyclic/JJ -/HYPH shift/NN operation/NN ./.
Experimental/JJ results/NNS on/IN nonlinear/JJ dimensionality/NN reduction/NN show/VBP that/IN the/DT proposed/JJ method/NN can/MD learn/VB abstract/JJ representations/NNS on/IN both/DT large/JJ -/HYPH scale/NN and/CC small/JJ -/HYPH scale/NN problems/NNS ,/, and/CC meanwhile/RB is/VBZ much/JJ faster/JJR than/IN deep/JJ neural/JJ networks/NNS on/IN large/JJ -/HYPH scale/NN problems/NNS ./.
