We/PRP introduce/VBP a/DT new/JJ structured/JJ kernel/NN interpolation/NN (/-LRB- SKI/NN )/-RRB- framework/NN ,/, which/WDT generalises/VBZ and/CC unifies/VBZ inducing/VBG point/NN methods/NNS for/IN scalable/JJ Gaussian/JJ processes/NNS (/-LRB- GPs/NNS )/-RRB- ./.
SKI/NN methods/NNS produce/VBP kernel/NN approximations/NNS for/IN fast/JJ computations/NNS through/IN kernel/NN interpolation/NN ./.
The/DT SKI/NNP framework/NN clarifies/VBZ how/WRB the/DT quality/NN of/IN an/DT inducing/VBG point/NN approach/NN depends/VBZ on/IN the/DT number/NN of/IN inducing/VBG (/-LRB- aka/RB interpolation/NN )/-RRB- points/NNS ,/, interpolation/NN strategy/NN ,/, and/CC GP/NNP covariance/NNP kernel/NN ./.
SKI/NNP also/RB provides/VBZ a/DT mechanism/NN to/TO create/VB new/JJ scalable/JJ kernel/NN methods/NNS ,/, through/IN choosing/VBG different/JJ kernel/NN interpolation/NN strategies/NNS ./.
Using/VBG SKI/NNP ,/, with/IN local/JJ cubic/JJ kernel/NN interpolation/NN ,/, we/PRP introduce/VBP KISS/NNP -/HYPH GP/NNP ,/, which/WDT is/VBZ 1/CD )/-RRB- more/JJR scalable/JJ than/IN inducing/VBG point/NN alternatives/NNS ,/, 2/LS )/-RRB- naturally/RB enables/VBZ Kronecker/NNP and/CC Toeplitz/NNP algebra/NN for/IN substantial/JJ additional/JJ gains/NNS in/IN scalability/NN ,/, without/IN requiring/VBG any/DT grid/NN data/NNS ,/, and/CC 3/LS )/-RRB- can/MD be/VB used/VBN for/IN fast/JJ and/CC expressive/JJ kernel/NN learning/NN ./.
KISS/NNP -/HYPH GP/NNP costs/VBZ O/NN (/-LRB- n/NN )/-RRB- time/NN and/CC storage/NN for/IN GP/NNP inference/NN ./.
We/PRP evaluate/VBP KISS/NNP -/HYPH GP/NNP for/IN kernel/NN matrix/NN approximation/NN ,/, kernel/NN learning/NN ,/, and/CC natural/JJ sound/NN modelling/NN ./.
