Generating/NNP descriptions/NNS for/IN videos/NNS has/VBZ many/JJ applications/NNS including/VBG assisting/VBG blind/JJ people/NNS and/CC human/JJ -/HYPH robot/NN interaction/NN ./.
The/DT recent/JJ advances/NNS in/IN image/NN captioning/NN as/RB well/RB as/IN the/DT release/NN of/IN large/JJ -/HYPH scale/NN movie/NN description/NN datasets/NNS such/JJ as/IN MPII/NN Movie/NN Description/NN allow/VBP to/TO study/VB this/DT task/NN in/IN more/JJR depth/NN ./.
Many/JJ of/IN the/DT proposed/VBN methods/NNS for/IN image/NN captioning/NN rely/VBP on/IN pre-trained/JJ object/NN classifier/NN CNNs/NNS and/CC Long/JJ -/HYPH Short/JJ Term/NN Memory/NN recurrent/JJ networks/NNS (/-LRB- LSTMs/NNS )/-RRB- for/IN generating/VBG descriptions/NNS ./.
While/IN image/NN description/NN focuses/VBZ on/IN objects/NNS ,/, we/PRP argue/VBP that/IN it/PRP is/VBZ important/JJ to/TO distinguish/VB verbs/NNS ,/, objects/NNS ,/, and/CC places/NNS in/IN the/DT challenging/JJ setting/NN of/IN movie/NN description/NN ./.
In/IN this/DT work/NN we/PRP show/VBP how/WRB to/TO learn/VB robust/JJ visual/JJ classifiers/NNS from/IN the/DT weak/JJ annotations/NNS of/IN the/DT sentence/NN descriptions/NNS ./.
Based/VBN on/IN these/DT visual/JJ classifiers/NNS we/PRP learn/VBP how/WRB to/TO generate/VB a/DT description/NN using/VBG an/DT LSTM/NN ./.
We/PRP explore/VBP different/JJ design/NN choices/NNS to/TO build/VB and/CC train/VB the/DT LSTM/NNP and/CC achieve/VB the/DT best/JJS performance/NN to/IN date/NN on/IN the/DT challenging/JJ MPII/NN -/HYPH MD/NN dataset/NN ./.
We/PRP compare/VBP and/CC analyze/VBP our/PRP$ approach/NN and/CC prior/JJ work/NN along/IN various/JJ dimensions/NNS to/TO better/RBR understand/VB the/DT key/JJ challenges/NNS of/IN the/DT movie/NN description/NN task/NN ./.
