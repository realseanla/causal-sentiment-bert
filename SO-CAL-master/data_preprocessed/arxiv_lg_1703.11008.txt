One/CD of/IN the/DT defining/VBG properties/NNS of/IN deep/JJ learning/NN is/VBZ that/IN models/NNS are/VBP chosen/VBN to/TO have/VB many/JJ more/JJR parameters/NNS than/IN available/JJ training/NN data/NNS ./.
In/IN light/NN of/IN this/DT capacity/NN for/IN overfitting/NN ,/, it/PRP is/VBZ remarkable/JJ that/IN simple/JJ algorithms/NNS like/IN SGD/NNP reliably/RB return/VB solutions/NNS with/IN low/JJ test/NN error/NN ./.
One/CD roadblock/NN to/IN explaining/VBG these/DT phenomena/NNS in/IN terms/NNS of/IN implicit/JJ regularization/NN ,/, structural/JJ properties/NNS of/IN the/DT solution/NN ,/, and/CC //HYPH or/CC easiness/NN of/IN the/DT data/NNS is/VBZ that/IN many/JJ learning/NN bounds/NNS are/VBP quantitatively/RB vacuous/JJ in/IN this/DT "/`` deep/JJ learning/NN "/'' regime/NN ./.
In/IN order/NN to/TO explain/VB generalization/NN ,/, we/PRP need/VBP nonvacuous/JJ bounds/NNS ./.
We/PRP return/VBP to/IN an/DT idea/NN by/IN Langford/NNP and/CC Caruana/NNP (/-LRB- 2001/CD )/-RRB- ,/, who/WP used/VBD PAC/NN -/HYPH Bayes/NNS bounds/NNS to/IN compute/VB nonvacuous/JJ numerical/JJ bounds/NNS on/IN generalization/NN error/NN for/IN stochastic/JJ two/CD -/HYPH layer/NN two/CD -/HYPH hidden/VBN -/HYPH unit/NN neural/JJ networks/NNS via/IN a/DT sensitivity/NN analysis/NN ./.
By/IN optimizing/VBG the/DT PAC/NN -/HYPH Bayes/NNS bound/VBN directly/RB ,/, we/PRP are/VBP able/JJ to/TO extend/VB their/PRP$ approach/NN and/CC obtain/VB nonvacuous/JJ generalization/NN bounds/NNS for/IN deep/JJ stochastic/JJ neural/JJ network/NN classifiers/NNS with/IN millions/NNS of/IN parameters/NNS trained/VBN on/IN only/JJ tens/NNS of/IN thousands/NNS of/IN examples/NNS ./.
We/PRP connect/VBP our/PRP$ findings/NNS to/IN recent/JJ and/CC old/JJ work/NN on/IN flat/JJ minima/NN and/CC MDL/NN -/HYPH based/VBN explanations/NNS of/IN generalization/NN ./.
