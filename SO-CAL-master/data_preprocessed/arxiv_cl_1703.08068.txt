Feedforward/JJ Neural/JJ Network/NN (/-LRB- FNN/NN )/-RRB- -/HYPH based/VBN language/NN models/NNS estimate/VBP the/DT probability/NN of/IN the/DT next/JJ word/NN based/VBN on/IN the/DT history/NN of/IN the/DT last/JJ N/NN words/NNS ,/, whereas/IN Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNN/NN )/-RRB- perform/VBP the/DT same/JJ task/NN based/VBN only/RB on/IN the/DT last/JJ word/NN and/CC some/DT context/NN information/NN that/WDT cycles/NNS in/IN the/DT network/NN ./.
This/DT paper/NN presents/VBZ a/DT novel/JJ approach/NN ,/, which/WDT bridges/VBZ the/DT gap/NN between/IN these/DT two/CD categories/NNS of/IN networks/NNS ./.
In/IN particular/JJ ,/, we/PRP propose/VBP an/DT architecture/NN which/WDT takes/VBZ advantage/NN of/IN the/DT explicit/JJ ,/, sequential/JJ enumeration/NN of/IN the/DT word/NN history/NN in/IN FNN/NNP structure/NN while/IN enhancing/VBG each/DT word/NN representation/NN at/IN the/DT projection/NN layer/NN through/IN recurrent/JJ context/NN information/NN that/WDT evolves/VBZ in/IN the/DT network/NN ./.
The/DT context/NN integration/NN is/VBZ performed/VBN using/VBG an/DT additional/JJ word/NN -/HYPH dependent/JJ weight/NN matrix/NN that/WDT is/VBZ also/RB learned/VBN during/IN the/DT training/NN ./.
Extensive/JJ experiments/NNS conducted/VBN on/IN the/DT Penn/NNP Treebank/NNP (/-LRB- PTB/NNP )/-RRB- and/CC the/DT Large/JJ Text/VB Compression/NNP Benchmark/NNP (/-LRB- LTCB/NNP )/-RRB- corpus/NN showed/VBD a/DT significant/JJ reduction/NN of/IN the/DT perplexity/NN when/WRB compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN feedforward/NN as/RB well/RB as/IN recurrent/JJ neural/JJ network/NN architectures/NNS ./.
