Finite/JJ -/HYPH sum/NN optimization/NN problems/NNS are/VBP ubiquitous/JJ in/IN machine/NN learning/NN ,/, and/CC are/VBP commonly/RB solved/VBN using/VBG first/RB -/HYPH order/NN methods/NNS which/WDT rely/VBP on/IN gradient/NN computations/NNS ./.
Recently/RB ,/, there/EX has/VBZ been/VBN growing/VBG interest/NN in/IN \/NN emph/NN {/-LRB- second/JJ -/HYPH order/NN }/-RRB- methods/NNS ,/, which/WDT rely/VBP on/IN both/DT gradients/NNS and/CC Hessians/NNPS ./.
In/IN principle/NN ,/, second/JJ -/HYPH order/NN methods/NNS can/MD require/VB much/JJ fewer/JJR iterations/NNS than/IN first/JJ -/HYPH order/NN methods/NNS ,/, and/CC hold/VB the/DT promise/NN for/IN more/RBR efficient/JJ algorithms/NNS ./.
Although/IN computing/VBG and/CC manipulating/VBG Hessians/NNPS is/VBZ prohibitive/JJ for/IN high/JJ -/HYPH dimensional/JJ problems/NNS in/IN general/JJ ,/, the/DT Hessians/NNPS of/IN individual/JJ functions/NNS in/IN finite/NN -/HYPH sum/NN problems/NNS can/MD often/RB be/VB efficiently/RB computed/VBN ,/, e.g./FW because/IN they/PRP possess/VBP a/DT low/JJ -/HYPH rank/NN structure/NN ./.
Can/MD second/RB -/HYPH order/NN information/NN indeed/RB be/VB used/VBN to/TO solve/VB such/JJ problems/NNS more/RBR efficiently/RB ?/.
In/IN this/DT paper/NN ,/, we/PRP provide/VBP evidence/NN that/IN the/DT answer/NN --/: perhaps/RB surprisingly/RB --/: is/VBZ negative/JJ ,/, at/IN least/RBS in/IN terms/NNS of/IN worst/JJS -/HYPH case/NN guarantees/NNS ./.
However/RB ,/, we/PRP also/RB discuss/VBP what/WP additional/JJ assumptions/NNS and/CC algorithmic/JJ approaches/NNS might/MD potentially/RB circumvent/VB this/DT negative/JJ result/NN ./.
