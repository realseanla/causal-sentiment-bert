Deep/JJ neural/JJ networks/NNS have/VBP proved/VBN very/RB successful/JJ in/IN domains/NNS where/WRB large/JJ training/NN sets/NNS are/VBP available/JJ ,/, but/CC when/WRB the/DT number/NN of/IN training/NN samples/NNS is/VBZ small/JJ ,/, their/PRP$ performance/NN suffers/VBZ from/IN overfitting/VBG ./.
Prior/JJ methods/NNS of/IN reducing/VBG overfitting/VBG such/JJ as/IN weight/NN decay/NN ,/, Dropout/NN and/CC DropConnect/NN are/VBP data/NNS -/HYPH independent/JJ ./.
This/DT paper/NN proposes/VBZ a/DT new/JJ method/NN ,/, GraphConnect/NNP ,/, that/DT is/VBZ data/NN -/HYPH dependent/JJ ,/, and/CC is/VBZ motivated/VBN by/IN the/DT observation/NN that/IN data/NNS of/IN interest/NN lie/VBP close/RB to/IN a/DT manifold/NN ./.
The/DT new/JJ method/NN encourages/VBZ the/DT relationships/NNS between/IN the/DT learned/VBN decisions/NNS to/TO resemble/VB a/DT graph/NN representing/VBG the/DT manifold/JJ structure/NN ./.
Essentially/RB GraphConnect/NNP is/VBZ designed/VBN to/TO learn/VB attributes/NNS that/WDT are/VBP present/JJ in/IN data/NNS samples/NNS in/IN contrast/NN to/IN weight/NN decay/NN ,/, Dropout/NN and/CC DropConnect/NN which/WDT are/VBP simply/RB designed/VBN to/TO make/VB it/PRP more/RBR difficult/JJ to/TO fit/VB to/IN random/JJ error/NN or/CC noise/NN ./.
Empirical/JJ Rademacher/NNP complexity/NN is/VBZ used/VBN to/TO connect/VB the/DT generalization/NN error/NN of/IN the/DT neural/JJ network/NN to/IN spectral/JJ properties/NNS of/IN the/DT graph/NN learned/VBD from/IN the/DT input/NN data/NNS ./.
This/DT framework/NN is/VBZ used/VBN to/TO show/VB that/IN GraphConnect/NNP is/VBZ superior/JJ to/IN weight/NN decay/NN ./.
Experimental/JJ results/NNS on/IN several/JJ benchmark/NN datasets/NNS validate/VBP the/DT theoretical/JJ analysis/NN ,/, and/CC show/VBP that/IN when/WRB the/DT number/NN of/IN training/NN samples/NNS is/VBZ small/JJ ,/, GraphConnect/NNP is/VBZ able/JJ to/TO significantly/RB improve/VB performance/NN over/IN weight/NN decay/NN ./.
