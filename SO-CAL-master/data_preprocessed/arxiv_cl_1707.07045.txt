We/PRP introduce/VBP the/DT first/JJ end/NN -/HYPH to/IN -/HYPH end/NN coreference/NN resolution/NN model/NN and/CC show/VBP that/IN it/PRP significantly/RB outperforms/VBZ all/DT previous/JJ work/NN without/IN using/VBG a/DT syntactic/JJ parser/NN or/CC hand/NN -/HYPH engineered/VBN mention/NN detector/NN ./.
The/DT key/JJ idea/NN is/VBZ to/TO directly/RB consider/VB all/DT spans/NNS in/IN a/DT document/NN as/IN potential/NN mentions/VBZ and/CC learn/VB distributions/NNS over/IN possible/JJ antecedents/NNS for/IN each/DT ./.
The/DT model/NN computes/VBZ span/NN embeddings/NNS that/WDT combine/VBP context/NN -/HYPH dependent/JJ boundary/NN representations/NNS with/IN a/DT head/NN -/HYPH finding/VBG attention/NN mechanism/NN ./.
It/PRP is/VBZ trained/VBN to/TO maximize/VB the/DT marginal/JJ likelihood/NN of/IN gold/NN antecedent/NN spans/NNS from/IN coreference/NN clusters/NNS and/CC is/VBZ factored/VBN to/TO enable/VB aggressive/JJ pruning/NN of/IN potential/NN mentions/VBZ ./.
Experiments/NNS demonstrate/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN ,/, with/IN a/DT gain/NN of/IN 1.5/CD F1/NN on/IN the/DT OntoNotes/NNP benchmark/NN and/CC by/IN 3.1/CD F1/NN using/VBG a/DT 5/CD -/HYPH model/NN ensemble/NN ,/, despite/IN the/DT fact/NN that/IN this/DT is/VBZ the/DT first/JJ approach/NN to/TO be/VB successfully/RB trained/VBN with/IN no/DT external/JJ resources/NNS ./.
