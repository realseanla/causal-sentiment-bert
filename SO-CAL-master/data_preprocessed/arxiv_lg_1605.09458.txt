Auto/NN -/HYPH encoders/NNS are/VBP often/RB used/VBN as/IN building/NN blocks/NNS of/IN deep/JJ network/NN classifier/NN to/TO learn/VB feature/NN extractors/NNS ,/, but/CC task/NN -/HYPH irrelevant/JJ information/NN in/IN the/DT input/NN data/NNS may/MD lead/VB to/IN bad/JJ extractors/NNS and/CC result/VBP in/IN poor/JJ generalization/NN performance/NN of/IN the/DT network/NN ./.
In/IN this/DT paper/NN ,/, via/IN dropping/VBG the/DT task/NN -/HYPH irrelevant/JJ input/NN variables/NNS the/DT performance/NN of/IN auto/NN -/HYPH encoders/NNS can/MD be/VB obviously/RB improved/VBN ./.
Specifically/RB ,/, an/DT importance/NN -/HYPH based/VBN variable/JJ selection/NN method/NN is/VBZ proposed/VBN to/IN aim/NN at/IN finding/VBG the/DT task/NN -/HYPH irrelevant/JJ input/NN variables/NNS and/CC dropping/VBG them.It/NNP firstly/RB estimates/VBZ importance/NN of/IN each/DT variable/NN ,/, and/CC then/RB drops/VBZ the/DT variables/NNS with/IN importance/NN value/NN lower/JJR than/IN a/DT threshold/NN ./.
In/IN order/NN to/TO obtain/VB better/JJR performance/NN ,/, the/DT method/NN can/MD be/VB employed/VBN for/IN each/DT layer/NN of/IN stacked/VBN auto/NN -/HYPH encoders/NNS ./.
Experimental/JJ results/NNS show/VBP that/IN when/WRB combined/VBN with/IN our/PRP$ method/NN the/DT stacked/VBN denoising/NN auto/NN -/HYPH encoders/NNS achieves/VBZ significantly/RB improved/VBN performance/NN on/IN three/CD challenging/JJ datasets/NNS ./.
