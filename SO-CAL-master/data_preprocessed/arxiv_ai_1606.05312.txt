Transfer/NN in/IN reinforcement/NN learning/NN refers/VBZ to/IN the/DT notion/NN that/IN generalization/NN should/MD occur/VB not/RB only/RB within/IN a/DT task/NN but/CC also/RB across/IN tasks/NNS ./.
Our/PRP$ focus/NN is/VBZ on/IN transfer/NN where/WRB the/DT reward/NN functions/NNS vary/VBP across/IN tasks/NNS while/IN the/DT environment/NN 's/POS dynamics/NNS remain/VBP the/DT same/JJ ./.
The/DT method/NN we/PRP propose/VBP rests/VBZ on/IN two/CD key/JJ ideas/NNS :/: "/`` successor/NN features/NNS ,/, "/'' a/DT value/NN function/NN representation/NN that/WDT decouples/VBZ the/DT dynamics/NNS of/IN the/DT environment/NN from/IN the/DT rewards/NNS ,/, and/CC "/`` generalized/VBN policy/NN improvement/NN ,/, "/'' a/DT generalization/NN of/IN dynamic/JJ programming/NN 's/POS policy/NN improvement/NN step/NN that/WDT considers/VBZ a/DT set/NN of/IN policies/NNS rather/RB than/IN a/DT single/JJ one/NN ./.
Put/VB together/RP ,/, the/DT two/CD ideas/NNS lead/VBP to/IN an/DT approach/NN that/WDT integrates/VBZ seamlessly/RB within/IN the/DT reinforcement/NN learning/VBG framework/NN and/CC allows/VBZ transfer/NN to/TO take/VB place/NN between/IN tasks/NNS without/IN any/DT restriction/NN ./.
The/DT proposed/JJ method/NN also/RB provides/VBZ performance/NN guarantees/NNS for/IN the/DT transferred/VBN policy/NN even/RB before/IN any/DT learning/NN has/VBZ taken/VBN place/NN ./.
We/PRP derive/VBP two/CD theorems/NNS that/WDT set/VBP our/PRP$ approach/NN in/IN firm/JJ theoretical/JJ ground/NN and/CC present/JJ experiments/NNS that/WDT show/VBP that/IN it/PRP successfully/RB promotes/VBZ transfer/NN in/IN practice/NN ./.
