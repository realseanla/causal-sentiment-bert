How/WRB much/RB can/MD pruning/NN algorithms/NNS teach/VBP us/PRP about/IN the/DT fundamentals/NNS of/IN learning/VBG representations/NNS in/IN neural/JJ networks/NNS ?/.
A/DT lot/NN ,/, it/PRP turns/VBZ out/RP ./.
Neural/JJ network/NN model/NN compression/NN has/VBZ become/VBN a/DT topic/NN of/IN great/JJ interest/NN in/IN recent/JJ years/NNS ,/, and/CC many/JJ different/JJ techniques/NNS have/VBP been/VBN proposed/VBN to/TO address/VB this/DT problem/NN ./.
In/IN general/JJ ,/, this/DT is/VBZ motivated/VBN by/IN the/DT idea/NN that/IN smaller/JJR models/NNS typically/RB lead/VBP to/IN better/JJR generalization/NN ./.
At/IN the/DT same/JJ time/NN ,/, the/DT decision/NN of/IN what/WP to/TO prune/VB and/CC when/WRB to/TO prune/VB necessarily/RB forces/VBZ us/PRP to/TO confront/VB our/PRP$ assumptions/NNS about/IN how/WRB neural/JJ networks/NNS actually/RB learn/VBP to/TO represent/VB patterns/NNS in/IN data/NNS ./.
In/IN this/DT work/NN we/PRP set/VBD out/RP to/TO test/VB several/JJ long/JJ -/HYPH held/JJ hypotheses/NNS about/IN neural/JJ network/NN learning/VBG representations/NNS and/CC numerical/JJ approaches/NNS to/IN pruning/NN ./.
To/TO accomplish/VB this/DT we/PRP first/RB reviewed/VBD the/DT historical/JJ literature/NN and/CC derived/VBD a/DT novel/JJ algorithm/NN to/TO prune/VB whole/JJ neurons/NNS (/-LRB- as/IN opposed/VBN to/IN the/DT traditional/JJ method/NN of/IN pruning/NN weights/NNS )/-RRB- from/IN optimally/RB trained/VBN networks/NNS using/VBG a/DT second/JJ -/HYPH order/NN Taylor/NNP method/NN ./.
We/PRP then/RB set/VBD about/RB testing/VBG the/DT performance/NN of/IN our/PRP$ algorithm/NN and/CC analyzing/VBG the/DT quality/NN of/IN the/DT decisions/NNS it/PRP made/VBD ./.
As/IN a/DT baseline/NN for/IN comparison/NN we/PRP used/VBD a/DT first/JJ -/HYPH order/NN Taylor/NNP method/NN based/VBN on/IN the/DT Skeletonization/NN algorithm/NN and/CC an/DT exhaustive/JJ brute/JJ -/HYPH force/NN serial/JJ pruning/NN algorithm/NN ./.
Our/PRP$ proposed/VBN algorithm/NN worked/VBD well/RB compared/VBN to/IN a/DT first/JJ -/HYPH order/NN method/NN ,/, but/CC not/RB nearly/RB as/RB well/RB as/IN the/DT brute/JJ -/HYPH force/NN method/NN ./.
Our/PRP$ error/NN analysis/NN led/VBD us/PRP to/TO question/VB the/DT validity/NN of/IN many/JJ widely/RB -/HYPH held/VBN assumptions/NNS behind/IN pruning/NN algorithms/NNS in/IN general/JJ and/CC the/DT trade/NN -/HYPH offs/NNS we/PRP often/RB make/VBP in/IN the/DT interest/NN of/IN reducing/VBG computational/JJ complexity/NN ./.
We/PRP discovered/VBD that/IN there/EX is/VBZ a/DT straightforward/JJ way/NN ,/, however/RB expensive/JJ ,/, to/TO serially/RB prune/VB 40/CD -/HYPH 70/CD percent/NN of/IN the/DT neurons/NNS in/IN a/DT trained/VBN network/NN with/IN minimal/JJ effect/NN on/IN the/DT learning/NN representation/NN and/CC without/IN any/DT re-training/NN ./.
