Decision/NN tree/NN (/-LRB- and/CC its/PRP$ extensions/NNS such/JJ as/IN Gradient/NN Boosting/VBG Decision/NN Trees/NNS and/CC Random/NNP Forest/NNP )/-RRB- is/VBZ a/DT widely/RB used/VBN machine/NN learning/NN algorithm/NN ,/, due/IN to/IN its/PRP$ practical/JJ effectiveness/NN and/CC model/NN interpretability/NN ./.
With/IN the/DT emergence/NN of/IN big/JJ data/NNS ,/, there/EX is/VBZ an/DT increasing/VBG need/NN to/TO parallelize/VB the/DT training/NN process/NN of/IN decision/NN tree/NN ./.
However/RB ,/, most/JJS existing/VBG attempts/NNS along/IN this/DT line/NN suffer/VB from/IN high/JJ communication/NN costs/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ algorithm/NN ,/, called/VBN \/SYM emph/NN {/-LRB- Parallel/JJ Voting/NNP Decision/NN Tree/NNP (/-LRB- PV/NN -/HYPH Tree/NN )/-RRB- }/-RRB- ,/, to/TO tackle/VB this/DT challenge/NN ./.
After/IN partitioning/VBG the/DT training/NN data/NNS onto/IN a/DT number/NN of/IN (/-LRB- e.g./FW ,/, $/$ M$/$ )/-RRB- machines/NNS ,/, this/DT algorithm/NN performs/VBZ both/CC local/JJ voting/NN and/CC global/JJ voting/NN in/IN each/DT iteration/NN ./.
For/IN local/JJ voting/NN ,/, the/DT top/JJ -/HYPH $/$ k/CD $/$ attributes/NNS are/VBP selected/VBN from/IN each/DT machine/NN according/VBG to/IN its/PRP$ local/JJ data/NNS ./.
Then/RB ,/, globally/RB top/JJ -/: $/NN 2k/NN $/$ attributes/NNS are/VBP determined/VBN by/IN a/DT majority/NN voting/NN among/IN these/DT local/JJ candidates/NNS ./.
Finally/RB ,/, the/DT full/JJ -/HYPH grained/JJ histograms/NNS of/IN the/DT globally/RB top/JJ -/: $/NN 2k/NN $/$ attributes/NNS are/VBP collected/VBN from/IN local/JJ machines/NNS in/IN order/NN to/TO identify/VB the/DT best/JJS (/-LRB- most/RBS informative/JJ )/-RRB- attribute/NN and/CC its/PRP$ split/NN point/NN ./.
PV/NN -/HYPH Tree/NN can/MD achieve/VB a/DT very/RB low/JJ communication/NN cost/NN (/-LRB- independent/JJ of/IN the/DT total/JJ number/NN of/IN attributes/NNS )/-RRB- and/CC thus/RB can/MD scale/VB out/RP very/RB well/RB ./.
Furthermore/RB ,/, theoretical/JJ analysis/NN shows/VBZ that/IN this/DT algorithm/NN can/MD learn/VB a/DT near/JJ optimal/JJ decision/NN tree/NN ,/, since/IN it/PRP can/MD find/VB the/DT best/JJS attribute/NN with/IN a/DT large/JJ probability/NN ./.
Our/PRP$ experiments/NNS on/IN real/JJ -/HYPH world/NN datasets/NNS show/VBP that/IN PV/NN -/HYPH Tree/NN significantly/RB outperforms/VBZ the/DT existing/VBG parallel/JJ decision/NN tree/NN algorithms/NNS in/IN the/DT trade/NN -/HYPH off/NN between/IN accuracy/NN and/CC efficiency/NN ./.
