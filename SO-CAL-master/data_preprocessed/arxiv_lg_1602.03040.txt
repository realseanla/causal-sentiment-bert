We/PRP present/VBP the/DT Structured/VBN Weighted/JJ Violations/NNS Perceptron/NNP (/-LRB- SWVP/NNP )/-RRB- algorithm/NN ,/, a/DT new/JJ perceptron/NN algorithm/NN for/IN structured/JJ prediction/NN ,/, that/IN generalizes/VBZ the/DT Collins/NNP Structured/NNP Perceptron/NNP (/-LRB- CSP/NNP ,/, (/-LRB- Collins/NNP ,/, 2002/CD )/-RRB- )/-RRB- ./.
Unlike/IN CSP/NNP ,/, the/DT update/NN rule/NN of/IN SWVP/NN explicitly/RB exploits/VBZ the/DT internal/JJ structure/NN of/IN the/DT predicted/VBN labels/NNS ./.
We/PRP prove/VBP that/IN for/IN linearly/RB separable/JJ training/NN sets/NNS ,/, SWVP/NN converges/VBZ to/IN a/DT weight/NN vector/NN that/WDT separates/VBZ the/DT data/NNS ,/, under/IN certain/JJ conditions/NNS on/IN the/DT parameters/NNS of/IN the/DT algorithm/NN ./.
We/PRP further/RB prove/VBP bounds/NNS for/IN SWVP/NN on/IN :/: (/-LRB- a/LS )/-RRB- the/DT number/NN of/IN updates/NNS in/IN the/DT separable/JJ case/NN ;/: (/-LRB- b/LS )/-RRB- mistakes/NNS in/IN the/DT non-separable/JJ case/NN ;/: and/CC (/-LRB- c/LS )/-RRB- the/DT probability/NN to/IN misclassify/VB an/DT unseen/JJ example/NN (/-LRB- generalization/NN )/-RRB- ,/, and/CC show/VBP that/IN for/IN most/JJS SWVP/NN variants/NNS these/DT bounds/NNS are/VBP tighter/JJR than/IN those/DT of/IN the/DT CSP/NNP special/JJ case/NN ./.
In/IN synthetic/JJ data/NNS experiments/NNS where/WRB data/NNS is/VBZ drawn/VBN from/IN a/DT generative/JJ hidden/JJ variable/JJ model/NN ,/, SWVP/NN provides/VBZ substantial/JJ improvements/NNS over/IN CSP/NNP ./.
