We/PRP propose/VBP a/DT technique/NN for/IN learning/VBG representations/NNS of/IN parser/NN states/NNS in/IN transition/NN -/HYPH based/VBN dependency/NN parsers/NNS ./.
Our/PRP$ primary/JJ innovation/NN is/VBZ a/DT new/JJ control/NN structure/NN for/IN sequence/NN -/HYPH to/IN -/HYPH sequence/NN neural/JJ networks/NNS ---/, the/DT stack/NN LSTM/NN ./.
Like/IN the/DT conventional/JJ stack/NN data/NNS structures/NNS used/VBN in/IN transition/NN -/HYPH based/VBN parsing/VBG ,/, elements/NNS can/MD be/VB pushed/VBN to/TO or/CC popped/VBN from/IN the/DT top/NN of/IN the/DT stack/NN in/IN constant/JJ time/NN ,/, but/CC ,/, in/IN addition/NN ,/, an/DT LSTM/NNP maintains/VBZ a/DT continuous/JJ space/NN embedding/NN of/IN the/DT stack/NN contents/NNS ./.
This/DT lets/VBZ us/PRP formulate/VB an/DT efficient/JJ parsing/VBG model/NN that/WDT captures/VBZ three/CD facets/NNS of/IN a/DT parser/NN 's/POS state/NN :/: (/-LRB- i/LS )/-RRB- unbounded/JJ look/NN -/HYPH ahead/NN into/IN the/DT buffer/NN of/IN incoming/JJ words/NNS ,/, (/-LRB- ii/LS )/-RRB- the/DT complete/JJ history/NN of/IN actions/NNS taken/VBN by/IN the/DT parser/NN ,/, and/CC (/-LRB- iii/LS )/-RRB- the/DT complete/JJ contents/NNS of/IN the/DT stack/NN of/IN partially/RB built/VBN tree/NN fragments/NNS ,/, including/VBG their/PRP$ internal/JJ structures/NNS ./.
Standard/JJ backpropagation/NN techniques/NNS are/VBP used/VBN for/IN training/NN and/CC yield/NN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN parsing/VBG performance/NN ./.
