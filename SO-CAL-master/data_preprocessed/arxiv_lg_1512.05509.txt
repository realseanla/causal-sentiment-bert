This/DT paper/NN explores/VBZ the/DT performance/NN of/IN fitted/VBN neural/JJ Q/NN iteration/NN for/IN reinforcement/NN learning/VBG in/IN several/JJ partially/RB observable/JJ environments/NNS ,/, using/VBG three/CD recurrent/JJ neural/JJ network/NN architectures/NNS :/: Long/JJ Short/JJ -/HYPH Term/NN Memory/NN ,/, Gated/VBN Recurrent/JJ Unit/NN and/CC MUT1/NN ,/, a/DT recurrent/JJ neural/JJ architecture/NN evolved/VBD from/IN a/DT pool/NN of/IN several/JJ thousands/NNS candidate/NN architectures/NNS ./.
A/DT variant/NN of/IN fitted/VBN Q/NN iteration/NN ,/, based/VBN on/IN Advantage/NN values/NNS instead/RB of/IN Q/NN values/NNS ,/, is/VBZ also/RB explored/VBN ./.
The/DT results/NNS show/VBP that/IN GRU/NNP performs/VBZ significantly/RB better/JJR than/IN LSTM/NNP and/CC MUT1/NN for/IN most/JJS of/IN the/DT problems/NNS considered/VBN ,/, requiring/VBG less/JJR training/NN episodes/NNS and/CC less/JJR CPU/NN time/NN before/IN learning/VBG a/DT very/RB good/JJ policy/NN ./.
Advantage/NN learning/NN also/RB tends/VBZ to/TO produce/VB better/JJR results/NNS ./.
