We/PRP analyze/VBP in/IN this/DT paper/NN a/DT random/JJ feature/NN map/NN based/VBN on/IN a/DT theory/NN of/IN invariance/NN I/CD -/HYPH theory/NN introduced/VBN recently/RB ./.
More/RBR specifically/RB ,/, a/DT group/NN invariant/JJ signal/NN signature/NN is/VBZ obtained/VBN through/IN cumulative/JJ distributions/NNS of/IN group/NN transformed/VBN random/JJ projections/NNS ./.
Our/PRP$ analysis/NN bridges/NNS invariant/JJ feature/NN learning/NN with/IN kernel/NN methods/NNS ,/, as/IN we/PRP show/VBP that/IN this/DT feature/NN map/NN defines/VBZ an/DT expected/VBN Haar/NN integration/NN kernel/NN that/WDT is/VBZ invariant/JJ to/IN the/DT specified/VBN group/NN action/NN ./.
We/PRP show/VBP how/WRB this/DT non-linear/JJ random/JJ feature/NN map/NN approximates/VBZ this/DT group/NN invariant/JJ kernel/NN uniformly/RB on/IN a/DT set/NN of/IN $/$ N$/CD points/NNS ./.
Moreover/RB ,/, we/PRP show/VBP that/IN it/PRP defines/VBZ a/DT function/NN space/NN that/WDT is/VBZ dense/JJ in/IN the/DT equivalent/JJ Invariant/JJ Reproducing/VBG Kernel/NNP Hilbert/NNP Space/NNP ./.
Finally/RB ,/, we/PRP quantify/VBP error/NN rates/NNS of/IN the/DT convergence/NN of/IN the/DT empirical/JJ risk/NN minimization/NN ,/, as/RB well/RB as/IN the/DT reduction/NN in/IN the/DT sample/NN complexity/NN of/IN a/DT learning/NN algorithm/NN using/VBG such/PDT an/DT invariant/JJ representation/NN for/IN signal/NN classification/NN ,/, in/IN a/DT classical/JJ supervised/JJ learning/NN setting/NN ./.
