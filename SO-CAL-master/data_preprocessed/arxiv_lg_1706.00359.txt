Topic/NN models/NNS have/VBP been/VBN widely/RB explored/VBN as/IN probabilistic/JJ generative/JJ models/NNS of/IN documents/NNS ./.
Traditional/JJ inference/NN methods/NNS have/VBP sought/VBN closed/JJ -/HYPH form/NN derivations/NNS for/IN updating/VBG the/DT models/NNS ,/, however/RB as/IN the/DT expressiveness/NN of/IN these/DT models/NNS grows/VBZ ,/, so/RB does/VBZ the/DT difficulty/NN of/IN performing/VBG fast/RB and/CC accurate/JJ inference/NN over/IN their/PRP$ parameters/NNS ./.
This/DT paper/NN presents/VBZ alternative/JJ neural/JJ approaches/NNS to/IN topic/NN modelling/NN by/IN providing/VBG parameterisable/JJ distributions/NNS over/IN topics/NNS which/WDT permit/VBP training/NN by/IN backpropagation/NN in/IN the/DT framework/NN of/IN neural/JJ variational/JJ inference/NN ./.
In/IN addition/NN ,/, with/IN the/DT help/NN of/IN a/DT stick/NN -/HYPH breaking/VBG construction/NN ,/, we/PRP propose/VBP a/DT recurrent/JJ network/NN that/WDT is/VBZ able/JJ to/TO discover/VB a/DT notionally/RB unbounded/JJ number/NN of/IN topics/NNS ,/, analogous/JJ to/IN Bayesian/JJ non-parametric/JJ topic/NN models/NNS ./.
Experimental/JJ results/NNS on/IN the/DT MXM/NNP Song/NNP Lyrics/NNS ,/, 20NewsGroups/NNS and/CC Reuters/NNP News/NNP datasets/NNS demonstrate/VBP the/DT effectiveness/NN and/CC efficiency/NN of/IN these/DT neural/JJ topic/NN models/NNS ./.
