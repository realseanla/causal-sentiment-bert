Neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- heavily/RB relies/VBZ on/IN word/NN level/NN modelling/NN to/TO learn/VB semantic/JJ representations/NNS of/IN input/NN sentences/NNS ./.
However/RB ,/, for/IN languages/NNS without/IN natural/JJ word/NN delimiters/NNS (/-LRB- e.g./FW ,/, Chinese/JJ )/-RRB- where/WRB input/NN sentences/NNS have/VBP to/TO be/VB tokenized/VBN first/RB ,/, conventional/JJ NMT/NN is/VBZ confronted/VBN with/IN two/CD issues/NNS :/: 1/LS )/-RRB- it/PRP is/VBZ difficult/JJ to/TO find/VB an/DT optimal/JJ tokenization/NN granularity/NN for/IN source/NN sentence/NN modelling/NN ,/, and/CC 2/LS )/-RRB- errors/NNS in/IN 1/CD -/HYPH best/JJS tokenizations/NNS may/MD propagate/VB to/IN the/DT encoder/NN of/IN NMT/NNP ./.
To/TO handle/VB these/DT issues/NNS ,/, we/PRP propose/VBP word/NN -/HYPH lattice/NN based/VBN Recurrent/JJ Neural/JJ Network/NN (/-LRB- RNN/NN )/-RRB- encoders/NNS for/IN NMT/NNP ,/, which/WDT generalize/VBP the/DT standard/JJ RNN/NN to/IN word/NN lattice/NN topology/NN ./.
The/DT proposed/VBN encoders/NNS take/VBP as/IN input/NN a/DT word/NN lattice/NN that/WDT compactly/RB encodes/VBZ multiple/JJ tokenizations/NNS ,/, and/CC learn/VB to/TO generate/VB new/JJ hidden/JJ states/NNS from/IN arbitrarily/RB many/JJ inputs/NNS and/CC hidden/JJ states/NNS in/IN preceding/VBG time/NN steps/NNS ./.
As/IN such/JJ ,/, the/DT word/NN -/HYPH lattice/NN based/VBN encoders/NNS not/RB only/RB alleviate/VB the/DT negative/JJ impact/NN of/IN tokenization/NN errors/NNS but/CC also/RB are/VBP more/RBR expressive/JJ and/CC flexible/JJ to/TO embed/VB input/NN sentences/NNS ./.
Experiment/NN results/NNS on/IN Chinese/JJ -/HYPH English/JJ translation/NN demonstrate/VBP the/DT superiorities/NNS of/IN the/DT proposed/VBN encoders/NNS over/IN the/DT conventional/JJ encoder/NN ./.
