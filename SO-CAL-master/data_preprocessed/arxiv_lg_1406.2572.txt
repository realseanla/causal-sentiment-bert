A/DT central/JJ challenge/NN to/IN many/JJ fields/NNS of/IN science/NN and/CC engineering/NN involves/VBZ minimizing/VBG non-convex/JJ error/NN functions/VBZ over/IN continuous/JJ ,/, high/JJ dimensional/JJ spaces/NNS ./.
Gradient/NN descent/NN or/CC quasi-Newton/NN methods/NNS are/VBP almost/RB ubiquitously/RB used/VBN to/TO perform/VB such/JJ minimizations/NNS ,/, and/CC it/PRP is/VBZ often/RB thought/VBN that/IN a/DT main/JJ source/NN of/IN difficulty/NN for/IN these/DT local/JJ methods/NNS to/TO find/VB the/DT global/JJ minimum/NN is/VBZ the/DT proliferation/NN of/IN local/JJ minima/NN with/IN much/RB higher/JJR error/NN than/IN the/DT global/JJ minimum/NN ./.
Here/RB we/PRP argue/VBP ,/, based/VBN on/IN results/NNS from/IN statistical/JJ physics/NN ,/, random/JJ matrix/NN theory/NN ,/, neural/JJ network/NN theory/NN ,/, and/CC empirical/JJ evidence/NN ,/, that/IN a/DT deeper/JJR and/CC more/JJR profound/JJ difficulty/NN originates/VBZ from/IN the/DT proliferation/NN of/IN saddle/NN points/NNS ,/, not/RB local/JJ minima/NN ,/, especially/RB in/IN high/JJ dimensional/JJ problems/NNS of/IN practical/JJ interest/NN ./.
Such/JJ saddle/NN points/NNS are/VBP surrounded/VBN by/IN high/JJ error/NN plateaus/NN that/WDT can/MD dramatically/RB slow/VB down/RP learning/NN ,/, and/CC give/VB the/DT illusory/JJ impression/NN of/IN the/DT existence/NN of/IN a/DT local/JJ minimum/NN ./.
Motivated/VBN by/IN these/DT arguments/NNS ,/, we/PRP propose/VBP a/DT new/JJ approach/NN to/IN second/JJ -/HYPH order/NN optimization/NN ,/, the/DT saddle/NN -/HYPH free/JJ Newton/NNP method/NN ,/, that/WDT can/MD rapidly/RB escape/VB high/JJ dimensional/JJ saddle/NN points/NNS ,/, unlike/IN gradient/NN descent/NN and/CC quasi-Newton/NN methods/NNS ./.
We/PRP apply/VBP this/DT algorithm/NN to/IN deep/JJ or/CC recurrent/JJ neural/JJ network/NN training/NN ,/, and/CC provide/VB numerical/JJ evidence/NN for/IN its/PRP$ superior/JJ optimization/NN performance/NN ./.
