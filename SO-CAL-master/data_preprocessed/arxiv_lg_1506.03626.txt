Margin/NN -/HYPH Based/VBN Principle/NNP has/VBZ been/VBN proposed/VBN for/IN a/DT long/JJ time/NN ,/, it/PRP has/VBZ been/VBN proved/VBN that/IN this/DT principle/NN could/MD reduce/VB the/DT structural/JJ risk/NN and/CC improve/VB the/DT performance/NN in/IN both/CC theoretical/JJ and/CC practical/JJ aspects/NNS ./.
Meanwhile/RB ,/, feed/NN -/HYPH forward/JJ neural/JJ network/NN is/VBZ a/DT traditional/JJ classifier/NN ,/, which/WDT is/VBZ very/RB hot/JJ at/IN present/JJ with/IN a/DT deeper/JJR architecture/NN ./.
However/RB ,/, the/DT training/NN algorithm/NN of/IN feed/NN -/HYPH forward/JJ neural/JJ network/NN is/VBZ developed/VBN and/CC generated/VBN from/IN Widrow/NNP -/HYPH Hoff/NNP Principle/NNP that/DT means/VBZ to/TO minimize/VB the/DT squared/JJ error/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ training/NN algorithm/NN for/IN feed/NN -/HYPH forward/JJ neural/JJ networks/NNS based/VBN on/IN Margin/NN -/HYPH Based/VBN Principle/NNP ,/, which/WDT could/MD effectively/RB promote/VB the/DT accuracy/NN and/CC generalization/NN ability/NN of/IN neural/JJ network/NN classifiers/NNS with/IN less/JJR labelled/VBN samples/NNS and/CC flexible/JJ network/NN ./.
We/PRP have/VBP conducted/VBN experiments/NNS on/IN four/CD UCI/NNP open/JJ datasets/NNS and/CC achieved/VBD good/JJ results/NNS as/IN expected/VBN ./.
In/IN conclusion/NN ,/, our/PRP$ model/NN could/MD handle/VB more/JJR sparse/JJ labelled/VBN and/CC more/JJR high/JJ -/HYPH dimension/NN dataset/NN in/IN a/DT high/JJ accuracy/NN while/IN modification/NN from/IN old/JJ ANN/NNP method/NN to/IN our/PRP$ method/NN is/VBZ easy/JJ and/CC almost/RB free/JJ of/IN work/NN ./.
