In/IN the/DT last/JJ few/JJ years/NNS ,/, Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNNs/NNS )/-RRB- have/VBP proved/VBN effective/JJ on/IN several/JJ NLP/NN tasks/NNS ./.
Despite/IN such/JJ great/JJ success/NN ,/, their/PRP$ ability/NN to/TO model/VB \/SYM emph/NN {/-LRB- sequence/NN labeling/NN }/-RRB- is/VBZ still/RB limited/JJ ./.
This/DT lead/NN research/NN toward/IN solutions/NNS where/WRB RNNs/NNS are/VBP combined/VBN with/IN models/NNS which/WDT already/RB proved/VBD effective/JJ in/IN this/DT domain/NN ,/, such/JJ as/IN CRFs/NNS ./.
In/IN this/DT work/NN we/PRP propose/VBP a/DT solution/NN far/RB simpler/JJR but/CC very/RB effective/JJ :/: an/DT evolution/NN of/IN the/DT simple/JJ Jordan/NNP RNN/NNP ,/, where/WRB labels/NNS are/VBP re-injected/VBN as/IN input/NN into/IN the/DT network/NN ,/, and/CC converted/VBD into/IN embeddings/NNS ,/, in/IN the/DT same/JJ way/NN as/IN words/NNS ./.
We/PRP compare/VBP this/DT RNN/NN variant/NN to/IN all/PDT the/DT other/JJ RNN/NN models/NNS ,/, Elman/NNP and/CC Jordan/NNP RNN/NNP ,/, LSTM/NNP and/CC GRU/NNP ,/, on/IN two/CD well/RB -/HYPH known/VBN tasks/NNS of/IN Spoken/NNP Language/NNP Understanding/VBG (/-LRB- SLU/NNP )/-RRB- ./.
Thanks/NNS to/IN label/NN embeddings/NNS and/CC their/PRP$ combination/NN at/IN the/DT hidden/JJ layer/NN ,/, the/DT proposed/VBN variant/NN ,/, which/WDT uses/VBZ more/JJR parameters/NNS than/IN Elman/NNP and/CC Jordan/NNP RNNs/NNS ,/, but/CC far/RB fewer/JJR than/IN LSTM/NNP and/CC GRU/NNP ,/, is/VBZ more/RBR effective/JJ than/IN other/JJ RNNs/NNS ,/, but/CC also/RB outperforms/VBZ sophisticated/JJ CRF/NNP models/NNS ./.
