In/IN this/DT work/NN we/PRP consider/VBP the/DT learning/NN setting/VBG where/WRB in/IN addition/NN to/IN the/DT training/NN set/NN ,/, the/DT learner/NN receives/VBZ a/DT collection/NN of/IN auxiliary/JJ hypotheses/NNS originating/VBG from/IN other/JJ tasks/NNS ./.
This/DT paradigm/NN ,/, known/VBN as/IN Hypothesis/NN Transfer/NN Learning/NN (/-LRB- HTL/NN )/-RRB- ,/, has/VBZ been/VBN successfully/RB exploited/VBN in/IN empirical/JJ works/NNS ,/, but/CC only/RB recently/RB has/VBZ received/VBN a/DT theoretical/JJ attention/NN ./.
Here/RB ,/, we/PRP try/VBP to/TO understand/VB when/WRB HTL/NNP facilitates/VBZ accelerated/VBN generalization/NN --/: the/DT goal/NN of/IN the/DT transfer/NN learning/NN paradigm/NN ./.
Thus/RB ,/, we/PRP study/VBP a/DT broad/JJ class/NN of/IN algorithms/NNS ,/, a/DT Hypothesis/NN Transfer/NN Learning/VBG through/IN Regularized/VBN ERM/NNP ,/, that/DT can/MD be/VB instantiated/VBN with/IN any/DT non-negative/JJ smooth/JJ loss/NN function/NN and/CC any/DT strongly/RB convex/JJ regularizer/NN ./.
We/PRP establish/VBP generalization/NN and/CC excess/JJ risk/NN bounds/NNS ,/, showing/VBG that/IN if/IN the/DT algorithm/NN is/VBZ fed/VBN with/IN a/DT good/JJ source/NN hypotheses/NNS combination/NN ,/, generalization/NN happens/VBZ at/IN the/DT fast/JJ rate/NN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- 1/CD //SYM m/NN )/-RRB- $/$ instead/RB of/IN usual/JJ $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- 1/CD //SYM \/SYM sqrt/SYM {/-LRB- m/NN }/-RRB- )/-RRB- $/$ ./.
We/PRP also/RB observe/VBP that/IN if/IN the/DT combination/NN is/VBZ perfect/JJ ,/, our/PRP$ theory/NN formally/RB backs/VBZ up/RP the/DT intuition/NN that/WDT learning/NN is/VBZ not/RB necessary/JJ ./.
On/IN the/DT other/JJ hand/NN ,/, if/IN the/DT source/NN hypotheses/NNS combination/NN is/VBZ a/DT misfit/NN for/IN the/DT target/NN task/NN ,/, we/PRP recover/VBP the/DT usual/JJ learning/NN rate/NN ./.
As/IN a/DT byproduct/NN of/IN our/PRP$ study/NN ,/, we/PRP also/RB prove/VBP a/DT new/JJ bound/VBN on/IN the/DT Rademacher/NNP complexity/NN of/IN the/DT smooth/JJ loss/NN class/NN under/IN weaker/JJR assumptions/NNS compared/VBN to/IN previous/JJ works/NNS ./.
