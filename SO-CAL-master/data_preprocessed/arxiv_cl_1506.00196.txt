Sequence/NN -/HYPH to/IN -/HYPH sequence/NN translation/NN methods/NNS based/VBN on/IN generation/NN with/IN a/DT side/NN -/HYPH conditioned/VBN language/NN model/NN have/VBP recently/RB shown/VBN promising/JJ results/NNS in/IN several/JJ tasks/NNS ./.
In/IN machine/NN translation/NN ,/, models/NNS conditioned/VBN on/IN source/NN side/NN words/NNS have/VBP been/VBN used/VBN to/TO produce/VB target/NN -/HYPH language/NN text/NN ,/, and/CC in/IN image/NN captioning/NN ,/, models/NNS conditioned/VBN images/NNS have/VBP been/VBN used/VBN to/TO generate/VB caption/NN text/NN ./.
Past/JJ work/NN with/IN this/DT approach/NN has/VBZ focused/VBN on/IN large/JJ vocabulary/NN tasks/NNS ,/, and/CC measured/VBN quality/NN in/IN terms/NNS of/IN BLEU/NNP ./.
In/IN this/DT paper/NN ,/, we/PRP explore/VBP the/DT applicability/NN of/IN such/JJ models/NNS to/IN the/DT qualitatively/RB different/JJ grapheme/NN -/HYPH to/IN -/HYPH phoneme/NN task/NN ./.
Here/RB ,/, the/DT input/NN and/CC output/NN side/NN vocabularies/NNS are/VBP small/JJ ,/, plain/JJ n/NN -/HYPH gram/NN models/NNS do/VBP well/RB ,/, and/CC credit/NN is/VBZ only/RB given/VBN when/WRB the/DT output/NN is/VBZ exactly/RB correct/JJ ./.
We/PRP find/VBP that/IN the/DT simple/JJ side/NN -/HYPH conditioned/VBN generation/NN approach/NN is/VBZ able/JJ to/TO rival/VB the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ,/, and/CC we/PRP are/VBP able/JJ to/TO significantly/RB advance/VB the/DT stat/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN with/IN bi-directional/JJ long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- neural/JJ networks/NNS that/WDT use/VBP the/DT same/JJ alignment/NN information/NN that/WDT is/VBZ used/VBN in/IN conventional/JJ approaches/NNS ./.
