Slack/NN and/CC margin/NN rescaling/NN are/VBP variants/NNS of/IN the/DT structured/JJ output/NN SVM/NN ./.
They/PRP define/VBP convex/NN surrogates/NNS to/IN task/NN specific/JJ loss/NN functions/NNS ,/, which/WDT ,/, when/WRB specialized/VBN to/IN non-additive/JJ loss/NN functions/NNS for/IN multi-label/JJ problems/NNS ,/, yield/NN extensions/NNS to/IN increasing/VBG set/VBN functions/NNS ./.
We/PRP demonstrate/VBP in/IN this/DT paper/NN that/WDT we/PRP may/MD use/VB these/DT concepts/NNS to/TO define/VB polynomial/JJ time/NN convex/NN extensions/NNS of/IN arbitrary/JJ supermodular/JJ functions/NNS ./.
We/PRP further/RB show/VBP that/IN slack/NN and/CC margin/NN rescaling/NN can/MD be/VB interpreted/VBN as/IN dominating/VBG convex/NN extensions/NNS over/IN multiplicative/JJ and/CC additive/JJ families/NNS ,/, and/CC that/IN margin/NN rescaling/NN is/VBZ strictly/RB dominated/VBN by/IN slack/NN rescaling/NN ./.
However/RB ,/, we/PRP also/RB demonstrate/VBP that/IN ,/, while/IN the/DT function/NN value/NN and/CC gradient/NN for/IN margin/NN rescaling/NN can/MD be/VB computed/VBN in/IN polynomial/JJ time/NN ,/, the/DT same/JJ for/IN slack/NN rescaling/VBG corresponds/VBZ to/IN a/DT non-supermodular/JJ maximization/NN problem/NN ./.
