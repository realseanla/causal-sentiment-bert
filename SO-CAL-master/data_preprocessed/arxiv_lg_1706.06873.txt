Convolution/NNP is/VBZ a/DT critical/JJ component/NN in/IN modern/JJ deep/JJ neural/JJ networks/NNS ,/, thus/RB several/JJ algorithms/NNS for/IN convolution/NN have/VBP been/VBN developed/VBN ./.
Direct/JJ convolution/NN is/VBZ simple/JJ but/CC suffers/VBZ from/IN poor/JJ performance/NN ./.
As/IN an/DT alternative/NN ,/, multiple/JJ indirect/JJ methods/NNS have/VBP been/VBN proposed/VBN including/VBG im2col/NN -/HYPH based/VBN convolution/NN ,/, FFT/NN -/HYPH based/VBN convolution/NN ,/, or/CC Winograd/NNP -/HYPH based/VBN algorithm/NN ./.
However/RB ,/, all/PDT these/DT indirect/JJ methods/NNS have/VBP high/JJ memory/NN -/HYPH overhead/NN ,/, which/WDT creates/VBZ performance/NN degradation/NN and/CC offers/VBZ a/DT poor/JJ trade/NN -/HYPH off/NN between/IN performance/NN and/CC memory/NN consumption/NN ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT memory/NN -/HYPH efficient/JJ convolution/NN or/CC MEC/NN with/IN compact/JJ lowering/NN ,/, which/WDT reduces/VBZ memory/NN -/HYPH overhead/NN substantially/RB and/CC accelerates/VBZ convolution/NN process/NN ./.
MEC/NNP lowers/VBZ the/DT input/NN matrix/NN in/IN a/DT simple/JJ yet/CC efficient/JJ //HYPH compact/JJ way/NN (/-LRB- i.e./FW ,/, much/RB less/JJR memory/NN -/HYPH overhead/NN )/-RRB- ,/, and/CC then/RB executes/VBZ multiple/JJ small/JJ matrix/NN multiplications/NNS in/IN parallel/JJ to/TO get/VB convolution/NN completed/VBN ./.
Additionally/RB ,/, the/DT reduced/VBN memory/NN footprint/NN improves/VBZ memory/NN sub-system/JJ efficiency/NN ,/, improving/VBG performance/NN ./.
Our/PRP$ experimental/JJ results/NNS show/VBP that/IN MEC/NNP reduces/VBZ memory/NN consumption/NN significantly/RB with/IN good/JJ speedup/NN on/IN both/DT mobile/JJ and/CC server/NN platforms/NNS ,/, compared/VBN with/IN other/JJ indirect/JJ convolution/NN algorithms/NNS ./.
