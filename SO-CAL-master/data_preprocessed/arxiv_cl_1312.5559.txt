There/EX are/VBP two/CD main/JJ approaches/NNS to/IN the/DT distributed/VBN representation/NN of/IN words/NNS :/: low/JJ -/HYPH dimensional/JJ deep/JJ learning/NN embeddings/NNS and/CC high/JJ -/HYPH dimensional/JJ distributional/JJ models/NNS ,/, in/IN which/WDT each/DT dimension/NN corresponds/VBZ to/IN a/DT context/NN word/NN ./.
In/IN this/DT paper/NN ,/, we/PRP combine/VBP these/DT two/CD approaches/NNS by/IN learning/VBG embeddings/NNS based/VBN on/IN distributional/JJ -/HYPH model/NN vectors/NNS -/, as/IN opposed/VBN to/IN one/CD -/HYPH hot/NN vectors/NNS as/RB is/VBZ standardly/RB done/VBN in/IN deep/JJ learning/NN ./.
We/PRP show/VBP that/IN the/DT combined/VBN approach/NN has/VBZ better/JJR performance/NN on/IN a/DT word/NN relatedness/NN judgment/NN task/NN ./.
