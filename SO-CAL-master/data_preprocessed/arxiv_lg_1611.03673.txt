Learning/VBG to/TO navigate/VB in/IN complex/JJ environments/NNS with/IN dynamic/JJ elements/NNS is/VBZ an/DT important/JJ milestone/NN in/IN developing/VBG AI/NN agents/NNS ./.
In/IN this/DT work/NN we/PRP formulate/VBP the/DT navigation/NN question/NN as/IN a/DT reinforcement/NN learning/VBG problem/NN and/CC show/VBP that/IN data/NNS efficiency/NN and/CC task/NN performance/NN can/MD be/VB dramatically/RB improved/VBN by/IN relying/VBG on/IN additional/JJ auxiliary/JJ tasks/NNS to/IN bootstrap/NN learning/NN ./.
In/IN particular/JJ we/PRP consider/VBP jointly/RB learning/VBG the/DT goal/NN -/HYPH driven/VBN reinforcement/NN learning/VBG problem/NN with/IN an/DT unsupervised/JJ depth/NN prediction/NN task/NN and/CC a/DT self/NN -/HYPH supervised/VBN loop/NN closure/NN classification/NN task/NN ./.
Using/VBG this/DT approach/NN we/PRP can/MD learn/VB to/TO navigate/VB from/IN raw/JJ sensory/JJ input/NN in/IN complicated/JJ 3D/NN mazes/NNS ,/, approaching/VBG human/JJ -/HYPH level/NN performance/NN even/RB under/IN conditions/NNS where/WRB the/DT goal/NN location/NN changes/NNS frequently/RB ./.
We/PRP provide/VBP detailed/JJ analysis/NN of/IN the/DT agent/NN behaviour/NN ,/, its/PRP$ ability/NN to/TO localise/VB ,/, and/CC its/PRP$ network/NN activity/NN dynamics/NNS ./.
We/PRP then/RB show/VBP that/IN the/DT agent/NN implicitly/RB learns/VBZ key/JJ navigation/NN abilities/NNS ,/, through/IN reinforcement/NN learning/VBG with/IN sparse/JJ rewards/NNS and/CC without/IN direct/JJ supervision/NN ./.
