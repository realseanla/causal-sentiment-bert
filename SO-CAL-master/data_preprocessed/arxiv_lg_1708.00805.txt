We/PRP develop/VBP an/DT approach/NN to/IN training/NN generative/NN models/NNS based/VBN on/IN unrolling/VBG a/DT variational/JJ auto/NN -/HYPH encoder/NN into/IN a/DT Markov/NNP chain/NN ,/, and/CC shaping/VBG the/DT chain/NN 's/POS trajectories/NNS using/VBG a/DT technique/NN inspired/VBN by/IN recent/JJ work/NN in/IN Approximate/JJ Bayesian/JJ computation/NN ./.
We/PRP show/VBP that/IN the/DT global/JJ minimizer/NN of/IN the/DT resulting/VBG objective/NN is/VBZ achieved/VBN when/WRB the/DT generative/JJ model/NN reproduces/VBZ the/DT target/NN distribution/NN ./.
To/TO allow/VB finer/JJR control/NN over/IN the/DT behavior/NN of/IN the/DT models/NNS ,/, we/PRP add/VBP a/DT regularization/NN term/NN inspired/VBN by/IN techniques/NNS used/VBN for/IN regularizing/VBG certain/JJ types/NNS of/IN policy/NN search/NN in/IN reinforcement/NN learning/NN ./.
We/PRP present/VBP empirical/JJ results/NNS on/IN the/DT MNIST/NN and/CC TFD/NN datasets/NNS which/WDT show/VBP that/IN our/PRP$ approach/NN offers/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN ,/, both/CC quantitatively/RB and/CC from/IN a/DT qualitative/JJ point/NN of/IN view/NN ./.
