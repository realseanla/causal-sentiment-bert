Reducing/VBG bit/NN -/HYPH widths/NNS of/IN weights/NNS ,/, activations/NNS ,/, and/CC gradients/NNS of/IN a/DT Neural/JJ Network/NN can/MD shrink/VB its/PRP$ storage/NN size/NN and/CC memory/NN usage/NN ,/, and/CC also/RB allow/VB for/IN faster/JJR training/NN and/CC inference/NN by/IN exploiting/VBG bitwise/RB operations/NNS ./.
However/RB ,/, previous/JJ attempts/NNS for/IN quantization/NN of/IN RNNs/NNS show/VBP considerable/JJ performance/NN degradation/NN when/WRB using/VBG low/JJ bit/NN -/HYPH width/NN weights/NNS and/CC activations/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP methods/NNS to/TO quantize/VB the/DT structure/NN of/IN gates/NNS and/CC interlinks/NNS in/IN LSTM/NN and/CC GRU/NN cells/NNS ./.
In/IN addition/NN ,/, we/PRP propose/VBP balanced/JJ quantization/NN methods/NNS for/IN weights/NNS to/TO further/RB reduce/VB performance/NN degradation/NN ./.
Experiments/NNS on/IN PTB/NNP and/CC IMDB/NNP datasets/NNS confirm/VBP effectiveness/NN of/IN our/PRP$ methods/NNS as/IN performances/NNS of/IN our/PRP$ models/NNS match/VBP or/CC surpass/VBP the/DT previous/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN of/IN quantized/VBN RNN/NN ./.
