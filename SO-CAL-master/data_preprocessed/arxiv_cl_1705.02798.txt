Recently/RB ,/, several/JJ end/NN -/HYPH to/IN -/HYPH end/NN neural/JJ models/NNS have/VBP been/VBN proposed/VBN for/IN machine/NN comprehension/NN tasks/NNS ./.
Typically/RB ,/, these/DT models/NNS use/VBP attention/NN mechanisms/NNS to/TO capture/VB the/DT complicated/JJ interaction/NN between/IN the/DT context/NN and/CC the/DT query/NN and/CC then/RB point/VB the/DT boundary/NN of/IN answer/NN ./.
To/TO better/RBR point/VB the/DT correct/JJ answer/NN ,/, we/PRP introduce/VBP the/DT Mnemonic/NNP Reader/NNP for/IN machine/NN comprehension/NN tasks/NNS ,/, which/WDT enhance/VBP the/DT attention/NN reader/NN in/IN two/CD aspects/NNS ./.
Firstly/RB ,/, we/PRP use/VBP a/DT self/NN -/HYPH alignment/NN attention/NN to/IN model/NN the/DT long/JJ -/HYPH distance/NN dependency/NN among/IN context/NN words/NNS ,/, and/CC obtain/VB query/NN -/HYPH aware/JJ and/CC self/NN -/HYPH aware/JJ contextual/JJ representation/NN for/IN each/DT word/NN in/IN the/DT context/NN ./.
Second/RB ,/, we/PRP use/VBP a/DT memory/NN -/HYPH based/VBN query/NN -/HYPH dependent/JJ pointer/NN to/TO predict/VB the/DT answer/NN ,/, which/WDT integrates/VBZ both/CC explicit/JJ and/CC implicit/JJ query/NN information/NN ,/, such/JJ as/IN query/NN category/NN ./.
Our/PRP$ experimental/JJ evaluations/NNS show/VBP that/IN our/PRP$ model/NN obtains/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN result/NN on/IN the/DT large/JJ -/HYPH scale/NN machine/NN comprehension/NN benchmarks/NNS SQuAD/NN ./.
