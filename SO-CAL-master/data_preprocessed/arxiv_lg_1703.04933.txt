Despite/IN their/PRP$ overwhelming/JJ capacity/NN to/TO overfit/VB ,/, deep/JJ learning/NN architectures/NNS tend/VBP to/TO generalize/VB relatively/RB well/RB to/IN unseen/JJ data/NNS ,/, allowing/VBG them/PRP to/TO be/VB deployed/VBN in/IN practice/NN ./.
However/RB ,/, explaining/VBG why/WRB this/DT is/VBZ the/DT case/NN is/VBZ still/RB an/DT open/JJ area/NN of/IN research/NN ./.
One/CD standing/VBG hypothesis/NN that/IN is/VBZ gaining/VBG popularity/NN ,/, e.g./FW Hochreiter/NNP &amp;/CC Schmidhuber/NNP (/-LRB- 1997/CD )/-RRB- ;/: Keskar/NNP et/FW al./FW (/-LRB- 2017/CD )/-RRB- ,/, is/VBZ that/IN the/DT flatness/NN of/IN minima/NN of/IN the/DT loss/NN function/NN found/VBN by/IN stochastic/JJ gradient/NN based/VBN methods/NNS results/VBZ in/IN good/JJ generalization/NN ./.
This/DT paper/NN argues/VBZ that/IN most/JJS notions/NNS of/IN flatness/NN are/VBP problematic/JJ for/IN deep/JJ models/NNS and/CC can/MD not/RB be/VB directly/RB applied/VBN to/TO explain/VB generalization/NN ./.
Specifically/RB ,/, when/WRB focusing/VBG on/IN deep/JJ networks/NNS with/IN rectifier/NN units/NNS ,/, we/PRP can/MD exploit/VB the/DT particular/JJ geometry/NN of/IN parameter/NN space/NN induced/VBN by/IN the/DT inherent/JJ symmetries/NNS that/WDT these/DT architectures/NNS exhibit/VBP to/TO build/VB equivalent/JJ models/NNS corresponding/VBG to/IN arbitrarily/RB sharper/JJR minima/NN ./.
Furthermore/RB ,/, if/IN we/PRP allow/VBP to/TO reparametrize/VB a/DT function/NN ,/, the/DT geometry/NN of/IN its/PRP$ parameters/NNS can/MD change/VB drastically/RB without/IN affecting/VBG its/PRP$ generalization/NN properties/NNS ./.
