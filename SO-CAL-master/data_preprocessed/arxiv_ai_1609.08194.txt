We/PRP introduce/VBP an/DT online/JJ neural/JJ sequence/NN to/IN sequence/NN model/NN that/WDT learns/VBZ to/IN alternate/JJ between/IN encoding/VBG and/CC decoding/VBG segments/NNS of/IN the/DT input/NN as/IN it/PRP is/VBZ read/VBN ./.
By/IN independently/RB tracking/VBG the/DT encoding/NN and/CC decoding/NN representations/NNS our/PRP$ algorithm/NN permits/VBZ exact/JJ polynomial/JJ marginalization/NN of/IN the/DT latent/JJ segmentation/NN during/IN training/NN ,/, and/CC during/IN decoding/NN beam/NN search/NN is/VBZ employed/VBN to/TO find/VB the/DT best/JJS alignment/NN path/NN together/RB with/IN the/DT predicted/VBN output/NN sequence/NN ./.
Our/PRP$ model/NN tackles/VBZ the/DT bottleneck/NN of/IN vanilla/NN encoder/NN -/HYPH decoders/NNS that/WDT have/VBP to/TO read/VB and/CC memorize/VB the/DT entire/JJ input/NN sequence/NN in/IN their/PRP$ fixed/VBN -/HYPH length/NN hidden/JJ states/NNS before/IN producing/VBG any/DT output/NN ./.
It/PRP is/VBZ different/JJ from/IN previous/JJ attentive/JJ models/NNS in/IN that/DT ,/, instead/RB of/IN treating/VBG the/DT attention/NN weights/NNS as/IN output/NN of/IN a/DT deterministic/JJ function/NN ,/, our/PRP$ model/NN assigns/VBZ attention/NN weights/NNS to/IN a/DT sequential/JJ latent/JJ variable/NN which/WDT can/MD be/VB marginalized/VBN out/RP and/CC permits/VBZ online/JJ generation/NN ./.
Experiments/NNS on/IN abstractive/JJ sentence/NN summarization/NN and/CC morphological/JJ inflection/NN show/VBP significant/JJ performance/NN gains/NNS over/IN the/DT baseline/NN encoder/NN -/HYPH decoders/NNS ./.
