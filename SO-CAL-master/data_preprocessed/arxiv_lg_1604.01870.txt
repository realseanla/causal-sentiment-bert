We/PRP study/VBP the/DT stochastic/JJ optimization/NN of/IN canonical/JJ correlation/NN analysis/NN (/-LRB- CCA/NN )/-RRB- ,/, whose/WP$ objective/NN is/VBZ nonconvex/JJ and/CC does/VBZ not/RB decouple/VB over/IN training/NN samples/NNS ./.
Although/IN several/JJ stochastic/JJ optimization/NN algorithms/NNS have/VBP been/VBN recently/RB proposed/VBN to/TO solve/VB this/DT problem/NN ,/, no/DT global/JJ convergence/NN guarantee/NN was/VBD provided/VBN by/IN any/DT of/IN them/PRP ./.
Based/VBN on/IN the/DT alternating/VBG least/JJS squares/NNS formulation/NN of/IN CCA/NN ,/, we/PRP propose/VBP a/DT globally/RB convergent/JJ stochastic/JJ algorithm/NN ,/, which/WDT solves/VBZ the/DT resulting/VBG least/JJS squares/NNS problems/NNS approximately/RB to/IN sufficient/JJ accuracy/NN with/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN stochastic/JJ gradient/NN methods/NNS for/IN convex/NN optimization/NN ./.
We/PRP provide/VBP the/DT overall/JJ time/NN complexity/NN of/IN our/PRP$ algorithm/NN which/WDT significantly/RB improves/VBZ upon/IN that/DT of/IN previous/JJ work/NN ./.
Experimental/JJ results/NNS demonstrate/VBP the/DT superior/JJ performance/NN of/IN our/PRP$ algorithm/NN ./.
