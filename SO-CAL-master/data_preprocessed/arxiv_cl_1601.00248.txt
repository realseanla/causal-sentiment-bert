Perplexity/NN (/-LRB- per/IN word/NN )/-RRB- is/VBZ the/DT most/RBS widely/RB used/VBN metric/JJ for/IN evaluating/VBG language/NN models/NNS ./.
This/DT is/VBZ mostly/RB due/JJ to/IN a/DT its/PRP$ ease/NN of/IN computation/NN ,/, lack/NN of/IN dependence/NN on/IN external/JJ tools/NNS like/IN speech/NN recognition/NN pipeline/NN and/CC a/DT good/JJ theoretical/JJ justification/NN for/IN why/WRB it/PRP should/MD work/VB ./.
Despite/IN this/DT ,/, there/EX has/VBZ been/VBN no/DT dearth/NN of/IN criticism/NN for/IN this/DT metric/JJ ./.
Most/JJS of/IN this/DT criticism/NN center/NN around/IN lack/NN of/IN correlation/NN with/IN extrinsic/JJ metrics/NNS like/IN word/NN error/NN rate/NN (/-LRB- WER/NN )/-RRB- ,/, dependence/NN upon/IN shared/VBN vocabulary/NN for/IN model/NN comparison/NN and/CC unsuitability/NN for/IN un-normalized/JJ language/NN model/NN evaluation/NN ./.
In/IN this/DT paper/NN we/PRP address/VBP the/DT last/JJ problem/NN of/IN inability/NN to/TO evaluate/VB un-normalized/JJ models/NNS by/IN introducing/VBG a/DT new/JJ discriminative/JJ evaluation/NN metric/NN that/WDT predicts/VBZ model/NN 's/POS performance/NN based/VBN on/IN its/PRP$ ability/NN to/TO discriminate/VB between/IN test/NN sentences/NNS and/CC their/PRP$ deformed/VBN version/NN ./.
Due/IN to/IN its/PRP$ discriminative/JJ formulation/NN ,/, this/DT approach/NN can/MD work/VB with/IN un-normalized/JJ probabilities/NNS while/IN retaining/VBG perplexity/NN 's/POS ease/NN of/IN computation/NN ./.
We/PRP show/VBP a/DT strong/JJ correlation/NN between/IN our/PRP$ new/JJ metric/JJ and/CC perplexity/NN across/IN a/DT range/NN of/IN models/NNS on/IN WSJ/NNP datasets/NNS ./.
We/PRP also/RB hypothesize/VBP a/DT stronger/JJR correlation/NN between/IN WER/NNP and/CC our/PRP$ new/JJ metric/JJ vis/FW -/HYPH a-vis/FW perplexity/NN due/IN to/IN similar/JJ discriminative/JJ objective/NN ./.
