We/PRP present/VBP a/DT new/JJ method/NN of/IN learning/VBG control/NN policies/NNS that/WDT successfully/RB operate/VBP under/IN unknown/JJ dynamic/JJ models/NNS ./.
We/PRP create/VBP such/JJ policies/NNS by/IN leveraging/VBG a/DT large/JJ number/NN of/IN training/NN examples/NNS that/WDT are/VBP generated/VBN using/VBG a/DT physical/JJ simulator/NN ./.
Our/PRP$ system/NN is/VBZ made/VBN of/IN two/CD components/NNS :/: a/DT Universal/NNP Policy/NNP (/-LRB- UP/NN )/-RRB- and/CC a/DT function/NN for/IN Online/NNP System/NNP Identification/NNP (/-LRB- OSI/NNP )/-RRB- ./.
We/PRP describe/VBP our/PRP$ control/NN policy/NN as/IN universal/JJ because/IN it/PRP is/VBZ trained/VBN over/IN a/DT wide/JJ array/NN of/IN dynamic/JJ models/NNS ./.
These/DT variations/NNS in/IN the/DT dynamic/JJ model/NN may/MD include/VB differences/NNS in/IN mass/NN and/CC inertia/NN of/IN the/DT robots/NNS '/POS components/NNS ,/, variable/JJ friction/NN coefficients/NNS ,/, or/CC unknown/JJ mass/NN of/IN an/DT object/NN to/TO be/VB manipulated/VBN ./.
By/IN training/VBG the/DT Universal/NNP Policy/NNP with/IN this/DT variation/NN ,/, the/DT control/NN policy/NN is/VBZ prepared/VBN for/IN a/DT wider/JJR array/NN of/IN possible/JJ conditions/NNS when/WRB executed/VBN in/IN an/DT unknown/JJ environment/NN ./.
The/DT second/JJ part/NN of/IN our/PRP$ system/NN uses/VBZ the/DT recent/JJ state/NN and/CC action/NN history/NN of/IN the/DT system/NN to/TO predict/VB the/DT dynamics/NNS model/NN parameters/NNS mu/NNS ./.
The/DT value/NN of/IN mu/NNS from/IN the/DT Online/NNP System/NNP Identification/NNP is/VBZ then/RB provided/VBN as/IN input/NN to/IN the/DT control/NN policy/NN (/-LRB- along/IN with/IN the/DT system/NN state/NN )/-RRB- ./.
Together/RB ,/, UP/NNP -/HYPH OSI/NNP is/VBZ a/DT robust/JJ control/NN policy/NN that/WDT can/MD be/VB used/VBN across/IN a/DT wide/JJ range/NN of/IN dynamic/JJ models/NNS ,/, and/CC that/DT is/VBZ also/RB responsive/JJ to/IN sudden/JJ changes/NNS in/IN the/DT environment/NN ./.
We/PRP have/VBP evaluated/VBN the/DT performance/NN of/IN this/DT system/NN on/IN a/DT variety/NN of/IN tasks/NNS ,/, including/VBG the/DT problem/NN of/IN cart/NN -/HYPH pole/NN swing/NN -/HYPH up/NN ,/, the/DT double/JJ inverted/VBN pendulum/NN ,/, locomotion/NN of/IN a/DT hopper/NN ,/, and/CC block/NN -/HYPH throwing/NN of/IN a/DT manipulator/NN ./.
UP/NN -/: OSI/NNP is/VBZ effective/JJ at/IN these/DT tasks/NNS across/IN a/DT wide/JJ range/NN of/IN dynamic/JJ models/NNS ./.
Moreover/RB ,/, when/WRB tested/VBN with/IN dynamic/JJ models/NNS outside/IN of/IN the/DT training/NN range/NN ,/, UP/NNP -/HYPH OSI/NNP outperforms/VBZ the/DT Universal/NNP Policy/NNP alone/RB ,/, even/RB when/WRB UP/NN is/VBZ given/VBN the/DT actual/JJ value/NN of/IN the/DT model/NN dynamics/NNS ./.
In/IN addition/NN to/IN the/DT benefits/NNS of/IN creating/VBG more/JJR robust/JJ controllers/NNS ,/, UP/NNP -/HYPH OSI/NNP also/RB holds/VBZ out/RP promise/NN of/IN narrowing/VBG the/DT Reality/NN Gap/NN between/IN simulated/JJ and/CC real/JJ physical/JJ systems/NNS ./.
