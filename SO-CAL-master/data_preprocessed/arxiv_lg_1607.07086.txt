We/PRP present/VBP an/DT approach/NN to/IN training/NN neural/JJ networks/NNS to/TO generate/VB sequences/NNS using/VBG actor/NN -/HYPH critic/NN methods/NNS from/IN reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- ./.
Current/JJ log/NN -/HYPH likelihood/NN training/NN methods/NNS are/VBP limited/VBN by/IN the/DT discrepancy/NN between/IN their/PRP$ training/NN and/CC testing/NN modes/NNS ,/, as/IN models/NNS must/MD generate/VB tokens/NNS conditioned/VBN on/IN their/PRP$ previous/JJ guesses/NNS rather/RB than/IN the/DT ground/NN -/HYPH truth/NN tokens/NNS ./.
We/PRP address/VBP this/DT problem/NN by/IN introducing/VBG a/DT \/NN textit/NN {/-LRB- critic/NN }/-RRB- network/NN that/WDT is/VBZ trained/VBN to/TO predict/VB the/DT value/NN of/IN an/DT output/NN token/NN ,/, given/VBN the/DT policy/NN of/IN an/DT \/NN textit/NN {/-LRB- actor/NN }/-RRB- network/NN ./.
This/DT results/VBZ in/IN a/DT training/NN procedure/NN that/WDT is/VBZ much/RB closer/JJR to/IN the/DT test/NN phase/NN ,/, and/CC allows/VBZ us/PRP to/TO directly/RB optimize/VB for/IN a/DT task/NN -/HYPH specific/JJ score/NN such/JJ as/IN BLEU/NN ./.
Crucially/RB ,/, since/IN we/PRP leverage/VBP these/DT techniques/NNS in/IN the/DT supervised/JJ learning/NN setting/VBG rather/RB than/IN the/DT traditional/JJ RL/NN setting/NN ,/, we/PRP condition/VBP the/DT critic/NN network/NN on/IN the/DT ground/NN -/HYPH truth/NN output/NN ./.
We/PRP show/VBP that/IN our/PRP$ method/NN leads/VBZ to/IN improved/VBN performance/NN on/IN both/DT a/DT synthetic/JJ task/NN ,/, and/CC for/IN German/JJ -/HYPH English/JJ machine/NN translation/NN ./.
Our/PRP$ analysis/NN paves/VBZ the/DT way/NN for/IN such/JJ methods/NNS to/TO be/VB applied/VBN in/IN natural/JJ language/NN generation/NN tasks/NNS ,/, such/JJ as/IN machine/NN translation/NN ,/, caption/NN generation/NN ,/, and/CC dialogue/NN modelling/NN ./.
