We/PRP present/VBP a/DT novel/NN and/CC scalable/JJ label/NN embedding/NN framework/NN for/IN large/JJ -/HYPH scale/NN multi-label/JJ learning/NN a.k.a/IN ExMLDS/NN (/-LRB- Extreme/NNP Multi-Label/NNP Learning/NNP using/VBG Distributional/NNP Semantics/NNP )/-RRB- ./.
Our/PRP$ approach/NN draws/VBZ inspiration/NN from/IN ideas/NNS rooted/VBN in/IN distributional/JJ semantics/NNS ,/, specifically/RB the/DT Skip/VB Gram/NNP Negative/JJ Sampling/NN (/-LRB- SGNS/NN )/-RRB- approach/NN ,/, widely/RB used/VBN to/TO learn/VB word/NN embeddings/NNS for/IN natural/JJ language/NN processing/NN tasks/NNS ./.
Learning/VBG such/JJ embeddings/NNS can/MD be/VB reduced/VBN to/IN a/DT certain/JJ matrix/NN factorization/NN ./.
Our/PRP$ approach/NN is/VBZ novel/JJ in/IN that/IN it/PRP highlights/VBZ interesting/JJ connections/NNS between/IN label/NN embedding/NN methods/NNS used/VBN for/IN multi-label/JJ learning/NN and/CC paragraph/NN //HYPH document/NN embedding/NN methods/NNS commonly/RB used/VBN for/IN learning/VBG representations/NNS of/IN text/NN data/NNS ./.
The/DT framework/NN can/MD also/RB be/VB easily/RB extended/VBN to/TO incorporate/VB auxiliary/JJ information/NN such/JJ as/IN label/NN -/HYPH label/NN correlations/NNS ;/: this/DT is/VBZ crucial/JJ especially/RB when/WRB there/EX are/VBP a/DT lot/NN of/IN missing/VBG labels/NNS in/IN the/DT training/NN data/NNS ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ approach/NN through/IN an/DT extensive/JJ set/NN of/IN experiments/NNS on/IN a/DT variety/NN of/IN benchmark/NN datasets/NNS ,/, and/CC show/VBP that/IN the/DT proposed/VBN learning/NN methods/NNS perform/VBP favorably/RB compared/VBN to/IN several/JJ baselines/NNS and/CC state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS for/IN large/JJ -/HYPH scale/NN multi-label/JJ learning/NN ./.
