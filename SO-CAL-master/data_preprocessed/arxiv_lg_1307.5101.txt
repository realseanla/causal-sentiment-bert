Multi-label/JJ classification/NN problems/NNS abound/VBP in/IN practice/NN ;/: as/IN a/DT result/NN ,/, many/JJ methods/NNS have/VBP recently/RB been/VBN proposed/VBN for/IN these/DT problems/NNS ./.
However/RB ,/, there/EX are/VBP two/CD key/JJ challenges/NNS that/WDT have/VBP not/RB been/VBN adequately/RB addressed/VBN :/: (/-LRB- a/LS )/-RRB- the/DT number/NN of/IN labels/NNS can/MD be/VB numerous/JJ ,/, for/IN example/NN ,/, in/IN the/DT millions/NNS ,/, and/CC (/-LRB- b/LS )/-RRB- the/DT test/NN data/NNS can/MD be/VB riddled/VBN with/IN missing/VBG labels/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP study/VBP a/DT generic/JJ framework/NN for/IN multi-label/JJ classification/NN that/WDT directly/RB addresses/VBZ the/DT above/JJ challenges/NNS ./.
In/IN particular/JJ ,/, we/PRP pose/VBP the/DT problem/NN as/IN one/CD of/IN empirical/JJ risk/NN minimization/NN ,/, where/WRB the/DT prediction/NN function/NN is/VBZ parameterized/VBN by/IN a/DT low/JJ -/HYPH rank/NN matrix/NN ./.
We/PRP show/VBP that/IN our/PRP$ approach/NN derives/VBZ several/JJ existing/VBG label/NN -/HYPH compression/NN based/VBN algorithms/NNS (/-LRB- such/JJ as/IN the/DT recently/RB proposed/VBN CPLST/NN method/NN (/-LRB- Chen/NNP and/CC Lin/NNP ,/, 2012/CD )/-RRB- in/IN a/DT principled/JJ manner/NN ./.
A/DT key/JJ facet/NN of/IN our/PRP$ approach/NN is/VBZ that/IN we/PRP handle/VBP missing/VBG labels/NNS in/IN the/DT training/NN set/VBN by/IN applying/VBG techniques/NNS from/IN the/DT domain/NN of/IN matrix/NN completion/NN ./.
To/TO develop/VB a/DT scalable/JJ algorithm/NN that/WDT can/MD handle/VB a/DT larger/JJR number/NN of/IN classes/NNS ,/, we/PRP use/VBP the/DT alternating/VBG minimization/NN method/NN to/TO find/VB the/DT low/JJ -/HYPH rank/NN parameter/NN matrix/NN ./.
Furthermore/RB ,/, for/IN the/DT special/JJ case/NN of/IN $/$ L_2/CD $/$ loss/NN ,/, we/PRP show/VBP that/IN special/JJ structure/NN in/IN the/DT problem/NN can/MD be/VB exploited/VBN and/CC the/DT alternating/VBG minimization/NN algorithm/NN can/MD be/VB efficiently/RB implemented/VBN ./.
Finally/RB ,/, we/PRP present/VBP empirical/JJ results/NNS on/IN a/DT variety/NN of/IN benchmark/NN datasets/NNS and/CC show/VBP that/IN our/PRP$ methods/NNS perform/VBP significantly/RB better/JJR than/IN existing/VBG label/NN compression/NN based/VBN methods/NNS ./.
Moreover/RB ,/, we/PRP demonstrate/VBP scalability/NN of/IN our/PRP$ approach/NN by/IN applying/VBG it/PRP to/IN a/DT large/JJ Wikipedia/NNP based/VBN dataset/NN that/WDT has/VBZ 117,564/CD training/NN data/NNS instances/NNS and/CC 207,386/CD labels/NNS ./.
