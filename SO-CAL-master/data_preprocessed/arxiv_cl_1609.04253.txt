Machine/NN transliteration/NN is/VBZ the/DT process/NN of/IN automatically/RB transforming/VBG the/DT script/NN of/IN a/DT word/NN from/IN a/DT source/NN language/NN to/IN a/DT target/NN language/NN ,/, while/IN preserving/VBG pronunciation/NN ./.
Sequence/NN to/IN sequence/NN learning/NN has/VBZ recently/RB emerged/VBN as/IN a/DT new/JJ paradigm/NN in/IN supervised/JJ learning/NN ./.
In/IN this/DT paper/NN a/DT character/NN -/HYPH based/VBN encoder/NN -/HYPH decoder/NN model/NN has/VBZ been/VBN proposed/VBN that/IN consists/VBZ of/IN two/CD Recurrent/JJ Neural/JJ Networks/NNS ./.
The/DT encoder/NN is/VBZ a/DT Bidirectional/JJ recurrent/JJ neural/JJ network/NN that/WDT encodes/VBZ a/DT sequence/NN of/IN symbols/NNS into/IN a/DT fixed/VBN -/HYPH length/NN vector/NN representation/NN ,/, and/CC the/DT decoder/NN generates/VBZ the/DT target/NN sequence/NN using/VBG an/DT attention/NN -/HYPH based/VBN recurrent/JJ neural/JJ network/NN ./.
The/DT encoder/NN ,/, the/DT decoder/NN and/CC the/DT attention/NN mechanism/NN are/VBP jointly/RB trained/VBN to/TO maximize/VB the/DT conditional/JJ probability/NN of/IN a/DT target/NN sequence/NN given/VBN a/DT source/NN sequence/NN ./.
Our/PRP$ experiments/NNS on/IN different/JJ datasets/NNS show/VBP that/IN the/DT proposed/VBN encoder/NN -/HYPH decoder/NN model/NN is/VBZ able/JJ to/TO achieve/VB significantly/RB higher/JJR transliteration/NN quality/NN over/IN traditional/JJ statistical/JJ models/NNS ./.
