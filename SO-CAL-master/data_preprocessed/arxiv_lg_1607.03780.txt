Distributional/JJ semantics/NNS creates/VBZ vector/NN -/HYPH space/NN representations/NNS that/WDT capture/VBP many/JJ forms/NNS of/IN semantic/JJ similarity/NN ,/, but/CC their/PRP$ relation/NN to/IN semantic/JJ entailment/NN has/VBZ been/VBN less/RBR clear/JJ ./.
We/PRP propose/VBP a/DT vector/NN -/HYPH space/NN model/NN which/WDT provides/VBZ a/DT formal/JJ foundation/NN for/IN a/DT distributional/JJ semantics/NNS of/IN entailment/NN ./.
Using/VBG a/DT mean/NN -/HYPH field/NN approximation/NN ,/, we/PRP develop/VBP approximate/JJ inference/NN procedures/NNS and/CC entailment/NN operators/NNS over/IN vectors/NNS of/IN probabilities/NNS of/IN features/NNS being/VBG known/VBN (/-LRB- versus/CC unknown/JJ )/-RRB- ./.
We/PRP use/VBP this/DT framework/NN to/IN reinterpret/VB an/DT existing/VBG distributional/JJ -/HYPH semantic/JJ model/NN (/-LRB- Word2Vec/NN )/-RRB- as/IN approximating/VBG an/DT entailment/NN -/HYPH based/VBN model/NN of/IN the/DT distributions/NNS of/IN words/NNS in/IN contexts/NNS ,/, thereby/RB predicting/VBG lexical/JJ entailment/NN relations/NNS ./.
In/IN both/DT unsupervised/JJ and/CC semi-supervised/JJ experiments/NNS on/IN hyponymy/NN detection/NN ,/, we/PRP get/VBP substantial/JJ improvements/NNS over/IN previous/JJ results/NNS ./.
