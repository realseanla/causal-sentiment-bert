Recursive/JJ neural/JJ networks/NNS (/-LRB- RNN/NN )/-RRB- and/CC their/PRP$ recently/RB proposed/VBN extension/NN recursive/JJ long/JJ short/JJ term/NN memory/NN networks/NNS (/-LRB- RLSTM/NNP )/-RRB- are/VBP models/NNS that/WDT compute/VBP representations/NNS for/IN sentences/NNS ,/, by/IN recursively/RB combining/VBG word/NN embeddings/NNS according/VBG to/IN an/DT externally/RB provided/VBN parse/VB tree/NN ./.
Both/DT models/NNS thus/RB ,/, unlike/IN recurrent/JJ networks/NNS ,/, explicitly/RB make/VB use/NN of/IN the/DT hierarchical/JJ structure/NN of/IN a/DT sentence/NN ./.
In/IN this/DT paper/NN ,/, we/PRP demonstrate/VBP that/IN RNNs/NNS nevertheless/RB suffer/VBP from/IN the/DT vanishing/VBG gradient/NN and/CC long/JJ distance/NN dependency/NN problem/NN ,/, and/CC that/IN RLSTMs/NNPS greatly/RB improve/VBP over/IN RNN/NNP 's/POS on/IN these/DT problems/NNS ./.
We/PRP present/VBP an/DT artificial/JJ learning/NN task/NN that/WDT allows/VBZ us/PRP to/TO quantify/VB the/DT severity/NN of/IN these/DT problems/NNS for/IN both/DT models/NNS ./.
We/PRP further/RB show/VBP that/IN a/DT ratio/NN of/IN gradients/NNS (/-LRB- at/IN the/DT root/NN node/NN and/CC a/DT focal/JJ leaf/NN node/NN )/-RRB- is/VBZ highly/RB indicative/JJ of/IN the/DT success/NN of/IN backpropagation/NN at/IN optimizing/VBG the/DT relevant/JJ weights/NNS low/JJ in/IN the/DT tree/NN ./.
This/DT paper/NN thus/RB provides/VBZ an/DT explanation/NN for/IN existing/VBG ,/, superior/JJ results/NNS of/IN RLSTMs/NNS on/IN tasks/NNS such/JJ as/IN sentiment/NN analysis/NN ,/, and/CC suggests/VBZ that/IN the/DT benefits/NNS of/IN including/VBG hierarchical/JJ structure/NN and/CC of/IN including/VBG LSTM/NN -/HYPH style/NN gating/NN are/VBP complementary/JJ ./.
