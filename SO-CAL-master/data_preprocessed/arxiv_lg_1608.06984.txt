There/EX is/VBZ evidence/NN that/IN humans/NNS can/MD be/VB more/RBR efficient/JJ than/IN existing/VBG algorithms/NNS at/IN searching/VBG for/IN good/JJ solutions/NNS in/IN high/JJ -/HYPH dimensional/JJ and/CC non-convex/JJ design/NN or/CC control/NN spaces/NNS ,/, potentially/RB due/IN to/IN our/PRP$ prior/JJ knowledge/NN and/CC learning/NN capability/NN ./.
This/DT work/NN attempts/VBZ to/TO quantify/VB the/DT search/NN strategy/NN of/IN human/JJ beings/NNS to/TO enhance/VB a/DT Bayesian/JJ optimization/NN (/-LRB- BO/NN )/-RRB- algorithm/NN for/IN an/DT optimal/JJ design/NN and/CC control/NN problem/NN ./.
We/PRP consider/VBP the/DT sequence/NN of/IN human/JJ solutions/NNS (/-LRB- called/VBN a/DT search/NN trajectory/NN )/-RRB- as/IN generated/VBN from/IN BO/NNP ,/, and/CC propose/VB to/TO recover/VB the/DT algorithmic/JJ parameters/NNS of/IN BO/NNP through/IN maximum/JJ likelihood/NN estimation/NN ./.
The/DT method/NN is/VBZ first/JJ verified/VBD through/IN simulation/NN studies/NNS and/CC then/RB applied/VBD to/IN human/JJ solutions/NNS crowdsourced/VBN from/IN a/DT gamified/VBN design/NN problem/NN ./.
We/PRP learn/VBP BO/NNP parameters/NNS from/IN a/DT player/NN who/WP achieved/VBD fast/JJ improvement/NN in/IN his/PRP$ //SYM her/PRP$ solutions/NNS and/CC show/VBP that/IN applying/VBG the/DT learned/VBN parameters/NNS to/IN BO/NNP achieves/VBZ better/JJR convergence/NN than/IN using/VBG a/DT self/NN -/HYPH adaptive/JJ BO/NN ./.
The/DT proposed/JJ method/NN is/VBZ different/JJ from/IN inverse/JJ reinforcement/NN learning/VBG in/IN that/IN it/PRP only/RB requires/VBZ a/DT good/JJ search/NN strategy/NN ,/, rather/RB than/IN near/IN -/HYPH optimal/JJ solutions/NNS from/IN humans/NNS ./.
