Deep/JJ conditional/JJ generative/JJ models/NNS are/VBP developed/VBN to/IN simultaneously/RB learn/VB the/DT temporal/JJ dependencies/NNS of/IN multiple/JJ sequences/NNS ./.
The/DT model/NN is/VBZ designed/VBN by/IN introducing/VBG a/DT three/CD -/HYPH way/NN weight/NN tensor/NN to/TO capture/VB the/DT multiplicative/JJ interactions/NNS between/IN side/NN information/NN and/CC sequences/NNS ./.
The/DT proposed/VBN model/NN builds/VBZ on/IN the/DT Temporal/JJ Sigmoid/NN Belief/NNP Network/NNP (/-LRB- TSBN/NNP )/-RRB- ,/, a/DT sequential/JJ stack/NN of/IN Sigmoid/NNP Belief/NNP Networks/NNP (/-LRB- SBNs/NNS )/-RRB- ./.
The/DT transition/NN matrices/NNS are/VBP further/JJ factored/VBD to/TO reduce/VB the/DT number/NN of/IN parameters/NNS and/CC improve/VB generalization/NN ./.
When/WRB side/NN information/NN is/VBZ not/RB available/JJ ,/, a/DT general/JJ framework/NN for/IN semi-supervised/JJ learning/NN based/VBN on/IN the/DT proposed/VBN model/NN is/VBZ constituted/VBN ,/, allowing/VBG robust/JJ sequence/NN classification/NN ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT proposed/VBN approach/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN predictive/JJ and/CC classification/NN performance/NN on/IN sequential/JJ data/NNS ,/, and/CC has/VBZ the/DT capacity/NN to/TO synthesize/VB sequences/NNS ,/, with/IN controlled/JJ style/NN transitioning/VBG and/CC blending/VBG ./.
