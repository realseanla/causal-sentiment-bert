Monte/NNP Carlo/NNP Tree/NNP Search/NNP (/-LRB- MCTS/NNP )/-RRB- has/VBZ improved/VBN the/DT performance/NN of/IN game/NN playing/VBG engines/NNS in/IN domains/NNS such/JJ as/IN Go/NNP ,/, Hex/NNP ,/, and/CC general/JJ -/HYPH game/NN playing/NN ./.
MCTS/NNP has/VBZ been/VBN shown/VBN to/TO outperform/VB outperform/VBP classic/JJ alpha/NN -/HYPH beta/NN search/NN in/IN games/NNS where/WRB good/JJ heuristic/NN evaluations/NNS are/VBP difficult/JJ to/TO obtain/VB ./.
In/IN recent/JJ years/NNS ,/, combining/VBG ideas/NNS from/IN traditional/JJ minimax/NN search/NN in/IN MCTS/NNP has/VBZ been/VBN shown/VBN to/TO be/VB advantageous/JJ in/IN some/DT domains/NNS ,/, such/JJ as/IN Lines/NNS of/IN Action/NN ,/, Amazons/NNS ,/, and/CC Breakthrough/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ way/NN to/TO use/VB heuristic/NN evaluations/NNS to/TO guide/VB the/DT MCTS/NNP search/NN by/IN storing/VBG the/DT two/CD sources/NNS of/IN information/NN ,/, estimated/VBN win/NN rates/NNS and/CC heuristic/NN evaluations/NNS ,/, separately/RB ./.
Rather/RB than/IN using/VBG the/DT heuristic/NN evaluations/NNS to/TO replace/VB the/DT playouts/NNS ,/, our/PRP$ technique/NN backs/VBZ them/PRP up/RP implicitly/RB during/IN its/PRP$ MCTS/NNP simulations/NNS ./.
These/DT learned/VBD evaluation/NN values/NNS are/VBP then/RB used/VBN to/TO guide/VB future/JJ simulations/NNS ./.
Compared/VBN to/IN current/JJ techniques/NNS ,/, we/PRP show/VBP that/IN using/VBG implicit/JJ minimax/NN backups/NNS leads/VBZ to/IN stronger/JJR play/NN performance/NN in/IN Breakthrough/NN ,/, Lines/NNS of/IN Action/NN ,/, and/CC Kalah/NNP ./.
