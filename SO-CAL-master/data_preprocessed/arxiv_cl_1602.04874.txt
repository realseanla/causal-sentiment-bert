Recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- has/VBZ been/VBN broadly/RB applied/VBN to/IN natural/JJ language/NN processing/NN (/-LRB- NLP/NN )/-RRB- problems/NNS ./.
This/DT kind/NN of/IN neural/JJ network/NN is/VBZ designed/VBN for/IN modeling/VBG sequential/JJ data/NNS and/CC has/VBZ been/VBN testified/VBN to/TO be/VB quite/RB efficient/JJ in/IN sequential/JJ tagging/NN tasks/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO use/VB bi-directional/JJ RNN/NN with/IN long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- units/NNS for/IN Chinese/JJ word/NN segmentation/NN ,/, which/WDT is/VBZ a/DT crucial/JJ preprocess/NN task/NN for/IN modeling/VBG Chinese/JJ sentences/NNS and/CC articles/NNS ./.
Classical/JJ methods/NNS focus/VBP on/IN designing/VBG and/CC combining/VBG hand/NN -/HYPH craft/NN features/NNS from/IN context/NN ,/, whereas/IN bi-directional/JJ LSTM/NN network/NN (/-LRB- BLSTM/NN )/-RRB- does/VBZ not/RB need/VB any/DT prior/JJ knowledge/NN or/CC pre-designing/NN ,/, and/CC it/PRP is/VBZ expert/JJ in/IN keeping/VBG the/DT contextual/JJ information/NN in/IN both/CC directions/NNS ./.
Experiment/NN result/NN shows/VBZ that/IN our/PRP$ approach/NN gets/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN in/IN word/NN segmentation/NN on/IN both/DT traditional/JJ Chinese/JJ datasets/NNS and/CC simplified/VBN Chinese/JJ datasets/NNS ./.
