Deep/JJ learning/NN models/NNS (/-LRB- DLMs/NNS )/-RRB- are/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN techniques/NNS in/IN speech/NN recognition/NN ./.
However/RB ,/, training/VBG good/JJ DLMs/NNS can/MD be/VB time/NN consuming/VBG especially/RB for/IN production/NN -/HYPH size/NN models/NNS and/CC corpora/NNS ./.
Although/IN several/JJ parallel/JJ training/NN algorithms/NNS have/VBP been/VBN proposed/VBN to/TO improve/VB training/NN efficiency/NN ,/, there/EX is/VBZ no/DT clear/JJ guidance/NN on/IN which/WDT one/CD to/TO choose/VB for/IN the/DT task/NN in/IN hand/NN due/IN to/IN lack/NN of/IN systematic/JJ and/CC fair/JJ comparison/NN among/IN them/PRP ./.
In/IN this/DT paper/NN we/PRP aim/VBP at/IN filling/VBG this/DT gap/NN by/IN comparing/VBG four/CD popular/JJ parallel/JJ training/NN algorithms/NNS in/IN speech/NN recognition/NN ,/, namely/RB asynchronous/JJ stochastic/JJ gradient/NN descent/NN (/-LRB- ASGD/NN )/-RRB- ,/, blockwise/JJ model/NN -/HYPH update/NN filtering/NN (/-LRB- BMUF/NN )/-RRB- ,/, bulk/JJ synchronous/JJ parallel/NN (/-LRB- BSP/NN )/-RRB- and/CC elastic/JJ averaging/NN stochastic/JJ gradient/NN descent/NN (/-LRB- EASGD/NN )/-RRB- ,/, on/IN 1000/CD -/HYPH hour/NN LibriSpeech/NNP corpora/NNS using/VBG feed/NN -/HYPH forward/JJ deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- and/CC convolutional/JJ ,/, long/JJ short/JJ -/HYPH term/NN memory/NN ,/, DNNs/NNS (/-LRB- CLDNNs/NNS )/-RRB- ./.
Based/VBN on/IN our/PRP$ experiments/NNS ,/, we/PRP recommend/VBP using/VBG BMUF/NNP as/IN the/DT top/JJ choice/NN to/IN train/NN acoustic/JJ models/NNS since/IN it/PRP is/VBZ most/RBS stable/JJ ,/, scales/NNS well/RB with/IN number/NN of/IN GPUs/NNS ,/, can/MD achieve/VB reproducible/JJ results/NNS ,/, and/CC in/IN many/JJ cases/NNS even/RB outperforms/VBZ single/JJ -/HYPH GPU/NN SGD/NN ./.
ASGD/NN can/MD be/VB used/VBN as/IN a/DT substitute/NN in/IN some/DT cases/NNS ./.
