Distributed/VBN word/NN representations/NNS have/VBP been/VBN demonstrated/VBN to/TO be/VB effective/JJ in/IN capturing/VBG se/FW -/HYPH mantic/FW and/CC syntactic/JJ regularities/NNS ./.
Unsuper/NN -/HYPH vised/VBN representation/NN learning/NN from/IN large/JJ un/JJ -/HYPH labeled/VBN corpora/NNS can/MD learn/VB similar/JJ representa/NN -/HYPH tions/NNS for/IN those/DT words/NNS that/WDT present/VBP similar/JJ co/NN -/HYPH occurrence/NN statistics/NNS ./.
Besides/IN local/JJ occurrence/NN statistics/NNS ,/, global/JJ topical/JJ information/NN is/VBZ also/RB important/JJ knowledge/NN that/WDT may/MD help/VB discrimi/VB -/HYPH nate/VB a/DT word/NN from/IN another/DT ./.
In/IN this/DT paper/NN ,/, we/PRP in/IN -/HYPH corporate/JJ category/NN information/NN of/IN documents/NNS in/IN the/DT learning/NN of/IN word/NN representations/NNS and/CC to/TO learn/VB the/DT proposed/VBN models/NNS in/IN a/DT document/NN -/HYPH wise/JJ manner/NN ./.
Our/PRP$ models/NNS outperform/VBP several/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN models/NNS in/IN word/NN analogy/NN and/CC word/NN similarity/NN tasks/NNS ./.
Moreover/RB ,/, we/PRP evaluate/VBP the/DT learned/VBN word/NN vectors/NNS on/IN sentiment/NN analysis/NN and/CC text/NN classification/NN tasks/NNS ,/, which/WDT shows/VBZ the/DT superiority/NN of/IN our/PRP$ learned/VBN word/NN vectors/NNS ./.
We/PRP also/RB learn/VBP high/JJ -/HYPH quality/NN category/NN embeddings/NNS that/WDT reflect/VBP topical/JJ meanings/NNS ./.
