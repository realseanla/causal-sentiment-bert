In/IN this/DT paper/NN we/PRP present/VBP the/DT greedy/JJ step/NN averaging/NN (/-LRB- GSA/NNP )/-RRB- method/NN ,/, a/DT parameter/NN -/HYPH free/JJ stochastic/JJ optimization/NN algorithm/NN for/IN a/DT variety/NN of/IN machine/NN learning/NN problems/NNS ./.
As/IN a/DT gradient/NN -/HYPH based/VBN optimization/NN method/NN ,/, GSA/NNP makes/VBZ use/NN of/IN the/DT information/NN from/IN the/DT minimizer/NN of/IN a/DT single/JJ sample/NN 's/POS loss/NN function/NN ,/, and/CC takes/VBZ average/JJ strategy/NN to/TO calculate/VB reasonable/JJ learning/NN rate/NN sequence/NN ./.
While/IN most/JJS existing/VBG gradient/NN -/HYPH based/VBN algorithms/NNS introduce/VBP an/DT increasing/VBG number/NN of/IN hyper/JJ parameters/NNS or/CC try/VB to/TO make/VB a/DT trade/NN -/HYPH off/NN between/IN computational/JJ cost/NN and/CC convergence/NN rate/NN ,/, GSA/NNP avoids/VBZ the/DT manual/JJ tuning/NN of/IN learning/NN rate/NN and/CC brings/VBZ in/IN no/DT more/RBR hyper/JJ parameters/NNS or/CC extra/JJ cost/NN ./.
We/PRP perform/VBP exhaustive/JJ numerical/JJ experiments/NNS for/IN logistic/JJ and/CC softmax/JJ regression/NN to/TO compare/VB our/PRP$ method/NN with/IN the/DT other/JJ state/NN of/IN the/DT art/NN ones/NNS on/IN 16/CD datasets/NNS ./.
Results/NNS show/VBP that/IN GSA/NNP is/VBZ robust/JJ on/IN various/JJ scenarios/NNS ./.
