We/PRP investigate/VBP the/DT usage/NN of/IN convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- for/IN the/DT slot/NN filling/VBG task/NN in/IN spoken/VBN language/NN understanding/NN ./.
We/PRP propose/VBP a/DT novel/JJ CNN/NNP architecture/NN for/IN sequence/NN labeling/NN which/WDT takes/VBZ into/IN account/NN the/DT previous/JJ context/NN words/NNS with/IN preserved/VBN order/NN information/NN and/CC pays/VBZ special/JJ attention/NN to/IN the/DT current/JJ word/NN with/IN its/PRP$ surrounding/VBG context/NN ./.
Moreover/RB ,/, it/PRP combines/VBZ the/DT information/NN from/IN the/DT past/NN and/CC the/DT future/JJ words/NNS for/IN classification/NN ./.
Our/PRP$ proposed/VBN CNN/NNP architecture/NN outperforms/VBZ even/RB the/DT previously/RB best/JJS ensembling/VBG recurrent/JJ neural/JJ network/NN model/NN and/CC achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS with/IN an/DT F1/NN -/HYPH score/NN of/IN 95.61/CD percent/NN on/IN the/DT ATIS/NNP benchmark/NN dataset/NN without/IN using/VBG any/DT additional/JJ linguistic/JJ knowledge/NN and/CC resources/NNS ./.
