Deep/JJ reinforcement/NN learning/NN has/VBZ achieved/VBN many/JJ impressive/JJ results/NNS in/IN recent/JJ years/NNS ./.
However/RB ,/, tasks/NNS with/IN sparse/JJ rewards/NNS or/CC long/JJ horizons/NNS continue/VBP to/TO pose/VB significant/JJ challenges/NNS ./.
To/TO tackle/VB these/DT important/JJ problems/NNS ,/, we/PRP propose/VBP a/DT general/JJ framework/NN that/WDT first/RB learns/VBZ useful/JJ skills/NNS in/IN a/DT pre-training/JJ environment/NN ,/, and/CC then/RB leverages/VBZ the/DT acquired/VBN skills/NNS for/IN learning/VBG faster/RBR in/IN downstream/JJ tasks/NNS ./.
Our/PRP$ approach/NN brings/VBZ together/RB some/DT of/IN the/DT strengths/NNS of/IN intrinsic/JJ motivation/NN and/CC hierarchical/JJ methods/NNS :/: the/DT learning/NN of/IN useful/JJ skill/NN is/VBZ guided/VBN by/IN a/DT single/JJ proxy/NN reward/NN ,/, the/DT design/NN of/IN which/WDT requires/VBZ very/RB minimal/JJ domain/NN knowledge/NN about/IN the/DT downstream/JJ tasks/NNS ./.
Then/RB a/DT high/JJ -/HYPH level/NN policy/NN is/VBZ trained/VBN on/IN top/NN of/IN these/DT skills/NNS ,/, providing/VBG a/DT significant/JJ improvement/NN of/IN the/DT exploration/NN and/CC allowing/VBG to/TO tackle/VB sparse/JJ rewards/NNS in/IN the/DT downstream/JJ tasks/NNS ./.
To/TO efficiently/RB pre-train/VB a/DT large/JJ span/NN of/IN skills/NNS ,/, we/PRP use/VBP Stochastic/JJ Neural/JJ Networks/NNS combined/VBN with/IN an/DT information/NN -/HYPH theoretic/JJ regularizer/NN ./.
Our/PRP$ experiments/NNS show/VBP that/IN this/DT combination/NN is/VBZ effective/JJ in/IN learning/VBG a/DT wide/JJ span/NN of/IN interpretable/JJ skills/NNS in/IN a/DT sample/NN -/HYPH efficient/JJ way/NN ,/, and/CC can/MD significantly/RB boost/VB the/DT learning/NN performance/NN uniformly/RB across/IN a/DT wide/JJ range/NN of/IN downstream/JJ tasks/NNS ./.
