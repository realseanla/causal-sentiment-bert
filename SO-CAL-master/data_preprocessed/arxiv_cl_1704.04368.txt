Neural/JJ sequence/NN -/HYPH to/IN -/HYPH sequence/NN models/NNS have/VBP provided/VBN a/DT viable/JJ new/JJ approach/NN for/IN abstractive/JJ text/NN summarization/NN (/-LRB- meaning/VBG they/PRP are/VBP not/RB restricted/VBN to/IN simply/RB selecting/VBG and/CC rearranging/VBG passages/NNS from/IN the/DT original/JJ text/NN )/-RRB- ./.
However/RB ,/, these/DT models/NNS have/VBP two/CD shortcomings/NNS :/: they/PRP are/VBP liable/JJ to/TO reproduce/VB factual/JJ details/NNS inaccurately/RB ,/, and/CC they/PRP tend/VBP to/TO repeat/VB themselves/PRP ./.
In/IN this/DT work/NN we/PRP propose/VBP a/DT novel/JJ architecture/NN that/WDT augments/VBZ the/DT standard/JJ sequence/NN -/HYPH to/IN -/HYPH sequence/NN attentional/JJ model/NN in/IN two/CD orthogonal/JJ ways/NNS ./.
First/RB ,/, we/PRP use/VBP a/DT hybrid/NN pointer/NN -/HYPH generator/NN network/NN that/WDT can/MD copy/VB words/NNS from/IN the/DT source/NN text/NN via/IN pointing/VBG ,/, which/WDT aids/VBZ accurate/JJ reproduction/NN of/IN information/NN ,/, while/IN retaining/VBG the/DT ability/NN to/TO produce/VB novel/JJ words/NNS through/IN the/DT generator/NN ./.
Second/RB ,/, we/PRP use/VBP coverage/NN to/TO keep/VB track/NN of/IN what/WP has/VBZ been/VBN summarized/VBN ,/, which/WDT discourages/VBZ repetition/NN ./.
We/PRP apply/VBP our/PRP$ model/NN to/IN the/DT CNN/NNP //HYPH Daily/NNP Mail/NNP summarization/NN task/NN ,/, outperforming/VBG the/DT current/JJ abstractive/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN by/IN at/IN least/RBS 2/CD ROUGE/NN points/NNS ./.
