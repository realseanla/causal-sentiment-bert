An/DT efficient/JJ way/NN to/TO learn/VB deep/JJ density/NN models/NNS that/WDT have/VBP many/JJ layers/NNS of/IN latent/JJ variables/NNS is/VBZ to/TO learn/VB one/CD layer/NN at/IN a/DT time/NN using/VBG a/DT model/NN that/WDT has/VBZ only/RB one/CD layer/NN of/IN latent/JJ variables/NNS ./.
After/IN learning/VBG each/DT layer/NN ,/, samples/NNS from/IN the/DT posterior/JJ distributions/NNS for/IN that/DT layer/NN are/VBP used/VBN as/IN training/NN data/NNS for/IN learning/VBG the/DT next/JJ layer/NN ./.
This/DT approach/NN is/VBZ commonly/RB used/VBN with/IN Restricted/VBN Boltzmann/NNP Machines/NNPS ,/, which/WDT are/VBP undirected/JJ graphical/JJ models/NNS with/IN a/DT single/JJ hidden/JJ layer/NN ,/, but/CC it/PRP can/MD also/RB be/VB used/VBN with/IN Mixtures/NNS of/IN Factor/NN Analysers/NNS (/-LRB- MFAs/NNS )/-RRB- which/WDT are/VBP directed/VBN graphical/JJ models/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT greedy/JJ layer-wise/NN learning/VBG algorithm/NN for/IN Deep/JJ Mixtures/NNS of/IN Factor/NN Analysers/NNS (/-LRB- DMFAs/NNS )/-RRB- ./.
Even/RB though/IN a/DT DMFA/NN can/MD be/VB converted/VBN to/IN an/DT equivalent/JJ shallow/JJ MFA/NN by/IN multiplying/VBG together/RB the/DT factor/NN loading/NN matrices/NNS at/IN different/JJ levels/NNS ,/, learning/VBG and/CC inference/NN are/VBP much/RB more/RBR efficient/JJ in/IN a/DT DMFA/NN and/CC the/DT sharing/NN of/IN each/DT lower/JJR -/HYPH level/NN factor/NN loading/NN matrix/NN by/IN many/JJ different/JJ higher/JJR level/NN MFAs/NNS prevents/VBZ overfitting/NN ./.
We/PRP demonstrate/VBP empirically/RB that/IN DMFAs/NNPS learn/VBP better/JJR density/NN models/NNS than/IN both/DT MFAs/NNS and/CC two/CD types/NNS of/IN Restricted/VBN Boltzmann/NNP Machine/NNP on/IN a/DT wide/JJ variety/NN of/IN datasets/NNS ./.
