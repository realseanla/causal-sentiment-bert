We/PRP demonstrate/VBP that/IN a/DT continuous/JJ relaxation/NN of/IN the/DT argmax/NN operation/NN can/MD be/VB used/VBN to/TO create/VB a/DT differentiable/JJ approximation/NN to/IN greedy/JJ decoding/NN for/IN sequence/NN -/HYPH to/IN -/HYPH sequence/NN (/-LRB- seq2seq/NN )/-RRB- models/NNS ./.
By/IN incorporating/VBG this/DT approximation/NN into/IN the/DT scheduled/VBN sampling/NN training/NN procedure/NN (/-LRB- Bengio/NNP et/FW al./FW ,/, 2015/CD )/-RRB- --/: a/DT well/RB -/HYPH known/VBN technique/NN for/IN correcting/VBG exposure/NN bias/NN --/: we/PRP introduce/VBP a/DT new/JJ training/NN objective/NN that/WDT is/VBZ continuous/JJ and/CC differentiable/JJ everywhere/RB and/CC that/DT can/MD provide/VB informative/JJ gradients/NNS near/IN points/NNS where/WRB previous/JJ decoding/NN decisions/NNS change/VBP their/PRP$ value/NN ./.
In/IN addition/NN ,/, by/IN using/VBG a/DT related/JJ approximation/NN ,/, we/PRP demonstrate/VBP a/DT similar/JJ approach/NN to/IN sampled/VBN -/HYPH based/JJ training/NN ./.
Finally/RB ,/, we/PRP show/VBP that/IN our/PRP$ approach/NN outperforms/VBZ cross-entropy/JJ training/NN and/CC scheduled/VBN sampling/NN procedures/NNS in/IN two/CD sequence/NN prediction/NN tasks/NNS :/: named/VBN entity/NN recognition/NN and/CC machine/NN translation/NN ./.
