In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/IN employ/VB the/DT convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- for/IN learning/VBG to/TO answer/VB questions/NNS from/IN the/DT image/NN ./.
Our/PRP$ proposed/VBN CNN/NNP provides/VBZ an/DT end/NN -/HYPH to/IN -/HYPH end/NN framework/NN for/IN learning/VBG not/RB only/RB the/DT image/NN representation/NN ,/, the/DT composition/NN model/NN for/IN question/NN ,/, but/CC also/RB the/DT inter-modal/JJ interaction/NN between/IN the/DT image/NN and/CC question/NN ,/, for/IN the/DT generation/NN of/IN answer/NN ./.
More/RBR specifically/RB ,/, the/DT proposed/VBN model/NN consists/VBZ of/IN three/CD components/NNS :/: an/DT image/NN CNN/NNP to/TO extract/VB the/DT image/NN representation/NN ,/, one/CD sentence/NN CNN/NNP to/TO encode/VB the/DT question/NN ,/, and/CC one/CD multimodal/JJ convolution/NN layer/NN to/IN fuse/NN the/DT multimodal/JJ input/NN of/IN the/DT image/NN and/CC question/NN to/TO obtain/VB the/DT joint/JJ representation/NN for/IN the/DT classification/NN in/IN the/DT space/NN of/IN candidate/NN answer/NN words/NNS ./.
We/PRP demonstrate/VBP the/DT efficacy/NN of/IN our/PRP$ proposed/VBN model/NN on/IN DAQUAR/NNP and/CC COCO/NNP -/HYPH QA/NNP datasets/NNS ,/, two/CD datasets/NNS recently/RB created/VBN for/IN the/DT image/NN question/NN answering/VBG (/-LRB- QA/NNP )/-RRB- ,/, with/IN performance/NN substantially/RB outperforming/VBG the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH arts/NNS ./.
