We/PRP introduce/VBP recurrent/JJ additive/JJ networks/NNS (/-LRB- RANs/NNS )/-RRB- ,/, a/DT new/JJ gated/VBN RNN/NN which/WDT is/VBZ distinguished/VBN by/IN the/DT use/NN of/IN purely/RB additive/JJ latent/JJ state/NN updates/NNS ./.
At/IN every/DT time/NN step/NN ,/, the/DT new/JJ state/NN is/VBZ computed/VBN as/IN a/DT gated/VBN component-wise/JJ sum/NN of/IN the/DT input/NN and/CC the/DT previous/JJ state/NN ,/, without/IN any/DT of/IN the/DT non-linearities/NNS commonly/RB used/VBN in/IN RNN/NN transition/NN dynamics/NNS ./.
We/PRP formally/RB show/VBP that/IN RAN/JJ states/NNS are/VBP weighted/JJ sums/NNS of/IN the/DT input/NN vectors/NNS ,/, and/CC that/IN the/DT gates/NNS only/RB contribute/VBP to/IN computing/VBG the/DT weights/NNS of/IN these/DT sums/NNS ./.
Despite/IN this/DT relatively/RB simple/JJ functional/JJ form/NN ,/, experiments/NNS demonstrate/VBP that/IN RANs/NNS outperform/VBP both/DT LSTMs/NNS and/CC GRUs/NNS on/IN benchmark/NN language/NN modeling/NN problems/NNS ./.
This/DT result/NN shows/VBZ that/IN many/JJ of/IN the/DT non-linear/JJ computations/NNS in/IN LSTMs/NNPS and/CC related/VBN networks/NNS are/VBP not/RB essential/JJ ,/, at/IN least/RBS for/IN the/DT problems/NNS we/PRP consider/VBP ,/, and/CC suggests/VBZ that/IN the/DT gates/NNS are/VBP doing/VBG more/JJR of/IN the/DT computational/JJ work/NN than/IN previously/RB understood/VBN ./.
