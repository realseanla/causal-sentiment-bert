Despite/IN their/PRP$ advantages/NNS in/IN terms/NNS of/IN computational/JJ resources/NNS ,/, latency/NN ,/, and/CC power/NN consumption/NN ,/, event/NN -/HYPH based/VBN implementations/NNS of/IN neural/JJ networks/NNS have/VBP not/RB been/VBN able/JJ to/TO achieve/VB the/DT same/JJ performance/NN figures/NNS as/IN their/PRP$ equivalent/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN deep/JJ network/NN models/NNS ./.
We/PRP propose/VBP counter/RB neurons/NNS as/IN minimal/JJ spiking/NN neuron/NN models/NNS which/WDT only/RB require/VBP addition/NN and/CC comparison/NN operations/NNS ,/, thus/RB avoiding/VBG costly/JJ multiplications/NNS ./.
We/PRP show/VBP how/WRB inference/NN carried/VBD out/RP in/IN deep/JJ counter/NN networks/NNS converges/VBZ to/IN the/DT same/JJ accuracy/NN levels/NNS as/IN are/VBP achieved/VBN with/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN conventional/JJ networks/NNS ./.
As/IN their/PRP$ event/NN -/HYPH based/VBN style/NN of/IN computation/NN leads/VBZ to/IN reduced/VBN latency/NN and/CC sparse/JJ updates/NNS ,/, counter/NN networks/NNS are/VBP ideally/RB suited/VBN for/IN efficient/JJ compact/JJ and/CC low/JJ -/HYPH power/NN hardware/NN implementation/NN ./.
We/PRP present/VBP theory/NN and/CC training/NN methods/NNS for/IN counter/NN networks/NNS ,/, and/CC demonstrate/VBP on/IN the/DT MNIST/NNP benchmark/NN that/WDT counter/VBP networks/NNS converge/VBP quickly/RB ,/, both/CC in/IN terms/NNS of/IN time/NN and/CC number/NN of/IN operations/NNS required/VBN ,/, to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN classification/NN accuracy/NN ./.
