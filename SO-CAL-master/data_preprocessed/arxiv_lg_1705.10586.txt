Despite/IN the/DT success/NN of/IN deep/JJ learning/NN on/IN many/JJ fronts/NNS especially/RB image/NN and/CC speech/NN ,/, its/PRP$ application/NN in/IN text/NN classification/NN often/RB is/VBZ still/RB not/RB as/RB good/JJ as/IN a/DT simple/JJ linear/JJ SVM/NN on/IN n/NN -/HYPH gram/NN TF/NNP -/HYPH IDF/NNP representation/NN especially/RB for/IN smaller/JJR datasets/NNS ./.
Deep/JJ learning/NN tends/VBZ to/TO emphasize/VB on/IN sentence/NN level/NN semantics/NNS when/WRB learning/VBG a/DT representation/NN with/IN models/NNS like/IN recurrent/JJ neural/JJ network/NN or/CC recursive/JJ neural/JJ network/NN ,/, however/RB from/IN the/DT success/NN of/IN TF/NNP -/HYPH IDF/NNP representation/NN ,/, it/PRP seems/VBZ a/DT bag/NN -/HYPH of/IN -/HYPH words/NNS type/NN of/IN representation/NN has/VBZ its/PRP$ strength/NN ./.
Taking/VBG advantage/NN of/IN both/DT representions/NNS ,/, we/PRP present/VBP a/DT model/NN known/VBN as/IN TDSM/NNP (/-LRB- Top/NNP Down/NNP Semantic/NNP Model/NNP )/-RRB- for/IN extracting/VBG a/DT sentence/NN representation/NN that/WDT considers/VBZ both/CC the/DT word/NN -/HYPH level/NN semantics/NNS by/IN linearly/RB combining/VBG the/DT words/NNS with/IN attention/NN weights/NNS and/CC the/DT sentence/NN -/HYPH level/NN semantics/NNS with/IN BiLSTM/NN and/CC use/VB it/PRP on/IN text/NN classification/NN ./.
We/PRP apply/VBP the/DT model/NN on/IN characters/NNS and/CC our/PRP$ results/NNS show/VBP that/IN our/PRP$ model/NN is/VBZ better/JJR than/IN all/PDT the/DT other/JJ character/NN -/HYPH based/VBN and/CC word/NN -/HYPH based/VBN convolutional/JJ neural/JJ network/NN models/NNS by/IN across/IN seven/CD different/JJ datasets/NNS with/IN only/RB 1/CD \/SYM percent/NN of/IN their/PRP$ parameters/NNS ./.
We/PRP also/RB demonstrate/VBP that/IN this/DT model/NN beats/VBZ traditional/JJ linear/JJ models/NNS on/IN TF/NNP -/HYPH IDF/NNP vectors/NNS on/IN small/JJ and/CC polished/JJ datasets/NNS like/IN news/NN article/NN in/IN which/WDT typically/RB deep/RB learning/VBG models/NNS surrender/VB ./.
