In/IN many/JJ probabilistic/JJ first/JJ -/HYPH order/NN representation/NN systems/NNS ,/, inference/NN is/VBZ performed/VBN by/IN "/`` grounding/NN "/'' ---/, i.e./FW ,/, mapping/VBG it/PRP to/IN a/DT propositional/JJ representation/NN ,/, and/CC then/RB performing/VBG propositional/JJ inference/NN ./.
With/IN a/DT large/JJ database/NN of/IN facts/NNS ,/, groundings/NNS can/MD be/VB very/RB large/JJ ,/, making/VBG inference/NN and/CC learning/NN computationally/RB expensive/JJ ./.
Here/RB we/PRP present/VBP a/DT first/JJ -/HYPH order/NN probabilistic/JJ language/NN which/WDT is/VBZ well/RB -/HYPH suited/VBN to/IN approximate/JJ "/`` local/JJ "/'' grounding/NN :/: every/DT query/NN $/$ Q$/CD can/MD be/VB approximately/RB grounded/VBN with/IN a/DT small/JJ graph/NN ./.
The/DT language/NN is/VBZ an/DT extension/NN of/IN stochastic/JJ logic/NN programs/NNS where/WRB inference/NN is/VBZ performed/VBN by/IN a/DT variant/NN of/IN personalized/VBN PageRank/NNP ./.
Experimentally/RB ,/, we/PRP show/VBP that/IN the/DT approach/NN performs/VBZ well/RB without/IN weight/NN learning/NN on/IN an/DT entity/NN resolution/NN task/NN ;/: that/DT supervised/JJ weight/NN -/HYPH learning/NN improves/VBZ accuracy/NN ;/: and/CC that/DT grounding/NN time/NN is/VBZ independent/JJ of/IN DB/NN size/NN ./.
We/PRP also/RB show/VBP that/IN order/NN -/HYPH of/IN -/HYPH magnitude/NN speedups/NNS are/VBP possible/JJ by/IN parallelizing/VBG learning/NN ./.
