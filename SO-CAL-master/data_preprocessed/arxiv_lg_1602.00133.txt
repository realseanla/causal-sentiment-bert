Many/JJ machine/NN learning/NN models/NNS ,/, such/JJ as/IN logistic/JJ regression/NN ~/SYM (/-LRB- LR/NN )/-RRB- and/CC support/NN vector/NN machine/NN ~/SYM (/-LRB- SVM/NN )/-RRB- ,/, can/MD be/VB formulated/VBN as/IN composite/JJ optimization/NN problems/NNS ./.
Recently/RB ,/, many/JJ distributed/VBN stochastic/JJ optimization/NN ~/SYM (/-LRB- DSO/NN )/-RRB- methods/NNS have/VBP been/VBN proposed/VBN to/TO solve/VB the/DT large/JJ -/HYPH scale/NN composite/JJ optimization/NN problems/NNS ,/, which/WDT have/VBP shown/VBN better/JJR performance/NN than/IN traditional/JJ batch/NN methods/NNS ./.
However/RB ,/, most/JJS of/IN these/DT DSO/NN methods/NNS are/VBP not/RB scalable/JJ enough/RB ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ DSO/NN method/NN ,/, called/VBN \/SYM underline/VB {/-LRB- s/POS }/-RRB- calable/JJ \/SYM underline/VB {/-LRB- c/NN }/-RRB- omposite/NN \/SYM underline/VB {/-LRB- op/NN }/-RRB- timization/NN for/IN l/NN \/SYM underline/VB {/-LRB- e/NN }/-RRB- arning/NN ~/SYM (/-LRB- {/-LRB- SCOPE/NN }/-RRB- )/-RRB- ,/, and/CC implement/VB it/PRP on/IN the/DT fault/NN -/HYPH tolerant/JJ distributed/VBN platform/NN \/SYM mbox/NN {/-LRB- Spark/NN }/-RRB- ./.
SCOPE/NN is/VBZ both/DT computation/NN -/HYPH efficient/JJ and/CC communication/NN -/HYPH efficient/JJ ./.
Theoretical/JJ analysis/NN shows/VBZ that/IN SCOPE/NNP is/VBZ convergent/JJ with/IN linear/JJ convergence/NN rate/NN when/WRB the/DT objective/JJ function/NN is/VBZ convex/NN ./.
Furthermore/RB ,/, empirical/JJ results/NNS on/IN real/JJ datasets/NNS show/VBP that/IN SCOPE/NNP can/MD outperform/VB other/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN distributed/VBN learning/NN methods/NNS on/IN Spark/NN ,/, including/VBG both/DT batch/NN learning/NN methods/NNS and/CC DSO/NN methods/NNS ./.
