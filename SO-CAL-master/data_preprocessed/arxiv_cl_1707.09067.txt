In/IN a/DT controlled/VBN experiment/NN of/IN sequence/NN -/HYPH to/IN -/HYPH sequence/NN approaches/NNS for/IN the/DT task/NN of/IN sentence/NN correction/NN ,/, we/PRP find/VBP that/IN character/NN -/HYPH based/VBN models/NNS are/VBP generally/RB more/RBR effective/JJ than/IN word/NN -/HYPH based/VBN models/NNS and/CC models/NNS that/WDT encode/VBP subword/JJ information/NN via/IN convolutions/NNS ,/, and/CC that/IN modeling/VBG the/DT output/NN data/NNS as/IN a/DT series/NN of/IN diffs/NNS improves/VBZ effectiveness/NN over/IN standard/JJ approaches/NNS ./.
Our/PRP$ strongest/JJS sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN improves/VBZ over/IN our/PRP$ strongest/JJS phrase/NN -/HYPH based/VBN statistical/JJ machine/NN translation/NN model/NN ,/, with/IN access/NN to/IN the/DT same/JJ data/NNS ,/, by/IN 6/CD M2/NN (/-LRB- 0.5/CD GLEU/NN )/-RRB- points/NNS ./.
Additionally/RB ,/, in/IN the/DT data/NNS environment/NN of/IN the/DT standard/JJ CoNLL/NN -/HYPH 2014/CD setup/NN ,/, we/PRP demonstrate/VBP that/IN modeling/NN (/-LRB- and/CC tuning/VB against/IN )/-RRB- diffs/NNS yields/NNS similar/JJ or/CC better/JJR M2/NN scores/NNS with/IN simpler/JJR models/NNS and/CC //HYPH or/CC significantly/RB less/JJR data/NNS than/IN previous/JJ sequence/NN -/HYPH to/IN -/HYPH sequence/NN approaches/NNS ./.
