Words/NNS in/IN natural/JJ language/NN follow/VB a/DT Zipfian/JJ distribution/NN whereby/WRB some/DT words/NNS are/VBP frequent/JJ but/CC most/JJS are/VBP rare/JJ ./.
Learning/VBG representations/NNS for/IN words/NNS in/IN the/DT "/`` long/JJ tail/NN "/'' of/IN this/DT distribution/NN requires/VBZ enormous/JJ amounts/NNS of/IN data/NNS ./.
Representations/NNS of/IN rare/JJ words/NNS trained/VBN directly/RB on/IN end/NN -/HYPH tasks/NNS are/VBP usually/RB poor/JJ ,/, requiring/VBG us/PRP to/TO pre-train/VB embeddings/NNS on/IN external/JJ data/NNS ,/, or/CC treat/VB all/DT rare/JJ words/NNS as/RB out/RB -/HYPH of/IN -/HYPH vocabulary/NN words/NNS with/IN a/DT unique/JJ representation/NN ./.
We/PRP provide/VBP a/DT method/NN for/IN predicting/VBG embeddings/NNS of/IN rare/JJ words/NNS on/IN the/DT fly/NN from/IN small/JJ amounts/NNS of/IN auxiliary/JJ data/NNS with/IN a/DT network/NN trained/VBN against/IN the/DT end/NN task/NN ./.
We/PRP show/VBP that/IN this/DT improves/VBZ results/NNS against/IN baselines/NNS where/WRB embeddings/NNS are/VBP trained/VBN on/IN the/DT end/NN task/NN in/IN a/DT reading/NN comprehension/NN task/NN ,/, a/DT recognizing/VBG textual/JJ entailment/NN task/NN ,/, and/CC in/IN language/NN modelling/NN ./.
