This/DT paper/NN provides/VBZ lower/JJR bounds/NNS on/IN the/DT convergence/NN rate/NN of/IN Derivative/JJ Free/JJ Optimization/NN (/-LRB- DFO/NN )/-RRB- with/IN noisy/JJ function/NN evaluations/NNS ,/, exposing/VBG a/DT fundamental/JJ and/CC unavoidable/JJ gap/NN between/IN the/DT performance/NN of/IN algorithms/NNS with/IN access/NN to/IN gradients/NNS and/CC those/DT with/IN access/NN to/IN only/JJ function/NN evaluations/NNS ./.
However/RB ,/, there/EX are/VBP situations/NNS in/IN which/WDT DFO/NNP is/VBZ unavoidable/JJ ,/, and/CC for/IN such/JJ situations/NNS we/PRP propose/VBP a/DT new/JJ DFO/NNP algorithm/NN that/WDT is/VBZ proved/VBN to/TO be/VB near/IN optimal/JJ for/IN the/DT class/NN of/IN strongly/RB convex/JJ objective/JJ functions/NNS ./.
A/DT distinctive/JJ feature/NN of/IN the/DT algorithm/NN is/VBZ that/IN it/PRP uses/VBZ only/RB Boolean/JJ -/HYPH valued/VBN function/NN comparisons/NNS ,/, rather/RB than/IN function/NN evaluations/NNS ./.
This/DT makes/VBZ the/DT algorithm/NN useful/JJ in/IN an/DT even/RB wider/JJR range/NN of/IN applications/NNS ,/, such/JJ as/IN optimization/NN based/VBN on/IN paired/VBN comparisons/NNS from/IN human/JJ subjects/NNS ,/, for/IN example/NN ./.
We/PRP also/RB show/VBP that/IN regardless/RB of/IN whether/IN DFO/NNP is/VBZ based/VBN on/IN noisy/JJ function/NN evaluations/NNS or/CC Boolean/JJ -/HYPH valued/VBN function/NN comparisons/NNS ,/, the/DT convergence/NN rate/NN is/VBZ the/DT same/JJ ./.
