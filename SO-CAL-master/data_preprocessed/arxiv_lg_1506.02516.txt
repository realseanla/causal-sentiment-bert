Recently/RB ,/, strong/JJ results/NNS have/VBP been/VBN demonstrated/VBN by/IN Deep/JJ Recurrent/JJ Neural/JJ Networks/NNS on/IN natural/JJ language/NN transduction/NN problems/NNS ./.
In/IN this/DT paper/NN we/PRP explore/VBP the/DT representational/JJ power/NN of/IN these/DT models/NNS using/VBG synthetic/JJ grammars/NNS designed/VBN to/TO exhibit/VB phenomena/NN similar/JJ to/IN those/DT found/VBN in/IN real/JJ transduction/NN problems/NNS such/JJ as/IN machine/NN translation/NN ./.
These/DT experiments/NNS lead/VBP us/PRP to/TO propose/VB new/JJ memory/NN -/HYPH based/VBN recurrent/JJ networks/NNS that/WDT implement/VBP continuously/RB differentiable/JJ analogues/NNS of/IN traditional/JJ data/NNS structures/NNS such/JJ as/IN Stacks/NNP ,/, Queues/NNPS ,/, and/CC DeQues/NNS ./.
We/PRP show/VBP that/IN these/DT architectures/NNS exhibit/VBP superior/JJ generalisation/NN performance/NN to/IN Deep/JJ RNNs/NNS and/CC are/VBP often/RB able/JJ to/TO learn/VB the/DT underlying/VBG generating/VBG algorithms/NNS in/IN our/PRP$ transduction/NN experiments/NNS ./.
