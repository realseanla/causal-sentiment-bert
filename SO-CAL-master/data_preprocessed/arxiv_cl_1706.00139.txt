Natural/JJ language/NN generation/NN (/-LRB- NLG/NN )/-RRB- is/VBZ a/DT critical/JJ component/NN in/IN a/DT spoken/VBN dialogue/NN system/NN ./.
This/DT paper/NN presents/VBZ a/DT Recurrent/JJ Neural/JJ Network/NN based/VBN Encoder/NNP -/HYPH Decoder/NNP architecture/NN ,/, in/IN which/WDT an/DT LSTM/NN -/HYPH based/VBN decoder/NN is/VBZ introduced/VBN to/TO select/VB ,/, aggregate/JJ semantic/JJ elements/NNS produced/VBN by/IN an/DT attention/NN mechanism/NN over/IN the/DT input/NN elements/NNS ,/, and/CC to/TO produce/VB the/DT required/VBN utterances/NNS ./.
The/DT proposed/VBN generator/NN can/MD be/VB jointly/RB trained/VBN both/DT sentence/NN planning/NN and/CC surface/NN realization/NN to/TO produce/VB natural/JJ language/NN sentences/NNS ./.
The/DT proposed/VBN model/NN was/VBD extensively/RB evaluated/VBN on/IN four/CD different/JJ NLG/NN datasets/NNS ./.
The/DT experimental/JJ results/NNS showed/VBD that/IN the/DT proposed/VBN generators/NNS not/RB only/RB consistently/RB outperform/VB the/DT previous/JJ methods/NNS across/IN all/PDT the/DT NLG/NN domains/NNS but/CC also/RB show/VBP an/DT ability/NN to/TO generalize/VB from/IN a/DT new/JJ ,/, unseen/JJ domain/NN and/CC learn/VB from/IN multi-domain/JJ datasets/NNS ./.
