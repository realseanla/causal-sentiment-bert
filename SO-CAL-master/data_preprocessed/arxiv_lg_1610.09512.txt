This/DT paper/NN studies/NNS systematic/JJ exploration/NN for/IN reinforcement/NN learning/VBG with/IN rich/JJ observations/NNS and/CC function/NN approximation/NN ./.
We/PRP introduce/VBP a/DT new/JJ formulation/NN ,/, called/VBN contextual/JJ decision/NN processes/NNS ,/, that/IN unifies/VBZ and/CC generalizes/VBZ most/RBS prior/JJ settings/NNS ./.
Our/PRP$ first/JJ contribution/NN is/VBZ a/DT new/JJ complexity/NN measure/NN ,/, the/DT Bellman/NNP Rank/NNP ,/, that/IN we/PRP show/VBP enables/VBZ tractable/JJ learning/NN of/IN near/JJ -/HYPH optimal/JJ behavior/NN in/IN these/DT processes/NNS and/CC is/VBZ naturally/RB small/JJ for/IN many/JJ well/RB -/HYPH studied/VBN reinforcement/NN learning/VBG settings/NNS ./.
Our/PRP$ second/JJ contribution/NN is/VBZ a/DT new/JJ reinforcement/NN learning/VBG algorithm/NN that/WDT engages/VBZ in/IN systematic/JJ exploration/NN to/TO learn/VB contextual/JJ decision/NN processes/NNS with/IN low/JJ Bellman/NNP Rank/NNP ./.
The/DT algorithm/NN provably/RB learns/VBZ near/IN -/HYPH optimal/JJ behavior/NN with/IN a/DT number/NN of/IN samples/NNS that/WDT is/VBZ polynomial/JJ in/IN all/DT relevant/JJ parameters/NNS but/CC independent/JJ of/IN the/DT number/NN of/IN unique/JJ observations/NNS ./.
The/DT algorithm/NN uses/VBZ Bellman/NNP error/NN minimization/NN with/IN optimistic/JJ exploration/NN and/CC provides/VBZ new/JJ insights/NNS into/IN efficient/JJ exploration/NN for/IN reinforcement/NN learning/VBG with/IN function/NN approximation/NN ./.
