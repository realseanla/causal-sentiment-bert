Exploiting/VBG the/DT great/JJ expressive/JJ power/NN of/IN Deep/JJ Neural/JJ Network/NN architectures/NNS ,/, relies/VBZ on/IN the/DT ability/NN to/TO train/VB them/PRP ./.
While/IN current/JJ theoretical/JJ work/NN provides/VBZ ,/, mostly/RB ,/, results/NNS showing/VBG the/DT hardness/NN of/IN this/DT task/NN ,/, empirical/JJ evidence/NN usually/RB differs/VBZ from/IN this/DT line/NN ,/, with/IN success/NN stories/NNS in/IN abundance/NN ./.
A/DT strong/JJ position/NN among/IN empirically/RB successful/JJ architectures/NNS is/VBZ captured/VBN by/IN networks/NNS where/WRB extensive/JJ weight/NN sharing/NN is/VBZ used/VBN ,/, either/CC by/IN Convolutional/NNP or/CC Recurrent/JJ layers/NNS ./.
Additionally/RB ,/, characterizing/VBG specific/JJ aspects/NNS of/IN different/JJ tasks/NNS ,/, making/VBG them/PRP "/`` harder/JJR "/'' or/CC "/`` easier/JJR "/'' ,/, is/VBZ an/DT interesting/JJ direction/NN explored/VBN both/DT theoretically/RB and/CC empirically/RB ./.
We/PRP consider/VBP a/DT family/NN of/IN ConvNet/NNP architectures/NNS ,/, and/CC prove/VB that/IN weight/NN sharing/NN can/MD be/VB crucial/JJ ,/, from/IN an/DT optimization/NN point/NN of/IN view/NN ./.
We/PRP explore/VBP different/JJ notions/NNS of/IN the/DT frequency/NN ,/, of/IN the/DT target/NN function/NN ,/, proving/VBG necessity/NN of/IN the/DT target/NN function/NN having/VBG some/DT low/JJ frequency/NN components/NNS ./.
This/DT necessity/NN is/VBZ not/RB sufficient/JJ -/HYPH only/JJ with/IN weight/NN sharing/NN can/MD it/PRP be/VB exploited/VBN ,/, thus/RB theoretically/RB separating/VBG architectures/NNS using/VBG it/PRP ,/, from/IN others/NNS which/WDT do/VBP not/RB ./.
Our/PRP$ theoretical/JJ results/NNS are/VBP aligned/VBN with/IN empirical/JJ experiments/NNS in/IN an/DT even/RB more/RBR general/JJ setting/NN ,/, suggesting/VBG viability/NN of/IN examination/NN of/IN the/DT role/NN played/VBN by/IN interleaving/VBG those/DT aspects/NNS in/IN broader/JJR families/NNS of/IN tasks/NNS ./.
