We/PRP study/VBP the/DT impact/NN of/IN big/JJ models/NNS (/-LRB- in/IN terms/NNS of/IN the/DT degree/NN of/IN lexicalization/NN )/-RRB- and/CC big/JJ data/NNS (/-LRB- in/IN terms/NNS of/IN the/DT training/NN corpus/NN size/NN )/-RRB- on/IN dependency/NN grammar/NN induction/NN ./.
We/PRP experimented/VBD with/IN L/NNP -/HYPH DMV/NNP ,/, a/DT lexicalized/VBN version/NN of/IN Dependency/NN Model/NN with/IN Valence/NNP and/CC L/NNP -/HYPH NDMV/NNP ,/, our/PRP$ lexicalized/VBN extension/NN of/IN the/DT Neural/JJ Dependency/NN Model/NN with/IN Valence/NNP ./.
We/PRP find/VBP that/IN L/NNP -/HYPH DMV/NNP only/RB benefits/VBZ from/IN very/RB small/JJ degrees/NNS of/IN lexicalization/NN and/CC moderate/JJ sizes/NNS of/IN training/NN corpora/NNS ./.
L/NN -/HYPH NDMV/NN can/MD benefit/VB from/IN big/JJ training/NN data/NNS and/CC lexicalization/NN of/IN greater/JJR degrees/NNS ,/, especially/RB when/WRB enhanced/VBN with/IN good/JJ model/NN initialization/NN ,/, and/CC it/PRP achieves/VBZ a/DT result/NN that/WDT is/VBZ competitive/JJ with/IN the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
