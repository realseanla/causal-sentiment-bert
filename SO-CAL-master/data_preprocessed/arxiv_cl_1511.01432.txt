We/PRP present/VBP two/CD approaches/NNS that/WDT use/VBP unlabeled/JJ data/NNS to/TO improve/VB sequence/NN learning/NN with/IN recurrent/JJ networks/NNS ./.
The/DT first/JJ approach/NN is/VBZ to/TO predict/VB what/WP comes/VBZ next/RB in/IN a/DT sequence/NN ,/, which/WDT is/VBZ a/DT conventional/JJ language/NN model/NN in/IN natural/JJ language/NN processing/NN ./.
The/DT second/JJ approach/NN is/VBZ to/TO use/VB a/DT sequence/NN autoencoder/NN ,/, which/WDT reads/VBZ the/DT input/NN sequence/NN into/IN a/DT vector/NN and/CC predicts/VBZ the/DT input/NN sequence/NN again/RB ./.
These/DT two/CD algorithms/NNS can/MD be/VB used/VBN as/IN a/DT "/`` pretraining/NN "/'' step/NN for/IN a/DT later/RBR supervised/JJ sequence/NN learning/VBG algorithm/NN ./.
In/IN other/JJ words/NNS ,/, the/DT parameters/NNS obtained/VBN from/IN the/DT unsupervised/JJ step/NN can/MD be/VB used/VBN as/IN a/DT starting/NN point/NN for/IN other/JJ supervised/JJ training/NN models/NNS ./.
In/IN our/PRP$ experiments/NNS ,/, we/PRP find/VBP that/DT long/JJ short/JJ term/NN memory/NN recurrent/JJ networks/NNS after/IN being/VBG pretrained/VBN with/IN the/DT two/CD approaches/NNS are/VBP more/RBR stable/JJ and/CC generalize/VB better/JJR ./.
With/IN pretraining/NN ,/, we/PRP are/VBP able/JJ to/TO train/VB long/RB short/JJ term/NN memory/NN recurrent/JJ networks/NNS up/IN to/IN a/DT few/JJ hundred/CD timesteps/NNS ,/, thereby/RB achieving/VBG strong/JJ performance/NN in/IN many/JJ text/NN classification/NN tasks/NNS ,/, such/JJ as/IN IMDB/NNP ,/, DBpedia/NNP and/CC 20/CD Newsgroups/NNS ./.
