This/DT paper/NN presents/VBZ a/DT novel/JJ neural/JJ machine/NN translation/NN model/NN which/WDT jointly/RB learns/VBZ translation/NN and/CC source/NN -/HYPH side/NN latent/JJ graph/NN representations/NNS of/IN sentences/NNS ./.
Unlike/IN existing/VBG pipelined/JJ approaches/NNS using/VBG syntactic/JJ parsers/NNS ,/, our/PRP$ end/NN -/HYPH to/IN -/HYPH end/NN model/NN learns/VBZ a/DT latent/JJ graph/NN parser/NN as/IN part/NN of/IN the/DT encoder/NN of/IN an/DT attention/NN -/HYPH based/VBN neural/JJ machine/NN translation/NN model/NN ,/, so/IN the/DT parser/NN is/VBZ optimized/VBN according/VBG to/IN the/DT translation/NN objective/NN ./.
Experimental/JJ results/NNS show/VBP that/IN our/PRP$ model/NN significantly/RB outperforms/VBZ the/DT previous/JJ best/JJS results/NNS on/IN the/DT standard/JJ English/NNP -/HYPH to/IN -/HYPH Japanese/JJ translation/NN dataset/NN ./.
