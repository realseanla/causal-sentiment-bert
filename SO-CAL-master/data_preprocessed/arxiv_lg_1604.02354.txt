Learning/VBG a/DT good/JJ distance/NN metric/JJ in/IN feature/NN space/NN potentially/RB improves/VBZ the/DT performance/NN of/IN the/DT KNN/NNP classifier/NN and/CC is/VBZ useful/JJ in/IN many/JJ real/JJ -/HYPH world/NN applications/NNS ./.
Many/JJ metric/JJ learning/NN algorithms/NNS are/VBP however/RB based/VBN on/IN the/DT point/NN estimation/NN of/IN a/DT quadratic/JJ optimization/NN problem/NN ,/, which/WDT is/VBZ time/NN -/HYPH consuming/VBG ,/, susceptible/JJ to/IN overfitting/VBG ,/, and/CC lack/VBP a/DT natural/JJ mechanism/NN to/IN reason/NN with/IN parameter/NN uncertainty/NN ,/, an/DT important/JJ property/NN useful/JJ especially/RB when/WRB the/DT training/NN set/NN is/VBZ small/JJ and/CC //HYPH or/CC noisy/JJ ./.
To/TO deal/VB with/IN these/DT issues/NNS ,/, we/PRP present/VBP a/DT novel/JJ Bayesian/JJ metric/JJ learning/NN method/NN ,/, called/VBN Bayesian/JJ NCA/NN ,/, based/VBN on/IN the/DT well/NN -/HYPH known/VBN Neighbourhood/NNP Component/NN Analysis/NN method/NN ,/, in/IN which/WDT the/DT metric/JJ posterior/NN is/VBZ characterized/VBN by/IN the/DT local/JJ label/NN consistency/NN constraints/NNS of/IN observations/NNS ,/, encoded/VBN with/IN a/DT similarity/NN graph/NN instead/RB of/IN independent/JJ pairwise/JJ constraints/NNS ./.
For/IN efficient/JJ Bayesian/JJ optimization/NN ,/, we/PRP explore/VBP the/DT variational/JJ lower/JJR bound/VBN over/IN the/DT log/NN -/HYPH likelihood/NN of/IN the/DT original/JJ NCA/NN objective/NN ./.
Experiments/NNS on/IN several/JJ publicly/RB available/JJ datasets/NNS demonstrate/VBP that/IN the/DT proposed/JJ method/NN is/VBZ able/JJ to/TO learn/VB robust/JJ metric/JJ measures/NNS from/IN small/JJ size/NN dataset/NN and/CC //HYPH or/CC from/IN challenging/JJ training/NN set/VBN with/IN labels/NNS contaminated/VBN by/IN errors/NNS ./.
The/DT proposed/JJ method/NN is/VBZ also/RB shown/VBN to/TO outperform/VB a/DT previous/JJ pairwise/JJ constrained/VBN Bayesian/JJ metric/JJ learning/NN method/NN ./.
