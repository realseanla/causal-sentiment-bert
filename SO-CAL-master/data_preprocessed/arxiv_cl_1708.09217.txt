The/DT attention/NN model/NN has/VBZ become/VBN a/DT standard/JJ component/NN in/IN neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- and/CC it/PRP guides/VBZ translation/NN process/NN by/IN selectively/RB focusing/VBG on/IN parts/NNS of/IN the/DT source/NN sentence/NN when/WRB predicting/VBG each/DT target/NN word/NN ./.
However/RB ,/, we/PRP find/VBP that/IN the/DT generation/NN of/IN a/DT target/NN word/NN does/VBZ not/RB only/RB depend/VB on/IN the/DT source/NN sentence/NN ,/, but/CC also/RB rely/VBP heavily/RB on/IN the/DT previous/JJ generated/VBN target/NN words/NNS ,/, especially/RB the/DT distant/JJ words/NNS which/WDT are/VBP difficult/JJ to/TO model/VB by/IN using/VBG recurrent/JJ neural/JJ networks/NNS ./.
To/TO solve/VB this/DT problem/NN ,/, we/PRP propose/VBP in/IN this/DT paper/NN a/DT novel/JJ look/NN -/HYPH ahead/NN attention/NN mechanism/NN for/IN generation/NN in/IN NMT/NNP ,/, which/WDT aims/VBZ at/IN directly/RB capturing/VBG the/DT dependency/NN relationship/NN between/IN target/NN words/NNS ./.
We/PRP further/RB design/VB three/CD patterns/NNS to/TO integrate/VB our/PRP$ look/NN -/HYPH ahead/NN attention/NN into/IN the/DT conventional/JJ attention/NN model/NN ./.
Experiments/NNS on/IN NIST/NNP Chinese/NNP -/HYPH to/IN -/HYPH English/NNP and/CC WMT/NNP English/NNP -/HYPH to/IN -/HYPH German/JJ translation/NN tasks/NNS show/VBP that/IN our/PRP$ proposed/VBN look/NN -/HYPH ahead/NN attention/NN mechanism/NN achieves/VBZ substantial/JJ improvements/NNS over/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baselines/NNS ./.
