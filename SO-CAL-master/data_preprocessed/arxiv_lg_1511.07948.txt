We/PRP study/VBP non-convex/JJ empirical/JJ risk/NN minimization/NN for/IN learning/VBG halfspaces/NNS and/CC neural/JJ networks/NNS ./.
For/IN loss/NN functions/NNS that/WDT are/VBP $/$ L$/CD -/HYPH Lipschitz/NNP continuous/JJ ,/, we/PRP present/VBP algorithms/NNS to/TO learn/VB halfspaces/NNS and/CC multi-layer/JJ neural/JJ networks/NNS that/WDT achieve/VBP arbitrarily/RB small/JJ excess/JJ risk/NN $/$ \/SYM epsilon/NN &gt;/SYM 0/CD $/$ ./.
The/DT time/NN complexity/NN is/VBZ polynomial/JJ in/IN the/DT input/NN dimension/NN $/$ d/LS $/$ and/CC the/DT sample/NN size/NN $/$ n/NN $/$ ,/, but/CC exponential/JJ in/IN the/DT quantity/NN $/$ (/-LRB- L/NN //HYPH \/SYM epsilon/NN ^/SYM 2/CD )/-RRB- \/SYM log/NN (/-LRB- L/NN //HYPH \/SYM epsilon/NN )/-RRB- $/$ ./.
These/DT algorithms/NNS run/VBP multiple/JJ rounds/NNS of/IN random/JJ initialization/NN followed/VBN by/IN arbitrary/JJ optimization/NN steps/NNS ./.
We/PRP further/RB show/VBP that/IN if/IN the/DT data/NNS is/VBZ separable/JJ by/IN some/DT neural/JJ network/NN with/IN constant/JJ margin/NN $/$ \/SYM gamma/NN &gt;/SYM 0/CD $/$ ,/, then/RB there/EX is/VBZ a/DT polynomial/JJ -/HYPH time/NN algorithm/NN for/IN learning/VBG a/DT neural/JJ network/NN that/WDT separates/VBZ the/DT training/NN data/NNS with/IN margin/NN $/$ \/SYM Omega/NN (/-LRB- \/SYM gamma/NN )/-RRB- $/$ ./.
As/IN a/DT consequence/NN ,/, the/DT algorithm/NN achieves/VBZ arbitrary/JJ generalization/NN error/NN $/$ \/SYM epsilon/NN &gt;/SYM 0/CD $/$ with/IN $/$ {/-LRB- \/SYM rm/NN poly/NN }/-RRB- (/-LRB- d/NN ,1/NN //HYPH \/SYM epsilon/NN )/-RRB- $/$ sample/NN and/CC time/NN complexity/NN ./.
We/PRP establish/VBP the/DT same/JJ learnability/NN result/NN when/WRB the/DT labels/NNS are/VBP randomly/RB flipped/VBN with/IN probability/NN $/$ \/SYM eta/NN &lt;/SYM 1/2/CD $/$ ./.
