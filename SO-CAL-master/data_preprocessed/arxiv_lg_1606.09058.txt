In/IN the/DT present/JJ paper/NN we/PRP show/VBP that/IN distributional/JJ information/NN is/VBZ particularly/RB important/JJ when/WRB considering/VBG concept/NN availability/NN under/IN implicit/JJ language/NN learning/NN conditions/NNS ./.
Based/VBN on/IN results/NNS from/IN different/JJ behavioural/JJ experiments/NNS we/PRP argue/VBP that/IN the/DT implicit/JJ learnability/NN of/IN semantic/JJ regularities/NNS depends/VBZ on/IN the/DT degree/NN to/TO which/WDT the/DT relevant/JJ concept/NN is/VBZ reflected/VBN in/IN language/NN use/NN ./.
In/IN our/PRP$ simulations/NNS ,/, we/PRP train/VBP a/DT Vector/NNP -/HYPH Space/NNP model/NN on/IN either/CC an/DT English/NNP or/CC a/DT Chinese/JJ corpus/NN and/CC then/RB feed/VB the/DT resulting/VBG representations/NNS to/IN a/DT feed/NN -/HYPH forward/JJ neural/JJ network/NN ./.
The/DT task/NN of/IN the/DT neural/JJ network/NN was/VBD to/TO find/VB a/DT mapping/NN between/IN the/DT word/NN representations/NNS and/CC the/DT novel/JJ words/NNS ./.
Using/VBG datasets/NNS from/IN four/CD behavioural/JJ experiments/NNS ,/, which/WDT used/VBD different/JJ semantic/JJ manipulations/NNS ,/, we/PRP were/VBD able/JJ to/TO obtain/VB learning/NN patterns/NNS very/RB similar/JJ to/IN those/DT obtained/VBN by/IN humans/NNS ./.
