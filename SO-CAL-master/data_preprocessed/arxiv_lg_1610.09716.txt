Building/NNP large/JJ models/NNS with/IN parameter/NN sharing/NN accounts/VBZ for/IN most/JJS of/IN the/DT success/NN of/IN deep/JJ convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP doubly/RB convolutional/JJ neural/JJ networks/NNS (/-LRB- DCNNs/NNP )/-RRB- ,/, which/WDT significantly/RB improve/VBP the/DT performance/NN of/IN CNNs/NNS by/IN further/JJ exploring/VBG this/DT idea/NN ./.
In/IN stead/NN of/IN allocating/VBG a/DT set/NN of/IN convolutional/JJ filters/NNS that/WDT are/VBP independently/RB learned/VBN ,/, a/DT DCNN/NNP maintains/VBZ groups/NNS of/IN filters/NNS where/WRB filters/NNS within/IN each/DT group/NN are/VBP translated/VBN versions/NNS of/IN each/DT other/JJ ./.
Practically/RB ,/, a/DT DCNN/NNP can/MD be/VB easily/RB implemented/VBN by/IN a/DT two/CD -/HYPH step/NN convolution/NN procedure/NN ,/, which/WDT is/VBZ supported/VBN by/IN most/JJS modern/JJ deep/JJ learning/NN libraries/NNS ./.
We/PRP perform/VBP extensive/JJ experiments/NNS on/IN three/CD image/NN classification/NN benchmarks/NNS :/: CIFAR/NNP -/HYPH 10/CD ,/, CIFAR/NN -/HYPH 100/CD and/CC ImageNet/NNP ,/, and/CC show/VBP that/IN DCNNs/NNS consistently/RB outperform/VBP other/JJ competing/VBG architectures/NNS ./.
We/PRP have/VBP also/RB verified/VBN that/IN replacing/VBG a/DT convolutional/JJ layer/NN with/IN a/DT doubly/RB convolutional/JJ layer/NN at/IN any/DT depth/NN of/IN a/DT CNN/NNP can/MD improve/VB its/PRP$ performance/NN ./.
Moreover/RB ,/, various/JJ design/NN choices/NNS of/IN DCNNs/NNS are/VBP demonstrated/VBN ,/, which/WDT shows/VBZ that/IN DCNN/NNP can/MD serve/VB the/DT dual/JJ purpose/NN of/IN building/VBG more/RBR accurate/JJ models/NNS and/CC //HYPH or/CC reducing/VBG the/DT memory/NN footprint/NN without/IN sacrificing/VBG the/DT accuracy/NN ./.
