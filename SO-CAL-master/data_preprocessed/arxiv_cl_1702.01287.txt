We/PRP introduce/VBP a/DT Multi-modal/JJ Neural/JJ Machine/NN Translation/NN model/NN in/IN which/WDT a/DT doubly/RB -/HYPH attentive/JJ decoder/NN naturally/RB incorporates/VBZ spatial/JJ visual/JJ features/NNS obtained/VBN using/VBG pre-trained/JJ convolutional/JJ neural/JJ networks/NNS ,/, bridging/VBG the/DT gap/NN between/IN image/NN description/NN and/CC translation/NN ./.
Our/PRP$ decoder/NN learns/VBZ to/TO attend/VB to/IN source/NN -/HYPH language/NN words/NNS and/CC parts/NNS of/IN an/DT image/NN independently/RB by/IN means/NNS of/IN two/CD separate/JJ attention/NN mechanisms/NNS as/IN it/PRP generates/VBZ words/NNS in/IN the/DT target/NN language/NN ./.
We/PRP find/VBP that/IN our/PRP$ model/NN can/MD efficiently/RB exploit/VB not/RB just/RB back/RB -/HYPH translated/VBN in/IN -/HYPH domain/NN multi-modal/JJ data/NNS but/CC also/RB large/JJ general/JJ -/HYPH domain/NN text/NN -/HYPH only/JJ MT/NN corpora/NNS ./.
We/PRP also/RB report/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN the/DT Multi30k/NN data/NNS set/VBN ./.
