Distributional/JJ word/NN representation/NN methods/NNS exploit/VBP word/NN co-occurrences/NNS to/TO build/VB compact/JJ vector/NN encodings/NNS of/IN words/NNS ./.
While/IN these/DT representations/NNS enjoy/VBP widespread/JJ use/NN in/IN modern/JJ natural/JJ language/NN processing/NN ,/, it/PRP is/VBZ unclear/JJ whether/IN they/PRP accurately/RB encode/VBP all/DT necessary/JJ facets/NNS of/IN conceptual/JJ meaning/NN ./.
In/IN this/DT paper/NN ,/, we/PRP evaluate/VBP how/WRB well/RB these/DT representations/NNS can/MD predict/VB perceptual/JJ and/CC conceptual/JJ features/NNS of/IN concrete/JJ concepts/NNS ,/, drawing/VBG on/IN two/CD semantic/JJ norm/NN datasets/NNS sourced/VBN from/IN human/JJ participants/NNS ./.
We/PRP find/VBP that/IN several/JJ standard/JJ word/NN representations/NNS fail/VBP to/TO encode/VB many/JJ salient/JJ perceptual/JJ features/NNS of/IN concepts/NNS ,/, and/CC show/VBP that/IN these/DT deficits/NNS correlate/VBP with/IN word/NN -/HYPH word/NN similarity/NN prediction/NN errors/NNS ./.
Our/PRP$ analyses/NNS provide/VBP motivation/NN for/IN grounded/VBN and/CC embodied/VBN language/NN learning/NN approaches/NNS ,/, which/WDT may/MD help/VB to/TO remedy/VB these/DT deficits/NNS ./.
