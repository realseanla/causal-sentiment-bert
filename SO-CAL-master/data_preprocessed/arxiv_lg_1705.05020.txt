This/DT paper/NN proposes/VBZ an/DT approach/NN for/IN tackling/VBG an/DT abstract/JJ formulation/NN of/IN weakly/RB supervised/JJ learning/NN ,/, which/WDT is/VBZ posed/VBN as/IN a/DT joint/JJ optimization/NN problem/NN in/IN the/DT continuous/JJ model/NN parameters/NNS and/CC discrete/JJ label/NN variables/NNS ./.
We/PRP devise/VBP a/DT novel/JJ decomposition/NN of/IN the/DT latter/JJ into/IN purely/RB discrete/JJ and/CC continuous/JJ subproblems/NNS within/IN the/DT framework/NN of/IN the/DT Alternating/NNP Direction/NNP Method/NNP of/IN Multipliers/NNPS (/-LRB- ADMM/NNP )/-RRB- ,/, which/WDT allows/VBZ to/TO efficiently/RB compute/VB a/DT local/JJ minimum/NN of/IN the/DT nonconvex/JJ objective/NN function/NN ./.
Our/PRP$ approach/NN preserves/VBZ integrality/NN of/IN the/DT discrete/JJ label/NN variables/NNS and/CC admits/VBZ a/DT globally/RB convergent/JJ kernel/NN formulation/NN ./.
The/DT resulting/VBG method/NN implicitly/RB alternates/VBZ between/IN a/DT discrete/JJ and/CC a/DT continuous/JJ variable/JJ update/NN ,/, however/RB ,/, it/PRP is/VBZ inherently/RB different/JJ from/IN simple/JJ alternating/VBG optimization/NN (/-LRB- hard/JJ EM/NN )/-RRB- ./.
In/IN numerous/JJ experiments/NNS we/PRP illustrate/VBP that/IN our/PRP$ method/NN can/MD learn/VB a/DT classifier/NN from/IN weak/JJ and/CC abstract/JJ combinatorial/JJ supervision/NN thereby/RB being/VBG superior/JJ towards/IN hard/JJ EM/NN ./.
