Although/IN attention/NN -/HYPH based/VBN Neural/JJ Machine/NN Translation/NN have/VBP achieved/VBN great/JJ success/NN ,/, attention/NN -/, mechanism/NN can/MD not/RB capture/VB the/DT entire/JJ meaning/NN of/IN the/DT source/NN sentence/NN because/IN the/DT attention/NN mechanism/NN generates/VBZ a/DT target/NN word/NN depending/VBG heavily/RB on/IN the/DT relevant/JJ parts/NNS of/IN the/DT source/NN sentence/NN ./.
The/DT report/NN of/IN earlier/JJR studies/NNS has/VBZ introduced/VBN a/DT latent/JJ variable/NN to/TO capture/VB the/DT entire/JJ meaning/NN of/IN sentence/NN and/CC achieved/VBD improvement/NN on/IN attention/NN -/HYPH based/VBN Neural/JJ Machine/NN Translation/NN ./.
We/PRP follow/VBP this/DT approach/NN and/CC we/PRP believe/VBP that/IN the/DT capturing/NN meaning/NN of/IN sentence/NN benefits/NNS from/IN image/NN information/NN because/IN human/JJ beings/NNS understand/VB the/DT meaning/NN of/IN language/NN not/RB only/RB from/IN textual/JJ information/NN but/CC also/RB from/IN perceptual/JJ information/NN such/JJ as/IN that/DT gained/VBN from/IN vision/NN ./.
As/IN described/VBN herein/RB ,/, we/PRP propose/VBP a/DT neural/JJ machine/NN translation/NN model/NN that/WDT introduces/VBZ a/DT continuous/JJ latent/NN variable/NN containing/VBG an/DT underlying/VBG semantic/JJ extracted/VBN from/IN texts/NNS and/CC images/NNS ./.
Our/PRP$ model/NN ,/, which/WDT can/MD be/VB trained/VBN end/NN -/HYPH to/IN -/HYPH end/NN ,/, requires/VBZ image/NN information/NN only/RB when/WRB training/NN ./.
Experiments/NNS conducted/VBN with/IN an/DT English/NNP --/: German/JJ translation/NN task/NN show/VBP that/IN our/PRP$ model/NN outperforms/VBZ over/IN the/DT baseline/NN ./.
