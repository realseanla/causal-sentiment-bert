In/IN real/JJ -/HYPH time/NN speech/NN recognition/NN applications/NNS ,/, the/DT latency/NN is/VBZ an/DT important/JJ issue/NN ./.
We/PRP have/VBP developed/VBN a/DT character/NN -/HYPH level/NN incremental/JJ speech/NN recognition/NN (/-LRB- ISR/NN )/-RRB- system/NN that/WDT responds/VBZ quickly/RB even/RB during/IN the/DT speech/NN ,/, where/WRB the/DT hypotheses/NNS are/VBP gradually/RB improved/VBN while/IN the/DT speaking/VBG proceeds/NNS ./.
The/DT algorithm/NN employs/VBZ a/DT speech/NN -/HYPH to/IN -/HYPH character/NN unidirectional/JJ recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- ,/, which/WDT is/VBZ end/NN -/HYPH to/IN -/HYPH end/NN trained/VBN with/IN connectionist/JJ temporal/JJ classification/NN (/-LRB- CTC/NN )/-RRB- ,/, and/CC an/DT RNN/NN -/HYPH based/VBN character/NN -/HYPH level/NN language/NN model/NN (/-LRB- LM/NN )/-RRB- ./.
The/DT output/NN values/NNS of/IN the/DT CTC/NN -/HYPH trained/VBN RNN/NN are/VBP character/NN -/HYPH level/NN probabilities/NNS ,/, which/WDT are/VBP processed/VBN by/IN beam/NN search/NN decoding/NN ./.
The/DT RNN/NN LM/NN augments/VBZ the/DT decoding/NN by/IN providing/VBG long/JJ -/HYPH term/NN dependency/NN information/NN ./.
We/PRP propose/VBP tree/NN -/HYPH based/VBN online/JJ beam/NN search/NN with/IN additional/JJ depth/NN -/HYPH pruning/NN ,/, which/WDT enables/VBZ the/DT system/NN to/TO process/VB infinitely/RB long/JJ input/NN speech/NN with/IN low/JJ latency/NN ./.
This/DT system/NN not/RB only/RB responds/VBZ quickly/RB on/IN speech/NN but/CC also/RB can/MD dictate/VB out/RB -/HYPH of/IN -/HYPH vocabulary/NN (/-LRB- OOV/NN )/-RRB- words/NNS according/VBG to/IN pronunciation/NN ./.
The/DT proposed/VBN model/NN achieves/VBZ the/DT word/NN error/NN rate/NN (/-LRB- WER/NN )/-RRB- of/IN 8.90/CD percent/NN on/IN the/DT Wall/NNP Street/NNP Journal/NNP (/-LRB- WSJ/NNP )/-RRB- Nov/NNP '92/CD 20K/NN evaluation/NN set/VBN when/WRB trained/VBN on/IN the/DT WSJ/NNP SI/NN -/HYPH 284/CD training/NN set/NN ./.
