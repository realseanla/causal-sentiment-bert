Although/IN deep/JJ neural/JJ networks/NNS (/-LRB- DNN/NN )/-RRB- are/VBP able/JJ to/TO scale/VB with/IN direct/JJ advances/NNS in/IN computational/JJ power/NN (/-LRB- e.g./FW ,/, memory/NN and/CC processing/NN speed/NN )/-RRB- ,/, they/PRP are/VBP not/RB well/RB suited/JJ to/TO exploit/VB the/DT recent/JJ trends/NNS for/IN parallel/JJ architectures/NNS ./.
In/IN particular/JJ ,/, gradient/NN descent/NN is/VBZ a/DT sequential/JJ process/NN and/CC the/DT resulting/VBG serial/JJ dependencies/NNS mean/VBP that/IN DNN/NNP training/NN can/MD not/RB be/VB parallelized/VBN effectively/RB ./.
Here/RB ,/, we/PRP show/VBP that/IN a/DT DNN/NN may/MD be/VB replicated/VBN over/IN a/DT massive/JJ parallel/JJ architecture/NN and/CC used/VBN to/TO provide/VB a/DT cumulative/JJ sampling/NN of/IN local/JJ solution/NN space/NN which/WDT results/VBZ in/IN rapid/JJ and/CC robust/JJ learning/NN ./.
We/PRP introduce/VBP a/DT complimentary/JJ convolutional/JJ bootstrapping/NN approach/NN that/WDT enhances/VBZ performance/NN of/IN the/DT parallel/JJ architecture/NN further/RB ./.
Our/PRP$ parallelized/JJ convolutional/JJ bootstrapping/NN DNN/NN out/IN -/HYPH performs/VBZ an/DT identical/JJ fully/RB -/HYPH trained/VBN traditional/JJ DNN/NN after/IN only/RB a/DT single/JJ iteration/NN of/IN training/NN ./.
