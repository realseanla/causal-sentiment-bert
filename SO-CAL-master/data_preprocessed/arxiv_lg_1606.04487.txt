We/PRP perform/VBP a/DT study/NN of/IN the/DT factors/NNS affecting/VBG training/NN time/NN in/IN multi-device/JJ deep/JJ learning/NN systems/NNS ./.
Given/VBN a/DT specification/NN of/IN a/DT convolutional/JJ neural/JJ network/NN ,/, we/PRP study/VBP how/WRB to/TO minimize/VB the/DT time/NN to/TO train/VB this/DT model/NN on/IN a/DT cluster/NN of/IN commodity/NN CPUs/NNS and/CC GPUs/NNS ./.
Our/PRP$ first/JJ contribution/NN focuses/VBZ on/IN the/DT single/JJ -/HYPH node/NN setting/NN ,/, in/IN which/WDT we/PRP show/VBP that/IN by/IN using/VBG standard/JJ batching/NN and/CC data/NN -/HYPH parallel/JJ techniques/NNS throughput/NN can/MD be/VB improved/VBN by/IN at/IN least/RBS 5.5/CD x/SYM over/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS when/WRB training/VBG on/IN CPUs/NNS ./.
This/DT ensures/VBZ an/DT end/NN -/HYPH to/IN -/HYPH end/NN training/NN time/NN directly/RB proportional/JJ to/IN the/DT throughput/NN of/IN a/DT device/NN regardless/RB of/IN its/PRP$ underlying/VBG hardware/NN ,/, allowing/VBG each/DT node/NN in/IN the/DT cluster/NN to/TO be/VB treated/VBN as/IN a/DT black/JJ box/NN ./.
Our/PRP$ second/JJ contribution/NN is/VBZ a/DT theoretical/JJ and/CC empirical/JJ study/NN of/IN the/DT tradeoffs/NNS affecting/VBG end/NN -/HYPH to/IN -/HYPH end/NN training/NN time/NN in/IN a/DT multiple/JJ -/HYPH device/NN setting/NN ./.
We/PRP identify/VBP the/DT degree/NN of/IN asynchronous/JJ parallelization/NN as/IN a/DT key/JJ feature/NN affecting/VBG both/CC hardware/NN and/CC statistical/JJ efficiency/NN ./.
We/PRP show/VBP that/IN asynchrony/NN can/MD be/VB viewed/VBN as/IN introducing/VBG a/DT momentum/NN parameter/NN ,/, which/WDT we/PRP use/VBP to/TO limit/VB our/PRP$ search/NN space/NN ;/: in/IN turn/NN ,/, this/DT leads/VBZ to/IN a/DT simpler/JJR optimizer/NN ,/, which/WDT is/VBZ our/PRP$ third/JJ contribution/NN ./.
Our/PRP$ optimizer/NN involves/VBZ a/DT predictive/JJ model/NN for/IN the/DT total/JJ time/NN to/IN convergence/NN and/CC selects/VBZ an/DT allocation/NN of/IN resources/NNS to/TO minimize/VB that/DT time/NN ./.
We/PRP demonstrate/VBP that/IN the/DT most/RBS popular/JJ distributed/VBN deep/JJ learning/NN systems/NNS fall/VBP within/IN our/PRP$ tradeoff/NN space/NN but/CC do/VBP not/RB optimize/VB within/IN the/DT space/NN ./.
By/IN doing/VBG such/JJ optimization/NN ,/, our/PRP$ prototype/NN runs/VBZ 1.9/CD x/NN to/IN 12x/NN faster/RBR than/IN the/DT fastest/JJS state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS ./.
