We/PRP consider/VBP the/DT problem/NN of/IN principal/JJ component/NN analysis/NN (/-LRB- PCA/NN )/-RRB- in/IN a/DT streaming/NN stochastic/JJ setting/NN ,/, where/WRB our/PRP$ goal/NN is/VBZ to/TO find/VB a/DT direction/NN of/IN approximate/JJ maximal/JJ variance/NN ,/, based/VBN on/IN a/DT stream/NN of/IN i.i.d./NN data/NNS points/NNS in/IN $/$ \/CD mathbb/NN {/-LRB- R/NN }/-RRB- ^/SYM d/NN $/$ ./.
A/DT simple/JJ and/CC computationally/RB cheap/JJ algorithm/NN for/IN this/DT is/VBZ stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- ,/, which/WDT incrementally/RB updates/NNS its/PRP$ estimate/NN based/VBN on/IN each/DT new/JJ data/NNS point/NN ./.
However/RB ,/, due/IN to/IN the/DT non-convex/JJ nature/NN of/IN the/DT problem/NN ,/, analyzing/VBG its/PRP$ performance/NN has/VBZ been/VBN a/DT challenge/NN ./.
In/IN particular/JJ ,/, existing/VBG guarantees/NNS rely/VBP on/IN a/DT non-trivial/JJ eigengap/NN assumption/NN on/IN the/DT covariance/NN matrix/NN ,/, which/WDT is/VBZ intuitively/RB unnecessary/JJ ./.
In/IN this/DT note/NN ,/, we/PRP provide/VBP (/-LRB- to/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN )/-RRB- the/DT first/JJ eigengap/NN -/HYPH free/JJ convergence/NN guarantees/NNS for/IN SGD/NNP in/IN the/DT context/NN of/IN PCA/NNP ./.
This/DT also/RB partially/RB resolves/VBZ an/DT open/JJ problem/NN posed/VBN in/IN [/-LRB- Hardt/NNP and/CC Price/NNP ,/, 2014/CD ]/-RRB- ./.
