We/PRP propose/VBP DoReFa/NNP -/HYPH Net/NNP ,/, a/DT method/NN to/TO train/VB convolutional/JJ neural/JJ networks/NNS that/WDT have/VBP low/JJ bitwidth/JJ weights/NNS and/CC activations/NNS using/VBG low/JJ bitwidth/JJ parameter/NN gradients/NNS ./.
In/IN particular/JJ ,/, during/IN backward/JJ pass/NN ,/, parameter/NN gradients/NNS are/VBP stochastically/RB quantized/VBN to/IN low/JJ bitwidth/JJ numbers/NNS before/IN being/VBG propagated/VBN to/IN convolutional/JJ layers/NNS ./.
As/IN convolutions/NNS during/IN forward/JJ //HYPH backward/JJ passes/NNS can/MD now/RB operate/VB on/IN low/JJ bitwidth/JJ weights/NNS and/CC activations/NNS //, gradients/NNS respectively/RB ,/, DoReFa/NN -/HYPH Net/NN can/MD use/VB bit/NN convolution/NN kernels/NNS to/TO accelerate/VB both/DT training/NN and/CC inference/NN ./.
Moreover/RB ,/, as/IN bit/NN convolutions/NNS can/MD be/VB efficiently/RB implemented/VBN on/IN CPU/NN ,/, FPGA/NNP ,/, ASIC/NNP and/CC GPU/NNP ,/, DoReFatNet/NNP opens/VBZ the/DT way/NN to/TO accelerate/VB training/NN of/IN low/JJ bitwidth/JJ neural/JJ network/NN on/IN these/DT hardware/NN ./.
Our/PRP$ experiments/NNS on/IN SVHN/NNP and/CC ImageNet/NNP datasets/NNS prove/VBP that/IN DoReFa/NNP -/HYPH Net/NNP can/MD achieve/VB comparable/JJ prediction/NN accuracy/NN as/IN 32/CD -/HYPH bit/NN counterparts/NNS ./.
For/IN example/NN ,/, a/DT DoReFa/NN -/HYPH Net/NN derived/VBN from/IN AlexNet/NNP that/WDT has/VBZ 1/CD -/HYPH bit/NN weights/NNS ,/, 2/CD -/HYPH bit/NN activations/NNS ,/, can/MD be/VB trained/VBN from/IN scratch/NN using/VBG 4/CD -/HYPH bit/NN gradients/NNS to/TO get/VB 47/CD \/SYM percent/NN top/JJ -/HYPH 1/CD accuracy/NN on/IN ImageNet/NNP validation/NN set/NN ./.
The/DT DoReFa/NNP -/HYPH Net/NNP AlexNet/NNP model/NN is/VBZ released/VBN publicly/RB ./.
