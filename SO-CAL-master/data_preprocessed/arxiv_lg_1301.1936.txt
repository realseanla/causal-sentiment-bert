Stochastic/JJ multi-armed/JJ bandits/NNS solve/VB the/DT Exploration/NNP -/HYPH Exploitation/NNP dilemma/NN and/CC ultimately/RB maximize/VB the/DT expected/VBN reward/NN ./.
Nonetheless/RB ,/, in/IN many/JJ practical/JJ problems/NNS ,/, maximizing/VBG the/DT expected/VBN reward/NN is/VBZ not/RB the/DT most/RBS desirable/JJ objective/NN ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP a/DT novel/JJ setting/NN based/VBN on/IN the/DT principle/NN of/IN risk/NN -/HYPH aversion/NN where/WRB the/DT objective/NN is/VBZ to/TO compete/VB against/IN the/DT arm/NN with/IN the/DT best/JJS risk/NN -/HYPH return/NN trade/NN -/HYPH off/NN ./.
This/DT setting/NN proves/VBZ to/TO be/VB intrinsically/RB more/RBR difficult/JJ than/IN the/DT standard/JJ multi-arm/JJ bandit/NN setting/VBG due/JJ in/IN part/NN to/IN an/DT exploration/NN risk/NN which/WDT introduces/VBZ a/DT regret/NN associated/VBN to/IN the/DT variability/NN of/IN an/DT algorithm/NN ./.
Using/VBG variance/NN as/IN a/DT measure/NN of/IN risk/NN ,/, we/PRP introduce/VBP two/CD new/JJ algorithms/NNS ,/, investigate/VB their/PRP$ theoretical/JJ guarantees/NNS ,/, and/CC report/NN preliminary/JJ empirical/JJ results/NNS ./.
