We/PRP consider/VBP the/DT task/NN of/IN evaluating/VBG a/DT policy/NN for/IN a/DT Markov/NNP decision/NN process/NN (/-LRB- MDP/NN )/-RRB- ./.
The/DT standard/JJ unbiased/JJ technique/NN for/IN evaluating/VBG a/DT policy/NN is/VBZ to/TO deploy/VB the/DT policy/NN and/CC observe/VB its/PRP$ performance/NN ./.
We/PRP show/VBP that/IN the/DT data/NNS collected/VBN from/IN deploying/VBG a/DT different/JJ policy/NN ,/, commonly/RB called/VBN the/DT behavior/NN policy/NN ,/, can/MD be/VB used/VBN to/TO produce/VB unbiased/JJ estimates/NNS with/IN lower/JJR mean/JJ squared/VBD error/NN than/IN this/DT standard/JJ technique/NN ./.
We/PRP derive/VBP an/DT analytic/JJ expression/NN for/IN the/DT optimal/JJ behavior/NN policy/NN ---/, the/DT behavior/NN policy/NN that/WDT minimizes/VBZ the/DT mean/JJ squared/JJ error/NN of/IN the/DT resulting/VBG estimates/NNS ./.
Because/IN this/DT expression/NN depends/VBZ on/IN terms/NNS that/WDT are/VBP unknown/JJ in/IN practice/NN ,/, we/PRP propose/VBP a/DT novel/JJ policy/NN evaluation/NN sub-problem/NN ,/, behavior/NN policy/NN search/NN :/: searching/VBG for/IN a/DT behavior/NN policy/NN that/WDT reduces/VBZ mean/JJ squared/JJ error/NN ./.
We/PRP present/VBP a/DT behavior/NN policy/NN search/NN algorithm/NN and/CC empirically/RB demonstrate/VBP its/PRP$ effectiveness/NN in/IN lowering/VBG the/DT mean/NN squared/VBD error/NN of/IN policy/NN performance/NN estimates/NNS ./.
