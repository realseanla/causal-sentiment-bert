In/IN distributed/VBN ,/, or/CC privacy/NN -/HYPH preserving/VBG learning/NN ,/, we/PRP are/VBP often/RB given/VBN a/DT set/NN of/IN probabilistic/JJ models/NNS estimated/VBN from/IN different/JJ local/JJ repositories/NNS ,/, and/CC asked/VBD to/TO combine/VB them/PRP into/IN a/DT single/JJ model/NN that/WDT gives/VBZ efficient/JJ statistical/JJ estimation/NN ./.
A/DT simple/JJ method/NN is/VBZ to/TO linearly/RB average/VB the/DT parameters/NNS of/IN the/DT local/JJ models/NNS ,/, which/WDT ,/, however/RB ,/, tends/VBZ to/TO be/VB degenerate/JJ or/CC not/RB applicable/JJ on/IN non-convex/JJ models/NNS ,/, or/CC models/NNS with/IN different/JJ parameter/NN dimensions/NNS ./.
One/CD more/JJR practical/JJ strategy/NN is/VBZ to/TO generate/VB bootstrap/NN samples/NNS from/IN the/DT local/JJ models/NNS ,/, and/CC then/RB learn/VB a/DT joint/JJ model/NN based/VBN on/IN the/DT combined/VBN bootstrap/NN set/NN ./.
Unfortunately/RB ,/, the/DT bootstrap/NN procedure/NN introduces/VBZ additional/JJ noise/NN and/CC can/MD significantly/RB deteriorate/VB the/DT performance/NN ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP two/CD variance/NN reduction/NN methods/NNS to/TO correct/VB the/DT bootstrap/NN noise/NN ,/, including/VBG a/DT weighted/JJ M/NN -/HYPH estimator/NN that/WDT is/VBZ both/CC statistically/RB efficient/JJ and/CC practically/RB powerful/JJ ./.
Both/DT theoretical/JJ and/CC empirical/JJ analysis/NN is/VBZ provided/VBN to/TO demonstrate/VB our/PRP$ methods/NNS ./.
