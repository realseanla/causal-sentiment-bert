A/DT core/NN challenge/NN for/IN an/DT agent/NN learning/NN to/TO interact/VB with/IN the/DT world/NN is/VBZ to/TO predict/VB how/WRB its/PRP$ actions/NNS affect/VBP objects/NNS in/IN its/PRP$ environment/NN ./.
Many/JJ existing/VBG methods/NNS for/IN learning/VBG the/DT dynamics/NNS of/IN physical/JJ interactions/NNS require/VBP labeled/JJ object/NN information/NN ./.
However/RB ,/, to/TO scale/VB real/JJ -/HYPH world/NN interaction/NN learning/NN to/IN a/DT variety/NN of/IN scenes/NNS and/CC objects/NNS ,/, acquiring/VBG labeled/VBN data/NNS becomes/VBZ increasingly/RB impractical/JJ ./.
To/TO learn/VB about/IN physical/JJ object/NN motion/NN without/IN labels/NNS ,/, we/PRP develop/VBP an/DT action/NN -/HYPH conditioned/VBN video/NN prediction/NN model/NN that/WDT explicitly/RB models/NNS pixel/NN motion/NN ,/, by/IN predicting/VBG a/DT distribution/NN over/IN pixel/NN motion/NN from/IN previous/JJ frames/NNS ./.
Because/IN our/PRP$ model/NN explicitly/RB predicts/VBZ motion/NN ,/, it/PRP is/VBZ partially/RB invariant/JJ to/TO object/VB appearance/NN ,/, enabling/VBG it/PRP to/TO generalize/VB to/IN previously/RB unseen/JJ objects/NNS ./.
To/TO explore/VB video/NN prediction/NN for/IN real/JJ -/HYPH world/NN interactive/JJ agents/NNS ,/, we/PRP also/RB introduce/VBP a/DT dataset/NN of/IN 50,000/CD robot/NN interactions/NNS involving/VBG pushing/VBG motions/NNS ,/, including/VBG a/DT test/NN set/VBN with/IN novel/JJ objects/NNS ./.
In/IN this/DT dataset/NN ,/, accurate/JJ prediction/NN of/IN videos/NNS conditioned/VBN on/IN the/DT robot/NN 's/POS future/JJ actions/NNS amounts/VBZ to/IN learning/VBG a/DT "/`` visual/JJ imagination/NN "/'' of/IN different/JJ futures/NNS based/VBN on/IN different/JJ courses/NNS of/IN action/NN ./.
Our/PRP$ experiments/NNS show/VBP that/IN our/PRP$ proposed/JJ method/NN not/RB only/RB produces/VBZ more/RBR accurate/JJ video/NN predictions/NNS ,/, but/CC also/RB more/RBR accurately/RB predicts/VBZ object/NN motion/NN ,/, when/WRB compared/VBN to/IN prior/JJ methods/NNS ./.
