The/DT paper/NN studies/NNS machine/NN learning/NN problems/NNS where/WRB each/DT example/NN is/VBZ described/VBN using/VBG a/DT set/NN of/IN Boolean/JJ features/NNS and/CC where/WRB hypotheses/NNS are/VBP represented/VBN by/IN linear/JJ threshold/NN elements/NNS ./.
One/CD method/NN of/IN increasing/VBG the/DT expressiveness/NN of/IN learned/VBN hypotheses/NNS in/IN this/DT context/NN is/VBZ to/TO expand/VB the/DT feature/NN set/VBN to/TO include/VB conjunctions/NNS of/IN basic/JJ features/NNS ./.
This/DT can/MD be/VB done/VBN explicitly/RB or/CC where/WRB possible/JJ by/IN using/VBG a/DT kernel/NN function/NN ./.
Focusing/VBG on/IN the/DT well/RB known/VBN Perceptron/NNP and/CC Winnow/NNP algorithms/NNS ,/, the/DT paper/NN demonstrates/VBZ a/DT tradeoff/NN between/IN the/DT computational/JJ efficiency/NN with/IN which/WDT the/DT algorithm/NN can/MD be/VB run/VBN over/IN the/DT expanded/VBN feature/NN space/NN and/CC the/DT generalization/NN ability/NN of/IN the/DT corresponding/VBG learning/NN algorithm/NN ./.
We/PRP first/RB describe/VB several/JJ kernel/NN functions/NNS which/WDT capture/VBP either/CC limited/JJ forms/NNS of/IN conjunctions/NNS or/CC all/DT conjunctions/NNS ./.
We/PRP show/VBP that/IN these/DT kernels/NNS can/MD be/VB used/VBN to/TO efficiently/RB run/VB the/DT Perceptron/NNP algorithm/NN over/IN a/DT feature/NN space/NN of/IN exponentially/RB many/JJ conjunctions/NNS ;/: however/RB we/PRP also/RB show/VBP that/IN using/VBG such/JJ kernels/NNS ,/, the/DT Perceptron/NNP algorithm/NN can/MD provably/RB make/VB an/DT exponential/JJ number/NN of/IN mistakes/NNS even/RB when/WRB learning/VBG simple/JJ functions/NNS ./.
We/PRP then/RB consider/VBP the/DT question/NN of/IN whether/IN kernel/NN functions/NNS can/MD analogously/RB be/VB used/VBN to/TO run/VB the/DT multiplicative/JJ -/HYPH update/NN Winnow/NNP algorithm/NN over/IN an/DT expanded/VBN feature/NN space/NN of/IN exponentially/RB many/JJ conjunctions/NNS ./.
Known/JJ upper/JJ bounds/NNS imply/VBP that/IN the/DT Winnow/NNP algorithm/NN can/MD learn/VB Disjunctive/JJ Normal/JJ Form/NN (/-LRB- DNF/NN )/-RRB- formulae/NNS with/IN a/DT polynomial/JJ mistake/NN bound/VBN in/IN this/DT setting/NN ./.
However/RB ,/, we/PRP prove/VBP that/IN it/PRP is/VBZ computationally/RB hard/JJ to/TO simulate/VB Winnows/NNP behavior/NN for/IN learning/VBG DNF/NN over/IN such/PDT a/DT feature/NN set/NN ./.
This/DT implies/VBZ that/IN the/DT kernel/NN functions/NNS which/WDT correspond/VBP to/IN running/VBG Winnow/NNP for/IN this/DT problem/NN are/VBP not/RB efficiently/RB computable/JJ ,/, and/CC that/IN there/EX is/VBZ no/DT general/JJ construction/NN that/WDT can/MD run/VB Winnow/NNP with/IN kernels/NNS ./.
