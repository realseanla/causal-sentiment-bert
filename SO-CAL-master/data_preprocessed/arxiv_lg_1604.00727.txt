We/PRP show/VBP that/IN an/DT encoder/NN -/HYPH decoder/NN framework/NN can/MD be/VB successfully/RB be/VB applied/VBN to/IN question/NN -/HYPH answering/VBG with/IN a/DT structured/JJ knowledge/NN base/NN ./.
In/IN addition/NN ,/, we/PRP propose/VBP a/DT new/JJ character/NN -/HYPH level/NN modeling/NN approach/NN for/IN this/DT task/NN ,/, which/WDT we/PRP use/VBP to/TO make/VB our/PRP$ model/NN robust/JJ to/IN unseen/JJ entities/NNS and/CC predicates/NNS ./.
We/PRP use/VBP our/PRP$ model/NN for/IN single/JJ -/HYPH relation/NN question/NN answering/VBG ,/, and/CC demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ novel/JJ approach/NN on/IN the/DT SimpleQuestions/NNPS dataset/NN ,/, where/WRB we/PRP improve/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN accuracy/NN by/IN 2/CD percent/NN for/IN both/DT Freebase2M/NN and/CC Freebase5M/NN subsets/NNS proposed/VBN ./.
Importantly/RB ,/, we/PRP achieve/VBP these/DT results/NNS even/RB though/IN our/PRP$ character/NN -/HYPH level/NN model/NN has/VBZ 16x/CD less/JJR parameters/NNS than/IN an/DT equivalent/JJ word/NN -/HYPH embedding/NN model/NN ,/, uses/VBZ significantly/RB less/JJR training/NN data/NNS than/IN previous/JJ work/NN which/WDT relies/VBZ on/IN data/NNS augmentation/NN ,/, and/CC encounters/VBZ only/RB 1.18/CD percent/NN of/IN the/DT entities/NNS seen/VBN during/IN training/NN when/WRB testing/NN ./.
