In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ image/NN representation/NN based/VBN on/IN a/DT multilayer/JJ kernel/NN machine/NN that/WDT performs/VBZ end/NN -/HYPH to/IN -/HYPH end/NN learning/NN ./.
Unlike/IN traditional/JJ kernel/NN methods/NNS ,/, where/WRB the/DT kernel/NN is/VBZ handcrafted/VBN or/CC adapted/VBN to/IN data/NNS in/IN an/DT unsupervised/JJ manner/NN ,/, we/PRP learn/VBP how/WRB to/TO shape/VB the/DT kernel/NN for/IN a/DT supervised/JJ prediction/NN problem/NN ./.
We/PRP proceed/VBP by/IN generalizing/VBG convolutional/JJ kernel/NN networks/NNS ,/, which/WDT originally/RB provide/VBP unsupervised/JJ image/NN representations/NNS ,/, and/CC we/PRP derive/VBP backpropagation/NN rules/NNS to/TO optimize/VB model/NN parameters/NNS ./.
As/IN a/DT result/NN ,/, we/PRP obtain/VBP a/DT new/JJ type/NN of/IN convolutional/JJ neural/JJ network/NN with/IN the/DT following/VBG properties/NNS :/: (/-LRB- i/LS )/-RRB- at/IN each/DT layer/NN ,/, learning/NN filters/NNS is/VBZ equivalent/JJ to/IN optimizing/VBG a/DT linear/JJ subspace/NN in/IN a/DT reproducing/VBG kernel/NN Hilbert/NNP space/NN (/-LRB- RKHS/NN )/-RRB- ,/, where/WRB we/PRP project/VBP data/NNS ,/, (/-LRB- ii/LS )/-RRB- the/DT network/NN may/MD be/VB learned/VBN with/IN supervision/NN or/CC without/IN ,/, (/-LRB- iii/LS )/-RRB- the/DT model/NN comes/VBZ with/IN a/DT natural/JJ regularization/NN function/NN (/-LRB- the/DT norm/NN in/IN the/DT RKHS/NN )/-RRB- ./.
We/PRP show/VBP that/IN our/PRP$ method/NN achieves/VBZ reasonably/RB competitive/JJ performance/NN on/IN some/DT standard/JJ "/`` deep/JJ learning/NN "/'' image/NN classification/NN datasets/NNS such/JJ as/IN CIFAR/NN -/HYPH 10/CD and/CC SVHN/NNP ,/, and/CC also/RB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS for/IN image/NN super-resolution/NN ,/, demonstrating/VBG the/DT applicability/NN of/IN our/PRP$ approach/NN to/IN a/DT large/JJ variety/NN of/IN image/NN -/HYPH related/VBN tasks/NNS ./.
