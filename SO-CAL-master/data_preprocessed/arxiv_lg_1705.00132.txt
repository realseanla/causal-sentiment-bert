We/PRP consider/VBP a/DT general/JJ framework/NN of/IN online/JJ learning/NN with/IN expert/NN advice/NN where/WRB the/DT regret/NN is/VBZ defined/VBN with/IN respect/NN to/IN a/DT competitor/NN class/NN defined/VBN by/IN a/DT weighted/JJ automaton/NN over/IN sequences/NNS of/IN experts/NNS ./.
Our/PRP$ framework/NN covers/VBZ several/JJ problems/NNS previously/RB studied/VBN ,/, in/IN particular/JJ that/IN of/IN competing/VBG against/IN k/CD -/HYPH shifting/NN experts/NNS ./.
We/PRP give/VBP a/DT series/NN of/IN algorithms/NNS for/IN this/DT problem/NN ,/, including/VBG an/DT automata/NN -/HYPH based/VBN algorithm/NN extending/VBG weighted/JJ -/HYPH majority/NN and/CC more/RBR efficient/JJ algorithms/NNS based/VBN on/IN the/DT notion/NN of/IN failure/NN transitions/NNS ./.
We/PRP further/RB present/JJ efficient/JJ algorithms/NNS based/VBN on/IN a/DT compact/JJ approximation/NN of/IN the/DT competitor/NN automaton/NN ,/, in/IN particular/JJ efficient/JJ n/NN -/HYPH gram/NN models/NNS obtained/VBN by/IN minimizing/VBG the/DT Renyi/NNP divergence/NN ,/, and/CC present/VB an/DT extensive/JJ study/NN of/IN the/DT approximation/NN properties/NNS of/IN such/JJ models/NNS ./.
We/PRP also/RB extend/VBP our/PRP$ algorithms/NNS and/CC results/NNS to/IN the/DT framework/NN of/IN sleeping/VBG experts/NNS ./.
Finally/RB ,/, we/PRP describe/VBP the/DT extension/NN of/IN our/PRP$ approximation/NN methods/NNS to/IN online/JJ convex/NN optimization/NN and/CC a/DT general/JJ mirror/NN descent/NN setting/NN ./.
