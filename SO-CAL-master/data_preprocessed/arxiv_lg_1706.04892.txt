Kernel/NNP online/RB convex/JJ optimization/NN (/-LRB- KOCO/NNP )/-RRB- is/VBZ a/DT framework/NN combining/VBG the/DT expressiveness/NN of/IN non-parametric/JJ kernel/NN models/NNS with/IN the/DT regret/NN guarantees/NNS of/IN online/JJ learning/NN ./.
First/RB -/HYPH order/NN KOCO/NNP methods/NNS such/JJ as/IN functional/JJ gradient/NN descent/NN require/VBP only/RB $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- t/NN )/-RRB- $/$ time/NN and/CC space/NN per/IN iteration/NN ,/, and/CC ,/, when/WRB the/DT only/JJ information/NN on/IN the/DT losses/NNS is/VBZ their/PRP$ convexity/NN ,/, achieve/VB a/DT minimax/NN optimal/JJ $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ regret/NN ./.
Nonetheless/RB ,/, many/JJ common/JJ losses/NNS in/IN kernel/NN problems/NNS ,/, such/JJ as/IN squared/JJ loss/NN ,/, logistic/JJ loss/NN ,/, and/CC squared/VBD hinge/NN loss/NN posses/NNS stronger/JJR curvature/NN that/WDT can/MD be/VB exploited/VBN ./.
In/IN this/DT case/NN ,/, second/JJ -/HYPH order/NN KOCO/NNP methods/NNS achieve/VBP $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM log/NN (/-LRB- \/SYM text/NN {/-LRB- Det/NN }/-RRB- (/-LRB- \/SYM boldsymbol/NN {/-LRB- K/NN }/-RRB- )/-RRB- )/-RRB- )/-RRB- $/$ regret/NN ,/, which/WDT we/PRP show/VBP scales/NNS as/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- d/NN _/NFP {/-LRB- \/SYM text/NN {/-LRB- eff/NN }/-RRB- }/-RRB- \/SYM log/NN T/NN )/-RRB- $/$ ,/, where/WRB $/$ d/LS _/NFP {/-LRB- \/SYM text/NN {/-LRB- eff/NN }/-RRB- }/-RRB- $/$ is/VBZ the/DT effective/JJ dimension/NN of/IN the/DT problem/NN and/CC is/VBZ usually/RB much/RB smaller/JJR than/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ ./.
The/DT main/JJ drawback/NN of/IN second/JJ -/HYPH order/NN methods/NNS is/VBZ their/PRP$ much/JJ higher/JJR $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- t/NN ^/SYM 2/CD )/-RRB- $/$ space/NN and/CC time/NN complexity/NN ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP kernel/NN online/JJ Newton/NNP step/NN (/-LRB- KONS/NN )/-RRB- ,/, a/DT new/JJ second/JJ -/HYPH order/NN KOCO/NNP method/NN that/WDT also/RB achieves/VBZ $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- d/NN _/NFP {/-LRB- \/SYM text/NN {/-LRB- eff/NN }/-RRB- }/-RRB- \/SYM log/NN T/NN )/-RRB- $/$ regret/NN ./.
To/TO address/VB the/DT computational/JJ complexity/NN of/IN second/JJ -/HYPH order/NN methods/NNS ,/, we/PRP introduce/VBP a/DT new/JJ matrix/NN sketching/VBG algorithm/NN for/IN the/DT kernel/NN matrix/NN $/$ \/CD boldsymbol/NN {/-LRB- K/NN }/-RRB- _/NFP t/NN $/$ ,/, and/CC show/VBP that/IN for/IN a/DT chosen/VBN parameter/NN $/$ \/SYM gamma/NN \/SYM leq/NN 1/CD $/$ our/PRP$ Sketched/NNP -/HYPH KONS/NNP reduces/VBZ the/DT space/NN and/CC time/NN complexity/NN by/IN a/DT factor/NN of/IN $/$ \/SYM gamma/NN ^/SYM 2/CD $/$ to/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- t/NN ^/SYM 2/CD \/SYM gamma/NN ^/SYM 2/CD )/-RRB- $/$ space/NN and/CC time/NN per/IN iteration/NN ,/, while/IN incurring/VBG only/RB $/$ 1/CD //HYPH \/SYM gamma/NN $/$ times/CC more/JJR regret/NN ./.
