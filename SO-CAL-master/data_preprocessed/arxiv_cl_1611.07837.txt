A/DT new/JJ model/NN for/IN video/NN captioning/NN is/VBZ developed/VBN ,/, using/VBG a/DT deep/JJ three/CD -/HYPH dimensional/JJ Convolutional/JJ Neural/JJ Network/NN (/-LRB- C3D/NN )/-RRB- as/IN an/DT encoder/NN for/IN videos/NNS and/CC a/DT Recurrent/JJ Neural/JJ Network/NN (/-LRB- RNN/NN )/-RRB- as/IN a/DT decoder/NN for/IN captions/NNS ./.
We/PRP consider/VBP both/DT "/`` hard/JJ "/'' and/CC "/`` soft/JJ "/'' attention/NN mechanisms/NNS ,/, to/IN adaptively/RB and/CC sequentially/RB focus/VB on/IN different/JJ layers/NNS of/IN features/NNS (/-LRB- levels/NNS of/IN feature/NN "/'' abstraction/NN "/'' )/-RRB- ,/, as/RB well/RB as/IN local/JJ spatiotemporal/JJ regions/NNS of/IN the/DT feature/NN maps/VBZ at/IN each/DT layer/NN ./.
The/DT proposed/VBN approach/NN is/VBZ evaluated/VBN on/IN three/CD benchmark/NN datasets/NNS :/: YouTube2Text/NN ,/, M/NN -/HYPH VAD/NN and/CC MSR/NN -/HYPH VTT/NN ./.
Along/IN with/IN visualizing/VBG the/DT results/NNS and/CC how/WRB the/DT model/NN works/VBZ ,/, these/DT experiments/NNS quantitatively/RB demonstrate/VBP the/DT effectiveness/NN of/IN the/DT proposed/VBN adaptive/JJ spatiotemporal/JJ feature/NN abstraction/NN for/IN translating/VBG videos/NNS to/IN sentences/NNS with/IN rich/JJ semantics/NNS ./.
