A/DT determinantal/JJ point/NN process/NN (/-LRB- DPP/NNP )/-RRB- is/VBZ a/DT probabilistic/JJ model/NN of/IN set/VBN diversity/NN compactly/RB parameterized/VBN by/IN a/DT positive/JJ semi-definite/JJ kernel/NN matrix/NN ./.
To/TO fit/VB a/DT DPP/NNP to/IN a/DT given/VBN task/NN ,/, we/PRP would/MD like/VB to/TO learn/VB the/DT entries/NNS of/IN its/PRP$ kernel/NN matrix/NN by/IN maximizing/VBG the/DT log/NN -/HYPH likelihood/NN of/IN the/DT available/JJ data/NNS ./.
However/RB ,/, log/NN -/HYPH likelihood/NN is/VBZ non-convex/JJ in/IN the/DT entries/NNS of/IN the/DT kernel/NN matrix/NN ,/, and/CC this/DT learning/NN problem/NN is/VBZ conjectured/VBN to/TO be/VB NP/NNP -/HYPH hard/JJ ./.
Thus/RB ,/, previous/JJ work/NN has/VBZ instead/RB focused/VBN on/IN more/RBR restricted/VBN convex/NN learning/NN settings/NNS :/: learning/VBG only/RB a/DT single/JJ weight/NN for/IN each/DT row/NN of/IN the/DT kernel/NN matrix/NN ,/, or/CC learning/VBG weights/NNS for/IN a/DT linear/JJ combination/NN of/IN DPPs/NNS with/IN fixed/VBN kernel/NN matrices/NNS ./.
In/IN this/DT work/NN we/PRP propose/VBP a/DT novel/JJ algorithm/NN for/IN learning/VBG the/DT full/JJ kernel/NN matrix/NN ./.
By/IN changing/VBG the/DT kernel/NN parameterization/NN from/IN matrix/NN entries/NNS to/IN eigenvalues/NNS and/CC eigenvectors/NNS ,/, and/CC then/RB lower/JJR -/HYPH bounding/VBG the/DT likelihood/NN in/IN the/DT manner/NN of/IN expectation/NN -/HYPH maximization/NN algorithms/NNS ,/, we/PRP obtain/VBP an/DT effective/JJ optimization/NN procedure/NN ./.
We/PRP test/VBP our/PRP$ method/NN on/IN a/DT real/JJ -/HYPH world/NN product/NN recommendation/NN task/NN ,/, and/CC achieve/VB relative/JJ gains/NNS of/IN up/RB to/IN 16.5/CD percent/NN in/IN test/NN log/NN -/HYPH likelihood/NN compared/VBN to/IN the/DT naive/JJ approach/NN of/IN maximizing/VBG likelihood/NN by/IN projected/VBN gradient/NN ascent/NN on/IN the/DT entries/NNS of/IN the/DT kernel/NN matrix/NN ./.
