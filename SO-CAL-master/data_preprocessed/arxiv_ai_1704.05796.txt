We/PRP propose/VBP a/DT general/JJ framework/NN called/VBN Network/NNP Dissection/NNP for/IN quantifying/VBG the/DT interpretability/NN of/IN latent/JJ representations/NNS of/IN CNNs/NNS by/IN evaluating/VBG the/DT alignment/NN between/IN individual/JJ hidden/JJ units/NNS and/CC a/DT set/NN of/IN semantic/JJ concepts/NNS ./.
Given/VBN any/DT CNN/NNP model/NN ,/, the/DT proposed/JJ method/NN draws/VBZ on/IN a/DT broad/JJ data/NNS set/NN of/IN visual/JJ concepts/NNS to/TO score/VB the/DT semantics/NNS of/IN hidden/VBN units/NNS at/IN each/DT intermediate/JJ convolutional/JJ layer/NN ./.
The/DT units/NNS with/IN semantics/NNS are/VBP given/VBN labels/NNS across/IN a/DT range/NN of/IN objects/NNS ,/, parts/NNS ,/, scenes/NNS ,/, textures/NNS ,/, materials/NNS ,/, and/CC colors/NNS ./.
We/PRP use/VBP the/DT proposed/JJ method/NN to/TO test/VB the/DT hypothesis/NN that/IN interpretability/NN of/IN units/NNS is/VBZ equivalent/JJ to/IN random/JJ linear/JJ combinations/NNS of/IN units/NNS ,/, then/RB we/PRP apply/VBP our/PRP$ method/NN to/TO compare/VB the/DT latent/JJ representations/NNS of/IN various/JJ networks/NNS when/WRB trained/VBN to/TO solve/VB different/JJ supervised/JJ and/CC self/NN -/HYPH supervised/JJ training/NN tasks/NNS ./.
We/PRP further/RB analyze/VB the/DT effect/NN of/IN training/NN iterations/NNS ,/, compare/VBP networks/NNS trained/VBN with/IN different/JJ initializations/NNS ,/, examine/VBP the/DT impact/NN of/IN network/NN depth/NN and/CC width/NN ,/, and/CC measure/VB the/DT effect/NN of/IN dropout/NN and/CC batch/NN normalization/NN on/IN the/DT interpretability/NN of/IN deep/JJ visual/JJ representations/NNS ./.
We/PRP demonstrate/VBP that/IN the/DT proposed/JJ method/NN can/MD shed/VB light/NN on/IN characteristics/NNS of/IN CNN/NNP models/NNS and/CC training/NN methods/NNS that/WDT go/VBP beyond/IN measurements/NNS of/IN their/PRP$ discriminative/JJ power/NN ./.
