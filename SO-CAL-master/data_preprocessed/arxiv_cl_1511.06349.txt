The/DT standard/JJ unsupervised/JJ recurrent/JJ neural/JJ network/NN language/NN model/NN (/-LRB- RNNLM/NN )/-RRB- generates/VBZ sentences/NNS one/CD word/NN at/IN a/DT time/NN and/CC does/VBZ not/RB work/VB from/IN an/DT explicit/JJ global/JJ distributed/VBN sentence/NN representation/NN ./.
In/IN this/DT work/NN ,/, we/PRP present/VBP an/DT RNN/NN -/HYPH based/VBN variational/JJ autoencoder/NN language/NN model/NN that/WDT incorporates/VBZ distributed/VBN latent/JJ representations/NNS of/IN entire/JJ sentences/NNS ./.
This/DT factorization/NN allows/VBZ it/PRP to/IN explicitly/RB model/NN holistic/JJ properties/NNS of/IN sentences/NNS such/JJ as/IN style/NN ,/, topic/NN ,/, and/CC high/JJ -/HYPH level/NN syntactic/JJ features/NNS ./.
Samples/NNS from/IN the/DT prior/JJ over/IN these/DT sentence/NN representations/NNS remarkably/RB produce/VBP diverse/JJ and/CC well/RB -/HYPH formed/VBN sentences/NNS through/IN simple/JJ deterministic/JJ decoding/NN ./.
By/IN examining/VBG paths/NNS through/IN this/DT latent/JJ space/NN ,/, we/PRP are/VBP able/JJ to/TO generate/VB coherent/JJ novel/JJ sentences/NNS that/WDT interpolate/VBP between/IN known/JJ sentences/NNS ./.
We/PRP present/VBP techniques/NNS for/IN solving/VBG the/DT difficult/JJ learning/NN problem/NN presented/VBN by/IN this/DT model/NN ,/, demonstrate/VBP strong/JJ performance/NN in/IN the/DT imputation/NN of/IN missing/VBG tokens/NNS ,/, and/CC explore/VB many/JJ interesting/JJ properties/NNS of/IN the/DT latent/JJ sentence/NN space/NN ./.
