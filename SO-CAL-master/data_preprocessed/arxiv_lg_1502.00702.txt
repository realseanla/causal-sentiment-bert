In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT universal/JJ model/NN for/IN high/JJ -/HYPH dimensional/JJ data/NNS ,/, called/VBD the/DT Hybrid/NN Orthogonal/NNP Projection/NNP and/CC Estimation/NNP (/-LRB- HOPE/NN )/-RRB- model/NN ,/, which/WDT combines/VBZ a/DT linear/JJ orthogonal/JJ projection/NN and/CC a/DT finite/JJ mixture/NN model/NN under/IN a/DT unified/JJ generative/JJ modelling/NN framework/NN ./.
The/DT HOPE/NN model/NN itself/PRP can/MD be/VB learned/VBN unsupervisedly/RB from/IN un-labelled/JJ data/NNS based/VBN on/IN the/DT maximum/JJ likelihood/NN estimation/NN as/RB well/RB as/IN trained/VBN discriminatively/RB from/IN labelled/VBN data/NNS ./.
More/RBR interestingly/RB ,/, we/PRP have/VBP shown/VBN the/DT proposed/VBN HOPE/NN models/NNS are/VBP closely/RB related/VBN to/IN neural/JJ networks/NNS (/-LRB- NNs/NNS )/-RRB- in/IN a/DT sense/NN that/IN each/DT NN/NNP hidden/VBN layer/NN can/MD be/VB reformulated/VBN as/IN a/DT HOPE/NN model/NN ./.
As/IN a/DT result/NN ,/, the/DT HOPE/NN framework/NN can/MD be/VB used/VBN as/IN a/DT novel/JJ tool/NN to/TO probe/VB why/WRB and/CC how/WRB NNs/NNPS work/VBP ,/, more/RBR importantly/RB ,/, it/PRP also/RB provides/VBZ several/JJ new/JJ learning/NN algorithms/NNS to/TO learn/VB NNs/NNP either/CC supervisedly/RB or/CC unsupervisedly/RB ./.
In/IN this/DT work/NN ,/, we/PRP have/VBP investigated/VBN the/DT HOPE/NN framework/NN in/IN learning/VBG NNs/NNS for/IN several/JJ standard/JJ tasks/NNS ,/, including/VBG image/NN recognition/NN on/IN MNIST/NNP and/CC speech/NN recognition/NN on/IN TIMIT/NNP ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT HOPE/NN framework/NN yields/NNS significant/JJ performance/NN gains/NNS over/IN the/DT current/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS in/IN various/JJ types/NNS of/IN NN/NNP learning/NN problems/NNS ,/, including/VBG unsupervised/JJ feature/NN learning/NN ,/, supervised/VBD or/CC semi-supervised/VBD learning/NN ./.
