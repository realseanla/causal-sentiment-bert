We/PRP consider/VBP the/DT problem/NN of/IN online/JJ combinatorial/JJ optimization/NN under/IN semi-bandit/NN feedback/NN ./.
The/DT goal/NN of/IN the/DT learner/NN is/VBZ to/TO sequentially/RB select/VB its/PRP$ actions/NNS from/IN a/DT combinatorial/JJ decision/NN set/VBN so/RB as/IN to/TO minimize/VB its/PRP$ cumulative/JJ loss/NN ./.
We/PRP propose/VBP a/DT learning/NN algorithm/NN for/IN this/DT problem/NN based/VBN on/IN combining/VBG the/DT Follow/VB -/HYPH the/DT -/HYPH Perturbed/VBN -/HYPH Leader/NN (/-LRB- FPL/NNP )/-RRB- prediction/NN method/NN with/IN a/DT novel/JJ loss/NN estimation/NN procedure/NN called/VBN Geometric/NNP Resampling/NNP (/-LRB- GR/NNP )/-RRB- ./.
Contrary/JJ to/IN previous/JJ solutions/NNS ,/, the/DT resulting/VBG algorithm/NN can/MD be/VB efficiently/RB implemented/VBN for/IN any/DT decision/NN set/NN where/WRB efficient/JJ offline/RB combinatorial/JJ optimization/NN is/VBZ possible/JJ at/IN all/DT ./.
Assuming/VBG that/IN the/DT elements/NNS of/IN the/DT decision/NN set/NN can/MD be/VB described/VBN with/IN d/NN -/HYPH dimensional/JJ binary/NN vectors/NNS with/IN at/IN most/RBS m/NN non-zero/JJ entries/NNS ,/, we/PRP show/VBP that/IN the/DT expected/VBN regret/NN of/IN our/PRP$ algorithm/NN after/IN T/NN rounds/NNS is/VBZ O/NN (/-LRB- m/NN sqrt/NN (/-LRB- dT/NN log/NN d/NN )/-RRB- )/-RRB- ./.
As/IN a/DT side/NN result/NN ,/, we/PRP also/RB improve/VBP the/DT best/JJS known/VBN regret/NN bounds/NNS for/IN FPL/NNP in/IN the/DT full/JJ information/NN setting/VBG to/IN O/NN (/-LRB- m/NN ^/SYM (/-LRB- 3/2/CD )/-RRB- sqrt/NN (/-LRB- T/NN log/NN d/NN )/-RRB- )/-RRB- ,/, gaining/VBG a/DT factor/NN of/IN sqrt/NN (/-LRB- d/NN //HYPH m/NN )/-RRB- over/IN previous/JJ bounds/NNS for/IN this/DT algorithm/NN ./.
