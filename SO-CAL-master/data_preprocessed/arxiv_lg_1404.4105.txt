We/PRP propose/VBP a/DT new/JJ approach/NN for/IN metric/JJ learning/NN by/IN framing/VBG it/PRP as/IN learning/VBG a/DT sparse/JJ combination/NN of/IN locally/RB discriminative/JJ metrics/NNS that/WDT are/VBP inexpensive/JJ to/TO generate/VB from/IN the/DT training/NN data/NNS ./.
This/DT flexible/JJ framework/NN allows/VBZ us/PRP to/TO naturally/RB derive/VB formulations/NNS for/IN global/JJ ,/, multi-task/VB and/CC local/JJ metric/JJ learning/NN ./.
The/DT resulting/VBG algorithms/NNS have/VBP several/JJ advantages/NNS over/IN existing/VBG methods/NNS in/IN the/DT literature/NN :/: a/DT much/RB smaller/JJR number/NN of/IN parameters/NNS to/TO be/VB estimated/VBN and/CC a/DT principled/JJ way/NN to/TO generalize/VB learned/VBN metrics/NNS to/IN new/JJ testing/NN data/NNS points/NNS ./.
To/TO analyze/VB the/DT approach/NN theoretically/RB ,/, we/PRP derive/VBP a/DT generalization/NN bound/VBD that/IN justifies/VBZ the/DT sparse/JJ combination/NN ./.
Empirically/RB ,/, we/PRP evaluate/VBP our/PRP$ algorithms/NNS on/IN several/JJ datasets/NNS against/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN metric/JJ learning/NN methods/NNS ./.
The/DT results/NNS are/VBP consistent/JJ with/IN our/PRP$ theoretical/JJ findings/NNS and/CC demonstrate/VBP the/DT superiority/NN of/IN our/PRP$ approach/NN in/IN terms/NNS of/IN classification/NN performance/NN and/CC scalability/NN ./.
