We/PRP study/VBP the/DT prevalent/JJ problem/NN when/WRB a/DT test/NN distribution/NN differs/VBZ from/IN the/DT training/NN distribution/NN ./.
We/PRP consider/VBP a/DT setting/NN where/WRB our/PRP$ training/NN set/NN consists/VBZ of/IN a/DT small/JJ number/NN of/IN sample/NN domains/NNS ,/, but/CC where/WRB we/PRP have/VBP many/JJ samples/NNS in/IN each/DT domain/NN ./.
Our/PRP$ goal/NN is/VBZ to/TO generalize/VB to/IN a/DT new/JJ domain/NN ./.
For/IN example/NN ,/, we/PRP may/MD want/VB to/TO learn/VB a/DT similarity/NN function/NN using/VBG only/RB certain/JJ classes/NNS of/IN objects/NNS ,/, but/CC we/PRP desire/VBP that/IN this/DT similarity/NN function/NN be/VB applicable/JJ to/IN object/NN classes/NNS not/RB present/JJ in/IN our/PRP$ training/NN sample/NN (/-LRB- e.g./FW we/PRP might/MD seek/VB to/TO learn/VB that/IN "/`` dogs/NNS are/VBP similar/JJ to/IN dogs/NNS "/'' even/RB though/IN images/NNS of/IN dogs/NNS were/VBD absent/JJ from/IN our/PRP$ training/NN set/NN )/-RRB- ./.
Our/PRP$ theoretical/JJ analysis/NN shows/VBZ that/IN we/PRP can/MD select/VB many/JJ more/JJR features/NNS than/IN domains/NNS while/IN avoiding/VBG overfitting/NN by/IN utilizing/VBG data/NNS -/HYPH dependent/JJ variance/NN properties/NNS ./.
We/PRP present/VBP a/DT greedy/JJ feature/NN selection/NN algorithm/NN based/VBN on/IN using/VBG T/NN -/HYPH statistics/NNS ./.
Our/PRP$ experiments/NNS validate/VBP this/DT theory/NN showing/VBG that/IN our/PRP$ T/NN -/HYPH statistic/NN based/VBN greedy/JJ feature/NN selection/NN is/VBZ more/RBR robust/JJ at/IN avoiding/VBG overfitting/NN than/IN the/DT classical/JJ greedy/JJ procedure/NN ./.
