We/PRP study/VBP a/DT stochastic/JJ and/CC distributed/VBN algorithm/NN for/IN nonconvex/JJ problems/NNS whose/WP$ objective/NN consists/VBZ of/IN a/DT sum/NN of/IN $/$ N$/CD nonconvex/JJ $/$ L_i/CD //SYM N$/NN -/HYPH smooth/JJ functions/NNS ,/, plus/CC a/DT nonsmooth/JJ regularizer/NN ./.
The/DT proposed/VBN NonconvEx/NN primal/JJ -/HYPH dual/JJ SpliTTing/NN (/-LRB- NESTT/NN )/-RRB- algorithm/NN splits/VBZ the/DT problem/NN into/IN $/$ N$/CD subproblems/NNS ,/, and/CC utilizes/VBZ an/DT augmented/VBN Lagrangian/NNP based/VBN primal/JJ -/HYPH dual/JJ scheme/NN to/TO solve/VB it/PRP in/IN a/DT distributed/VBN and/CC stochastic/JJ manner/NN ./.
With/IN a/DT special/JJ non-uniform/JJ sampling/NN ,/, a/DT version/NN of/IN NESTT/NN achieves/VBZ $/$ \/CD epsilon/CD $/$ -/HYPH stationary/JJ solution/NN using/VBG $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- (/-LRB- \/SYM sum/NN _/NFP {/-LRB- i/PRP =/SYM 1/CD }/-RRB- ^/SYM N/NN \/SYM sqrt/NN {/-LRB- L_i/NN //HYPH N/NN }/-RRB- )/-RRB- ^/SYM 2/CD //SYM \/SYM epsilon/SYM )/-RRB- $/$ gradient/NN evaluations/NNS ,/, which/WDT can/MD be/VB up/RB to/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- N/NN )/-RRB- $/$ times/NNS better/JJR than/IN the/DT (/-LRB- proximal/JJ )/-RRB- gradient/NN descent/NN methods/NNS ./.
It/PRP also/RB achieves/VBZ Q/NN -/HYPH linear/JJ convergence/NN rate/NN for/IN nonconvex/JJ $/$ \/CD ell_1/CD $/$ penalized/VBN quadratic/JJ problems/NNS with/IN polyhedral/JJ constraints/NNS ./.
Further/RB ,/, we/PRP reveal/VBP a/DT fundamental/JJ connection/NN between/IN {/-LRB- \/SYM it/PRP primal/JJ -/HYPH dual/JJ }/-RRB- based/VBN methods/NNS and/CC a/DT few/JJ {/-LRB- \/SYM it/PRP primal/JJ only/RB }/-RRB- methods/NNS such/JJ as/IN IAG/NN //HYPH SAG/NN //HYPH SAGA/NN ./.
