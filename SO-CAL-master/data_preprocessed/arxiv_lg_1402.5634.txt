To/TO achieve/VB acceptable/JJ performance/NN for/IN AI/NN tasks/NNS ,/, one/PRP can/MD either/RB use/VB sophisticated/JJ feature/NN extraction/NN methods/NNS as/IN the/DT first/JJ layer/NN in/IN a/DT two/CD -/HYPH layered/VBN supervised/JJ learning/NN model/NN ,/, or/CC learn/VB the/DT features/NNS directly/RB using/VBG a/DT deep/JJ (/-LRB- multi-layered/JJ )/-RRB- model/NN ./.
While/IN the/DT first/JJ approach/NN is/VBZ very/RB problem/NN -/HYPH specific/JJ ,/, the/DT second/JJ approach/NN has/VBZ computational/JJ overheads/NNS in/IN learning/VBG multiple/JJ layers/NNS and/CC fine/JJ -/HYPH tuning/NN of/IN the/DT model/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP an/DT approach/NN called/VBD wide/JJ learning/NN based/VBN on/IN arc/NN -/HYPH cosine/NN kernels/NNS ,/, that/DT learns/VBZ a/DT single/JJ layer/NN of/IN infinite/JJ width/NN ./.
We/PRP propose/VBP exact/JJ and/CC inexact/JJ learning/NN strategies/NNS for/IN wide/JJ learning/NN and/CC show/VBP that/IN wide/JJ learning/NN with/IN single/JJ layer/NN outperforms/VBZ single/JJ layer/NN as/RB well/RB as/IN deep/JJ architectures/NNS of/IN finite/JJ width/NN for/IN some/DT benchmark/NN datasets/NNS ./.
