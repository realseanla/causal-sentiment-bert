Nowadays/RB ,/, the/DT number/NN of/IN layers/NNS and/CC of/IN neurons/NNS in/IN each/DT layer/NN of/IN a/DT deep/JJ network/NN are/VBP typically/RB set/VBN manually/RB ./.
While/IN very/RB deep/JJ and/CC wide/JJ networks/NNS have/VBP proven/VBN effective/JJ in/IN general/JJ ,/, they/PRP come/VBP at/IN a/DT high/JJ memory/NN and/CC computation/NN cost/NN ,/, thus/RB making/VBG them/PRP impractical/JJ for/IN constrained/VBN platforms/NNS ./.
These/DT networks/NNS ,/, however/RB ,/, are/VBP known/VBN to/TO have/VB many/JJ redundant/JJ parameters/NNS ,/, and/CC could/MD thus/RB ,/, in/IN principle/NN ,/, be/VB replaced/VBN by/IN more/JJR compact/JJ architectures/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP an/DT approach/NN to/IN automatically/RB determining/VBG the/DT number/NN of/IN neurons/NNS in/IN each/DT layer/NN of/IN a/DT deep/JJ network/NN during/IN learning/NN ./.
To/IN this/DT end/NN ,/, we/PRP propose/VBP to/TO make/VB use/NN of/IN a/DT group/NN sparsity/NN regularizer/NN on/IN the/DT parameters/NNS of/IN the/DT network/NN ,/, where/WRB each/DT group/NN is/VBZ defined/VBN to/TO act/VB on/IN a/DT single/JJ neuron/NN ./.
Starting/VBG from/IN an/DT overcomplete/JJ network/NN ,/, we/PRP show/VBP that/IN our/PRP$ approach/NN can/MD reduce/VB the/DT number/NN of/IN parameters/NNS by/IN up/RB to/IN 80/CD \/SYM percent/NN while/IN retaining/VBG or/CC even/RB improving/VBG the/DT network/NN accuracy/NN ./.
