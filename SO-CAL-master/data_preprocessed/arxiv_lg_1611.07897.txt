We/PRP propose/VBP a/DT new/JJ encoder/NN -/HYPH decoder/NN approach/NN to/TO learn/VB distributed/VBN sentence/NN representations/NNS from/IN unlabeled/JJ sentences/NNS ./.
The/DT word/NN -/HYPH to/IN -/HYPH vector/NN representation/NN is/VBZ used/VBN ,/, and/CC convolutional/JJ neural/JJ networks/NNS are/VBP employed/VBN as/IN sentence/NN encoders/NNS ,/, mapping/VBG an/DT input/NN sentence/NN into/IN a/DT fixed/VBN -/HYPH length/NN vector/NN ./.
This/DT representation/NN is/VBZ decoded/VBN using/VBG long/JJ short/JJ -/HYPH term/NN memory/NN recurrent/JJ neural/JJ networks/NNS ,/, considering/VBG several/JJ tasks/NNS ,/, such/JJ as/IN reconstructing/VBG the/DT input/NN sentence/NN ,/, or/CC predicting/VBG the/DT future/JJ sentence/NN ./.
We/PRP further/RB describe/VBP a/DT hierarchical/JJ encoder/NN -/HYPH decoder/NN model/NN to/TO encode/VB a/DT sentence/NN to/TO predict/VB multiple/JJ future/JJ sentences/NNS ./.
By/IN training/VBG our/PRP$ models/NNS on/IN a/DT large/JJ collection/NN of/IN novels/NNS ,/, we/PRP obtain/VBP a/DT highly/RB generic/JJ convolutional/JJ sentence/NN encoder/NN that/WDT performs/VBZ well/RB in/IN practice/NN ./.
Experimental/JJ results/NNS on/IN several/JJ benchmark/NN datasets/NNS ,/, and/CC across/IN a/DT broad/JJ range/NN of/IN applications/NNS ,/, demonstrate/VBP the/DT superiority/NN of/IN the/DT proposed/VBN model/NN over/IN competing/VBG methods/NNS ./.
