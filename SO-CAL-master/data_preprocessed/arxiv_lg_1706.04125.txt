We/PRP consider/VBP prediction/NN with/IN expert/NN advice/NN when/WRB the/DT loss/NN vectors/NNS are/VBP assumed/VBN to/TO lie/VB in/IN a/DT set/NN described/VBN by/IN the/DT sum/NN of/IN atomic/JJ norm/NN balls/NNS ./.
We/PRP derive/VBP a/DT regret/NN bound/VBN for/IN a/DT general/JJ version/NN of/IN the/DT online/JJ mirror/NN descent/NN (/-LRB- OMD/NN )/-RRB- algorithm/NN that/WDT uses/VBZ a/DT combination/NN of/IN regularizers/NNS ,/, each/DT adapted/VBN to/IN the/DT constituent/JJ atomic/JJ norms/NNS ./.
The/DT general/JJ result/NN recovers/VBZ standard/JJ OMD/NNP regret/NN bounds/NNS ,/, and/CC yields/NNS regret/VBP bounds/NNS for/IN new/JJ structured/JJ settings/NNS where/WRB the/DT loss/NN vectors/NNS are/VBP (/-LRB- i/LS )/-RRB- noisy/JJ versions/NNS of/IN points/NNS from/IN a/DT low/JJ -/HYPH rank/NN subspace/NN ,/, (/-LRB- ii/LS )/-RRB- sparse/JJ vectors/NNS corrupted/VBN with/IN noise/NN ,/, and/CC (/-LRB- iii/LS )/-RRB- sparse/JJ perturbations/NNS of/IN low/JJ -/HYPH rank/NN vectors/NNS ./.
For/IN the/DT problem/NN of/IN online/JJ learning/NN with/IN structured/JJ losses/NNS ,/, we/PRP also/RB show/VBP lower/JJR bounds/NNS on/IN regret/NN in/IN terms/NNS of/IN rank/NN and/CC sparsity/NN of/IN the/DT source/NN set/NN of/IN the/DT loss/NN vectors/NNS ,/, which/WDT implies/VBZ lower/JJR bounds/NNS for/IN the/DT above/JJ additive/JJ loss/NN settings/NNS as/RB well/RB ./.
