We/PRP present/VBP a/DT deep/JJ layered/JJ architecture/NN that/WDT generalizes/VBZ classical/JJ convolutional/JJ neural/JJ networks/NNS (/-LRB- ConvNets/NNP )/-RRB- ./.
The/DT architecture/NN ,/, called/VBN SimNets/NNP ,/, is/VBZ driven/VBN by/IN two/CD operators/NNS ,/, one/CD being/VBG a/DT similarity/NN function/NN whose/WP$ family/NN contains/VBZ the/DT convolution/NN operator/NN used/VBN in/IN ConvNets/NNP ,/, and/CC the/DT other/JJ is/VBZ a/DT new/JJ '/`` soft/JJ max/NN -/HYPH min/NN -/HYPH mean/NN '/'' operator/NN called/VBN MMECS/NN that/WDT realizes/VBZ classical/JJ operators/NNS like/IN ReLU/NN and/CC max/NN -/HYPH pooling/VBG ,/, but/CC has/VBZ additional/JJ capabilities/NNS that/WDT make/VBP SimNets/NNS a/DT powerful/JJ generalization/NN of/IN ConvNets/NNP ./.
Two/CD interesting/JJ properties/NNS that/WDT emerge/VBP from/IN the/DT architecture/NN are/VBP :/: (/-LRB- i/LS )/-RRB- the/DT basic/JJ input/NN to/IN hidden/JJ -/HYPH units/NNS to/IN output/NN -/HYPH nodes/NNS machinery/NN contains/VBZ as/IN special/JJ case/NN a/DT kernel/NN machine/NN ,/, and/CC (/-LRB- ii/LS )/-RRB- initializing/VBG networks/NNS using/VBG unsupervised/JJ learning/NN is/VBZ natural/JJ ./.
Experiments/NNS demonstrate/VBP the/DT capability/NN of/IN achieving/VBG state/NN of/IN the/DT art/NN accuracy/NN with/IN networks/NNS that/WDT are/VBP 1/8/CD the/DT size/NN of/IN comparable/JJ ConvNets/NNPS ./.
