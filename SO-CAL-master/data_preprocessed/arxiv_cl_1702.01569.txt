A/DT fundamental/JJ challenge/NN in/IN developing/VBG semantic/JJ parsers/NNS is/VBZ the/DT paucity/NN of/IN strong/JJ supervision/NN in/IN the/DT form/NN of/IN language/NN utterances/NNS annotated/VBN with/IN logical/JJ form/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO exploit/VB structural/JJ regularities/NNS in/IN language/NN in/IN different/JJ domains/NNS ,/, and/CC train/NN semantic/JJ parsers/NNS over/IN multiple/JJ knowledge/NN -/HYPH bases/NNS (/-LRB- KBs/NNS )/-RRB- ,/, while/IN sharing/VBG information/NN across/IN datasets/NNS ./.
We/PRP find/VBP that/IN we/PRP can/MD substantially/RB improve/VB parsing/VBG accuracy/NN by/IN training/VBG a/DT single/JJ sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN over/IN multiple/JJ KBs/NNS ,/, when/WRB providing/VBG an/DT encoding/NN of/IN the/DT domain/NN at/IN decoding/NN time/NN ./.
Our/PRP$ model/NN achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN the/DT Overnight/RB dataset/NN (/-LRB- containing/VBG eight/CD domains/NNS )/-RRB- ,/, improves/VBZ performance/NN over/IN a/DT single/JJ KB/NN baseline/NN from/IN 75.6/CD percent/NN to/IN 79.6/CD percent/NN ,/, while/IN obtaining/VBG a/DT 7x/NN reduction/NN in/IN the/DT number/NN of/IN model/NN parameters/NNS ./.
