Two/CD types/NNS of/IN low/JJ cost/NN -/HYPH per/IN -/HYPH iteration/NN gradient/NN descent/NN methods/NNS have/VBP been/VBN extensively/RB studied/VBN in/IN parallel/NN ./.
One/CD is/VBZ online/JJ or/CC stochastic/JJ gradient/NN descent/NN (/-LRB- OGD/NN //HYPH SGD/NN )/-RRB- ,/, and/CC the/DT other/JJ is/VBZ randomzied/VBN coordinate/JJ descent/NN (/-LRB- RBCD/NN )/-RRB- ./.
In/IN this/DT paper/NN ,/, for/IN the/DT first/JJ time/NN ,/, we/PRP combine/VBP the/DT two/CD types/NNS of/IN methods/NNS together/RB and/CC propose/VB online/JJ randomized/JJ block/NN coordinate/NN descent/NN (/-LRB- ORBCD/NN )/-RRB- ./.
At/IN each/DT iteration/NN ,/, ORBCD/NN only/RB computes/VBZ the/DT partial/JJ gradient/NN of/IN one/CD block/NN coordinate/NN of/IN one/CD mini-batch/NN samples/NNS ./.
ORBCD/NN is/VBZ well/RB suited/JJ for/IN the/DT composite/JJ minimization/NN problem/NN where/WRB one/CD function/NN is/VBZ the/DT average/NN of/IN the/DT losses/NNS of/IN a/DT large/JJ number/NN of/IN samples/NNS and/CC the/DT other/JJ is/VBZ a/DT simple/JJ regularizer/NN defined/VBN on/IN high/JJ dimensional/JJ variables/NNS ./.
We/PRP show/VBP that/IN the/DT iteration/NN complexity/NN of/IN ORBCD/NN has/VBZ the/DT same/JJ order/NN as/IN OGD/NNP or/CC SGD/NNP ./.
For/IN strongly/RB convex/JJ functions/NNS ,/, by/IN reducing/VBG the/DT variance/NN of/IN stochastic/JJ gradients/NNS ,/, we/PRP show/VBP that/IN ORBCD/NN can/MD converge/VB at/IN a/DT geometric/JJ rate/NN in/IN expectation/NN ,/, matching/VBG the/DT convergence/NN rate/NN of/IN SGD/NNP with/IN variance/NN reduction/NN and/CC RBCD/NN ./.
