Deep/JJ neural/JJ network/NN is/VBZ difficult/JJ to/TO train/VB and/CC this/DT predicament/NN becomes/VBZ worse/JJR as/IN the/DT depth/NN increases/NNS ./.
The/DT essence/NN of/IN this/DT problem/NN exists/VBZ in/IN the/DT magnitude/NN of/IN backpropagated/JJ errors/NNS that/WDT will/MD result/VB in/IN gradient/NN vanishing/VBG or/CC exploding/VBG phenomenon/NN ./.
We/PRP show/VBP that/IN a/DT variant/NN of/IN regularizer/NN which/WDT utilizes/VBZ orthonormality/NN among/IN different/JJ filter/NN banks/NNS can/MD alleviate/VB this/DT problem/NN ./.
Moreover/RB ,/, we/PRP design/VBP a/DT backward/JJ error/NN modulation/NN mechanism/NN based/VBN on/IN the/DT quasi-isometry/NN assumption/NN between/IN two/CD consecutive/JJ parametric/JJ layers/NNS ./.
Equipped/VBN with/IN these/DT two/CD ingredients/NNS ,/, we/PRP propose/VBP several/JJ novel/JJ optimization/NN solutions/NNS that/WDT can/MD be/VB utilized/VBN for/IN training/VBG a/DT specific/JJ -/HYPH structured/JJ (/-LRB- repetitively/RB triple/JJ modules/NNS of/IN Conv/NNP -/HYPH BNReLU/NNP )/-RRB- extremely/RB deep/JJ convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- WITHOUT/IN any/DT shortcuts/NNS //, identity/NN mappings/NNS from/IN scratch/NN ./.
Experiments/NNS show/VBP that/IN our/PRP$ proposed/VBN solutions/NNS can/MD achieve/VB 4/CD percent/NN improvement/NN for/IN a/DT 44/CD -/HYPH layer/NN plain/JJ network/NN and/CC almost/RB 50/CD percent/NN improvement/NN for/IN a/DT 110/CD -/HYPH layer/NN plain/JJ network/NN on/IN the/DT CIFAR/NN -/HYPH 10/CD dataset/NN ./.
Moreover/RB ,/, we/PRP can/MD successfully/RB train/VB plain/JJ CNNs/NNS to/TO match/VB the/DT performance/NN of/IN the/DT residual/JJ counterparts/NNS ./.
Besides/RB ,/, we/PRP propose/VBP new/JJ principles/NNS for/IN designing/VBG network/NN structure/NN from/IN the/DT insights/NNS evoked/VBN by/IN orthonormality/NN ./.
Combined/VBN with/IN residual/JJ structure/NN ,/, we/PRP achieve/VBP comparative/JJ performance/NN on/IN the/DT ImageNet/NNP dataset/NN ./.
