We/PRP consider/VBP the/DT problem/NN of/IN off/RB -/HYPH policy/NN evaluation/NN ---/, estimating/VBG the/DT value/NN of/IN a/DT target/NN policy/NN using/VBG data/NNS collected/VBN by/IN another/DT policy/NN ---/, under/IN the/DT contextual/JJ bandit/NN model/NN ./.
We/PRP establish/VBP a/DT minimax/NN lower/JJR bound/VBN on/IN the/DT mean/JJ squared/JJ error/NN (/-LRB- MSE/NN )/-RRB- ,/, and/CC show/VBP that/IN it/PRP is/VBZ matched/VBN up/RP to/IN constant/JJ factors/NNS by/IN the/DT inverse/JJ propensity/NN scoring/NN (/-LRB- IPS/NNP )/-RRB- estimator/NN ./.
Since/IN in/IN the/DT multi-armed/JJ bandit/NN problem/NN the/DT IPS/NNP is/VBZ suboptimal/JJ (/-LRB- Li/NNP et/NNP ./.
al/NNP ,/, 2015/CD )/-RRB- ,/, our/PRP$ result/NN highlights/VBZ the/DT difficulty/NN of/IN the/DT contextual/JJ setting/NN with/IN non-degenerate/JJ context/NN distributions/NNS ./.
We/PRP further/RB consider/VBP improvements/NNS on/IN this/DT minimax/NN MSE/NN bound/VBN ,/, given/VBN access/NN to/IN a/DT reward/NN model/NN ./.
We/PRP show/VBP that/IN the/DT existing/VBG doubly/RB robust/JJ approach/NN ,/, which/WDT utilizes/VBZ such/PDT a/DT reward/NN model/NN ,/, may/MD continue/VB to/TO suffer/VB from/IN high/JJ variance/NN even/RB when/WRB the/DT reward/NN model/NN is/VBZ perfect/JJ ./.
We/PRP propose/VBP a/DT new/JJ estimator/NN called/VBN SWITCH/NNP which/WDT more/RBR effectively/RB uses/VBZ the/DT reward/NN model/NN and/CC achieves/VBZ a/DT superior/JJ bias/NN -/HYPH variance/NN tradeoff/NN compared/VBN with/IN prior/JJ work/NN ./.
We/PRP prove/VBP an/DT upper/JJ bound/VBN on/IN its/PRP$ MSE/NNP and/CC demonstrate/VB its/PRP$ benefits/NNS empirically/RB on/IN a/DT diverse/JJ collection/NN of/IN datasets/NNS ,/, often/RB seeing/VBG orders/NNS of/IN magnitude/NN improvements/NNS over/IN a/DT number/NN of/IN baselines/NNS ./.
