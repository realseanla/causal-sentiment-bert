Mobile/NNP robots/NNS are/VBP increasingly/RB being/VBG employed/VBN for/IN performing/VBG complex/JJ tasks/NNS in/IN dynamic/JJ environments/NNS ./.
Reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- methods/NNS are/VBP recognized/VBN to/TO be/VB promising/VBG for/IN specifying/VBG such/JJ tasks/NNS in/IN a/DT relatively/RB simple/JJ manner/NN ./.
However/RB ,/, the/DT strong/JJ dependency/NN between/IN the/DT learning/NN method/NN and/CC the/DT task/NN to/TO learn/VB is/VBZ a/DT well/RB -/HYPH known/VBN problem/NN that/WDT restricts/VBZ practical/JJ implementations/NNS of/IN RL/NN in/IN robotics/NNS ,/, often/RB requiring/VBG major/JJ modifications/NNS of/IN parameters/NNS and/CC adding/VBG other/JJ techniques/NNS for/IN each/DT particular/JJ task/NN ./.
In/IN this/DT paper/NN we/PRP present/VBP a/DT practical/JJ core/NN implementation/NN of/IN RL/NN which/WDT enables/VBZ the/DT learning/NN process/NN for/IN multiple/JJ robotic/JJ tasks/NNS with/IN minimal/JJ per/IN -/HYPH task/NN tuning/NN or/CC none/NN ./.
Based/VBN on/IN value/NN iteration/NN methods/NNS ,/, this/DT implementation/NN includes/VBZ a/DT novel/JJ approach/NN for/IN action/NN selection/NN ,/, called/VBN Q/NN -/HYPH biased/VBN softmax/NN regression/NN (/-LRB- QBIASSR/NN )/-RRB- ,/, which/WDT avoids/VBZ poor/JJ performance/NN of/IN the/DT learning/NN process/NN when/WRB the/DT robot/NN reaches/VBZ new/JJ unexplored/JJ states/NNS ./.
Our/PRP$ approach/NN takes/VBZ advantage/NN of/IN the/DT structure/NN of/IN the/DT state/NN space/NN by/IN attending/VBG the/DT physical/JJ variables/NNS involved/VBN (/-LRB- e.g./FW ,/, distances/NNS to/IN obstacles/NNS ,/, X/NN ,/, Y/NN ,/, {/-LRB- \/SYM theta/NN }/-RRB- pose/NN ,/, etc./FW )/-RRB- ,/, thus/RB experienced/JJ sets/NNS of/IN states/NNS may/MD favor/VB the/DT decision/NN -/HYPH making/VBG process/NN of/IN unexplored/JJ or/CC rarely/RB -/HYPH explored/VBN states/NNS ./.
This/DT improvement/NN has/VBZ a/DT relevant/JJ role/NN in/IN reducing/VBG the/DT tuning/NN of/IN the/DT algorithm/NN for/IN particular/JJ tasks/NNS ./.
Experiments/NNS with/IN real/JJ and/CC simulated/JJ robots/NNS ,/, performed/VBN with/IN the/DT software/NN framework/NN also/RB introduced/VBN here/RB ,/, show/VBP that/IN our/PRP$ implementation/NN is/VBZ effectively/RB able/JJ to/TO learn/VB different/JJ robotic/JJ tasks/NNS without/IN tuning/VBG the/DT learning/NN method/NN ./.
Results/NNS also/RB suggest/VBP that/IN the/DT combination/NN of/IN true/JJ online/JJ SARSA/NNP (/-LRB- {/-LRB- \/SYM lambda/NN }/-RRB- )/-RRB- with/IN QBIASSR/NN can/MD outperform/VB the/DT existing/VBG RL/NN core/NN algorithms/NNS in/IN low/JJ -/HYPH dimensional/JJ robotic/JJ tasks/NNS ./.
