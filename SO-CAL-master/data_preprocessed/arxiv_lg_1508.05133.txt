Contemporary/JJ deep/JJ neural/JJ networks/NNS exhibit/VBP impressive/JJ results/NNS on/IN practical/JJ problems/NNS ./.
These/DT networks/NNS generalize/VB well/RB although/IN their/PRP$ inherent/JJ capacity/NN may/MD extend/VB significantly/RB beyond/IN the/DT number/NN of/IN training/NN examples/NNS ./.
We/PRP analyze/VBP this/DT behavior/NN in/IN the/DT context/NN of/IN deep/JJ ,/, infinite/JJ neural/JJ networks/NNS ./.
We/PRP show/VBP that/IN deep/JJ infinite/JJ layers/NNS are/VBP naturally/RB aligned/VBN with/IN Gaussian/JJ processes/NNS and/CC kernel/NN methods/NNS ,/, and/CC devise/VB stochastic/JJ kernels/NNS that/WDT encode/VBP the/DT information/NN of/IN these/DT networks/NNS ./.
We/PRP show/VBP that/IN stability/NN results/NNS apply/VBP despite/IN the/DT size/NN ,/, offering/VBG an/DT explanation/NN for/IN their/PRP$ empirical/JJ success/NN ./.
