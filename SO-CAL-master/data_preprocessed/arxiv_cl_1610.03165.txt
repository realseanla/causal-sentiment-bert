Long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- have/VBP been/VBN shown/VBN to/TO give/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN many/JJ speech/NN recognition/NN tasks/NNS ,/, as/IN they/PRP are/VBP able/JJ to/TO provide/VB the/DT learned/VBN dynamically/RB changing/VBG contextual/JJ window/NN of/IN all/DT sequence/NN history/NN ./.
On/IN the/DT other/JJ hand/NN ,/, the/DT convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- have/VBP brought/VBN significant/JJ improvements/NNS to/TO deep/RB feed/VB -/HYPH forward/RP neural/JJ networks/NNS (/-LRB- FFNNs/NNS )/-RRB- ,/, as/IN they/PRP are/VBP able/JJ to/TO better/RBR reduce/VB spectral/JJ variation/NN in/IN the/DT input/NN signal/NN ./.
In/IN this/DT paper/NN ,/, a/DT network/NN architecture/NN called/VBN as/IN convolutional/JJ recurrent/JJ neural/JJ network/NN (/-LRB- CRNN/NNP )/-RRB- is/VBZ proposed/VBN by/IN combining/VBG the/DT CNN/NNP and/CC LSTM/NNP RNN/NNP ./.
In/IN the/DT proposed/VBN CRNNs/NNS ,/, each/DT speech/NN frame/NN ,/, without/IN adjacent/JJ context/NN frames/NNS ,/, is/VBZ organized/VBN as/IN a/DT number/NN of/IN local/JJ feature/NN patches/NNS along/IN the/DT frequency/NN axis/NN ,/, and/CC then/RB a/DT LSTM/NNP network/NN is/VBZ performed/VBN on/IN each/DT feature/NN patch/NN along/IN the/DT time/NN axis/NN ./.
We/PRP train/VBP and/CC compare/VBP FFNNs/NNS ,/, LSTM/NNP RNNs/NNS and/CC the/DT proposed/VBN LSTM/NN CRNNs/NNS at/IN various/JJ number/NN of/IN configurations/NNS ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT LSTM/NNP CRNNs/NNPS can/MD exceed/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN speech/NN recognition/NN performance/NN ./.
