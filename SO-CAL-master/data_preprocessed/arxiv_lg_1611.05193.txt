We/PRP describe/VBP a/DT method/NN for/IN searching/VBG the/DT optimal/JJ hyper/JJ -/HYPH parameters/NNS in/IN reservoir/NN computing/NN ,/, which/WDT consists/VBZ of/IN a/DT Gaussian/JJ process/NN with/IN Bayesian/JJ optimization/NN ./.
It/PRP provides/VBZ an/DT alternative/NN to/IN other/JJ frequently/RB used/VBN optimization/NN methods/NNS such/JJ as/IN grid/NN ,/, random/JJ ,/, or/CC manual/JJ search/NN ./.
In/IN addition/NN to/IN a/DT set/NN of/IN optimal/JJ hyper/JJ -/HYPH parameters/NNS ,/, the/DT method/NN also/RB provides/VBZ a/DT probability/NN distribution/NN of/IN the/DT cost/NN function/NN as/IN a/DT function/NN of/IN the/DT hyper/JJ -/HYPH parameters/NNS ./.
We/PRP apply/VBP this/DT method/NN to/IN two/CD types/NNS of/IN reservoirs/NNS :/: nonlinear/JJ delay/NN nodes/NNS and/CC echo/VBP state/NN networks/NNS ./.
It/PRP shows/VBZ excellent/JJ performance/NN on/IN all/DT considered/VBN benchmarks/NNS ,/, either/CC matching/VBG or/CC significantly/RB surpassing/VBG expert/JJ human/JJ optimization/NN ./.
We/PRP find/VBP that/IN some/DT values/NNS for/IN hyper/JJ -/HYPH parameters/NNS that/WDT have/VBP become/VBN standard/JJ in/IN the/DT research/NN community/NN ,/, are/VBP in/IN fact/NN suboptimal/JJ for/IN most/JJS of/IN the/DT problems/NNS we/PRP considered/VBD ./.
In/IN general/JJ ,/, the/DT algorithm/NN achieves/VBZ optimal/JJ results/NNS in/IN fewer/JJR iterations/NNS when/WRB compared/VBN to/IN other/JJ optimization/NN methods/NNS ,/, and/CC scales/VBZ well/RB with/IN increasing/VBG dimensionality/NN of/IN the/DT hyper/JJ -/HYPH parameter/NN space/NN ./.
Due/IN to/IN its/PRP$ automated/VBN nature/NN ,/, this/DT method/NN significantly/RB reduces/VBZ the/DT need/NN for/IN expert/JJ knowledge/NN when/WRB optimizing/VBG the/DT hyper/JJ -/HYPH parameters/NNS in/IN reservoir/NN computing/NN ./.
Existing/VBG software/NN libraries/NNS for/IN Bayesian/JJ optimization/NN make/VB the/DT implementation/NN of/IN the/DT algorithm/NN straightforward/JJ ./.
