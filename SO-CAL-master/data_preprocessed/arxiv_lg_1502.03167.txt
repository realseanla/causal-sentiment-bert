Training/VBG Deep/JJ Neural/JJ Networks/NNS is/VBZ complicated/VBN by/IN the/DT fact/NN that/IN the/DT distribution/NN of/IN each/DT layer/NN 's/POS inputs/NNS changes/NNS during/IN training/NN ,/, as/IN the/DT parameters/NNS of/IN the/DT previous/JJ layers/NNS change/VBP ./.
This/DT slows/VBZ down/IN the/DT training/NN by/IN requiring/VBG lower/JJR learning/NN rates/NNS and/CC careful/JJ parameter/NN initialization/NN ,/, and/CC makes/VBZ it/PRP notoriously/RB hard/RB to/TO train/VB models/NNS with/IN saturating/VBG nonlinearities/NNS ./.
We/PRP refer/VBP to/IN this/DT phenomenon/NN as/IN internal/JJ covariate/NN shift/NN ,/, and/CC address/VB the/DT problem/NN by/IN normalizing/VBG layer/NN inputs/NNS ./.
Our/PRP$ method/NN draws/VBZ its/PRP$ strength/NN from/IN making/VBG normalization/NN a/DT part/NN of/IN the/DT model/NN architecture/NN and/CC performing/VBG the/DT normalization/NN for/IN each/DT training/NN mini-batch/NN }/-RRB- ./.
Batch/NN Normalization/NN allows/VBZ us/PRP to/TO use/VB much/RB higher/JJR learning/NN rates/NNS and/CC be/VB less/RBR careful/JJ about/IN initialization/NN ./.
It/PRP also/RB acts/VBZ as/IN a/DT regularizer/NN ,/, in/IN some/DT cases/NNS eliminating/VBG the/DT need/NN for/IN Dropout/NNP ./.
Applied/NNP to/IN a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN image/NN classification/NN model/NN ,/, Batch/NN Normalization/NN achieves/VBZ the/DT same/JJ accuracy/NN with/IN 14/CD times/NNS fewer/JJR training/NN steps/NNS ,/, and/CC beats/VBZ the/DT original/JJ model/NN by/IN a/DT significant/JJ margin/NN ./.
Using/VBG an/DT ensemble/NN of/IN batch/NN -/HYPH normalized/VBN networks/NNS ,/, we/PRP improve/VBP upon/IN the/DT best/JJS published/VBN result/NN on/IN ImageNet/NNP classification/NN :/: reaching/VBG 4.9/CD percent/NN top/NN -/HYPH 5/CD validation/NN error/NN (/-LRB- and/CC 4.8/CD percent/NN test/NN error/NN )/-RRB- ,/, exceeding/VBG the/DT accuracy/NN of/IN human/JJ raters/NNS ./.
