The/DT success/NN of/IN long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- neural/JJ networks/NNS in/IN language/NN processing/NN is/VBZ typically/RB attributed/VBN to/IN their/PRP$ ability/NN to/TO capture/VB long/RB -/HYPH distance/NN statistical/JJ regularities/NNS ./.
Linguistic/JJ regularities/NNS are/VBP often/RB sensitive/JJ to/IN syntactic/JJ structure/NN ;/: can/MD such/JJ dependencies/NNS be/VB captured/VBN by/IN LSTMs/NNPS ,/, which/WDT do/VBP not/RB have/VB explicit/JJ structural/JJ representations/NNS ?/.
We/PRP begin/VBP addressing/VBG this/DT question/NN using/VBG number/NN agreement/NN in/IN English/NNP subject/NN -/, verb/VB dependencies/NNS ./.
We/PRP probe/VBP the/DT architecture/NN 's/POS grammatical/JJ competence/NN both/CC using/VBG training/NN objectives/NNS with/IN an/DT explicit/JJ grammatical/JJ target/NN (/-LRB- number/NN prediction/NN ,/, grammaticality/NN judgments/NNS )/-RRB- and/CC using/VBG language/NN models/NNS ./.
In/IN the/DT strongly/RB supervised/JJ settings/NNS ,/, the/DT LSTM/NNP achieved/VBD very/RB high/JJ overall/JJ accuracy/NN (/-LRB- less/JJR than/IN 1/CD percent/NN errors/NNS )/-RRB- ,/, but/CC errors/NNS increased/VBD when/WRB sequential/JJ and/CC structural/JJ information/NN conflicted/VBN ./.
The/DT frequency/NN of/IN such/JJ errors/NNS rose/VBD sharply/RB in/IN the/DT language/NN -/HYPH modeling/NN setting/NN ./.
We/PRP conclude/VBP that/IN LSTMs/NNPS can/MD capture/VB a/DT non-trivial/JJ amount/NN of/IN grammatical/JJ structure/NN given/VBN targeted/VBN supervision/NN ,/, but/CC stronger/JJR architectures/NNS may/MD be/VB required/VBN to/TO further/RB reduce/VB errors/NNS ;/: furthermore/RB ,/, the/DT language/NN modeling/NN signal/NN is/VBZ insufficient/JJ for/IN capturing/VBG syntax/NN -/HYPH sensitive/JJ dependencies/NNS ,/, and/CC should/MD be/VB supplemented/VBN with/IN more/JJR direct/JJ supervision/NN if/IN such/JJ dependencies/NNS need/VBP to/TO be/VB captured/VBN ./.
