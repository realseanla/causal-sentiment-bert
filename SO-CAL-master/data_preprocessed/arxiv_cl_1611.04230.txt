We/PRP present/VBP SummaRuNNer/NNP ,/, a/DT Recurrent/JJ Neural/JJ Network/NN (/-LRB- RNN/NN )/-RRB- based/VBN sequence/NN model/NN for/IN extractive/JJ summarization/NN of/IN documents/NNS and/CC show/VBP that/IN it/PRP achieves/VBZ performance/NN better/JJR than/IN or/CC comparable/JJ to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
Our/PRP$ model/NN has/VBZ the/DT additional/JJ advantage/NN of/IN being/VBG very/RB interpretable/JJ ,/, since/IN it/PRP allows/VBZ visualization/NN of/IN its/PRP$ predictions/NNS broken/VBN up/RP by/IN abstract/JJ features/NNS such/JJ as/IN information/NN content/NN ,/, salience/NN and/CC novelty/NN ./.
Another/DT novel/JJ contribution/NN of/IN our/PRP$ work/NN is/VBZ abstractive/JJ training/NN of/IN our/PRP$ extractive/JJ model/NN that/WDT can/MD train/VB on/IN human/JJ generated/VBN reference/NN summaries/NNS alone/RB ,/, eliminating/VBG the/DT need/NN for/IN sentence/NN -/HYPH level/NN extractive/JJ labels/NNS ./.
