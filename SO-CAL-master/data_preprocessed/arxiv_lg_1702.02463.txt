We/PRP address/VBP the/DT problem/NN of/IN synthesizing/VBG new/JJ video/NN frames/NNS in/IN an/DT existing/VBG video/NN ,/, either/CC in/IN -/HYPH between/IN existing/VBG frames/NNS (/-LRB- interpolation/NN )/-RRB- ,/, or/CC subsequent/JJ to/IN them/PRP (/-LRB- extrapolation/NN )/-RRB- ./.
This/DT problem/NN is/VBZ challenging/JJ because/IN video/JJ appearance/NN and/CC motion/NN can/MD be/VB highly/RB complex/JJ ./.
Traditional/JJ optical/JJ -/HYPH flow/NN -/HYPH based/VBN solutions/NNS often/RB fail/VBP where/WRB flow/NN estimation/NN is/VBZ challenging/JJ ,/, while/IN newer/JJR neural/JJ -/HYPH network/NN -/HYPH based/VBN methods/NNS that/WDT hallucinate/VBP pixel/NN values/NNS directly/RB often/RB produce/VBP blurry/JJ results/NNS ./.
We/PRP combine/VBP the/DT advantages/NNS of/IN these/DT two/CD methods/NNS by/IN training/VBG a/DT deep/JJ network/NN that/WDT learns/VBZ to/IN synthesize/VB video/NN frames/NNS by/IN flowing/VBG pixel/NN values/NNS from/IN existing/VBG ones/NNS ,/, which/WDT we/PRP call/VBP deep/JJ voxel/NN flow/NN ./.
Our/PRP$ method/NN requires/VBZ no/DT human/JJ supervision/NN ,/, and/CC any/DT video/NN can/MD be/VB used/VBN as/IN training/NN data/NNS by/IN dropping/VBG ,/, and/CC then/RB learning/VBG to/TO predict/VB ,/, existing/VBG frames/NNS ./.
The/DT technique/NN is/VBZ efficient/JJ ,/, and/CC can/MD be/VB applied/VBN at/IN any/DT video/NN resolution/NN ./.
We/PRP demonstrate/VBP that/IN our/PRP$ method/NN produces/VBZ results/NNS that/IN both/CC quantitatively/RB and/CC qualitatively/RB improve/VB upon/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
