Catastrophic/NNP forgetting/VBG is/VBZ a/DT problem/NN which/WDT refers/VBZ to/IN losing/VBG the/DT information/NN of/IN the/DT first/JJ task/NN after/IN training/VBG from/IN the/DT second/JJ task/NN in/IN continual/JJ learning/NN of/IN neural/JJ networks/NNS ./.
To/TO resolve/VB this/DT problem/NN ,/, we/PRP propose/VBP the/DT incremental/JJ moment/NN matching/NN (/-LRB- IMM/NNP )/-RRB- ,/, which/WDT uses/VBZ the/DT Bayesian/JJ neural/JJ network/NN framework/NN ./.
IMM/NN assumes/VBZ that/IN the/DT posterior/JJ distribution/NN of/IN parameters/NNS of/IN neural/JJ networks/NNS is/VBZ approximated/VBN with/IN Gaussian/JJ distribution/NN and/CC incrementally/RB matches/VBZ the/DT moment/NN of/IN the/DT posteriors/NNS ,/, which/WDT are/VBP trained/VBN for/IN the/DT first/JJ and/CC second/JJ task/NN ,/, respectively/RB ./.
To/TO make/VB our/PRP$ Gaussian/JJ assumption/NN reasonable/JJ ,/, the/DT IMM/NNP procedure/NN utilizes/VBZ various/JJ transfer/NN learning/NN techniques/NNS including/VBG weight/NN transfer/NN ,/, L2/NN -/HYPH norm/NN of/IN old/JJ and/CC new/JJ parameters/NNS ,/, and/CC a/DT newly/RB proposed/VBN variant/NN of/IN dropout/NN using/VBG old/JJ parameters/NNS ./.
We/PRP analyze/VBP our/PRP$ methods/NNS on/IN the/DT MNIST/NN and/CC CIFAR/NN -/HYPH 10/CD datasets/NNS ,/, and/CC then/RB evaluate/VB them/PRP on/IN a/DT real/JJ -/HYPH world/NN life/NN -/HYPH log/NN dataset/NN collected/VBN using/VBG Google/NNP Glass/NNP ./.
Experimental/JJ results/NNS show/VBP that/IN IMM/NNP produces/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN in/IN a/DT variety/NN of/IN datasets/NNS ./.
