In/IN a/DT variety/NN of/IN problems/NNS originating/VBG in/IN supervised/JJ ,/, unsupervised/JJ ,/, and/CC reinforcement/NN learning/NN ,/, the/DT loss/NN function/NN is/VBZ defined/VBN by/IN an/DT expectation/NN over/IN a/DT collection/NN of/IN random/JJ variables/NNS ,/, which/WDT might/MD be/VB part/NN of/IN a/DT probabilistic/JJ model/NN or/CC the/DT external/JJ world/NN ./.
Estimating/VBG the/DT gradient/NN of/IN this/DT loss/NN function/NN ,/, using/VBG samples/NNS ,/, lies/VBZ at/IN the/DT core/NN of/IN gradient/NN -/HYPH based/VBN learning/NN algorithms/NNS for/IN these/DT problems/NNS ./.
We/PRP introduce/VBP the/DT formalism/NN of/IN stochastic/JJ computation/NN graphs/NNS ---/, directed/VBN acyclic/JJ graphs/NNS that/WDT include/VBP both/DT deterministic/JJ functions/NNS and/CC conditional/JJ probability/NN distributions/NNS ---/, and/CC describe/VB how/WRB to/TO easily/RB and/CC automatically/RB derive/VBP an/DT unbiased/JJ estimator/NN of/IN the/DT loss/NN function/NN 's/POS gradient/NN ./.
The/DT resulting/VBG algorithm/NN for/IN computing/VBG the/DT gradient/NN estimator/NN is/VBZ a/DT simple/JJ modification/NN of/IN the/DT standard/JJ backpropagation/NN algorithm/NN ./.
The/DT generic/JJ scheme/NN we/PRP propose/VBP unifies/VBZ estimators/NNS derived/VBN in/IN variety/NN of/IN prior/JJ work/NN ,/, along/IN with/IN variance/NN -/HYPH reduction/NN techniques/NNS therein/RB ./.
It/PRP could/MD assist/VB researchers/NNS in/IN developing/VBG intricate/JJ models/NNS involving/VBG a/DT combination/NN of/IN stochastic/JJ and/CC deterministic/JJ operations/NNS ,/, enabling/VBG ,/, for/IN example/NN ,/, attention/NN ,/, memory/NN ,/, and/CC control/NN actions/NNS ./.
