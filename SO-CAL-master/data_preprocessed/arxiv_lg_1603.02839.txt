For/IN many/JJ machine/NN learning/NN problems/NNS ,/, data/NNS is/VBZ abundant/JJ and/CC it/PRP may/MD be/VB prohibitive/JJ to/TO make/VB multiple/JJ passes/NNS through/IN the/DT full/JJ training/NN set/NN ./.
In/IN this/DT context/NN ,/, we/PRP investigate/VBP strategies/NNS for/IN dynamically/RB increasing/VBG the/DT effective/JJ sample/NN size/NN ,/, when/WRB using/VBG iterative/JJ methods/NNS such/JJ as/IN stochastic/JJ gradient/NN descent/NN ./.
Our/PRP$ interest/NN is/VBZ motivated/VBN by/IN the/DT rise/NN of/IN variance/NN -/HYPH reduced/VBN methods/NNS ,/, which/WDT achieve/VBP linear/JJ convergence/NN rates/NNS that/WDT scale/VBP favorably/RB for/IN smaller/JJR sample/NN sizes/NNS ./.
Exploiting/VBG this/DT feature/NN ,/, we/PRP show/VBP --/: theoretically/RB and/CC empirically/RB --/: how/WRB to/TO obtain/VB significant/JJ speed/NN -/HYPH ups/NNS with/IN a/DT novel/JJ algorithm/NN that/WDT reaches/VBZ statistical/JJ accuracy/NN on/IN an/DT $/$ n/NN $/$ -/HYPH sample/NN in/IN $/$ 2n/CD $/$ ,/, instead/RB of/IN $/$ n/NN \/SYM log/NN n/NN $/$ steps/NNS ./.
