In/IN this/DT paper/NN ,/, we/PRP extend/VBP the/DT deep/JJ long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- DLSTM/NNP )/-RRB- recurrent/JJ neural/JJ networks/NNS by/IN introducing/VBG gated/VBN direct/JJ connections/NNS between/IN memory/NN cells/NNS in/IN adjacent/JJ layers/NNS ./.
These/DT direct/JJ links/NNS ,/, called/VBN highway/NN connections/NNS ,/, enable/VBP unimpeded/JJ information/NN flow/NN across/IN different/JJ layers/NNS and/CC thus/RB alleviate/VB the/DT gradient/NN vanishing/VBG problem/NN when/WRB building/VBG deeper/JJR LSTMs/NNS ./.
We/PRP further/RB introduce/VB the/DT latency/NN -/HYPH controlled/VBN bidirectional/JJ LSTMs/NNS (/-LRB- BLSTMs/NNS )/-RRB- which/WDT can/MD exploit/VB the/DT whole/JJ history/NN while/IN keeping/VBG the/DT latency/NN under/IN control/NN ./.
Efficient/JJ algorithms/NNS are/VBP proposed/VBN to/TO train/VB these/DT novel/JJ networks/NNS using/VBG both/CC frame/NN and/CC sequence/NN discriminative/JJ criteria/NNS ./.
Experiments/NNS on/IN the/DT AMI/NNP distant/JJ speech/NN recognition/NN (/-LRB- DSR/NN )/-RRB- task/NN indicate/VBP that/IN we/PRP can/MD train/VB deeper/JJR LSTMs/NNS and/CC achieve/VB better/JJR improvement/NN from/IN sequence/NN training/NN with/IN highway/NN LSTMs/NNS (/-LRB- HLSTMs/NNS )/-RRB- ./.
Our/PRP$ novel/JJ model/NN obtains/VBZ $/$ 43.9/CD //SYM 47.7/CD \/SYM percent/NN $/$ WER/CD on/IN AMI/NNP (/-LRB- SDM/NNP )/-RRB- dev/NN and/CC eval/NN sets/NNS ,/, outperforming/VBG all/DT previous/JJ works/NNS ./.
It/PRP beats/VBZ the/DT strong/JJ DNN/NN and/CC DLSTM/NN baselines/NNS with/IN $/$ 15.7/CD \/SYM percent/NN $/$ and/CC $/$ 5.3/CD \/SYM percent/NN $/$ relative/JJ improvement/NN respectively/RB ./.
