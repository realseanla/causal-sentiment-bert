Decision/NN tree/NN learning/NN is/VBZ a/DT popular/JJ approach/NN for/IN classification/NN and/CC regression/NN in/IN machine/NN learning/NN and/CC statistics/NNS ,/, and/CC Bayesian/JJ formulations/NNS ---/, which/WDT introduce/VBP a/DT prior/JJ distribution/NN over/IN decision/NN trees/NNS ,/, and/CC formulate/VB learning/NN as/IN posterior/JJ inference/NN given/VBN data/NNS ---/, have/VBP been/VBN shown/VBN to/TO produce/VB competitive/JJ performance/NN ./.
Unlike/IN classic/JJ decision/NN tree/NN learning/VBG algorithms/NNS like/IN ID3/NN ,/, C4/NN .5/NN and/CC CART/NN ,/, which/WDT work/VBP in/IN a/DT top/JJ -/HYPH down/JJ manner/NN ,/, existing/VBG Bayesian/JJ algorithms/NNS produce/VBP an/DT approximation/NN to/IN the/DT posterior/JJ distribution/NN by/IN evolving/VBG a/DT complete/JJ tree/NN (/-LRB- or/CC collection/NN thereof/RB )/-RRB- iteratively/RB via/IN local/JJ Monte/NNP Carlo/NNP modifications/NNS to/IN the/DT structure/NN of/IN the/DT tree/NN ,/, e.g./FW ,/, using/VBG Markov/NNP chain/NN Monte/NNP Carlo/NNP (/-LRB- MCMC/NNP )/-RRB- ./.
We/PRP present/VBP a/DT sequential/JJ Monte/NNP Carlo/NNP (/-LRB- SMC/NNP )/-RRB- algorithm/NN that/WDT instead/RB works/VBZ in/IN a/DT top/JJ -/HYPH down/JJ manner/NN ,/, mimicking/VBG the/DT behavior/NN and/CC speed/NN of/IN classic/JJ algorithms/NNS ./.
We/PRP demonstrate/VBP empirically/RB that/IN our/PRP$ approach/NN delivers/VBZ accuracy/NN comparable/JJ to/IN the/DT most/RBS popular/JJ MCMC/NN method/NN ,/, but/CC operates/VBZ more/JJR than/IN an/DT order/NN of/IN magnitude/NN faster/RBR ,/, and/CC thus/RB represents/VBZ a/DT better/JJR computation/NN -/HYPH accuracy/NN tradeoff/NN ./.
