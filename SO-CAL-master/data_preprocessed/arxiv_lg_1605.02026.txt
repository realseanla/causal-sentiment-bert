With/IN the/DT growing/VBG importance/NN of/IN large/JJ network/NN models/NNS and/CC enormous/JJ training/NN datasets/NNS ,/, GPUs/NNS have/VBP become/VBN increasingly/RB necessary/JJ to/TO train/VB neural/JJ networks/NNS ./.
This/DT is/VBZ largely/RB because/IN conventional/JJ optimization/NN algorithms/NNS rely/VBP on/IN stochastic/JJ gradient/NN methods/NNS that/WDT do/VBP n't/RB scale/VB well/RB to/IN large/JJ numbers/NNS of/IN cores/NNS in/IN a/DT cluster/NN setting/NN ./.
Furthermore/RB ,/, the/DT convergence/NN of/IN all/DT gradient/NN methods/NNS ,/, including/VBG batch/NN methods/NNS ,/, suffers/VBZ from/IN common/JJ problems/NNS like/IN saturation/NN effects/NNS ,/, poor/JJ conditioning/NN ,/, and/CC saddle/NN points/NNS ./.
This/DT paper/NN explores/VBZ an/DT unconventional/JJ training/NN method/NN that/WDT uses/VBZ alternating/VBG direction/NN methods/NNS and/CC Bregman/NNP iteration/NN to/TO train/VB networks/NNS without/IN gradient/NN descent/NN steps/NNS ./.
The/DT proposed/JJ method/NN reduces/VBZ the/DT network/NN training/NN problem/NN to/IN a/DT sequence/NN of/IN minimization/NN sub-steps/NNS that/WDT can/MD each/DT be/VB solved/VBN globally/RB in/IN closed/JJ form/NN ./.
The/DT proposed/JJ method/NN is/VBZ advantageous/JJ because/IN it/PRP avoids/VBZ many/JJ of/IN the/DT caveats/NNS that/WDT make/VBP gradient/NN methods/NNS slow/JJ on/IN highly/RB non-convex/JJ problems/NNS ./.
The/DT method/NN exhibits/VBZ strong/JJ scaling/NN in/IN the/DT distributed/VBN setting/NN ,/, yielding/VBG linear/JJ speedups/NNS even/RB when/WRB split/VBN over/IN thousands/NNS of/IN cores/NNS ./.
