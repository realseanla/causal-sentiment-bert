Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNN/NN )/-RRB- have/VBP recently/RB achieved/VBN the/DT best/JJS performance/NN in/IN off/RB -/HYPH line/NN Handwriting/NNP Text/VB Recognition/NNP ./.
At/IN the/DT same/JJ time/NN ,/, learning/VBG RNN/NN by/IN gradient/NN descent/NN leads/VBZ to/IN slow/JJ convergence/NN ,/, and/CC training/NN times/NNS are/VBP particularly/RB long/RB when/WRB the/DT training/NN database/NN consists/VBZ of/IN full/JJ lines/NNS of/IN text/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP an/DT easy/JJ way/NN to/TO accelerate/VB stochastic/JJ gradient/NN descent/NN in/IN this/DT set/NN -/HYPH up/NN ,/, and/CC in/IN the/DT general/JJ context/NN of/IN learning/NN to/TO recognize/VB sequences/NNS ./.
The/DT principle/NN is/VBZ called/VBN Curriculum/NNP Learning/NNP ,/, or/CC shaping/VBG ./.
The/DT idea/NN is/VBZ to/TO first/RB learn/VB to/TO recognize/VB short/JJ sequences/NNS before/IN training/NN on/IN all/DT available/JJ training/NN sequences/NNS ./.
Experiments/NNS on/IN three/CD different/JJ handwritten/JJ text/NN databases/NNS (/-LRB- Rimes/NNP ,/, IAM/NNP ,/, OpenHaRT/NNP )/-RRB- show/VBP that/IN a/DT simple/JJ implementation/NN of/IN this/DT strategy/NN can/MD significantly/RB speed/VB up/RP the/DT training/NN of/IN RNN/NN for/IN Text/VB Recognition/NNP ,/, and/CC even/RB significantly/RB improve/VB performance/NN in/IN some/DT cases/NNS ./.
