We/PRP present/VBP a/DT Bayesian/JJ sequential/JJ decision/NN -/HYPH making/VBG formulation/NN of/IN the/DT information/NN filtering/VBG problem/NN ,/, in/IN which/WDT an/DT algorithm/NN presents/VBZ items/NNS (/-LRB- news/NN articles/NNS ,/, scientific/JJ papers/NNS ,/, tweets/NNS )/-RRB- arriving/VBG in/IN a/DT stream/NN ,/, and/CC learns/VBZ relevance/NN from/IN user/NN feedback/NN on/IN presented/VBN items/NNS ./.
We/PRP model/VBP user/NN preferences/NNS using/VBG a/DT Bayesian/JJ linear/JJ model/NN ,/, similar/JJ in/IN spirit/NN to/IN a/DT Bayesian/JJ linear/JJ bandit/NN ./.
We/PRP compute/VBP a/DT computational/JJ upper/JJ bound/VBN on/IN the/DT value/NN of/IN the/DT optimal/JJ policy/NN ,/, which/WDT allows/VBZ computing/VBG an/DT optimality/NN gap/NN for/IN implementable/JJ policies/NNS ./.
We/PRP then/RB use/VB this/DT analysis/NN as/IN motivation/NN in/IN introducing/VBG a/DT pair/NN of/IN new/JJ Decompose/NNP -/HYPH Then/RB -/HYPH Decide/VB (/-LRB- DTD/NN )/-RRB- heuristic/NN policies/NNS ,/, DTD/NN -/HYPH Dynamic/JJ -/HYPH Programming/NN (/-LRB- DTD/NN -/HYPH DP/NN )/-RRB- and/CC DTD/NNP -/HYPH Upper/NNP -/HYPH Confidence/NN -/HYPH Bound/NN (/-LRB- DTD/NN -/HYPH UCB/NN )/-RRB- ./.
We/PRP compare/VBP DTD/NN -/HYPH DP/NN and/CC DTD/NN -/HYPH UCB/NN against/IN several/JJ benchmarks/NNS on/IN real/JJ and/CC simulated/JJ data/NNS ,/, demonstrating/VBG significant/JJ improvement/NN ,/, and/CC show/VBP that/IN the/DT achieved/VBN performance/NN is/VBZ close/JJ to/IN the/DT upper/JJ bound/JJ ./.
