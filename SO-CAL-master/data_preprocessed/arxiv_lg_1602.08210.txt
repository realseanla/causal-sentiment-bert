In/IN this/DT paper/NN ,/, we/PRP systematically/RB analyse/VB the/DT connecting/VBG architectures/NNS of/IN recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- ./.
Our/PRP$ main/JJ contribution/NN is/VBZ twofold/JJ :/: first/RB ,/, we/PRP present/VBP a/DT rigorous/JJ graph/NN -/HYPH theoretic/JJ framework/NN describing/VBG the/DT connecting/VBG architectures/NNS of/IN RNNs/NNS in/IN general/JJ ./.
Second/RB ,/, we/PRP propose/VBP three/CD architecture/NN complexity/NN measures/NNS of/IN RNNs/NNS :/: (/-LRB- a/LS )/-RRB- the/DT recurrent/JJ depth/NN ,/, which/WDT captures/VBZ the/DT RNN/NNP 's/POS over-time/NN nonlinear/JJ complexity/NN ,/, (/-LRB- b/LS )/-RRB- the/DT feedforward/NN depth/NN ,/, which/WDT captures/VBZ the/DT local/JJ input/NN -/HYPH output/NN nonlinearity/NN (/-LRB- similar/JJ to/IN the/DT "/`` depth/NN "/'' in/IN feedforward/JJ neural/JJ networks/NNS (/-LRB- FNNs/NNS )/-RRB- )/-RRB- ,/, and/CC (/-LRB- c/LS )/-RRB- the/DT recurrent/JJ skip/VB coefficient/NN which/WDT captures/VBZ how/WRB rapidly/RB the/DT information/NN propagates/VBZ over/IN time/NN ./.
Our/PRP$ experimental/JJ results/NNS show/VBP that/IN RNNs/NNS might/MD benefit/VB from/IN larger/JJR recurrent/JJ depth/NN and/CC feedforward/NN depth/NN ./.
We/PRP further/RB demonstrate/VBP that/IN increasing/VBG recurrent/JJ skip/VB coefficient/NN offers/NNS performance/NN boosts/VBZ on/IN long/JJ term/NN dependency/NN problems/NNS ,/, as/IN we/PRP improve/VBP the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN for/IN sequential/JJ MNIST/NN dataset/NN ./.
