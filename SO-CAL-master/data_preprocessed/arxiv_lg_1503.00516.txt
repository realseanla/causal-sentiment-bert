Big/JJ data/NNS consists/VBZ of/IN large/JJ multidimensional/JJ datasets/NNS that/WDT would/MD often/RB be/VB difficult/JJ to/TO analyze/VB if/IN working/VBG with/IN the/DT original/JJ tensor/NN ./.
There/EX is/VBZ a/DT rising/VBG interest/NN in/IN the/DT use/NN of/IN tensor/NN decompositions/NNS to/TO approximate/VB large/JJ tensors/NNS in/IN order/NN to/TO reduce/VB their/PRP$ dimensions/NNS by/IN selecting/VBG important/JJ features/NNS for/IN classification/NN ./.
Of/IN particular/JJ interest/NN is/VBZ the/DT Tucker/NNP decomposition/NN (/-LRB- TD/NN )/-RRB- that/WDT has/VBZ already/RB been/VBN applied/VBN in/IN neuroscience/NN ,/, geoscience/NN ,/, signal/NN processing/NN ,/, pattern/NN and/CC image/NN recognition/NN ./.
However/RB the/DT decomposition/NN itself/PRP leads/VBZ to/IN exponential/JJ computational/JJ time/NN for/IN high/JJ -/HYPH order/NN tensors/NNS ./.
To/TO circumvent/VB this/DT obstacle/NN we/PRP propose/VBP an/DT alternative/JJ known/VBN as/IN the/DT matrix/NN product/NN state/NN (/-LRB- MPS/NN )/-RRB- decomposition/NN for/IN the/DT data/NNS representation/NN of/IN big/JJ data/NNS tensors/NNS ./.
This/DT decomposition/NN has/VBZ been/VBN used/VBN extensively/RB in/IN quantum/NN physics/NN within/IN the/DT last/JJ decade/NN and/CC its/PRP$ benefit/NN has/VBZ surprisingly/RB not/RB been/VBN seen/VBN in/IN other/JJ areas/NNS of/IN research/NN ./.
We/PRP prove/VBP that/IN the/DT MPS/NNP decomposition/NN for/IN feature/NN extraction/NN and/CC classification/NN in/IN supervised/JJ learning/NN can/MD be/VB implemented/VBN efficiently/RB with/IN high/JJ classification/NN rates/NNS in/IN pattern/NN and/CC image/NN recognition/NN ./.
