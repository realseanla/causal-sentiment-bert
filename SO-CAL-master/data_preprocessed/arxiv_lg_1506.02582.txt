We/PRP consider/VBP emphatic/JJ temporal/JJ -/HYPH difference/NN learning/NN algorithms/NNS for/IN policy/NN evaluation/NN in/IN discounted/VBN Markov/NNP decision/NN processes/NNS with/IN finite/JJ spaces/NNS ./.
Such/JJ algorithms/NNS were/VBD recently/RB proposed/VBN by/IN Sutton/NNP ,/, Mahmood/NNP ,/, and/CC White/NNP (/-LRB- 2015/CD )/-RRB- as/IN an/DT improved/JJ solution/NN to/IN the/DT problem/NN of/IN divergence/NN of/IN off/RB -/HYPH policy/NN temporal/JJ -/HYPH difference/NN learning/NN with/IN linear/JJ function/NN approximation/NN ./.
We/PRP present/VBP in/IN this/DT paper/NN the/DT first/JJ convergence/NN proofs/NNS for/IN two/CD emphatic/JJ algorithms/NNS ,/, ETD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- and/CC ELSTD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- ./.
We/PRP prove/VBP ,/, under/IN general/JJ off/IN -/HYPH policy/NN conditions/NNS ,/, the/DT convergence/NN in/IN $/$ L/$ ^/SYM 1/CD $/$ for/IN ELSTD/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- iterates/NNS ,/, and/CC the/DT almost/RB sure/JJ convergence/NN of/IN the/DT approximate/JJ value/NN functions/VBZ calculated/VBN by/IN both/DT algorithms/NNS using/VBG a/DT single/JJ infinitely/RB long/JJ trajectory/NN ./.
Our/PRP$ analysis/NN involves/VBZ new/JJ techniques/NNS with/IN applications/NNS beyond/IN emphatic/JJ algorithms/NNS leading/VBG ,/, for/IN example/NN ,/, to/IN the/DT first/JJ proof/NN that/WDT standard/JJ TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- also/RB converges/VBZ under/IN off/IN -/HYPH policy/NN training/NN for/IN $/$ \/CD lambda/NN $/$ sufficiently/RB large/JJ ./.
