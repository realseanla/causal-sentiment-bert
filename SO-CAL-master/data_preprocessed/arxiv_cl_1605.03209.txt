In/IN order/NN to/TO capture/VB rich/JJ language/NN phenomena/NN ,/, neural/JJ machine/NN translation/NN models/NNS have/VBP to/TO use/VB a/DT large/JJ vocabulary/NN size/NN ,/, which/WDT requires/VBZ high/JJ computing/NN time/NN and/CC large/JJ memory/NN usage/NN ./.
In/IN this/DT paper/NN ,/, we/PRP alleviate/VBP this/DT issue/NN by/IN introducing/VBG a/DT sentence/NN -/HYPH level/NN or/CC batch/NN -/HYPH level/NN vocabulary/NN ,/, which/WDT is/VBZ only/RB a/DT very/RB small/JJ sub-set/NN of/IN the/DT full/JJ output/NN vocabulary/NN ./.
For/IN each/DT sentence/NN or/CC batch/NN ,/, we/PRP only/RB predict/VBP the/DT target/NN words/NNS in/IN its/PRP$ sentence/NN -/HYPH level/NN or/CC batch/NN -/HYPH level/NN vocabulary/NN ./.
Thus/RB ,/, we/PRP reduce/VBP both/CC the/DT computing/NN time/NN and/CC the/DT memory/NN usage/NN ./.
Our/PRP$ method/NN simply/RB takes/VBZ into/IN account/NN the/DT translation/NN options/NNS of/IN each/DT word/NN or/CC phrase/NN in/IN the/DT source/NN sentence/NN ,/, and/CC picks/VBZ a/DT very/RB small/JJ target/NN vocabulary/NN for/IN each/DT sentence/NN based/VBN on/IN a/DT word/NN -/HYPH to/IN -/HYPH word/NN translation/NN model/NN or/CC a/DT bilingual/JJ phrase/NN library/NN learned/VBN from/IN a/DT traditional/JJ machine/NN translation/NN model/NN ./.
Experimental/JJ results/NNS on/IN the/DT large/JJ -/HYPH scale/NN English/NNP -/HYPH to/IN -/HYPH French/JJ task/NN show/VBP that/IN our/PRP$ method/NN achieves/VBZ better/JJR translation/NN performance/NN by/IN 1/CD BLEU/NN point/NN over/IN the/DT large/JJ vocabulary/NN neural/JJ machine/NN translation/NN system/NN of/IN Jean/NNP et/FW al./FW (/-LRB- 2015/CD )/-RRB- ./.
