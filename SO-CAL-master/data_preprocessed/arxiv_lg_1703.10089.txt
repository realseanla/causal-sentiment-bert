In/IN this/DT paper/NN ,/, we/PRP study/VBP the/DT use/NN of/IN recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- for/IN modeling/NN and/CC forecasting/NN time/NN series/NN ./.
We/PRP first/RB illustrate/VBP the/DT fact/NN that/IN standard/JJ sequence/NN -/HYPH to/IN -/HYPH sequence/NN RNNs/NNS neither/CC capture/NN well/RB periods/NNS in/IN time/NN series/NN nor/CC handle/VB well/RB missing/VBG values/NNS ,/, even/RB though/IN many/JJ real/JJ life/NN times/NNS series/NNS are/VBP periodic/JJ and/CC contain/VBP missing/VBG values/NNS ./.
We/PRP then/RB propose/VB an/DT extended/JJ attention/NN mechanism/NN that/WDT can/MD be/VB deployed/VBN on/IN top/NN of/IN any/DT RNN/NN and/CC that/DT is/VBZ designed/VBN to/TO capture/VB periods/NNS and/CC make/VB the/DT RNN/NN more/RBR robust/JJ to/IN missing/VBG values/NNS ./.
We/PRP show/VBP the/DT effectiveness/NN of/IN this/DT novel/JJ model/NN through/IN extensive/JJ experiments/NNS with/IN multiple/JJ univariate/JJ and/CC multivariate/JJ datasets/NNS ./.
