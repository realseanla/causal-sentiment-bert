We/PRP model/VBP coherent/JJ conversation/NN continuation/NN via/IN RNN/NN -/HYPH based/VBN dialogue/NN models/NNS equipped/VBN with/IN a/DT dynamic/JJ attention/NN mechanism/NN ./.
Our/PRP$ attention/NN -/HYPH RNN/NN language/NN model/NN dynamically/RB increases/VBZ the/DT scope/NN of/IN attention/NN on/IN the/DT history/NN as/IN the/DT conversation/NN continues/VBZ ,/, as/IN opposed/VBN to/IN standard/JJ attention/NN (/-LRB- or/CC alignment/NN )/-RRB- models/NNS with/IN a/DT fixed/VBN input/NN scope/NN in/IN a/DT sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN ./.
This/DT allows/VBZ each/DT generated/VBN word/NN to/TO be/VB associated/VBN with/IN the/DT most/RBS relevant/JJ words/NNS in/IN its/PRP$ corresponding/VBG conversation/NN history/NN ./.
We/PRP evaluate/VBP the/DT model/NN on/IN two/CD popular/JJ dialogue/NN datasets/NNS ,/, the/DT open/JJ -/HYPH domain/NN MovieTriples/NNPS dataset/NN and/CC the/DT closed/JJ -/HYPH domain/NN Ubuntu/NNP Troubleshoot/NNP dataset/NN ,/, and/CC achieve/VB significant/JJ improvements/NNS over/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN and/CC baselines/NNS on/IN several/JJ metrics/NNS ,/, including/VBG complementary/JJ diversity/NN -/HYPH based/VBN metrics/NNS ,/, human/JJ evaluation/NN ,/, and/CC qualitative/JJ visualizations/NNS ./.
We/PRP also/RB show/VBP that/IN a/DT vanilla/NN RNN/NN with/IN dynamic/JJ attention/NN outperforms/VBZ more/RBR complex/JJ memory/NN models/NNS (/-LRB- e.g./FW ,/, LSTM/NN and/CC GRU/NN )/-RRB- by/IN allowing/VBG for/IN flexible/JJ ,/, long/JJ -/HYPH distance/NN memory/NN ./.
We/PRP promote/VBP further/RB coherence/NN via/IN topic/NN modeling/NN -/HYPH based/VBN reranking/NN ./.
