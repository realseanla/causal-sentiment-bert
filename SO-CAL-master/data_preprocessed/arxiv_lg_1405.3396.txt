We/PRP present/VBP algorithms/NNS for/IN reducing/VBG the/DT Dueling/VBG Bandits/NNPS problem/NN to/IN the/DT conventional/JJ (/-LRB- stochastic/JJ )/-RRB- Multi-Armed/JJ Bandits/NNPS problem/NN ./.
The/DT Dueling/VBG Bandits/NNPS problem/NN is/VBZ an/DT online/JJ model/NN of/IN learning/VBG with/IN ordinal/JJ feedback/NN of/IN the/DT form/NN "/`` A/NN is/VBZ preferred/VBN to/IN B/NN "/'' (/-LRB- as/IN opposed/VBN to/IN cardinal/JJ feedback/NN like/IN "/`` A/NNP has/VBZ value/NN 2.5/CD "/'' )/-RRB- ,/, giving/VBG it/PRP wide/JJ applicability/NN in/IN learning/VBG from/IN implicit/JJ user/NN feedback/NN and/CC revealed/VBD and/CC stated/VBD preferences/NNS ./.
In/IN contrast/NN to/IN existing/VBG algorithms/NNS for/IN the/DT Dueling/VBG Bandits/NNPS problem/NN ,/, our/PRP$ reductions/NNS --/: named/VBN $/$ \/CD Doubler/NNP $/$ ,/, $/$ \/SYM MultiSbm/NNP $/$ and/CC $/$ \/CD DoubleSbm/NNP $/$ --/: provide/VB a/DT generic/JJ schema/NN for/IN translating/VBG the/DT extensive/JJ body/NN of/IN known/VBN results/NNS about/IN conventional/JJ Multi-Armed/JJ Bandit/NN algorithms/NNS to/IN the/DT Dueling/VBG Bandits/NNPS setting/VBG ./.
For/IN $/$ \/CD Doubler/NNP $/$ and/CC $/$ \/CD MultiSbm/NNP $/$ we/PRP prove/VBP regret/NN upper/JJ bounds/NNS in/IN both/DT finite/JJ and/CC infinite/JJ settings/NNS ,/, and/CC conjecture/NN about/IN the/DT performance/NN of/IN $/$ \/CD DoubleSbm/NNP $/$ which/WDT empirically/RB outperforms/VBZ the/DT other/JJ two/CD as/RB well/RB as/IN previous/JJ algorithms/NNS in/IN our/PRP$ experiments/NNS ./.
In/IN addition/NN ,/, we/PRP provide/VBP the/DT first/JJ almost/RB optimal/JJ regret/NN bound/VBN in/IN terms/NNS of/IN second/JJ order/NN terms/NNS ,/, such/JJ as/IN the/DT differences/NNS between/IN the/DT values/NNS of/IN the/DT arms/NNS ./.
