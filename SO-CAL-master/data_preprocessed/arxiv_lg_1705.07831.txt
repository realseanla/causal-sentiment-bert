Training/VBG generative/JJ adversarial/JJ networks/NNS is/VBZ unstable/JJ in/IN high/JJ -/HYPH dimensions/NNS when/WRB the/DT true/JJ data/NNS distribution/NN lies/VBZ on/IN a/DT lower/JJR -/HYPH dimensional/JJ manifold/NN ./.
The/DT discriminator/NN is/VBZ then/RB easily/RB able/JJ to/TO separate/VB nearly/RB all/DT generated/VBN samples/NNS leaving/VBG the/DT generator/NN without/IN meaningful/JJ gradients/NNS ./.
We/PRP propose/VBP training/VBG a/DT single/JJ generator/NN simultaneously/RB against/IN an/DT array/NN of/IN discriminators/NNS ,/, each/DT of/IN which/WDT looks/VBZ at/IN a/DT different/JJ random/JJ low/JJ -/HYPH dimensional/JJ projection/NN of/IN the/DT data/NNS ./.
We/PRP show/VBP that/IN individual/JJ discriminators/NNS then/RB provide/VBP stable/JJ gradients/NNS to/IN the/DT generator/NN ,/, and/CC that/IN the/DT generator/NN learns/VBZ to/TO produce/VB samples/NNS consistent/JJ with/IN the/DT full/JJ data/NNS distribution/NN to/TO satisfy/VB all/DT discriminators/NNS ./.
We/PRP demonstrate/VBP the/DT practical/JJ utility/NN of/IN this/DT approach/NN experimentally/RB ,/, and/CC show/VBP that/IN it/PRP is/VBZ able/JJ to/TO produce/VB image/NN samples/NNS with/IN higher/JJR quality/NN than/IN traditional/JJ training/NN with/IN a/DT single/JJ discriminator/NN ./.
