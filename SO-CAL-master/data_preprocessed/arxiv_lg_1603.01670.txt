We/PRP present/VBP in/IN this/DT paper/NN a/DT systematic/JJ study/NN on/IN how/WRB to/TO morph/VB a/DT well/RB -/HYPH trained/VBN neural/JJ network/NN to/IN a/DT new/JJ one/NN so/IN that/IN its/PRP$ network/NN function/NN can/MD be/VB completely/RB preserved/VBN ./.
We/PRP define/VBP this/DT as/IN \/SYM emph/NN {/-LRB- network/NN morphism/NN }/-RRB- in/IN this/DT research/NN ./.
After/IN morphing/VBG a/DT parent/NN network/NN ,/, the/DT child/NN network/NN is/VBZ expected/VBN to/TO inherit/VB the/DT knowledge/NN from/IN its/PRP$ parent/NN network/NN and/CC also/RB has/VBZ the/DT potential/JJ to/TO continue/VB growing/VBG into/IN a/DT more/RBR powerful/JJ one/CD with/IN much/JJ shortened/VBN training/NN time/NN ./.
The/DT first/JJ requirement/NN for/IN this/DT network/NN morphism/NN is/VBZ its/PRP$ ability/NN to/TO handle/VB diverse/JJ morphing/VBG types/NNS of/IN networks/NNS ,/, including/VBG changes/NNS of/IN depth/NN ,/, width/NN ,/, kernel/NN size/NN ,/, and/CC even/RB subnet/NN ./.
To/TO meet/VB this/DT requirement/NN ,/, we/PRP first/RB introduce/VB the/DT network/NN morphism/NN equations/NNS ,/, and/CC then/RB develop/VB novel/JJ morphing/NN algorithms/NNS for/IN all/PDT these/DT morphing/VBG types/NNS for/IN both/DT classic/JJ and/CC convolutional/JJ neural/JJ networks/NNS ./.
The/DT second/JJ requirement/NN for/IN this/DT network/NN morphism/NN is/VBZ its/PRP$ ability/NN to/TO deal/VB with/IN non-linearity/NN in/IN a/DT network/NN ./.
We/PRP propose/VBP a/DT family/NN of/IN parametric/JJ -/HYPH activation/NN functions/NNS to/TO facilitate/VB the/DT morphing/NN of/IN any/DT continuous/JJ non-linear/JJ activation/NN neurons/NNS ./.
Experimental/JJ results/NNS on/IN benchmark/NN datasets/NNS and/CC typical/JJ neural/JJ networks/NNS demonstrate/VBP the/DT effectiveness/NN of/IN the/DT proposed/VBN network/NN morphism/NN scheme/NN ./.
