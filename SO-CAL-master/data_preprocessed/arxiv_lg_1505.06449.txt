We/PRP extend/VBP previous/JJ work/NN on/IN efficiently/RB training/VBG linear/JJ models/NNS by/IN applying/VBG stochastic/JJ updates/NNS to/IN non-zero/JJ features/NNS only/RB ,/, lazily/RB bringing/VBG weights/NNS current/JJ as/IN needed/VBN ./.
To/IN date/NN ,/, only/RB the/DT closed/JJ form/NN updates/NNS for/IN the/DT $/$ \/CD ell_1/CD $/$ ,/, $/$ \/SYM ell/NN _/NFP {/-LRB- \/SYM infty/JJ }/-RRB- $/$ ,/, and/CC the/DT rarely/RB used/VBN $/$ \/CD ell_2/CD $/$ norm/NN have/VBP been/VBN described/VBN ./.
We/PRP extend/VBP this/DT work/NN by/IN showing/VBG the/DT proper/JJ closed/JJ form/NN updates/NNS for/IN the/DT popular/JJ $/$ \/CD ell/CD ^/SYM 2_2/CD $/$ and/CC elastic/JJ net/JJ regularized/VBN models/NNS ./.
We/PRP show/VBP a/DT dynamic/JJ programming/NN algorithm/NN to/TO calculate/VB the/DT proper/JJ elastic/JJ net/JJ update/NN with/IN only/RB one/CD constant/JJ -/HYPH time/NN subproblem/NN computation/NN per/IN update/NN ./.
Our/PRP$ algorithm/NN handles/VBZ both/CC fixed/VBN and/CC decreasing/VBG learning/NN rates/NNS and/CC we/PRP derive/VBP the/DT result/NN for/IN both/DT stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- and/CC forward/RB backward/JJ splitting/NN (/-LRB- FoBoS/NN )/-RRB- ./.
We/PRP empirically/RB validate/VBP the/DT algorithm/NN ,/, showing/VBG that/IN on/IN a/DT bag/NN -/HYPH of/IN -/HYPH words/NNS dataset/NN with/IN $/$ 260,941/CD $/$ features/NNS and/CC $/$ 88/CD $/$ nonzero/CD features/NNS on/IN average/JJ per/IN example/NN ,/, our/PRP$ method/NN trains/NNS a/DT logistic/JJ regression/NN classifier/NN with/IN elastic/JJ net/JJ regularization/NN $/$ 612/CD $/$ times/NNS faster/JJR than/IN an/DT otherwise/RB identical/JJ implementation/NN with/IN dense/JJ updates/NNS ./.
