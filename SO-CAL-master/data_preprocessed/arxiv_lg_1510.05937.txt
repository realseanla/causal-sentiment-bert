The/DT popular/JJ i/NN -/HYPH vector/NN model/NN represents/VBZ speakers/NNS as/IN lowdimensional/JJ continuous/JJ vectors/NNS (/-LRB- i/CD -/SYM vectors/NNS )/-RRB- ,/, and/CC hence/RB is/VBZ a/DT way/NN of/IN continuous/JJ speaker/NN embedding/NN ./.
In/IN this/DT paper/NN ,/, we/PRP investigate/VBP binary/JJ speaker/NN embedding/NN ,/, which/WDT transforms/VBZ ivectors/NNS to/IN binary/NN vectors/NNS (/-LRB- codes/NNS )/-RRB- by/IN a/DT hash/NN function/NN ./.
We/PRP start/VBP from/IN locality/NN sensitive/JJ hashing/VBG (/-LRB- LSH/NN )/-RRB- ,/, a/DT simple/JJ binarization/NN approach/NN where/WRB binary/JJ codes/NNS are/VBP derived/VBN from/IN a/DT set/NN of/IN random/JJ hash/NN functions/NNS ./.
A/DT potential/JJ problem/NN of/IN LSH/NNP is/VBZ that/IN the/DT randomly/RB sampled/VBN hash/NN functions/NNS might/MD be/VB suboptimal/JJ ,/, we/PRP therefore/RB propose/VBP an/DT improved/JJ Hamming/NN distance/NN learning/NN approach/NN ,/, where/WRB the/DT hash/NN function/NN is/VBZ learned/VBN by/IN a/DT variablesized/VBN block/NN training/NN that/WDT projects/VBZ each/DT dimension/NN of/IN the/DT original/JJ i/NN -/HYPH vectors/NNS to/IN variable/JJ -/HYPH sized/JJ binary/JJ codes/NNS independently/RB ./.
Our/PRP$ experiments/NNS show/VBP that/IN binary/JJ speaker/NN embedding/NN can/MD deliver/VB competitive/JJ or/CC even/RB better/JJR results/NNS on/IN both/DT speaker/NN verification/NN and/CC identification/NN tasks/NNS ,/, while/IN the/DT memory/NN usage/NN and/CC the/DT computation/NN cost/NN are/VBP significant/JJ reduced/VBN ./.
