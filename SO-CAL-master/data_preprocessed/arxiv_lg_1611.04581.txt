Training/NN time/NN on/IN large/JJ datasets/NNS for/IN deep/JJ neural/JJ networks/NNS is/VBZ the/DT principal/JJ workflow/NN bottleneck/NN in/IN a/DT number/NN of/IN important/JJ applications/NNS of/IN deep/JJ learning/NN ,/, such/JJ as/IN object/NN classification/NN and/CC detection/NN in/IN automatic/JJ driver/NN assistance/NN systems/NNS (/-LRB- ADAS/NN )/-RRB- ./.
To/TO minimize/VB training/NN time/NN ,/, the/DT training/NN of/IN a/DT deep/JJ neural/JJ network/NN must/MD be/VB scaled/VBN beyond/IN a/DT single/JJ machine/NN to/IN as/RB many/JJ machines/NNS as/IN possible/JJ by/IN distributing/VBG the/DT optimization/NN method/NN used/VBN for/IN training/NN ./.
While/IN a/DT number/NN of/IN approaches/NNS have/VBP been/VBN proposed/VBN for/IN distributed/VBN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- ,/, at/IN the/DT current/JJ time/NN synchronous/JJ approaches/NNS to/IN distributed/VBN SGD/NNP appear/VBP to/TO be/VB showing/VBG the/DT greatest/JJS performance/NN at/IN large/JJ scale/NN ./.
Synchronous/JJ scaling/NN of/IN SGD/NNP suffers/VBZ from/IN the/DT need/NN to/TO synchronize/VB all/DT processors/NNS on/IN each/DT gradient/NN step/NN and/CC is/VBZ not/RB resilient/JJ in/IN the/DT face/NN of/IN failing/VBG or/CC lagging/VBG processors/NNS ./.
In/IN asynchronous/JJ approaches/NNS using/VBG parameter/NN servers/NNS ,/, training/NN is/VBZ slowed/VBN by/IN contention/NN to/IN the/DT parameter/NN server/NN ./.
In/IN this/DT paper/NN we/PRP compare/VBP the/DT convergence/NN of/IN synchronous/JJ and/CC asynchronous/JJ SGD/NNP for/IN training/VBG a/DT modern/JJ ResNet/NNP network/NN architecture/NN on/IN the/DT ImageNet/NNP classification/NN problem/NN ./.
We/PRP also/RB propose/VBP an/DT asynchronous/JJ method/NN ,/, gossiping/VBG SGD/NNP ,/, that/DT aims/VBZ to/TO retain/VB the/DT positive/JJ features/NNS of/IN both/DT systems/NNS by/IN replacing/VBG the/DT all/RB -/HYPH reduce/VB collective/JJ operation/NN of/IN synchronous/JJ training/NN with/IN a/DT gossip/NN aggregation/NN algorithm/NN ./.
We/PRP find/VBP ,/, perhaps/RB counterintuitively/RB ,/, that/IN asynchronous/JJ SGD/NNP ,/, including/VBG both/DT elastic/JJ averaging/NN and/CC gossiping/VBG ,/, converges/VBZ faster/RBR at/IN fewer/JJR nodes/NNS (/-LRB- up/RB to/IN about/RB 32/CD nodes/NNS )/-RRB- ,/, whereas/IN synchronous/JJ SGD/NNP scales/NNS better/JJR to/IN more/JJR nodes/NNS (/-LRB- up/RB to/IN about/RB 100/CD nodes/NNS )/-RRB- ./.
