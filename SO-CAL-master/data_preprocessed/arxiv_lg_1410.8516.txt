We/PRP propose/VBP a/DT deep/JJ learning/NN framework/NN for/IN modeling/NN complex/JJ high/JJ -/HYPH dimensional/JJ densities/NNS via/IN Non-linear/JJ Independent/JJ Component/NN Estimation/NN (/-LRB- NICE/JJ )/-RRB- ./.
It/PRP is/VBZ based/VBN on/IN the/DT idea/NN that/IN a/DT good/JJ representation/NN is/VBZ one/CD in/IN which/WDT the/DT data/NNS has/VBZ a/DT distribution/NN that/WDT is/VBZ easy/JJ to/TO model/VB ./.
For/IN this/DT purpose/NN ,/, a/DT non-linear/JJ deterministic/JJ transformation/NN of/IN the/DT data/NNS is/VBZ learned/VBN that/IN maps/NNS it/PRP to/IN a/DT latent/JJ space/NN so/RB as/IN to/TO make/VB the/DT transformed/VBN data/NNS conform/VBP to/IN a/DT factorized/JJ distribution/NN ,/, i.e./FW ,/, resulting/VBG in/IN independent/JJ latent/JJ variables/NNS ./.
We/PRP parametrize/VBP this/DT transformation/NN so/IN that/IN computing/VBG the/DT determinant/NN of/IN the/DT Jacobian/NNP and/CC inverse/JJ Jacobian/NNP is/VBZ trivial/JJ ,/, yet/CC we/PRP maintain/VBP the/DT ability/NN to/TO learn/VB complex/JJ non-linear/JJ transformations/NNS ,/, via/IN a/DT composition/NN of/IN simple/JJ building/NN blocks/NNS ,/, each/DT based/VBN on/IN a/DT deep/JJ neural/JJ network/NN ./.
The/DT training/NN criterion/NN is/VBZ simply/RB the/DT exact/JJ log/NN -/HYPH likelihood/NN ,/, which/WDT is/VBZ tractable/JJ ,/, and/CC unbiased/JJ ancestral/JJ sampling/NN is/VBZ also/RB easy/JJ ./.
We/PRP show/VBP that/IN this/DT approach/NN yields/VBZ good/JJ generative/JJ models/NNS on/IN four/CD image/NN datasets/NNS and/CC can/MD be/VB used/VBN for/IN inpainting/NN ./.
