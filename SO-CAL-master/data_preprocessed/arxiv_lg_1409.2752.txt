We/PRP explore/VBP combining/VBG the/DT benefits/NNS of/IN convolutional/JJ architectures/NNS and/CC autoencoders/NNS for/IN learning/VBG deep/JJ representations/NNS in/IN an/DT unsupervised/JJ manner/NN ./.
A/DT major/JJ challenge/NN is/VBZ to/TO achieve/VB appropriate/JJ sparsity/NN among/IN hidden/JJ variables/NNS ,/, since/IN neighbouring/VBG variables/NNS in/IN each/DT feature/NN map/NN tend/VBP to/TO be/VB highly/RB correlated/VBN and/CC a/DT suppression/NN mechanism/NN is/VBZ therefore/RB needed/VBN ./.
Previously/RB ,/, deconvolutional/JJ networks/NNS and/CC convolutional/JJ predictive/JJ sparse/JJ decomposition/NN have/VBP been/VBN used/VBN to/TO construct/VB systems/NNS that/WDT have/VBP a/DT recognition/NN pathway/NN and/CC a/DT data/NN generation/NN pathway/NN that/WDT are/VBP trained/VBN so/IN that/IN they/PRP agree/VBP and/CC so/IN that/IN the/DT hidden/JJ representation/NN is/VBZ sparse/JJ ./.
We/PRP take/VBP a/DT more/RBR direct/JJ approach/NN and/CC describe/VB a/DT way/NN to/TO train/VB convolutional/JJ autoencoders/NNS layer/NN by/IN layer/NN ,/, where/WRB in/IN each/DT layer/NN sparsity/NN is/VBZ achieved/VBN using/VBG a/DT winner/NN -/HYPH take/NN -/HYPH all/DT activation/NN function/NN within/IN each/DT feature/NN map/NN ./.
Learning/NN is/VBZ computationally/RB efficient/JJ and/CC we/PRP show/VBP that/IN our/PRP$ method/NN can/MD be/VB used/VBN to/TO train/VB shallow/JJ and/CC deep/JJ convolutional/JJ autoencoders/NNS whose/WP$ representations/NNS can/MD be/VB used/VBN to/TO achieve/VB classification/NN rates/NNS on/IN the/DT MNIST/NNP ,/, CIFAR/NNP -/HYPH 10/CD and/CC NORB/NNP datasets/NNS that/WDT are/VBP competitive/JJ with/IN the/DT state/NN of/IN the/DT art/NN ./.
