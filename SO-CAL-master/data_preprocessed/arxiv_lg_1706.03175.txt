In/IN this/DT paper/NN ,/, we/PRP consider/VBP regression/NN problems/NNS with/IN one/CD -/HYPH hidden/VBN -/HYPH layer/NN neural/JJ networks/NNS (/-LRB- 1NNs/NNS )/-RRB- ./.
We/PRP distill/VBP some/DT properties/NNS of/IN activation/NN functions/NNS that/WDT lead/VBP to/IN $/$ \/SYM mathit/FW {/-LRB- local/JJ ~/SYM strong/JJ ~/SYM convexity/NN }/-RRB- $/$ in/IN the/DT neighborhood/NN of/IN the/DT ground/NN -/HYPH truth/NN parameters/NNS for/IN the/DT 1NN/NN squared/VBN -/HYPH loss/NN objective/NN ./.
Most/RBS popular/JJ nonlinear/JJ activation/NN functions/NNS satisfy/VBP the/DT distilled/JJ properties/NNS ,/, including/VBG rectified/VBN linear/JJ units/NNS (/-LRB- ReLUs/NNS )/-RRB- ,/, leaky/JJ ReLUs/NNS ,/, squared/VBD ReLUs/NNS and/CC sigmoids/NNS ./.
For/IN activation/NN functions/NNS that/WDT are/VBP also/RB smooth/JJ ,/, we/PRP show/VBP $/$ \/SYM mathit/FW {/-LRB- local/JJ ~/SYM linear/JJ ~/SYM convergence/NN }/-RRB- $/$ guarantees/NNS of/IN gradient/NN descent/NN under/IN a/DT resampling/NN rule/NN ./.
For/IN homogeneous/JJ activations/NNS ,/, we/PRP show/VBP tensor/NN methods/NNS are/VBP able/JJ to/TO initialize/VB the/DT parameters/NNS to/TO fall/VB into/IN the/DT local/JJ strong/JJ convexity/NN region/NN ./.
As/IN a/DT result/NN ,/, tensor/NN initialization/NN followed/VBN by/IN gradient/NN descent/NN is/VBZ guaranteed/VBN to/TO recover/VB the/DT ground/NN truth/NN with/IN sample/NN complexity/NN $/$ d/LS \/SYM cdot/NN \/SYM log/NN (/-LRB- 1/CD //SYM \/SYM epsilon/SYM )/-RRB- \/SYM cdot/NN \/SYM mathrm/NN {/-LRB- poly/NN }/-RRB- (/-LRB- k/CD ,/, \/SYM lambda/NN )/-RRB- $/$ and/CC computational/JJ complexity/NN $/$ n/NN \/SYM cdot/NN d/NN \/SYM cdot/NN \/SYM mathrm/NN {/-LRB- poly/NN }/-RRB- (/-LRB- k/CD ,/, \/SYM lambda/NN )/-RRB- $/$ for/IN smooth/JJ homogeneous/JJ activations/NNS with/IN high/JJ probability/NN ,/, where/WRB $/$ d/LS $/$ is/VBZ the/DT dimension/NN of/IN the/DT input/NN ,/, $/$ k/CD $/$ (/-LRB- $/$ k/CD \/SYM leq/NN d/NN $/$ )/-RRB- is/VBZ the/DT number/NN of/IN hidden/JJ nodes/NNS ,/, $/$ \/SYM lambda/NN $/$ is/VBZ a/DT conditioning/NN property/NN of/IN the/DT ground/NN -/HYPH truth/NN parameter/NN matrix/NN between/IN the/DT input/NN layer/NN and/CC the/DT hidden/JJ layer/NN ,/, $/$ \/SYM epsilon/NN $/$ is/VBZ the/DT targeted/VBN precision/NN and/CC $/$ n/NN $/$ is/VBZ the/DT number/NN of/IN samples/NNS ./.
To/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ work/NN that/WDT provides/VBZ recovery/NN guarantees/NNS for/IN 1NNs/NNS with/IN both/DT sample/NN complexity/NN and/CC computational/JJ complexity/NN $/$ \/SYM mathit/FW {/-LRB- linear/JJ }/-RRB- $/$ in/IN the/DT input/NN dimension/NN and/CC $/$ \/CD mathit/FW {/-LRB- logarithmic/JJ }/-RRB- $/$ in/IN the/DT precision/NN ./.
