Keyphrase/NN provides/VBZ highly/RB -/HYPH summative/JJ information/NN that/WDT can/MD be/VB effectively/RB used/VBN for/IN understanding/NN ,/, organizing/VBG and/CC retrieving/VBG text/NN content/NN ./.
Though/IN previous/JJ studies/NNS have/VBP provided/VBN many/JJ workable/JJ solutions/NNS for/IN automated/VBN keyphrase/NN extraction/NN ,/, they/PRP commonly/RB divided/VBD the/DT to/IN -/HYPH be-summarized/VBN content/NN into/IN multiple/JJ text/NN chunks/NNS ,/, then/RB ranked/VBD and/CC selected/VBD the/DT most/RBS meaningful/JJ ones/NNS ./.
These/DT approaches/NNS could/MD neither/CC identify/VB keyphrases/NNS that/WDT do/VBP not/RB appear/VB in/IN the/DT text/NN ,/, nor/CC capture/NN the/DT real/JJ semantic/JJ meaning/NN behind/IN the/DT text/NN ./.
We/PRP propose/VBP a/DT generative/JJ model/NN for/IN keyphrase/NN prediction/NN with/IN an/DT encoder/NN -/HYPH decoder/NN framework/NN ,/, which/WDT can/MD effectively/RB overcome/VB the/DT above/JJ drawbacks/NNS ./.
We/PRP name/VBP it/PRP as/IN deep/JJ keyphrase/NN generation/NN since/IN it/PRP attempts/VBZ to/TO capture/VB the/DT deep/JJ semantic/JJ meaning/NN of/IN the/DT content/NN with/IN a/DT deep/JJ learning/NN method/NN ./.
Empirical/JJ analysis/NN on/IN six/CD datasets/NNS demonstrates/VBZ that/IN our/PRP$ proposed/VBN model/NN not/RB only/RB achieves/VBZ a/DT significant/JJ performance/NN boost/NN on/IN extracting/VBG keyphrases/NNS that/WDT appear/VBP in/IN the/DT source/NN text/NN ,/, but/CC also/RB can/MD generate/VB absent/JJ keyphrases/NNS based/VBN on/IN the/DT semantic/JJ meaning/NN of/IN the/DT text/NN ./.
Code/NN and/CC dataset/NN are/VBP available/JJ at/IN
