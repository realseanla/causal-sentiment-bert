In/IN this/DT paper/NN ,/, we/PRP enhance/VBP the/DT attention/NN -/HYPH based/VBN neural/JJ machine/NN translation/NN by/IN adding/VBG an/DT explicit/JJ coverage/NN embedding/NN model/NN to/TO alleviate/VB issues/NNS of/IN repeating/VBG and/CC dropping/VBG translations/NNS in/IN NMT/NNP ./.
For/IN each/DT source/NN word/NN ,/, our/PRP$ model/NN starts/VBZ with/IN a/DT full/JJ coverage/NN embedding/NN vector/NN ,/, and/CC then/RB keeps/VBZ updating/VBG it/PRP with/IN a/DT gated/VBN recurrent/JJ unit/NN as/IN the/DT translation/NN goes/VBZ ./.
All/PDT the/DT initialized/JJ coverage/NN embeddings/NNS and/CC updating/VBG matrix/NN are/VBP learned/VBN in/IN the/DT training/NN procedure/NN ./.
Experiments/NNS on/IN the/DT large/JJ -/HYPH scale/NN Chinese/JJ -/HYPH to/IN -/HYPH English/NNP task/NN show/VBP that/IN our/PRP$ enhanced/VBN model/NN improves/VBZ the/DT translation/NN quality/NN significantly/RB on/IN various/JJ test/NN sets/NNS over/IN the/DT strong/JJ large/JJ vocabulary/NN NMT/NN system/NN ./.
