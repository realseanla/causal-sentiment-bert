Unifying/JJ seemingly/RB disparate/JJ algorithmic/JJ ideas/NNS to/TO produce/VB better/JJR performing/VBG algorithms/NNS has/VBZ been/VBN a/DT longstanding/JJ goal/NN in/IN reinforcement/NN learning/NN ./.
As/IN a/DT primary/JJ example/NN ,/, TD/NN (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- elegantly/RB unifies/VBZ one/CD -/HYPH step/NN TD/NN prediction/NN with/IN Monte/NNP Carlo/NNP methods/NNS through/IN the/DT use/NN of/IN eligibility/NN traces/NNS and/CC the/DT trace/NN -/HYPH decay/NN parameter/NN $/$ \/CD lambda/NN $/$ ./.
Currently/RB ,/, there/EX are/VBP a/DT multitude/NN of/IN algorithms/NNS that/WDT can/MD be/VB used/VBN to/TO perform/VB TD/NN control/NN ,/, including/VBG Sarsa/NNP ,/, $/$ Q$/CD -/HYPH learning/NN ,/, and/CC Expected/VBN Sarsa/NNP ./.
These/DT methods/NNS are/VBP often/RB studied/VBN in/IN the/DT one/CD -/HYPH step/NN case/NN ,/, but/CC they/PRP can/MD be/VB extended/VBN across/IN multiple/JJ time/NN steps/NNS to/TO achieve/VB better/JJR performance/NN ./.
Each/DT of/IN these/DT algorithms/NNS is/VBZ seemingly/RB distinct/JJ ,/, and/CC no/DT one/NN dominates/VBZ the/DT others/NNS for/IN all/DT problems/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP study/VBP a/DT new/JJ multi-step/JJ action/NN -/HYPH value/NN algorithm/NN called/VBD $/$ Q/NN (/-LRB- \/SYM sigma/NN )/-RRB- $/$ which/WDT unifies/VBZ and/CC generalizes/VBZ these/DT existing/VBG algorithms/NNS ,/, while/IN subsuming/VBG them/PRP as/IN special/JJ cases/NNS ./.
A/DT new/JJ parameter/NN ,/, $/$ \/SYM sigma/FW $/$ ,/, is/VBZ introduced/VBN to/TO allow/VB the/DT degree/NN of/IN sampling/NN performed/VBN by/IN the/DT algorithm/NN at/IN each/DT step/NN during/IN its/PRP$ backup/NN to/TO be/VB continuously/RB varied/JJ ,/, with/IN Sarsa/NNP existing/VBG at/IN one/CD extreme/NN (/-LRB- full/JJ sampling/NN )/-RRB- ,/, and/CC Expected/VBN Sarsa/NNP existing/VBG at/IN the/DT other/JJ (/-LRB- pure/JJ expectation/NN )/-RRB- ./.
$/$ Q/NN (/-LRB- \/SYM sigma/NN )/-RRB- $/$ is/VBZ generally/RB applicable/JJ to/IN both/DT on/IN -/HYPH and/CC off/RB -/HYPH policy/NN learning/NN ,/, but/CC in/IN this/DT work/NN we/PRP focus/VBP on/IN experiments/NNS in/IN the/DT on/IN -/HYPH policy/NN case/NN ./.
Our/PRP$ results/NNS show/VBP that/IN an/DT intermediate/JJ value/NN of/IN $/$ \/CD sigma/CD $/$ ,/, which/WDT results/VBZ in/IN a/DT mixture/NN of/IN the/DT existing/VBG algorithms/NNS ,/, performs/VBZ better/JJR than/IN either/CC extreme/JJ ./.
The/DT mixture/NN can/MD also/RB be/VB varied/JJ dynamically/RB which/WDT can/MD result/VB in/IN even/RB greater/JJR performance/NN ./.
