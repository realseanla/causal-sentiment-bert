Stochastic/JJ gradient/NN descent/NN algorithms/NNS for/IN training/NN linear/JJ and/CC kernel/NN predictors/NNS are/VBP gaining/VBG more/JJR and/CC more/JJR importance/NN ,/, thanks/NNS to/IN their/PRP$ scalability/NN ./.
While/IN various/JJ methods/NNS have/VBP been/VBN proposed/VBN to/TO speed/VB up/RP their/PRP$ convergence/NN ,/, the/DT model/NN selection/NN phase/NN is/VBZ often/RB ignored/VBN ./.
In/IN fact/NN ,/, in/IN theoretical/JJ works/NNS most/RBS of/IN the/DT time/NN assumptions/NNS are/VBP made/VBN ,/, for/IN example/NN ,/, on/IN the/DT prior/JJ knowledge/NN of/IN the/DT norm/NN of/IN the/DT optimal/JJ solution/NN ,/, while/IN in/IN the/DT practical/JJ world/NN validation/NN methods/NNS remain/VBP the/DT only/JJ viable/JJ approach/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ kernel/NN -/HYPH based/VBN stochastic/JJ gradient/NN descent/NN algorithm/NN that/WDT performs/VBZ model/NN selection/NN while/IN training/NN ,/, with/IN no/DT parameters/NNS to/IN tune/NN ,/, nor/CC any/DT form/NN of/IN cross-validation/NN ./.
The/DT algorithm/NN builds/VBZ on/IN recent/JJ advancement/NN in/IN online/JJ learning/NN theory/NN for/IN unconstrained/JJ settings/NNS ,/, to/TO estimate/VB over/IN time/NN the/DT right/JJ regularization/NN in/IN a/DT data/NN -/HYPH dependent/JJ way/NN ./.
Optimal/JJ rates/NNS of/IN convergence/NN are/VBP proved/VBN under/IN standard/JJ smoothness/NN assumptions/NNS on/IN the/DT target/NN function/NN ,/, using/VBG the/DT range/NN space/NN of/IN the/DT fractional/JJ integral/JJ operator/NN associated/VBN with/IN the/DT kernel/NN ./.
