Deep/JJ learning/NN researchers/NNS commonly/RB suggest/VBP that/IN converged/VBN models/NNS are/VBP stuck/VBN in/IN local/JJ minima/NN ./.
More/RBR recently/RB ,/, some/DT researchers/NNS observed/VBD that/IN under/IN reasonable/JJ assumptions/NNS ,/, the/DT vast/JJ majority/NN of/IN critical/JJ points/NNS are/VBP saddle/NN points/NNS ,/, not/RB true/JJ minima/NN ./.
Both/DT descriptions/NNS suggest/VBP that/IN weights/NNS converge/VBP around/IN a/DT point/NN in/IN weight/NN space/NN ,/, be/VB it/PRP a/DT local/JJ optima/NN or/CC merely/RB a/DT critical/JJ point/NN ./.
However/RB ,/, it/PRP 's/VBZ possible/JJ that/IN neither/CC interpretation/NN is/VBZ accurate/JJ ./.
As/IN neural/JJ networks/NNS are/VBP typically/RB over-complete/JJ ,/, it/PRP 's/VBZ easy/JJ to/TO show/VB the/DT existence/NN of/IN vast/JJ continuous/JJ regions/NNS through/IN weight/NN space/NN with/IN equal/JJ loss/NN ./.
In/IN this/DT paper/NN ,/, we/PRP build/VBP on/IN recent/JJ work/NN empirically/RB characterizing/VBG the/DT error/NN surfaces/NNS of/IN neural/JJ networks/NNS ./.
We/PRP analyze/VBP training/NN paths/NNS through/IN weight/NN space/NN ,/, presenting/VBG evidence/NN that/IN apparent/JJ convergence/NN of/IN loss/NN does/VBZ not/RB correspond/VB to/IN weights/NNS arriving/VBG at/IN critical/JJ points/NNS ,/, but/CC instead/RB to/IN large/JJ movements/NNS through/IN flat/JJ regions/NNS of/IN weight/NN space/NN ./.
While/IN it/PRP 's/VBZ trivial/JJ to/TO show/VB that/IN neural/JJ network/NN error/NN surfaces/NNS are/VBP globally/RB non-convex/JJ ,/, we/PRP show/VBP that/IN error/NN surfaces/NNS are/VBP also/RB locally/RB non-convex/JJ ,/, even/RB after/IN breaking/VBG symmetry/NN with/IN a/DT random/JJ initialization/NN and/CC also/RB after/IN partial/JJ training/NN ./.
