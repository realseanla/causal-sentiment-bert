Algorithms/NNS for/IN learning/VBG distributions/NNS over/IN weight/NN -/HYPH vectors/NNS ,/, such/JJ as/IN AROW/NNP were/VBD recently/RB shown/VBN empirically/RB to/TO achieve/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN at/IN various/JJ problems/NNS ,/, with/IN strong/JJ theoretical/JJ guaranties/NNS ./.
Extending/VBG these/DT algorithms/NNS to/IN matrix/NN models/NNS pose/VBP challenges/NNS since/IN the/DT number/NN of/IN free/JJ parameters/NNS in/IN the/DT covariance/NN of/IN the/DT distribution/NN scales/NNS as/IN $/$ n/NN ^/SYM 4/CD $/$ with/IN the/DT dimension/NN $/$ n/NN $/$ of/IN the/DT matrix/NN ,/, and/CC $/$ n/NN $/$ tends/VBZ to/TO be/VB large/JJ in/IN real/JJ applications/NNS ./.
We/PRP describe/VBP ,/, analyze/VB and/CC experiment/VB with/IN two/CD new/JJ algorithms/NNS for/IN learning/VBG distribution/NN of/IN matrix/NN models/NNS ./.
Our/PRP$ first/JJ algorithm/NN maintains/VBZ a/DT diagonal/JJ covariance/NN over/IN the/DT parameters/NNS and/CC can/MD handle/VB large/JJ covariance/NN matrices/NNS ./.
The/DT second/JJ algorithm/NN factors/NNS the/DT covariance/NN to/TO capture/VB inter-features/NNS correlation/NN while/IN keeping/VBG the/DT number/NN of/IN parameters/NNS linear/JJ in/IN the/DT size/NN of/IN the/DT original/JJ matrix/NN ./.
We/PRP analyze/VBP both/DT algorithms/NNS in/IN the/DT mistake/NN bound/VBN model/NN and/CC show/VB a/DT superior/JJ precision/NN performance/NN of/IN our/PRP$ approach/NN over/IN other/JJ algorithms/NNS in/IN two/CD tasks/NNS :/: retrieving/VBG similar/JJ images/NNS ,/, and/CC ranking/VBG similar/JJ documents/NNS ./.
The/DT factored/JJ algorithm/NN is/VBZ shown/VBN to/TO attain/VB faster/RBR convergence/NN rate/NN ./.
