In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ finetuning/NN algorithm/NN for/IN the/DT recently/RB introduced/VBN multi-way/NN ,/, mulitlingual/JJ neural/JJ machine/NN translate/VB that/DT enables/VBZ zero/CD -/HYPH resource/NN machine/NN translation/NN ./.
When/WRB used/VBN together/RB with/IN novel/NN many/JJ -/HYPH to/IN -/HYPH one/CD translation/NN strategies/NNS ,/, we/PRP empirically/RB show/VBP that/IN this/DT finetuning/NN algorithm/NN allows/VBZ the/DT multi-way/NN ,/, multilingual/JJ model/NN to/IN translate/VB a/DT zero/CD -/HYPH resource/NN language/NN pair/NN (/-LRB- 1/CD )/-RRB- as/RB well/RB as/IN a/DT single/JJ -/HYPH pair/NN neural/JJ translation/NN model/NN trained/VBN with/IN up/RB to/IN 1M/NN direct/JJ parallel/JJ sentences/NNS of/IN the/DT same/JJ language/NN pair/NN and/CC (/-LRB- 2/LS )/-RRB- better/JJR than/IN pivot/NN -/HYPH based/VBN translation/NN strategy/NN ,/, while/IN keeping/VBG only/RB one/CD additional/JJ copy/NN of/IN attention/NN -/HYPH related/VBN parameters/NNS ./.
