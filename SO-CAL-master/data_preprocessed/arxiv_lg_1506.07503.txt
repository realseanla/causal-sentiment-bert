Recurrent/JJ sequence/NN generators/NNS conditioned/VBN on/IN input/NN data/NNS through/IN an/DT attention/NN mechanism/NN have/VBP recently/RB shown/VBN very/RB good/JJ performance/NN on/IN a/DT range/NN of/IN tasks/NNS in/IN -/HYPH cluding/VBG machine/NN translation/NN ,/, handwriting/NN synthesis/NN and/CC image/NN caption/NN gen/NN -/HYPH eration/NN ./.
We/PRP extend/VBP the/DT attention/NN -/, mechanism/NN with/IN features/NNS needed/VBN for/IN speech/NN recognition/NN ./.
We/PRP show/VBP that/IN while/IN an/DT adaptation/NN of/IN the/DT model/NN used/VBN for/IN machine/NN translation/NN in/IN reaches/VBZ a/DT competitive/JJ 18.7/CD percent/NN phoneme/NN error/NN rate/NN (/-LRB- PER/NN )/-RRB- on/IN the/DT TIMIT/NNP phoneme/NN recognition/NN task/NN ,/, it/PRP can/MD only/RB be/VB applied/VBN to/IN utterances/NNS which/WDT are/VBP roughly/RB as/RB long/RB as/IN the/DT ones/NNS it/PRP was/VBD trained/VBN on/IN ./.
We/PRP offer/VBP a/DT qualitative/JJ explanation/NN of/IN this/DT failure/NN and/CC propose/VB a/DT novel/NN and/CC generic/JJ method/NN of/IN adding/VBG location/NN -/HYPH awareness/NN to/IN the/DT attention/NN mechanism/NN to/TO alleviate/VB this/DT issue/NN ./.
The/DT new/JJ method/NN yields/VBZ a/DT model/NN that/WDT is/VBZ robust/JJ to/IN long/JJ inputs/NNS and/CC achieves/VBZ 18/CD percent/NN PER/NN in/IN single/JJ utterances/NNS and/CC 20/CD percent/NN in/IN 10/CD -/HYPH times/NNS longer/JJR (/-LRB- repeated/VBN )/-RRB- utterances/NNS ./.
Finally/RB ,/, we/PRP propose/VBP a/DT change/NN to/IN the/DT at/IN -/HYPH tention/NN mechanism/NN that/WDT prevents/VBZ it/PRP from/IN concentrating/VBG too/RB much/RB on/IN single/JJ frames/NNS ,/, which/WDT further/RB reduces/VBZ PER/NN to/IN 17.6/CD percent/NN level/NN ./.
