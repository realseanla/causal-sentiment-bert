Given/VBN a/DT task/NN of/IN predicting/VBG $/$ Y$/CD from/IN $/$ X$/CD ,/, a/DT loss/NN function/NN $/$ L$/CD ,/, and/CC a/DT set/NN of/IN probability/NN distributions/NNS $/$ \/SYM Gamma/NNP $/$ ,/, what/WP is/VBZ the/DT optimal/JJ decision/NN rule/NN minimizing/VBG the/DT worst/JJS -/HYPH case/NN expected/VBN loss/NN over/IN $/$ \/SYM Gamma/NNP $/$ ?/.
In/IN this/DT paper/NN ,/, we/PRP address/VBP this/DT question/NN by/IN introducing/VBG a/DT generalization/NN of/IN the/DT principle/NN of/IN maximum/JJ entropy/NN ./.
Applying/VBG this/DT principle/NN to/IN sets/NNS of/IN distributions/NNS with/IN a/DT proposed/VBN structure/NN ,/, we/PRP develop/VBP a/DT general/JJ minimax/NN approach/NN for/IN supervised/JJ learning/NN problems/NNS ,/, that/IN reduces/VBZ to/IN the/DT maximum/JJ likelihood/NN problem/NN over/IN generalized/VBN linear/JJ models/NNS ./.
Through/IN this/DT framework/NN ,/, we/PRP develop/VBP two/CD classification/NN algorithms/NNS called/VBD the/DT minimax/NN SVM/NN and/CC the/DT minimax/NN Brier/NNP classifier/NN ./.
The/DT minimax/NN SVM/NN ,/, which/WDT is/VBZ a/DT relaxed/JJ version/NN of/IN the/DT standard/JJ SVM/NNP ,/, minimizes/VBZ the/DT worst/JJS -/HYPH case/NN 0/CD -/HYPH 1/CD loss/NN over/IN the/DT structured/VBN set/NN of/IN distribution/NN ,/, and/CC by/IN our/PRP$ numerical/JJ experiments/NNS can/MD outperform/VB the/DT SVM/NNP ./.
We/PRP also/RB explore/VB the/DT application/NN of/IN the/DT developed/VBN framework/NN in/IN robust/JJ feature/NN selection/NN ./.
