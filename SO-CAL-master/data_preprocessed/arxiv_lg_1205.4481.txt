In/IN this/DT work/NN we/PRP consider/VBP the/DT stochastic/JJ minimization/NN of/IN nonsmooth/JJ convex/NN loss/NN functions/NNS ,/, a/DT central/JJ problem/NN in/IN machine/NN learning/NN ./.
We/PRP propose/VBP a/DT novel/JJ algorithm/NN called/VBN Accelerated/NNP Nonsmooth/NNP Stochastic/NNP Gradient/NNP Descent/NNP (/-LRB- ANSGD/NNP )/-RRB- ,/, which/WDT exploits/VBZ the/DT structure/NN of/IN common/JJ nonsmooth/JJ loss/NN functions/VBZ to/TO achieve/VB optimal/JJ convergence/NN rates/NNS for/IN a/DT class/NN of/IN problems/NNS including/VBG SVMs/NNS ./.
It/PRP is/VBZ the/DT first/JJ stochastic/JJ algorithm/NN that/WDT can/MD achieve/VB the/DT optimal/JJ O/NN (/-LRB- 1/CD //SYM t/NN )/-RRB- rate/NN for/IN minimizing/VBG nonsmooth/JJ loss/NN functions/NNS (/-LRB- with/IN strong/JJ convexity/NN )/-RRB- ./.
The/DT fast/JJ rates/NNS are/VBP confirmed/VBN by/IN empirical/JJ comparisons/NNS ,/, in/IN which/WDT ANSGD/NNP significantly/RB outperforms/VBZ previous/JJ subgradient/JJ descent/NN algorithms/NNS including/VBG SGD/NNP ./.
