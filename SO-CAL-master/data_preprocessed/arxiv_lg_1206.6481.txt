In/IN many/JJ multilingual/JJ text/NN classification/NN problems/NNS ,/, the/DT documents/NNS in/IN different/JJ languages/NNS often/RB share/VBP the/DT same/JJ set/NN of/IN categories/NNS ./.
To/TO reduce/VB the/DT labeling/NN cost/NN of/IN training/VBG a/DT classification/NN model/NN for/IN each/DT individual/JJ language/NN ,/, it/PRP is/VBZ important/JJ to/TO transfer/VB the/DT label/NN knowledge/NN gained/VBD from/IN one/CD language/NN to/IN another/DT language/NN by/IN conducting/VBG cross/NN language/NN classification/NN ./.
In/IN this/DT paper/NN we/PRP develop/VBP a/DT novel/JJ subspace/NN co-regularized/VBN multi-view/JJ learning/NN method/NN for/IN cross/NN language/NN text/NN classification/NN ./.
This/DT method/NN is/VBZ built/VBN on/IN parallel/JJ corpora/NNS produced/VBN by/IN machine/NN translation/NN ./.
It/PRP jointly/RB minimizes/VBZ the/DT training/NN error/NN of/IN each/DT classifier/NN in/IN each/DT language/NN while/IN penalizing/VBG the/DT distance/NN between/IN the/DT subspace/NN representations/NNS of/IN parallel/JJ documents/NNS ./.
Our/PRP$ empirical/JJ study/NN on/IN a/DT large/JJ set/NN of/IN cross/NN language/NN text/NN classification/NN tasks/NNS shows/VBZ the/DT proposed/JJ method/NN consistently/RB outperforms/VBZ a/DT number/NN of/IN inductive/JJ methods/NNS ,/, domain/NN adaptation/NN methods/NNS ,/, and/CC multi-view/JJ learning/NN methods/NNS ./.
