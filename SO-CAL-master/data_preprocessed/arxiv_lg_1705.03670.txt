Recently/RB deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- have/VBP been/VBN used/VBN to/TO learn/VB speaker/NN features/NNS ./.
However/RB ,/, the/DT quality/NN of/IN the/DT learned/VBN features/NNS is/VBZ not/RB sufficiently/RB good/JJ ,/, so/RB a/DT complex/NN back/RB -/HYPH end/VB model/NN ,/, either/CC neural/JJ or/CC probabilistic/JJ ,/, has/VBZ to/TO be/VB used/VBN to/TO address/VB the/DT residual/JJ uncertainty/NN when/WRB applied/VBN to/IN speaker/NN verification/NN ,/, just/RB as/IN with/IN raw/JJ features/NNS ./.
This/DT paper/NN presents/VBZ a/DT convolutional/JJ time/NN -/HYPH delay/NN deep/JJ neural/JJ network/NN structure/NN (/-LRB- CT/NN -/HYPH DNN/NN )/-RRB- for/IN speaker/NN feature/NN learning/NN ./.
Our/PRP$ experimental/JJ results/NNS on/IN the/DT Fisher/NNP database/NN demonstrated/VBD that/IN this/DT CT/NN -/HYPH DNN/NN can/MD produce/VB high/JJ -/HYPH quality/NN speaker/NN features/VBZ :/: even/RB with/IN a/DT single/JJ feature/NN (/-LRB- 0.3/CD seconds/NNS including/VBG the/DT context/NN )/-RRB- ,/, the/DT EER/NNP can/MD be/VB as/RB low/JJ as/IN 7.68/CD percent/NN ./.
This/DT effectively/RB confirmed/VBD that/IN the/DT speaker/NN trait/NN is/VBZ largely/RB a/DT deterministic/JJ short/JJ -/HYPH time/NN property/NN rather/RB than/IN a/DT long/JJ -/HYPH time/NN distributional/JJ pattern/NN ,/, and/CC therefore/RB can/MD be/VB extracted/VBN from/IN just/RB dozens/NNS of/IN frames/NNS ./.
