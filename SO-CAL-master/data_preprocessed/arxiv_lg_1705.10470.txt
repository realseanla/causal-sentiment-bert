In/IN this/DT paper/NN ,/, we/PRP consider/VBP the/DT problem/NN of/IN machine/NN teaching/NN ,/, the/DT inverse/JJ problem/NN of/IN machine/NN learning/NN ./.
Different/JJ from/IN traditional/JJ machine/NN teaching/NN which/WDT views/VBZ the/DT learners/NNS as/IN batch/NN algorithms/NNS ,/, we/PRP study/VBP a/DT new/JJ paradigm/NN where/WRB the/DT learner/NN uses/VBZ an/DT iterative/JJ algorithm/NN and/CC a/DT teacher/NN can/MD feed/VB examples/NNS sequentially/RB and/CC intelligently/RB based/VBN on/IN the/DT current/JJ performance/NN of/IN the/DT learner/NN ./.
We/PRP show/VBP that/IN the/DT teaching/NN complexity/NN in/IN the/DT iterative/JJ case/NN is/VBZ very/RB different/JJ from/IN that/DT in/IN the/DT batch/NN case/NN ./.
Instead/RB of/IN constructing/VBG a/DT minimal/JJ training/NN set/NN for/IN learners/NNS ,/, our/PRP$ iterative/JJ machine/NN teaching/NN focuses/VBZ on/IN achieving/VBG fast/JJ convergence/NN in/IN the/DT learner/NN model/NN ./.
Depending/VBG on/IN the/DT level/NN of/IN information/NN the/DT teacher/NN has/VBZ from/IN the/DT learner/NN model/NN ,/, we/PRP design/VBP teaching/VBG algorithms/NNS which/WDT can/MD provably/RB reduce/VB the/DT number/NN of/IN teaching/NN examples/NNS and/CC achieve/VB faster/RBR convergence/NN than/IN learning/VBG without/IN teachers/NNS ./.
We/PRP also/RB validate/VBP our/PRP$ theoretical/JJ findings/NNS with/IN extensive/JJ experiments/NNS on/IN different/JJ data/NNS distribution/NN and/CC real/JJ image/NN datasets/NNS ./.
