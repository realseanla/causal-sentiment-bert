Canonical/JJ correlation/NN analysis/NN (/-LRB- CCA/NN )/-RRB- is/VBZ a/DT fundamental/JJ technique/NN in/IN multi-view/JJ data/NNS analysis/NN and/CC representation/NN learning/NN ./.
Several/JJ nonlinear/JJ extensions/NNS of/IN the/DT classical/JJ linear/JJ CCA/NN method/NN have/VBP been/VBN proposed/VBN ,/, including/VBG kernel/NN and/CC deep/JJ neural/JJ network/NN methods/NNS ./.
These/DT approaches/NNS restrict/VBP attention/NN to/IN certain/JJ families/NNS of/IN nonlinear/JJ projections/NNS ,/, which/WDT the/DT user/NN must/MD specify/VB (/-LRB- by/IN choosing/VBG a/DT kernel/NN or/CC a/DT neural/JJ network/NN architecture/NN )/-RRB- ,/, and/CC are/VBP computationally/RB demanding/VBG ./.
Interestingly/RB ,/, the/DT theory/NN of/IN nonlinear/JJ CCA/NN without/IN any/DT functional/JJ restrictions/NNS ,/, has/VBZ been/VBN studied/VBN in/IN the/DT population/NN setting/NN by/IN Lancaster/NNP already/RB in/IN the/DT 50's/NNS ./.
However/RB ,/, these/DT results/NNS ,/, have/VBP not/RB inspired/VBN practical/JJ algorithms/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP revisit/VBP Lancaster/NNP 's/POS theory/NN ,/, and/CC use/VB it/PRP to/TO devise/VB a/DT practical/JJ algorithm/NN for/IN nonparametric/JJ CCA/NN (/-LRB- NCCA/NN )/-RRB- ./.
Specifically/RB ,/, we/PRP show/VBP that/IN the/DT most/RBS correlated/JJ nonlinear/JJ projections/NNS of/IN two/CD random/JJ vectors/NNS can/MD be/VB expressed/VBN in/IN terms/NNS of/IN the/DT singular/JJ value/NN decomposition/NN of/IN a/DT certain/JJ operator/NN associated/VBN with/IN their/PRP$ joint/JJ density/NN ./.
Thus/RB ,/, by/IN estimating/VBG the/DT population/NN density/NN from/IN data/NNS ,/, NCCA/NNP reduces/VBZ to/IN solving/VBG an/DT eigenvalue/NN system/NN ,/, superficially/RB like/IN kernel/NN CCA/NN but/CC ,/, importantly/RB ,/, without/IN having/VBG to/TO compute/VB the/DT inverse/NN of/IN any/DT kernel/NN matrix/NN ./.
We/PRP also/RB derive/VBP a/DT partially/RB linear/JJ CCA/NN (/-LRB- PLCCA/NN )/-RRB- variant/NN in/IN which/WDT one/CD of/IN the/DT views/NNS undergoes/VBZ a/DT linear/JJ projection/NN while/IN the/DT other/JJ is/VBZ nonparametric/JJ ./.
PLCCA/NN turns/VBZ out/RP to/TO have/VB a/DT similar/JJ form/NN to/IN the/DT classical/JJ linear/JJ CCA/NN ,/, but/CC with/IN a/DT nonparametric/JJ regression/NN term/NN replacing/VBG the/DT linear/JJ regression/NN in/IN CCA/NN ./.
Using/VBG a/DT kernel/NN density/NN estimate/NN based/VBN on/IN a/DT small/JJ number/NN of/IN nearest/JJS neighbors/NNS ,/, our/PRP$ NCCA/NNP and/CC PLCCA/NNP algorithms/NNS are/VBP memory/NN -/HYPH efficient/JJ ,/, often/RB run/VBP much/RB faster/RBR ,/, and/CC achieve/VB better/JJR performance/NN than/IN kernel/NN CCA/NN and/CC comparable/JJ performance/NN to/IN deep/JJ CCA/NN ./.
