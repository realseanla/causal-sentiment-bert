Reinforcement/NN Learning/NN (/-LRB- RL/NN )/-RRB- can/MD model/VB complex/JJ behavior/NN policies/NNS for/IN goal/NN -/HYPH directed/VBN sequential/JJ decision/NN making/VBG tasks/NNS ./.
A/DT hallmark/NN of/IN RL/NNP algorithms/NNS is/VBZ Temporal/JJ Difference/NN (/-LRB- TD/NN )/-RRB- learning/NN :/: value/NN function/NN for/IN the/DT current/JJ state/NN is/VBZ moved/VBN towards/IN a/DT bootstrapped/JJ target/NN that/WDT is/VBZ estimated/VBN using/VBG next/JJ state/NN 's/POS value/NN function/NN ./.
$/$ \/CD lambda/NN $/$ -/HYPH returns/NNS generalize/VB beyond/IN 1/CD -/HYPH step/NN returns/NNS and/CC strike/VB a/DT balance/NN between/IN Monte/NNP Carlo/NNP and/CC TD/NNP learning/VBG methods/NNS ./.
While/IN lambda/NN -/HYPH returns/NNS have/VBP been/VBN extensively/RB studied/VBN in/IN RL/NNP ,/, they/PRP have/VBP n't/RB been/VBN explored/VBN a/DT lot/NN in/IN Deep/NNP RL/NNP ./.
This/DT paper/NN 's/POS first/JJ contribution/NN is/VBZ an/DT exhaustive/JJ benchmarking/NN of/IN lambda/NN -/HYPH returns/NNS ./.
Although/IN mathematically/RB tractable/JJ ,/, the/DT use/NN of/IN exponentially/RB decaying/VBG weighting/NN of/IN n/NN -/HYPH step/NN returns/NNS based/VBN targets/NNS in/IN lambda/NN -/HYPH returns/NNS is/VBZ a/DT rather/RB ad/FW -/HYPH hoc/FW design/NN choice/NN ./.
Our/PRP$ second/JJ major/JJ contribution/NN is/VBZ that/IN we/PRP propose/VBP a/DT generalization/NN of/IN lambda/NN -/HYPH returns/NNS called/VBN Confidence/NN -/HYPH based/VBN Autodidactic/NN Returns/NNS (/-LRB- CAR/NN )/-RRB- ,/, wherein/WRB the/DT RL/NNP agent/NN learns/VBZ the/DT weighting/NN of/IN the/DT n/NN -/HYPH step/NN returns/NNS in/IN an/DT end/NN -/HYPH to/IN -/HYPH end/NN manner/NN ./.
This/DT allows/VBZ the/DT agent/NN to/TO learn/VB to/TO decide/VB how/WRB much/JJ it/PRP wants/VBZ to/TO weigh/VB the/DT n/NN -/HYPH step/NN returns/NNS based/VBN targets/NNS ./.
In/IN contrast/NN ,/, lambda/NN -/HYPH returns/NNS restrict/VBP RL/NNP agents/NNS to/TO use/VB an/DT exponentially/RB decaying/VBG weighting/NN scheme/NN ./.
Autodidactic/JJ returns/NNS can/MD be/VB used/VBN for/IN improving/VBG any/DT RL/NN algorithm/NN which/WDT uses/VBZ TD/NN learning/NN ./.
We/PRP empirically/RB demonstrate/VBP that/IN using/VBG sophisticated/JJ weighted/JJ mixtures/NNS of/IN multi-step/JJ returns/NNS (/-LRB- like/IN CAR/NN and/CC lambda/NN -/HYPH returns/NNS )/-RRB- considerably/RB outperforms/VBZ the/DT use/NN of/IN n/NN -/HYPH step/NN returns/NNS ./.
We/PRP perform/VBP our/PRP$ experiments/NNS on/IN the/DT Asynchronous/JJ Advantage/NN Actor/NN Critic/NN (/-LRB- A3C/NN )/-RRB- algorithm/NN in/IN the/DT Atari/NNP 2600/CD domain/NN ./.
