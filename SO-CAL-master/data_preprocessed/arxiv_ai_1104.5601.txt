We/PRP consider/VBP finite/JJ horizon/NN Markov/NNP decision/NN processes/NNS under/IN performance/NN measures/NNS that/WDT involve/VBP both/CC the/DT mean/NN and/CC the/DT variance/NN of/IN the/DT cumulative/JJ reward/NN ./.
We/PRP show/VBP that/IN either/CC randomized/VBD or/CC history/NN -/HYPH based/VBN policies/NNS can/MD improve/VB performance/NN ./.
We/PRP prove/VBP that/IN the/DT complexity/NN of/IN computing/VBG a/DT policy/NN that/WDT maximizes/VBZ the/DT mean/JJ reward/NN under/IN a/DT variance/NN constraint/NN is/VBZ NP/NNP -/HYPH hard/JJ for/IN some/DT cases/NNS ,/, and/CC strongly/RB NP/NNP -/HYPH hard/JJ for/IN others/NNS ./.
We/PRP finally/RB offer/VBP pseudopolynomial/JJ exact/JJ and/CC approximation/NN algorithms/NNS ./.
