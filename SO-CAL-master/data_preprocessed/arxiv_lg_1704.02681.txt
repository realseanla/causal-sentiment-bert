This/DT paper/NN explores/VBZ the/DT use/NN of/IN Pyramid/NN Vector/NNP Quantization/NN (/-LRB- PVQ/NN )/-RRB- to/TO reduce/VB the/DT computational/JJ cost/NN for/IN a/DT variety/NN of/IN neural/JJ networks/NNS (/-LRB- NNs/NNS )/-RRB- while/IN ,/, at/IN the/DT same/JJ time/NN ,/, compressing/VBG the/DT weights/NNS that/WDT describe/VBP them/PRP ./.
This/DT is/VBZ based/VBN on/IN the/DT fact/NN that/IN the/DT dot/NN product/NN between/IN an/DT N/NN dimensional/JJ vector/NN of/IN real/JJ numbers/NNS and/CC an/DT N/NN dimensional/JJ PVQ/NN vector/NN can/MD be/VB calculated/VBN with/IN only/JJ additions/NNS and/CC subtractions/NNS and/CC one/CD multiplication/NN ./.
This/DT is/VBZ advantageous/JJ since/IN tensor/NN products/NNS ,/, commonly/RB used/VBN in/IN NNs/NNS ,/, can/MD be/VB re-conduced/VBN to/IN a/DT dot/NN product/NN or/CC a/DT set/NN of/IN dot/NN products/NNS ./.
Finally/RB ,/, it/PRP is/VBZ stressed/VBN that/IN any/DT NN/NNP architecture/NN that/WDT is/VBZ based/VBN on/IN an/DT operation/NN that/WDT can/MD be/VB re-conduced/VBN to/IN a/DT dot/NN product/NN can/MD benefit/VB from/IN the/DT techniques/NNS described/VBN here/RB ./.
