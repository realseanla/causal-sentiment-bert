The/DT efficiency/NN of/IN reinforcement/NN learning/VBG algorithms/NNS depends/VBZ critically/RB on/IN a/DT few/JJ meta/NN -/HYPH parameters/NNS that/WDT modulates/VBZ the/DT learning/NN updates/NNS and/CC the/DT trade/NN -/HYPH off/NN between/IN exploration/NN and/CC exploitation/NN ./.
The/DT adaptation/NN of/IN the/DT meta/NN -/HYPH parameters/NNS is/VBZ an/DT open/JJ question/NN in/IN reinforcement/NN learning/NN ,/, which/WDT arguably/RB has/VBZ become/VBN more/JJR of/IN an/DT issue/NN recently/RB with/IN the/DT success/NN of/IN deep/JJ reinforcement/NN learning/VBG in/IN high/JJ -/HYPH dimensional/JJ state/NN spaces/NNS ./.
The/DT long/JJ learning/NN times/NNS in/IN domains/NNS such/JJ as/IN Atari/NNP 2600/CD video/NN games/NNS makes/VBZ it/PRP not/RB feasible/JJ to/TO perform/VB comprehensive/JJ searches/NNS of/IN appropriate/JJ meta/NN -/HYPH parameter/NN values/NNS ./.
We/PRP propose/VBP the/DT Online/NN Meta/NN -/HYPH learning/NN by/IN Parallel/JJ Algorithm/NN Competition/NN (/-LRB- OMPAC/NN )/-RRB- method/NN ./.
In/IN the/DT OMPAC/NN method/NN ,/, several/JJ instances/NNS of/IN a/DT reinforcement/NN learning/VBG algorithm/NN are/VBP run/VBN in/IN parallel/JJ with/IN small/JJ differences/NNS in/IN the/DT initial/JJ values/NNS of/IN the/DT meta/NN -/HYPH parameters/NNS ./.
After/IN a/DT fixed/VBN number/NN of/IN episodes/NNS ,/, the/DT instances/NNS are/VBP selected/VBN based/VBN on/IN their/PRP$ performance/NN in/IN the/DT task/NN at/IN hand/NN ./.
Before/IN continuing/VBG the/DT learning/NN ,/, Gaussian/JJ noise/NN is/VBZ added/VBN to/IN the/DT meta/NN -/HYPH parameters/NNS with/IN a/DT predefined/JJ probability/NN ./.
We/PRP validate/VBP the/DT OMPAC/NN method/NN by/IN improving/VBG the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN stochastic/JJ SZ/NNP -/HYPH Tetris/NNP and/CC in/IN standard/JJ Tetris/NNP with/IN a/DT smaller/JJR ,/, 10/CD $/$ \/CD times/NNS $/$ 10/CD ,/, board/NN ,/, by/IN 31/CD percent/NN and/CC 84/CD percent/NN ,/, respectively/RB ,/, and/CC by/IN improving/VBG the/DT results/NNS for/IN deep/JJ Sarsa/NNP (/-LRB- $/$ \/CD lambda/NN $/$ )/-RRB- agents/NNS in/IN three/CD Atari/NNP 2600/CD games/NNS by/IN 62/CD percent/NN or/CC more/JJR ./.
The/DT experiments/NNS also/RB show/VBP the/DT ability/NN of/IN the/DT OMPAC/NN method/NN to/TO adapt/VB the/DT meta/NN -/HYPH parameters/NNS according/VBG to/IN the/DT learning/NN progress/NN in/IN different/JJ tasks/NNS ./.
