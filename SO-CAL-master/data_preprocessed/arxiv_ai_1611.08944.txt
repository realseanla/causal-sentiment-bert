Reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- problems/NNS are/VBP often/RB phrased/VBN in/IN terms/NNS of/IN Markov/NNP decision/NN processes/NNS (/-LRB- MDPs/NNS )/-RRB- ./.
In/IN this/DT thesis/NN we/PRP go/VBP beyond/IN MDPs/NNS and/CC consider/VB RL/NNP in/IN environments/NNS that/WDT are/VBP non-Markovian/JJ ,/, non-ergodic/JJ and/CC only/RB partially/RB observable/JJ ./.
Our/PRP$ focus/NN is/VBZ not/RB on/IN practical/JJ algorithms/NNS ,/, but/CC rather/RB on/IN the/DT fundamental/JJ underlying/JJ problems/NNS :/: How/WRB do/VBP we/PRP balance/VB exploration/NN and/CC exploitation/NN ?/.
How/WRB do/VBP we/PRP explore/VB optimally/RB ?/.
When/WRB is/VBZ an/DT agent/NN optimal/JJ ?/.
We/PRP follow/VBP the/DT nonparametric/JJ realizable/JJ paradigm/NN ./.
