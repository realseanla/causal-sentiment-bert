In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ neural/JJ network/NN model/NN called/VBN RNN/NNP Encoder/NNP --/: Decoder/NN that/WDT consists/VBZ of/IN two/CD recurrent/JJ neural/JJ networks/NNS (/-LRB- RNN/NN )/-RRB- ./.
One/CD RNN/NN encodes/VBZ a/DT sequence/NN of/IN symbols/NNS into/IN a/DT fixed/VBN -/HYPH length/NN vector/NN representation/NN ,/, and/CC the/DT other/JJ decodes/VBZ the/DT representation/NN into/IN another/DT sequence/NN of/IN symbols/NNS ./.
The/DT encoder/NN and/CC decoder/NN of/IN the/DT proposed/VBN model/NN are/VBP jointly/RB trained/VBN to/TO maximize/VB the/DT conditional/JJ probability/NN of/IN a/DT target/NN sequence/NN given/VBN a/DT source/NN sequence/NN ./.
The/DT performance/NN of/IN a/DT statistical/JJ machine/NN translation/NN system/NN is/VBZ empirically/RB found/VBN to/TO improve/VB by/IN using/VBG the/DT conditional/JJ probabilities/NNS of/IN phrase/NN pairs/NNS computed/VBN by/IN the/DT RNN/NNP Encoder/NNP --/: Decoder/NNP as/IN an/DT additional/JJ feature/NN in/IN the/DT existing/JJ log/NN -/HYPH linear/JJ model/NN ./.
Qualitatively/RB ,/, we/PRP show/VBP that/IN the/DT proposed/VBN model/NN learns/VBZ a/DT semantically/RB and/CC syntactically/RB meaningful/JJ representation/NN of/IN linguistic/JJ phrases/NNS ./.
