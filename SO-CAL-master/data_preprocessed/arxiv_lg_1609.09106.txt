This/DT work/NN explores/VBZ hypernetworks/NNS :/: an/DT approach/NN of/IN using/VBG a/DT small/JJ network/NN ,/, also/RB known/VBN as/IN a/DT hypernetwork/NN ,/, to/TO generate/VB the/DT weights/NNS for/IN a/DT larger/JJR network/NN ./.
Hypernetworks/NNS provide/VBP an/DT abstraction/NN that/WDT is/VBZ similar/JJ to/IN what/WP is/VBZ found/VBN in/IN nature/NN :/: the/DT relationship/NN between/IN a/DT genotype/NN -/HYPH the/DT hypernetwork/NN -/HYPH and/CC a/DT phenotype/NN -/HYPH the/DT main/JJ network/NN ./.
Though/IN they/PRP are/VBP also/RB reminiscent/JJ of/IN HyperNEAT/NN in/IN evolution/NN ,/, our/PRP$ hypernetworks/NNS are/VBP trained/VBN end/NN -/HYPH to/IN -/HYPH end/NN with/IN backpropagation/NN and/CC thus/RB are/VBP usually/RB faster/RBR ./.
The/DT focus/NN of/IN this/DT work/NN is/VBZ to/TO make/VB hypernetworks/NNS useful/JJ for/IN deep/JJ convolutional/JJ networks/NNS and/CC long/JJ recurrent/JJ networks/NNS ,/, where/WRB hypernetworks/NNS can/MD be/VB viewed/VBN as/IN relaxed/JJ form/NN of/IN weight/NN -/HYPH sharing/NN across/IN layers/NNS ./.
Our/PRP$ main/JJ result/NN is/VBZ that/IN hypernetworks/NNS can/MD generate/VB non-shared/JJ weights/NNS for/IN LSTM/NNP and/CC achieve/VB state/NN -/HYPH of/IN -/HYPH art/NN results/NNS on/IN a/DT variety/NN of/IN language/NN modeling/NN tasks/NNS with/IN Character/NN -/HYPH Level/NN Penn/NNP Treebank/NNP and/CC Hutter/NNP Prize/NNP Wikipedia/NNP datasets/NNS ,/, challenging/VBG the/DT weight/NN -/HYPH sharing/VBG paradigm/NN for/IN recurrent/JJ networks/NNS ./.
Our/PRP$ results/NNS also/RB show/VBP that/IN hypernetworks/NNS applied/VBN to/IN convolutional/JJ networks/NNS still/RB achieve/VBP respectable/JJ results/NNS for/IN image/NN recognition/NN tasks/NNS compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN baseline/NN models/NNS while/IN requiring/VBG fewer/JJR learnable/JJ parameters/NNS ./.
