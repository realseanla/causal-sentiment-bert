Training/VBG neural/JJ network/NN language/NN models/NNS over/IN large/JJ vocabularies/NNS is/VBZ still/RB computationally/RB very/RB costly/JJ compared/VBN to/IN count/NN -/HYPH based/VBN models/NNS such/JJ as/IN Kneser/NNP -/HYPH Ney/NNP ./.
At/IN the/DT same/JJ time/NN ,/, neural/JJ language/NN models/NNS are/VBP gaining/VBG popularity/NN for/IN many/JJ applications/NNS such/JJ as/IN speech/NN recognition/NN and/CC machine/NN translation/NN whose/WP$ success/NN depends/VBZ on/IN scalability/NN ./.
We/PRP present/VBP a/DT systematic/JJ comparison/NN of/IN strategies/NNS to/TO represent/VB and/CC train/VB large/JJ vocabularies/NNS ,/, including/VBG softmax/NN ,/, hierarchical/JJ softmax/NN ,/, target/NN sampling/NN ,/, noise/NN contrastive/JJ estimation/NN and/CC self/NN normalization/NN ./.
We/PRP further/RB extend/VBP self/NN normalization/NN to/TO be/VB a/DT proper/JJ estimator/NN of/IN likelihood/NN and/CC introduce/VB an/DT efficient/JJ variant/NN of/IN softmax/NN ./.
We/PRP evaluate/VBP each/DT method/NN on/IN three/CD popular/JJ benchmarks/NNS ,/, examining/VBG performance/NN on/IN rare/JJ words/NNS ,/, the/DT speed/NN //HYPH accuracy/NN trade/NN -/HYPH off/NN and/CC complementarity/NN to/IN Kneser/NNP -/HYPH Ney/NNP ./.
