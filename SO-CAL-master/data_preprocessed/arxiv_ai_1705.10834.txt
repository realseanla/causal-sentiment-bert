Experience/NN replay/NN is/VBZ one/CD of/IN the/DT most/RBS commonly/RB used/VBN approaches/NNS to/TO improve/VB the/DT sample/NN efficiency/NN of/IN reinforcement/NN learning/VBG algorithms/NNS ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP an/DT approach/NN to/IN select/JJ and/CC replay/NN sequences/NNS of/IN transitions/NNS in/IN order/NN to/TO accelerate/VB the/DT learning/NN of/IN a/DT reinforcement/NN learning/VBG agent/NN in/IN an/DT off/RB -/HYPH policy/NN setting/NN ./.
In/IN addition/NN to/IN selecting/VBG appropriate/JJ sequences/NNS ,/, we/PRP also/RB artificially/RB construct/VB transition/NN sequences/NNS using/VBG information/NN gathered/VBN from/IN previous/JJ agent/NN -/HYPH environment/NN interactions/NNS ./.
These/DT sequences/NNS ,/, when/WRB replayed/VBN ,/, allow/VB value/NN function/NN information/NN to/IN trickle/NN down/IN to/IN larger/JJR sections/NNS of/IN the/DT state/NN //HYPH state/NN -/HYPH action/NN space/NN ,/, thereby/RB making/VBG the/DT most/JJS of/IN the/DT agent/NN 's/POS experience/NN ./.
We/PRP demonstrate/VBP our/PRP$ approach/NN on/IN modified/VBN versions/NNS of/IN standard/JJ reinforcement/NN learning/VBG tasks/NNS such/JJ as/IN the/DT mountain/NN car/NN and/CC puddle/NN world/NN problems/NNS and/CC empirically/RB show/VBP that/IN it/PRP enables/VBZ better/JJR learning/NN of/IN value/NN functions/VBZ as/IN compared/VBN to/IN other/JJ forms/NNS of/IN experience/NN replay/NN ./.
Further/RB ,/, we/PRP briefly/RB discuss/VBP some/DT of/IN the/DT possible/JJ extensions/NNS to/IN this/DT work/NN ,/, as/RB well/RB as/IN applications/NNS and/CC situations/NNS where/WRB this/DT approach/NN could/MD be/VB particularly/RB useful/JJ ./.
