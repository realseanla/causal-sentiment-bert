With/IN the/DT advent/NN of/IN word/NN embeddings/NNS ,/, lexicons/NNS are/VBP no/RB longer/RBR fully/RB utilized/VBN for/IN sentiment/NN analysis/NN although/IN they/PRP still/RB provide/VBP important/JJ features/NNS in/IN the/DT traditional/JJ setting/NN ./.
This/DT paper/NN introduces/VBZ a/DT novel/JJ approach/NN to/IN sentiment/NN analysis/NN that/WDT integrates/VBZ lexicon/NN embeddings/NNS and/CC an/DT attention/NN mechanism/NN into/IN Convolutional/JJ Neural/JJ Networks/NNS ./.
Our/PRP$ approach/NN performs/VBZ separate/JJ convolutions/NNS for/IN word/NN and/CC lexicon/NN embeddings/NNS and/CC provides/VBZ a/DT global/JJ view/NN of/IN the/DT document/NN using/VBG attention/NN ./.
Our/PRP$ models/NNS are/VBP experimented/VBN on/IN both/CC the/DT SemEval/NNP '16/CD Task/NNP 4/CD dataset/NN and/CC the/DT Stanford/NNP Sentiment/NN Treebank/NNP ,/, and/CC show/VBP comparative/JJ or/CC better/JJR results/NNS against/IN the/DT existing/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS ./.
Our/PRP$ analysis/NN shows/VBZ that/IN lexicon/NN embeddings/NNS allow/VBP to/TO build/VB high/JJ -/HYPH performing/VBG models/NNS with/IN much/RB smaller/JJR word/NN embeddings/NNS ,/, and/CC the/DT attention/NN mechanism/NN effectively/RB dims/VBZ out/RP noisy/JJ words/NNS for/IN sentiment/NN analysis/NN ./.
