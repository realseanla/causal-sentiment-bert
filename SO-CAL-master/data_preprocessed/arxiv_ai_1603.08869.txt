Hierarchical/JJ Reinforcement/NN Learning/NN (/-LRB- HRL/NN )/-RRB- exploits/NNS temporal/JJ abstraction/NN to/TO solve/VB large/JJ Markov/NNP Decision/NN Processes/NNS (/-LRB- MDP/NN )/-RRB- and/CC provide/VB transferable/JJ subtask/NN policies/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP an/DT off/RB -/HYPH policy/NN HRL/NN algorithm/NN :/: Hierarchical/JJ Q/NN -/HYPH value/NN Iteration/NN (/-LRB- HQI/NN )/-RRB- ./.
We/PRP show/VBP that/IN it/PRP is/VBZ possible/JJ to/TO effectively/RB learn/VB recursive/JJ optimal/JJ policies/NNS for/IN any/DT valid/JJ hierarchical/JJ decomposition/NN of/IN the/DT original/JJ MDP/NN ,/, given/VBN a/DT fixed/VBN dataset/NN collected/VBN from/IN a/DT flat/JJ stochastic/JJ behavioral/JJ policy/NN ./.
We/PRP first/RB formally/RB prove/VB the/DT convergence/NN of/IN the/DT algorithm/NN for/IN tabular/JJ MDP/NN ./.
Then/RB our/PRP$ experiments/NNS on/IN the/DT Taxi/NN domain/NN show/VBP that/IN HQI/NN converges/VBZ faster/JJR than/IN a/DT flat/JJ Q/NN -/HYPH value/NN Iteration/NN and/CC enjoys/VBZ easy/JJ state/NN abstraction/NN ./.
Also/RB ,/, we/PRP demonstrate/VBP that/IN our/PRP$ algorithm/NN is/VBZ able/JJ to/TO learn/VB optimal/JJ policies/NNS for/IN different/JJ hierarchical/JJ structures/NNS from/IN the/DT same/JJ fixed/VBN dataset/NN ,/, which/WDT enables/VBZ model/NN comparison/NN without/IN recollecting/VBG data/NNS ./.
