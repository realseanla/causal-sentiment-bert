Exploration/NN in/IN complex/JJ domains/NNS is/VBZ a/DT key/JJ challenge/NN in/IN reinforcement/NN learning/NN ,/, especially/RB for/IN tasks/NNS with/IN very/RB sparse/JJ rewards/NNS ./.
Recent/JJ successes/NNS in/IN deep/JJ reinforcement/NN learning/NN have/VBP been/VBN achieved/VBN mostly/RB using/VBG simple/JJ heuristic/NN exploration/NN strategies/NNS such/JJ as/IN $/$ \/CD epsilon/CD $/$ -/HYPH greedy/JJ action/NN selection/NN or/CC Gaussian/JJ control/NN noise/NN ,/, but/CC there/EX are/VBP many/JJ tasks/NNS where/WRB these/DT methods/NNS are/VBP insufficient/JJ to/TO make/VB any/DT learning/NN progress/NN ./.
Here/RB ,/, we/PRP consider/VBP more/RBR complex/JJ heuristics/NNS :/: efficient/JJ and/CC scalable/JJ exploration/NN strategies/NNS that/WDT maximize/VBP a/DT notion/NN of/IN an/DT agent/NN 's/POS surprise/NN about/IN its/PRP$ experiences/NNS via/IN intrinsic/JJ motivation/NN ./.
We/PRP propose/VBP to/TO learn/VB a/DT model/NN of/IN the/DT MDP/NNP transition/NN probabilities/NNS concurrently/RB with/IN the/DT policy/NN ,/, and/CC to/TO form/VB intrinsic/JJ rewards/NNS that/WDT approximate/VBP the/DT KL/NN -/HYPH divergence/NN of/IN the/DT true/JJ transition/NN probabilities/NNS from/IN the/DT learned/VBN model/NN ./.
One/CD of/IN our/PRP$ approximations/NNS results/VBZ in/IN using/VBG surprisal/NN as/IN intrinsic/JJ motivation/NN ,/, while/IN the/DT other/JJ gives/VBZ the/DT $/$ k/CD $/$ -/HYPH step/NN learning/NN progress/NN ./.
We/PRP show/VBP that/IN our/PRP$ incentives/NNS enable/VBP agents/NNS to/TO succeed/VB in/IN a/DT wide/JJ range/NN of/IN environments/NNS with/IN high/JJ -/HYPH dimensional/JJ state/NN spaces/NNS and/CC very/RB sparse/JJ rewards/NNS ,/, including/VBG continuous/JJ control/NN tasks/NNS and/CC games/NNS in/IN the/DT Atari/NNP RAM/NNP domain/NN ,/, outperforming/VBG several/JJ other/JJ heuristic/NN exploration/NN techniques/NNS ./.
