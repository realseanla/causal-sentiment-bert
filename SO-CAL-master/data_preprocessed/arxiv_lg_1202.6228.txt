In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT PAC/NN -/HYPH Bayes/NNS bound/VBN for/IN the/DT generalization/NN risk/NN of/IN the/DT Gibbs/NNP classifier/NN in/IN the/DT multi-class/NN classification/NN framework/NN ./.
The/DT novelty/NN of/IN our/PRP$ work/NN is/VBZ the/DT critical/JJ use/NN of/IN the/DT confusion/NN matrix/NN of/IN a/DT classifier/NN as/IN an/DT error/NN measure/NN ;/: this/DT puts/VBZ our/PRP$ contribution/NN in/IN the/DT line/NN of/IN work/NN aiming/VBG at/IN dealing/VBG with/IN performance/NN measure/NN that/WDT are/VBP richer/JJR than/IN mere/JJ scalar/JJ criterion/NN such/JJ as/IN the/DT misclassification/NN rate/NN ./.
Thanks/NN to/IN very/RB recent/JJ and/CC beautiful/JJ results/NNS on/IN matrix/NN concentration/NN inequalities/NNS ,/, we/PRP derive/VBP two/CD bounds/NNS showing/VBG that/IN the/DT true/JJ confusion/NN risk/NN of/IN the/DT Gibbs/NNP classifier/NN is/VBZ upper/JJ -/HYPH bounded/VBN by/IN its/PRP$ empirical/JJ risk/NN plus/CC a/DT term/NN depending/VBG on/IN the/DT number/NN of/IN training/NN examples/NNS in/IN each/DT class/NN ./.
To/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ PAC/NN -/HYPH Bayes/NNS bounds/NNS based/VBN on/IN confusion/NN matrices/NNS ./.
