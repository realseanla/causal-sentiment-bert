Modeling/VBG attention/NN in/IN neural/JJ multi-source/NN sequence/NN -/HYPH to/IN -/HYPH sequence/NN learning/NN remains/VBZ a/DT relatively/RB unexplored/JJ area/NN ,/, despite/IN its/PRP$ usefulness/NN in/IN tasks/NNS that/WDT incorporate/VBP multiple/JJ source/NN languages/NNS or/CC modalities/NNS ./.
We/PRP propose/VBP two/CD novel/JJ approaches/NNS to/TO combine/VB the/DT outputs/NNS of/IN attention/NN mechanisms/NNS over/IN each/DT source/NN sequence/NN ,/, flat/JJ and/CC hierarchical/JJ ./.
We/PRP compare/VBP the/DT proposed/VBN methods/NNS with/IN existing/VBG techniques/NNS and/CC present/JJ results/NNS of/IN systematic/JJ evaluation/NN of/IN those/DT methods/NNS on/IN the/DT WMT16/NN Multimodal/NNP Translation/NNP and/CC Automatic/NNP Post-editing/NN tasks/NNS ./.
We/PRP show/VBP that/IN the/DT proposed/VBN methods/NNS achieve/VBP competitive/JJ results/NNS on/IN both/DT tasks/NNS ./.
