Regularization/NN is/VBZ a/DT well/RB studied/VBN problem/NN in/IN the/DT context/NN of/IN neural/JJ networks/NNS ./.
It/PRP is/VBZ usually/RB used/VBN to/TO improve/VB the/DT generalization/NN performance/NN when/WRB the/DT number/NN of/IN input/NN samples/NNS is/VBZ relatively/RB small/JJ or/CC heavily/RB contaminated/VBN with/IN noise/NN ./.
The/DT regularization/NN of/IN a/DT parametric/JJ model/NN can/MD be/VB achieved/VBN in/IN different/JJ manners/NNS some/DT of/IN which/WDT are/VBP early/JJ stopping/VBG (/-LRB- Morgan/NNP and/CC Bourlard/NNP ,/, 1990/CD )/-RRB- ,/, weight/NN decay/NN ,/, output/NN smoothing/NN that/WDT are/VBP used/VBN to/TO avoid/VB overfitting/VBG during/IN the/DT training/NN of/IN the/DT considered/VBN model/NN ./.
From/IN a/DT Bayesian/JJ point/NN of/IN view/NN ,/, many/JJ regularization/NN techniques/NNS correspond/VBP to/IN imposing/VBG certain/JJ prior/JJ distributions/NNS on/IN model/NN parameters/NNS (/-LRB- Krogh/NNP and/CC Hertz/NNP ,/, 1991/CD )/-RRB- ./.
Using/VBG Bishop/NNP 's/POS approximation/NN (/-LRB- Bishop/NNP ,/, 1995/CD )/-RRB- of/IN the/DT objective/JJ function/NN when/WRB a/DT restricted/JJ type/NN of/IN noise/NN is/VBZ added/VBN to/IN the/DT input/NN of/IN a/DT parametric/JJ function/NN ,/, we/PRP derive/VBP the/DT higher/JJR order/NN terms/NNS of/IN the/DT Taylor/NNP expansion/NN and/CC analyze/VB the/DT coefficients/NNS of/IN the/DT regularization/NN terms/NNS induced/VBN by/IN the/DT noisy/JJ input/NN ./.
In/IN particular/JJ we/PRP study/VBP the/DT effect/NN of/IN penalizing/VBG the/DT Hessian/NNP of/IN the/DT mapping/NN function/NN with/IN respect/NN to/IN the/DT input/NN in/IN terms/NNS of/IN generalization/NN performance/NN ./.
We/PRP also/RB show/VBP how/WRB we/PRP can/MD control/VB independently/RB this/DT coefficient/NN by/IN explicitly/RB penalizing/VBG the/DT Jacobian/NNP of/IN the/DT mapping/NN function/NN on/IN corrupted/VBN inputs/NNS ./.
