For/IN supervised/JJ and/CC unsupervised/JJ learning/NN ,/, positive/JJ definite/JJ kernels/NNS allow/VBP to/TO use/VB large/JJ and/CC potentially/RB infinite/JJ dimensional/JJ feature/NN spaces/NNS with/IN a/DT computational/JJ cost/NN that/WDT only/RB depends/VBZ on/IN the/DT number/NN of/IN observations/NNS ./.
This/DT is/VBZ usually/RB done/VBN through/IN the/DT penalization/NN of/IN predictor/NN functions/NNS by/IN Euclidean/NNP or/CC Hilbertian/NNP norms/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP explore/VBP penalizing/VBG by/IN sparsity/NN -/HYPH inducing/VBG norms/NNS such/JJ as/IN the/DT l1/NN -/HYPH norm/NN or/CC the/DT block/NN l1/NN -/HYPH norm/NN ./.
We/PRP assume/VBP that/IN the/DT kernel/NN decomposes/VBZ into/IN a/DT large/JJ sum/NN of/IN individual/JJ basis/NN kernels/NNS which/WDT can/MD be/VB embedded/VBN in/IN a/DT directed/VBN acyclic/JJ graph/NN ;/: we/PRP show/VBP that/IN it/PRP is/VBZ then/RB possible/JJ to/TO perform/VB kernel/NN selection/NN through/IN a/DT hierarchical/JJ multiple/JJ kernel/NN learning/VBG framework/NN ,/, in/IN polynomial/JJ time/NN in/IN the/DT number/NN of/IN selected/VBN kernels/NNS ./.
This/DT framework/NN is/VBZ naturally/RB applied/VBN to/IN non/AFX linear/JJ variable/JJ selection/NN ;/: our/PRP$ extensive/JJ simulations/NNS on/IN synthetic/JJ datasets/NNS and/CC datasets/NNS from/IN the/DT UCI/NNP repository/NN show/NN that/WDT efficiently/RB exploring/VBG the/DT large/JJ feature/NN space/NN through/IN sparsity/NN -/HYPH inducing/VBG norms/NNS leads/VBZ to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN predictive/JJ performance/NN ./.
