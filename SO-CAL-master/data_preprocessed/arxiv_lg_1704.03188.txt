It/PRP has/VBZ been/VBN believed/VBN that/IN stochastic/JJ feedforward/JJ neural/JJ networks/NNS (/-LRB- SFNNs/NNS )/-RRB- have/VBP several/JJ advantages/NNS beyond/IN deterministic/JJ deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- :/: they/PRP have/VBP more/JJR expressive/JJ power/NN allowing/VBG multi-modal/JJ mappings/NNS and/CC regularize/VB better/JJR due/IN to/IN their/PRP$ stochastic/JJ nature/NN ./.
However/RB ,/, training/VBG large/JJ -/HYPH scale/NN SFNN/NN is/VBZ notoriously/RB harder/RBR ./.
In/IN this/DT paper/NN ,/, we/PRP aim/VBP at/IN developing/VBG efficient/JJ training/NN methods/NNS for/IN SFNN/NNP ,/, in/IN particular/JJ using/VBG known/VBN architectures/NNS and/CC pre-trained/JJ parameters/NNS of/IN DNN/NNP ./.
To/IN this/DT end/NN ,/, we/PRP propose/VBP a/DT new/JJ intermediate/JJ stochastic/JJ model/NN ,/, called/VBN Simplified/VBN -/HYPH SFNN/NN ,/, which/WDT can/MD be/VB built/VBN upon/IN any/DT baseline/NN DNNand/NN approximates/VBZ certain/JJ SFNN/NNP by/IN simplifying/VBG its/PRP$ upper/JJ latent/JJ units/NNS above/IN stochastic/JJ ones/NNS ./.
The/DT main/JJ novelty/NN of/IN our/PRP$ approach/NN is/VBZ in/IN establishing/VBG the/DT connection/NN between/IN three/CD models/NNS ,/, i.e./FW ,/, DNN/NN -/HYPH &gt;/SYM Simplified/JJ -/HYPH SFNN/NN -/HYPH &gt;/SYM SFNN/NNP ,/, which/WDT naturally/RB leads/VBZ to/IN an/DT efficient/JJ training/NN procedure/NN of/IN the/DT stochastic/JJ models/NNS utilizing/VBG pre-trained/JJ parameters/NNS of/IN DNN/NNP ./.
Using/VBG several/JJ popular/JJ DNNs/NNS ,/, we/PRP show/VBP how/WRB they/PRP can/MD be/VB effectively/RB transferred/VBN to/IN the/DT corresponding/VBG stochastic/JJ models/NNS for/IN both/DT multi-modal/JJ and/CC classification/NN tasks/NNS on/IN MNIST/NNP ,/, TFD/NNP ,/, CASIA/NNP ,/, CIFAR/NNP -/HYPH 10/CD ,/, CIFAR/NN -/HYPH 100/CD and/CC SVHN/NN datasets/NNS ./.
In/IN particular/JJ ,/, we/PRP train/VBP a/DT stochastic/JJ model/NN of/IN 28/CD layers/NNS and/CC 36/CD million/CD parameters/NNS ,/, where/WRB training/NN such/PDT a/DT large/JJ -/HYPH scale/NN stochastic/JJ network/NN is/VBZ significantly/RB challenging/JJ without/IN using/VBG Simplified/VBN -/HYPH SFNN/NN
