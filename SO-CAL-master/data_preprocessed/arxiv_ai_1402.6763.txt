We/PRP consider/VBP the/DT problem/NN of/IN controlling/VBG a/DT Markov/NNP decision/NN process/NN (/-LRB- MDP/NN )/-RRB- with/IN a/DT large/JJ state/NN space/NN ,/, so/RB as/IN to/TO minimize/VB average/JJ cost/NN ./.
Since/IN it/PRP is/VBZ intractable/JJ to/TO compete/VB with/IN the/DT optimal/JJ policy/NN for/IN large/JJ scale/NN problems/NNS ,/, we/PRP pursue/VBP the/DT more/RBR modest/JJ goal/NN of/IN competing/VBG with/IN a/DT low/JJ -/HYPH dimensional/JJ family/NN of/IN policies/NNS ./.
We/PRP use/VBP the/DT dual/JJ linear/JJ programming/NN formulation/NN of/IN the/DT MDP/NNP average/JJ cost/NN problem/NN ,/, in/IN which/WDT the/DT variable/NN is/VBZ a/DT stationary/JJ distribution/NN over/IN state/NN -/HYPH action/NN pairs/NNS ,/, and/CC we/PRP consider/VBP a/DT neighborhood/NN of/IN a/DT low/JJ -/HYPH dimensional/JJ subset/NN of/IN the/DT set/NN of/IN stationary/JJ distributions/NNS (/-LRB- defined/VBN in/IN terms/NNS of/IN state/NN -/HYPH action/NN features/NNS )/-RRB- as/IN the/DT comparison/NN class/NN ./.
We/PRP propose/VBP two/CD techniques/NNS ,/, one/CD based/VBN on/IN stochastic/JJ convex/NN optimization/NN ,/, and/CC one/CD based/VBN on/IN constraint/NN sampling/NN ./.
In/IN both/DT cases/NNS ,/, we/PRP give/VBP bounds/NNS that/WDT show/VBP that/IN the/DT performance/NN of/IN our/PRP$ algorithms/NNS approaches/VBZ the/DT best/JJS achievable/JJ by/IN any/DT policy/NN in/IN the/DT comparison/NN class/NN ./.
Most/RBS importantly/RB ,/, these/DT results/NNS depend/VBP on/IN the/DT size/NN of/IN the/DT comparison/NN class/NN ,/, but/CC not/RB on/IN the/DT size/NN of/IN the/DT state/NN space/NN ./.
Preliminary/JJ experiments/NNS show/VBP the/DT effectiveness/NN of/IN the/DT proposed/VBN algorithms/NNS in/IN a/DT queuing/VBG application/NN ./.
