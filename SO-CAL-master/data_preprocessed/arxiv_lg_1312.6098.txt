This/DT paper/NN explores/VBZ the/DT complexity/NN of/IN deep/JJ feed/NN forward/JJ networks/NNS with/IN linear/JJ presynaptic/JJ couplings/NNS and/CC rectified/VBN linear/JJ activations/NNS ./.
This/DT is/VBZ a/DT contribution/NN to/IN the/DT growing/VBG body/NN of/IN work/NN contrasting/VBG the/DT representational/JJ power/NN of/IN deep/JJ and/CC shallow/JJ network/NN architectures/NNS ./.
In/IN particular/JJ ,/, we/PRP offer/VBP a/DT framework/NN for/IN comparing/VBG deep/JJ and/CC shallow/JJ models/NNS that/WDT belong/VBP to/IN the/DT family/NN of/IN piece-wise/JJ linear/JJ functions/NNS based/VBN on/IN computational/JJ geometry/NN ./.
We/PRP look/VBP at/IN a/DT deep/JJ (/-LRB- two/CD hidden/JJ layers/NNS )/-RRB- rectifier/JJR multilayer/JJ perceptron/NN (/-LRB- MLP/NN )/-RRB- with/IN linear/JJ outputs/NNS units/NNS and/CC compare/VB it/PRP with/IN a/DT single/JJ layer/NN version/NN of/IN the/DT model/NN ./.
In/IN the/DT asymptotic/JJ regime/NN as/IN the/DT number/NN of/IN units/NNS goes/VBZ to/IN infinity/NN ,/, if/IN the/DT shallow/JJ model/NN has/VBZ $/$ 2n/CD $/$ hidden/VBN units/NNS and/CC $/$ n_0/CD $/$ inputs/NNS ,/, then/RB the/DT number/NN of/IN linear/JJ regions/NNS is/VBZ $/$ O/UH (/-LRB- n/NN ^/SYM {/-LRB- n_0/NN }/-RRB- )/-RRB- $/$ ./.
A/DT two/CD layer/NN model/NN with/IN $/$ n/NN $/$ number/NN of/IN hidden/VBN units/NNS on/IN each/DT layer/NN has/VBZ $/$ \/SYM Omega/NN (/-LRB- n/NN ^/SYM {/-LRB- n_0/NN }/-RRB- )/-RRB- $/$ ./.
We/PRP consider/VBP this/DT as/IN a/DT first/JJ step/NN towards/IN understanding/VBG the/DT complexity/NN of/IN these/DT models/NNS and/CC argue/VBP that/IN better/JJR constructions/NNS in/IN this/DT framework/NN might/MD provide/VB more/RBR accurate/JJ comparisons/NNS (/-LRB- especially/RB for/IN the/DT interesting/JJ case/NN of/IN when/WRB the/DT number/NN of/IN hidden/JJ layers/NNS goes/VBZ to/IN infinity/NN )/-RRB- ./.
