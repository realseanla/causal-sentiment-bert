The/DT recently/RB proposed/VBN neural/JJ network/NN joint/JJ model/NN (/-LRB- NNJM/NN )/-RRB- (/-LRB- Devlin/NNP et/FW al./FW ,/, 2014/CD )/-RRB- arguments/NNS the/DT n/NN -/HYPH gram/NN target/NN language/NN model/NN with/IN a/DT heuristically/RB chosen/VBN source/NN context/NN window/NN ,/, achieving/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN in/IN SMT/NNP ./.
In/IN this/DT paper/NN ,/, we/PRP give/VBP a/DT more/RBR systematic/JJ treatment/NN by/IN summarizing/VBG the/DT relevant/JJ source/NN information/NN through/IN a/DT convolutional/JJ architecture/NN guided/VBN by/IN the/DT target/NN information/NN ./.
With/IN different/JJ guiding/VBG signals/NNS during/IN decoding/NN ,/, our/PRP$ specifically/RB designed/VBN convolution/NN gating/NN architectures/NNS can/MD pinpoint/VB the/DT parts/NNS of/IN a/DT source/NN sentence/NN that/WDT are/VBP relevant/JJ to/IN predicting/VBG a/DT target/NN word/NN ,/, and/CC fuse/VB them/PRP with/IN the/DT context/NN of/IN entire/JJ source/NN sentence/NN to/TO form/VB a/DT unified/JJ representation/NN ./.
This/DT representation/NN ,/, together/RB with/IN target/NN language/NN words/NNS ,/, are/VBP fed/VBN to/IN a/DT deep/JJ neural/JJ network/NN (/-LRB- DNN/NN )/-RRB- to/TO form/VB a/DT stronger/JJR NNJM/NN ./.
Experiments/NNS on/IN two/CD NIST/NNP Chinese/NNP -/HYPH English/NNP translation/NN tasks/NNS show/VBP that/IN the/DT proposed/VBN model/NN can/MD achieve/VB significant/JJ improvements/NNS over/IN the/DT previous/JJ NNJM/NNP by/IN up/RB to/IN 1.01/CD BLEU/NN points/NNS on/IN average/JJ
