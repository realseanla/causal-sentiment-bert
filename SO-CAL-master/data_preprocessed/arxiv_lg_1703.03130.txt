This/DT paper/NN proposes/VBZ a/DT new/JJ model/NN for/IN extracting/VBG an/DT interpretable/JJ sentence/NN embedding/NN by/IN introducing/VBG self/NN -/HYPH attention/NN ./.
Instead/RB of/IN using/VBG a/DT vector/NN ,/, we/PRP use/VBP a/DT 2/CD -/HYPH D/NN matrix/NN to/TO represent/VB the/DT embedding/NN ,/, with/IN each/DT row/NN of/IN the/DT matrix/NN attending/VBG on/IN a/DT different/JJ part/NN of/IN the/DT sentence/NN ./.
We/PRP also/RB propose/VBP a/DT self/NN -/HYPH attention/NN mechanism/NN and/CC a/DT special/JJ regularization/NN term/NN for/IN the/DT model/NN ./.
As/IN a/DT side/NN effect/NN ,/, the/DT embedding/NN comes/VBZ with/IN an/DT easy/JJ way/NN of/IN visualizing/VBG what/WP specific/JJ parts/NNS of/IN the/DT sentence/NN are/VBP encoded/VBN into/IN the/DT embedding/NN ./.
We/PRP evaluate/VBP our/PRP$ model/NN on/IN 3/CD different/JJ tasks/NNS :/: author/NN profiling/NN ,/, sentiment/NN classification/NN ,/, and/CC textual/JJ entailment/NN ./.
Results/NNS show/VBP that/IN our/PRP$ model/NN yields/VBZ a/DT significant/JJ performance/NN gain/NN compared/VBN to/IN other/JJ sentence/NN embedding/NN methods/NNS in/IN all/DT of/IN the/DT 3/CD tasks/NNS ./.
