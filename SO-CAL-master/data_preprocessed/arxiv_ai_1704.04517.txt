We/PRP introduce/VBP a/DT novel/JJ framework/NN for/IN evaluating/VBG multimodal/JJ deep/JJ learning/NN models/NNS with/IN respect/NN to/IN their/PRP$ language/NN understanding/NN and/CC generalization/NN abilities/NNS ./.
In/IN this/DT approach/NN ,/, artificial/JJ data/NNS is/VBZ automatically/RB generated/VBN according/VBG to/IN the/DT experimenter/NN 's/POS specifications/NNS ./.
The/DT content/NN of/IN the/DT data/NNS ,/, both/CC during/IN training/NN and/CC evaluation/NN ,/, can/MD be/VB controlled/VBN in/IN detail/NN ,/, which/WDT enables/VBZ tasks/NNS to/TO be/VB created/VBN that/WDT require/VBP true/JJ generalization/NN abilities/NNS ,/, in/IN particular/JJ the/DT combination/NN of/IN previously/RB introduced/VBN concepts/NNS in/IN novel/JJ ways/NNS ./.
We/PRP demonstrate/VBP the/DT potential/NN of/IN our/PRP$ methodology/NN by/IN evaluating/VBG various/JJ visual/JJ question/NN answering/VBG models/NNS on/IN four/CD different/JJ tasks/NNS ,/, and/CC show/VB how/WRB our/PRP$ framework/NN gives/VBZ us/PRP detailed/JJ insights/NNS into/IN their/PRP$ capabilities/NNS and/CC limitations/NNS ./.
By/IN open/JJ -/HYPH sourcing/VBG our/PRP$ framework/NN ,/, we/PRP hope/VBP to/TO stimulate/VB progress/NN in/IN the/DT field/NN of/IN multimodal/JJ language/NN understanding/NN ./.
