We/PRP study/VBP the/DT problem/NN of/IN learning/VBG local/JJ metrics/NNS for/IN nearest/JJS neighbor/NN classification/NN ./.
Most/JJS previous/JJ works/NNS on/IN local/JJ metric/JJ learning/NN learn/VB a/DT number/NN of/IN local/JJ unrelated/JJ metrics/NNS ./.
While/IN this/DT "/`` independence/NN "/'' approach/NN delivers/VBZ an/DT increased/VBN flexibility/NN its/PRP$ downside/NN is/VBZ the/DT considerable/JJ risk/NN of/IN overfitting/VBG ./.
We/PRP present/VBP a/DT new/JJ parametric/JJ local/JJ metric/JJ learning/NN method/NN in/IN which/WDT we/PRP learn/VBP a/DT smooth/JJ metric/JJ matrix/NN function/NN over/IN the/DT data/NNS manifold/NN ./.
Using/VBG an/DT approximation/NN error/NN bound/VBN of/IN the/DT metric/JJ matrix/NN function/NN we/PRP learn/VBP local/JJ metrics/NNS as/IN linear/JJ combinations/NNS of/IN basis/NN metrics/NNS defined/VBN on/IN anchor/NN points/NNS over/IN different/JJ regions/NNS of/IN the/DT instance/NN space/NN ./.
We/PRP constrain/VBP the/DT metric/JJ matrix/NN function/NN by/IN imposing/VBG on/IN the/DT linear/JJ combinations/NNS manifold/JJ regularization/NN which/WDT makes/VBZ the/DT learned/VBN metric/JJ matrix/NN function/NN vary/VBP smoothly/RB along/IN the/DT geodesics/NNS of/IN the/DT data/NNS manifold/NN ./.
Our/PRP$ metric/JJ learning/NN method/NN has/VBZ excellent/JJ performance/NN both/CC in/IN terms/NNS of/IN predictive/JJ power/NN and/CC scalability/NN ./.
We/PRP experimented/VBD with/IN several/JJ large/JJ -/HYPH scale/NN classification/NN problems/NNS ,/, tens/NNS of/IN thousands/NNS of/IN instances/NNS ,/, and/CC compared/VBN it/PRP with/IN several/JJ state/NN of/IN the/DT art/NN metric/JJ learning/NN methods/NNS ,/, both/CC global/JJ and/CC local/JJ ,/, as/RB well/RB as/IN to/IN SVM/NNP with/IN automatic/JJ kernel/NN selection/NN ,/, all/DT of/IN which/WDT it/PRP outperforms/VBZ in/IN a/DT significant/JJ manner/NN ./.
