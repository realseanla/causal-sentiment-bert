There/EX are/VBP many/JJ applications/NNS scenarios/NNS for/IN which/WDT the/DT computational/JJ performance/NN and/CC memory/NN footprint/NN of/IN the/DT prediction/NN phase/NN of/IN Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNNs/NNS )/-RRB- needs/VBZ to/TO be/VB optimized/VBN ./.
Binary/JJ Neural/JJ Networks/NNS (/-LRB- BDNNs/NNS )/-RRB- have/VBP been/VBN shown/VBN to/TO be/VB an/DT effective/JJ way/NN of/IN achieving/VBG this/DT objective/NN ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP how/WRB Convolutional/JJ Neural/JJ Networks/NNS (/-LRB- CNNs/NNS )/-RRB- can/MD be/VB implemented/VBN using/VBG binary/JJ representations/NNS ./.
Espresso/NN is/VBZ a/DT compact/JJ ,/, yet/RB powerful/JJ library/NN written/VBN in/IN C/NN //HYPH CUDA/NN that/WDT features/VBZ all/PDT the/DT functionalities/NNS required/VBN for/IN the/DT forward/JJ propagation/NN of/IN CNNs/NNS ,/, in/IN a/DT binary/JJ file/NN less/JJR than/IN 400KB/NN ,/, without/IN any/DT external/JJ dependencies/NNS ./.
Although/IN it/PRP is/VBZ mainly/RB designed/VBN to/TO take/VB advantage/NN of/IN massive/JJ GPU/NNP parallelism/NN ,/, Espresso/NNP also/RB provides/VBZ an/DT equivalent/JJ CPU/NN implementation/NN for/IN CNNs/NNS ./.
Espresso/NN provides/VBZ special/JJ convolutional/JJ and/CC dense/JJ layers/NNS for/IN BCNNs/NNS ,/, leveraging/VBG bit/NN -/HYPH packing/NN and/CC bit-wise/JJ computations/NNS for/IN efficient/JJ execution/NN ./.
These/DT techniques/NNS provide/VBP a/DT speed/NN -/HYPH up/NN of/IN matrix/NN -/HYPH multiplication/NN routines/NNS ,/, and/CC at/IN the/DT same/JJ time/NN ,/, reduce/VB memory/NN usage/NN when/WRB storing/VBG parameters/NNS and/CC activations/NNS ./.
We/PRP experimentally/RB show/VBP that/IN Espresso/NNP is/VBZ significantly/RB faster/JJR than/IN existing/VBG implementations/NNS of/IN optimized/VBN binary/JJ neural/JJ networks/NNS (/-LRB- $/$ \/CD approx/RB $/$ 2/CD orders/NNS of/IN magnitude/NN )/-RRB- ./.
Espresso/NN is/VBZ released/VBN under/IN the/DT Apache/NNP 2.0/CD license/NN and/CC is/VBZ available/JJ at/IN
