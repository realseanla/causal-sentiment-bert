This/DT paper/NN introduces/VBZ an/DT environment/NN for/IN simple/JJ 2D/NN maze/NN games/NNS ,/, designed/VBN as/IN a/DT sandbox/NN for/IN machine/NN learning/NN approaches/VBZ to/IN reasoning/NN and/CC planning/NN ./.
Within/IN it/PRP ,/, we/PRP create/VBP 10/CD simple/JJ games/NNS based/VBN on/IN algorithmic/JJ tasks/NNS (/-LRB- e.g./FW embodying/VBG simple/JJ if/IN -/HYPH then/RB statements/NNS )/-RRB- ./.
We/PRP deploy/VBP a/DT range/NN of/IN neural/JJ models/NNS (/-LRB- fully/RB connected/VBN ,/, convolutional/JJ network/NN ,/, memory/NN network/NN )/-RRB- on/IN these/DT games/NNS ,/, with/IN and/CC without/IN a/DT procedurally/RB generated/VBN curriculum/NN ./.
We/PRP show/VBP that/IN these/DT architectures/NNS can/MD be/VB trained/VBN with/IN reinforcement/NN to/IN respectable/JJ performance/NN on/IN these/DT tasks/NNS ,/, but/CC are/VBP still/RB far/RB from/IN optimal/JJ ,/, despite/IN their/PRP$ simplicity/NN ./.
We/PRP also/RB apply/VBP these/DT models/NNS to/IN games/NNS involving/VBG combat/NN ,/, including/VBG StarCraft/NNP ,/, demonstrating/VBG their/PRP$ ability/NN to/TO learn/VB non-trivial/JJ tactics/NNS which/WDT enable/VBP them/PRP to/TO consistently/RB beat/VB the/DT in/IN -/HYPH game/NN AI/NN ./.
