In/IN this/DT paper/NN ,/, we/PRP prove/VBP a/DT conjecture/NN published/VBN in/IN 1989/CD and/CC also/RB partially/RB address/VB an/DT open/JJ problem/NN announced/VBN at/IN the/DT Conference/NN on/IN Learning/VBG Theory/NN (/-LRB- COLT/NN )/-RRB- 2015/CD ./.
For/IN an/DT expected/JJ loss/NN function/NN of/IN a/DT deep/JJ nonlinear/JJ neural/JJ network/NN ,/, we/PRP prove/VBP the/DT following/JJ statements/NNS under/IN the/DT independence/NN assumption/NN adopted/VBN from/IN recent/JJ work/NN :/: 1/LS )/-RRB- the/DT function/NN is/VBZ non-convex/JJ and/CC non-concave/JJ ,/, 2/CD )/-RRB- every/DT local/JJ minimum/NN is/VBZ a/DT global/JJ minimum/NN ,/, 3/CD )/-RRB- every/DT critical/JJ point/NN that/WDT is/VBZ not/RB a/DT global/JJ minimum/NN is/VBZ a/DT saddle/NN point/NN ,/, and/CC 4/CD )/-RRB- the/DT property/NN of/IN saddle/NN points/NNS differs/VBZ for/IN shallow/JJ networks/NNS (/-LRB- with/IN three/CD layers/NNS )/-RRB- and/CC deeper/JJR networks/NNS (/-LRB- with/IN more/JJR than/IN three/CD layers/NNS )/-RRB- ./.
Moreover/RB ,/, we/PRP prove/VBP that/IN the/DT same/JJ four/CD statements/NNS hold/VBP for/IN deep/JJ linear/JJ neural/JJ networks/NNS with/IN any/DT depth/NN ,/, any/DT widths/NNS and/CC no/DT unrealistic/JJ assumptions/NNS ./.
As/IN a/DT result/NN ,/, we/PRP present/VBP an/DT instance/NN ,/, for/IN which/WDT we/PRP can/MD answer/VB to/IN the/DT following/VBG question/NN :/: how/WRB difficult/JJ to/TO directly/RB train/VB a/DT deep/JJ model/NN in/IN theory/NN ?/.
It/PRP is/VBZ more/RBR difficult/JJ than/IN the/DT classical/JJ machine/NN learning/NN models/NNS (/-LRB- because/IN of/IN the/DT non-convexity/NN )/-RRB- ,/, but/CC not/RB too/RB difficult/JJ (/-LRB- because/IN of/IN the/DT nonexistence/NN of/IN poor/JJ local/JJ minima/NN and/CC the/DT property/NN of/IN the/DT saddle/NN points/NNS )/-RRB- ./.
We/PRP note/VBP that/IN even/RB though/IN we/PRP have/VBP advanced/VBN the/DT theoretical/JJ foundations/NNS of/IN deep/JJ learning/NN ,/, there/EX is/VBZ still/RB a/DT gap/NN between/IN theory/NN and/CC practice/NN ./.
