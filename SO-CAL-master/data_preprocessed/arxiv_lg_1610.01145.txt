We/PRP study/VBP how/WRB approximation/NN errors/NNS of/IN neural/JJ networks/NNS with/IN ReLU/NN activation/NN functions/VBZ depend/VB on/IN the/DT depth/NN of/IN the/DT network/NN ./.
We/PRP establish/VBP rigorous/JJ error/NN bounds/NNS showing/VBG that/IN deep/JJ ReLU/NN networks/NNS are/VBP significantly/RB more/JJR expressive/JJ than/IN shallow/JJ ones/NNS as/RB long/RB as/IN approximations/NNS of/IN smooth/JJ functions/NNS are/VBP concerned/JJ ./.
At/IN the/DT same/JJ time/NN ,/, we/PRP show/VBP that/IN on/IN a/DT set/NN of/IN functions/NNS constrained/VBN only/RB by/IN their/PRP$ degree/NN of/IN smoothness/NN ,/, a/DT ReLU/NN network/NN architecture/NN can/MD not/RB in/IN general/JJ achieve/VB approximation/NN accuracy/NN with/IN better/JJR than/IN a/DT power/NN law/NN dependence/NN on/IN the/DT network/NN size/NN ,/, regardless/RB of/IN its/PRP$ depth/NN ./.
