Dropped/VBD Pronouns/NNPS (/-LRB- DP/NNP )/-RRB- in/IN which/WDT pronouns/NNS are/VBP frequently/RB dropped/VBN in/IN the/DT source/NN language/NN but/CC should/MD be/VB retained/VBN in/IN the/DT target/NN language/NN are/VBP challenge/NN in/IN machine/NN translation/NN ./.
In/IN response/NN to/IN this/DT problem/NN ,/, we/PRP propose/VBP a/DT semi-supervised/JJ approach/NN to/IN recall/NN possibly/RB missing/VBG pronouns/NNS in/IN the/DT translation/NN ./.
Firstly/RB ,/, we/PRP build/VBP training/NN data/NNS for/IN DP/NNP generation/NN in/IN which/WDT the/DT DPs/NNS are/VBP automatically/RB labelled/VBN according/VBG to/IN the/DT alignment/NN information/NN from/IN a/DT parallel/JJ corpus/NN ./.
Secondly/RB ,/, we/PRP build/VBP a/DT deep/JJ learning/NN -/HYPH based/VBN DP/NNP generator/NN for/IN input/NN sentences/NNS in/IN decoding/NN when/WRB no/DT corresponding/VBG references/NNS exist/VBP ./.
More/RBR specifically/RB ,/, the/DT generation/NN is/VBZ two/CD -/HYPH phase/NN :/: (/-LRB- 1/LS )/-RRB- DP/NN position/NN detection/NN ,/, which/WDT is/VBZ modeled/VBN as/IN a/DT sequential/JJ labelling/NN task/NN with/IN recurrent/JJ neural/JJ networks/NNS ;/: and/CC (/-LRB- 2/LS )/-RRB- DP/NN prediction/NN ,/, which/WDT employs/VBZ a/DT multilayer/JJ perceptron/NN with/IN rich/JJ features/NNS ./.
Finally/RB ,/, we/PRP integrate/VBP the/DT above/JJ outputs/NNS into/IN our/PRP$ translation/NN system/NN to/IN recall/NN missing/VBG pronouns/NNS by/IN both/DT extracting/VBG rules/NNS from/IN the/DT DP/NNP -/HYPH labelled/VBN training/NN data/NNS and/CC translating/VBG the/DT DP/NNP -/HYPH generated/VBN input/NN sentences/NNS ./.
Experimental/JJ results/NNS show/VBP that/IN our/PRP$ approach/NN achieves/VBZ a/DT significant/JJ improvement/NN of/IN 1.58/CD BLEU/NN points/NNS in/IN translation/NN performance/NN with/IN 66/CD percent/NN F/NN -/HYPH score/NN for/IN DP/NNP generation/NN accuracy/NN ./.
