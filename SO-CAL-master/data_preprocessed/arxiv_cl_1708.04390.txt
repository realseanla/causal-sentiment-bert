Image/NN captioning/NN has/VBZ so/RB far/RB been/VBN explored/VBN mostly/RB in/IN English/NNP ,/, as/IN most/JJS available/JJ datasets/NNS are/VBP in/IN this/DT language/NN ./.
However/RB ,/, the/DT application/NN of/IN image/NN captioning/NN should/MD not/RB be/VB restricted/VBN by/IN language/NN ./.
Only/RB few/JJ studies/NNS have/VBP been/VBN conducted/VBN for/IN image/NN captioning/NN in/IN a/DT cross-lingual/JJ setting/NN ./.
Different/JJ from/IN these/DT works/NNS that/WDT manually/RB build/VBP a/DT dataset/NN for/IN a/DT target/NN language/NN ,/, we/PRP aim/VBP to/TO learn/VB a/DT cross-lingual/JJ captioning/NN model/NN fully/RB from/IN machine/NN -/HYPH translated/VBN sentences/NNS ./.
To/TO conquer/VB the/DT lack/NN of/IN fluency/NN in/IN the/DT translated/VBN sentences/NNS ,/, we/PRP propose/VBP in/IN this/DT paper/NN a/DT fluency/NN -/HYPH guided/VBN learning/NN framework/NN ./.
The/DT framework/NN comprises/VBZ a/DT module/NN to/TO automatically/RB estimate/VB the/DT fluency/NN of/IN the/DT sentences/NNS and/CC another/DT module/NN to/TO utilize/VB the/DT estimated/VBN fluency/NN scores/NNS to/TO effectively/RB train/VB an/DT image/NN captioning/NN model/NN for/IN the/DT target/NN language/NN ./.
As/IN experiments/NNS on/IN two/CD bilingual/JJ (/-LRB- English/JJ -/HYPH Chinese/JJ )/-RRB- datasets/NNS show/VBP ,/, our/PRP$ approach/NN improves/VBZ both/CC fluency/NN and/CC relevance/NN of/IN the/DT generated/VBN captions/NNS in/IN Chinese/JJ ,/, but/CC without/IN using/VBG any/DT manually/RB written/VBN sentences/NNS from/IN the/DT target/NN language/NN ./.
