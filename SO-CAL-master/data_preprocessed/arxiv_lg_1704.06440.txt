Two/CD of/IN the/DT leading/VBG approaches/NNS for/IN model/NN -/HYPH free/JJ reinforcement/NN learning/NN are/VBP policy/NN gradient/NN methods/NNS and/CC $/$ Q$/CD -/HYPH learning/NN methods/NNS ./.
$/$ Q$/CD -/HYPH learning/NN methods/NNS can/MD be/VB effective/JJ and/CC sample/NN -/HYPH efficient/JJ when/WRB they/PRP work/VBP ,/, however/RB ,/, it/PRP is/VBZ not/RB well/RB -/HYPH understood/VBN why/WRB they/PRP work/VBP ,/, since/IN empirically/RB ,/, the/DT $/$ Q$/CD -/HYPH values/NNS they/PRP estimate/VBP are/VBP very/RB inaccurate/JJ ./.
A/DT partial/JJ explanation/NN may/MD be/VB that/IN $/$ Q$/CD -/HYPH learning/NN methods/NNS are/VBP secretly/RB implementing/VBG policy/NN gradient/NN updates/NNS :/: we/PRP show/VBP that/IN there/EX is/VBZ a/DT precise/JJ equivalence/NN between/IN $/$ Q$/CD -/HYPH learning/NN and/CC policy/NN gradient/NN methods/NNS in/IN the/DT setting/NN of/IN entropy/NN -/HYPH regularized/VBN reinforcement/NN learning/NN ,/, that/IN "/`` soft/JJ "/'' (/-LRB- entropy/NN -/HYPH regularized/VBN )/-RRB- $/$ Q$/CD -/HYPH learning/NN is/VBZ exactly/RB equivalent/JJ to/IN a/DT policy/NN gradient/NN method/NN ./.
We/PRP also/RB point/VBP out/RP a/DT connection/NN between/IN $/$ Q$/CD -/HYPH learning/NN methods/NNS and/CC natural/JJ policy/NN gradient/NN methods/NNS ./.
Experimentally/RB ,/, we/PRP explore/VBP the/DT entropy/NN -/HYPH regularized/VBN versions/NNS of/IN $/$ Q$/CD -/HYPH learning/NN and/CC policy/NN gradients/NNS ,/, and/CC we/PRP find/VBP them/PRP to/TO perform/VB as/RB well/RB as/IN (/-LRB- or/CC slightly/RB better/JJR than/IN )/-RRB- the/DT standard/JJ variants/NNS on/IN the/DT Atari/NNP benchmark/NN ./.
We/PRP also/RB show/VBP that/IN the/DT equivalence/NN holds/VBZ in/IN practical/JJ settings/NNS by/IN constructing/VBG a/DT $/$ Q$/CD -/HYPH learning/NN method/NN that/WDT closely/RB matches/VBZ the/DT learning/NN dynamics/NNS of/IN A3C/NN without/IN using/VBG a/DT target/NN network/NN or/CC $/$ \/CD epsilon/CD $/$ -/HYPH greedy/JJ exploration/NN schedule/NN ./.
