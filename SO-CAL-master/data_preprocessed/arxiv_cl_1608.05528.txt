Recent/JJ work/NN has/VBZ demonstrated/VBN that/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN word/NN embedding/NN models/NNS require/VBP different/JJ context/NN types/NNS to/TO produce/VB high/JJ -/HYPH quality/NN representations/NNS for/IN different/JJ word/NN classes/NNS such/JJ as/IN adjectives/NNS (/-LRB- A/NN )/-RRB- ,/, verbs/NNS (/-LRB- V/NN )/-RRB- ,/, and/CC nouns/NNS (/-LRB- N/NN )/-RRB- ./.
This/DT paper/NN is/VBZ concerned/VBN with/IN identifying/VBG contexts/NNS useful/JJ for/IN learning/VBG A/NN //HYPH V/NN //HYPH N/NN -/HYPH specific/JJ representations/NNS ./.
We/PRP introduce/VBP a/DT simple/JJ yet/CC effective/JJ framework/NN for/IN selecting/VBG class/NN -/HYPH specific/JJ context/NN configurations/NNS that/WDT yield/VBP improved/VBN representations/NNS for/IN each/DT class/NN ./.
We/PRP propose/VBP an/DT automatic/JJ A/NN */NFP style/NN selection/NN algorithm/NN that/WDT effectively/RB searches/VBZ only/RB a/DT fraction/NN of/IN the/DT large/JJ configuration/NN space/NN ./.
The/DT results/NNS on/IN predicting/VBG similarity/NN scores/NNS for/IN the/DT A/NN ,/, V/NN ,/, and/CC N/NN subsets/NNS of/IN the/DT benchmarking/NN SimLex/NN -/HYPH 999/CD evaluation/NN set/VBN indicate/VBP that/IN our/PRP$ method/NN is/VBZ useful/JJ for/IN each/DT class/NN :/: the/DT improvements/NNS are/VBP 6/CD percent/NN (/-LRB- A/NN )/-RRB- ,/, 6/CD percent/NN (/-LRB- V/NN )/-RRB- ,/, and/CC 5/CD percent/NN (/-LRB- N/NN )/-RRB- over/IN the/DT best/JJS previously/RB proposed/VBN context/NN type/NN for/IN each/DT class/NN ./.
At/IN the/DT same/JJ time/NN ,/, the/DT model/NN trains/NNS on/IN only/RB 14/CD percent/NN (/-LRB- A/NN )/-RRB- ,/, 26.2/CD percent/NN (/-LRB- V/NN )/-RRB- ,/, and/CC 33.6/CD percent/NN (/-LRB- N/NN )/-RRB- of/IN all/DT dependency/NN -/HYPH based/VBN contexts/NNS ,/, resulting/VBG in/IN much/JJ shorter/JJR training/NN time/NN ./.
