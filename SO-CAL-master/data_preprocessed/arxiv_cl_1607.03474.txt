Many/JJ sequential/JJ processing/NN tasks/NNS require/VBP complex/JJ nonlinear/JJ transition/NN functions/NNS from/IN one/CD step/NN to/IN the/DT next/JJ ./.
However/RB ,/, recurrent/JJ neural/JJ networks/NNS with/IN such/JJ '/`` deep/JJ '/'' transition/NN functions/VBZ remain/VBP difficult/JJ to/TO train/VB ,/, even/RB when/WRB using/VBG Long/JJ Short/JJ -/HYPH Term/NN Memory/NN networks/NNS ./.
We/PRP introduce/VBP a/DT novel/JJ theoretical/JJ analysis/NN of/IN recurrent/JJ networks/NNS based/VBN on/IN Ger/NN \/SYM v/NN {/-LRB- s/POS }/-RRB- gorin/NN 's/POS circle/NN theorem/NN that/WDT illuminates/VBZ several/JJ modeling/NN and/CC optimization/NN issues/NNS and/CC improves/VBZ our/PRP$ understanding/NN of/IN the/DT LSTM/NNP cell/NN ./.
Based/VBN on/IN this/DT analysis/NN we/PRP propose/VBP Recurrent/JJ Highway/NN Networks/NNS (/-LRB- RHN/NN )/-RRB- ,/, which/WDT are/VBP long/RB not/RB only/RB in/IN time/NN but/CC also/RB in/IN space/NN ,/, generalizing/VBG LSTMs/NNS to/IN larger/JJR step/NN -/HYPH to/IN -/HYPH step/NN depths/NNS ./.
Experiments/NNS indicate/VBP that/IN the/DT proposed/VBN architecture/NN results/NNS in/IN complex/JJ but/CC efficient/JJ models/NNS ,/, beating/VBG previous/JJ models/NNS for/IN character/NN prediction/NN on/IN the/DT Hutter/NNP Prize/NNP dataset/NN with/IN less/JJR than/IN half/NN of/IN the/DT parameters/NNS ./.
