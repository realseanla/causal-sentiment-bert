We/PRP analyze/VBP online/JJ and/CC mini-batch/JJ k/NN -/HYPH means/NN variants/NNS ./.
Both/DT scale/NN up/RP the/DT widely/RB used/VBN Lloyd/NNP 's/POS algorithm/NN via/IN stochastic/JJ approximation/NN ,/, and/CC have/VBP become/VBN popular/JJ for/IN large/JJ -/HYPH scale/NN clustering/NN and/CC unsupervised/JJ feature/NN learning/NN ./.
We/PRP show/VBP ,/, for/IN the/DT first/JJ time/NN ,/, that/IN they/PRP have/VBP global/JJ convergence/NN towards/IN local/JJ optima/NN at/IN $/$ O/UH (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- t/NN }/-RRB- )/-RRB- $/$ rate/NN under/IN general/JJ conditions/NNS ./.
In/IN addition/NN ,/, we/PRP show/VBP if/IN the/DT dataset/NN is/VBZ clusterable/JJ ,/, with/IN suitable/JJ initialization/NN ,/, mini-batch/NN k/CD -/HYPH means/NN converges/VBZ to/IN an/DT optimal/JJ k/NN -/HYPH means/NN solution/NN with/IN $/$ O/UH (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- t/NN }/-RRB- )/-RRB- $/$ convergence/NN rate/NN with/IN high/JJ probability/NN ./.
The/DT k/NN -/HYPH means/NN objective/NN is/VBZ non-convex/JJ and/CC non-differentiable/JJ :/: we/PRP exploit/VBP ideas/NNS from/IN non-convex/JJ gradient/NN -/HYPH based/VBN optimization/NN by/IN providing/VBG a/DT novel/JJ characterization/NN of/IN the/DT trajectory/NN of/IN k/CD -/HYPH means/NN algorithm/NN on/IN its/PRP$ solution/NN space/NN ,/, and/CC circumvent/VB its/PRP$ non-differentiability/NN via/IN geometric/JJ insights/NNS about/IN k/CD -/HYPH means/NN update/NN ./.
