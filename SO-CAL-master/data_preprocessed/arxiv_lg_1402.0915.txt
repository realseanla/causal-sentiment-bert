In/IN this/DT paper/NN ,/, we/PRP study/VBP ordered/VBN representations/NNS of/IN data/NNS in/IN which/WDT different/JJ dimensions/NNS have/VBP different/JJ degrees/NNS of/IN importance/NN ./.
To/TO learn/VB these/DT representations/NNS we/PRP introduce/VBP nested/VBN dropout/NN ,/, a/DT procedure/NN for/IN stochastically/RB removing/VBG coherent/JJ nested/VBN sets/NNS of/IN hidden/VBN units/NNS in/IN a/DT neural/JJ network/NN ./.
We/PRP first/RB present/VBP a/DT sequence/NN of/IN theoretical/JJ results/NNS in/IN the/DT simple/JJ case/NN of/IN a/DT semi-linear/JJ autoencoder/NN ./.
We/PRP rigorously/RB show/VBP that/IN the/DT application/NN of/IN nested/VBN dropout/NN enforces/VBZ identifiability/NN of/IN the/DT units/NNS ,/, which/WDT leads/VBZ to/IN an/DT exact/JJ equivalence/NN with/IN PCA/NN ./.
We/PRP then/RB extend/VBP the/DT algorithm/NN to/IN deep/JJ models/NNS and/CC demonstrate/VBP the/DT relevance/NN of/IN ordered/VBN representations/NNS to/IN a/DT number/NN of/IN applications/NNS ./.
Specifically/RB ,/, we/PRP use/VBP the/DT ordered/VBN property/NN of/IN the/DT learned/VBN codes/NNS to/TO construct/VB hash/NN -/HYPH based/VBN data/NNS structures/NNS that/WDT permit/VBP very/RB fast/JJ retrieval/NN ,/, achieving/VBG retrieval/NN in/IN time/NN logarithmic/JJ in/IN the/DT database/NN size/NN and/CC independent/JJ of/IN the/DT dimensionality/NN of/IN the/DT representation/NN ./.
This/DT allows/VBZ codes/NNS that/WDT are/VBP hundreds/NNS of/IN times/NNS longer/JJR than/IN currently/RB feasible/JJ for/IN retrieval/NN ./.
We/PRP therefore/RB avoid/VB the/DT diminished/JJ quality/NN associated/VBN with/IN short/JJ codes/NNS ,/, while/IN still/RB performing/VBG retrieval/NN that/WDT is/VBZ competitive/JJ in/IN speed/NN with/IN existing/VBG methods/NNS ./.
We/PRP also/RB show/VBP that/IN ordered/VBN representations/NNS are/VBP a/DT promising/JJ way/NN to/TO learn/VB adaptive/JJ compression/NN for/IN efficient/JJ online/JJ data/NNS reconstruction/NN ./.
