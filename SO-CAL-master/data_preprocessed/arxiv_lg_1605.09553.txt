Attention/NN mechanisms/NNS have/VBP recently/RB been/VBN introduced/VBN in/IN deep/JJ learning/NN for/IN various/JJ tasks/NNS in/IN natural/JJ language/NN processing/NN and/CC computer/NN vision/NN ./.
But/CC despite/IN their/PRP$ popularity/NN ,/, the/DT "/`` correctness/NN "/'' of/IN the/DT implicitly/RB -/HYPH learned/VBN attention/NN maps/NNS has/VBZ only/RB been/VBN assessed/VBN qualitatively/RB by/IN visualization/NN of/IN several/JJ examples/NNS ./.
In/IN this/DT paper/NN we/PRP focus/VBP on/IN evaluating/VBG and/CC improving/VBG the/DT correctness/NN of/IN attention/NN in/IN neural/JJ image/NN captioning/NN models/NNS ./.
Specifically/RB ,/, we/PRP propose/VBP a/DT quantitative/JJ evaluation/NN metric/JJ for/IN how/WRB well/RB the/DT attention/NN maps/VBZ align/VB with/IN human/JJ judgment/NN ,/, using/VBG recently/RB released/VBN datasets/NNS with/IN alignment/NN between/IN regions/NNS in/IN images/NNS and/CC entities/NNS in/IN captions/NNS ./.
We/PRP then/RB propose/VBP novel/JJ models/NNS with/IN different/JJ levels/NNS of/IN explicit/JJ supervision/NN for/IN learning/VBG attention/NN maps/NNS during/IN training/NN ./.
The/DT supervision/NN can/MD be/VB strong/JJ when/WRB alignment/NN between/IN regions/NNS and/CC caption/NN entities/NNS are/VBP available/JJ ,/, or/CC weak/JJ when/WRB only/JJ object/NN segments/NNS and/CC categories/NNS are/VBP provided/VBN ./.
We/PRP show/VBP on/IN the/DT popular/JJ Flickr30k/NN and/CC COCO/NN datasets/NNS that/WDT introducing/VBG supervision/NN of/IN attention/NN maps/NNS during/IN training/NN solidly/RB improves/VBZ both/DT attention/NN correctness/NN and/CC caption/NN quality/NN ./.
