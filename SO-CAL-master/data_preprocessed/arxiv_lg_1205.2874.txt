We/PRP consider/VBP a/DT multi-armed/JJ bandit/NN problem/NN where/WRB the/DT decision/NN maker/NN can/MD explore/VB and/CC exploit/VB different/JJ arms/NNS at/IN every/DT round/NN ./.
The/DT exploited/VBN arm/NN adds/VBZ to/IN the/DT decision/NN maker/NN 's/POS cumulative/JJ reward/NN (/-LRB- without/IN necessarily/RB observing/VBG the/DT reward/NN )/-RRB- while/IN the/DT explored/VBN arm/NN reveals/VBZ its/PRP$ value/NN ./.
We/PRP devise/VBP algorithms/NNS for/IN this/DT setup/NN and/CC show/VBP that/IN the/DT dependence/NN on/IN the/DT number/NN of/IN arms/NNS ,/, k/CD ,/, can/MD be/VB much/RB better/JJR than/IN the/DT standard/JJ square/JJ root/NN of/IN k/CD dependence/NN ,/, depending/VBG on/IN the/DT behavior/NN of/IN the/DT arms/NNS '/POS reward/NN sequences/NNS ./.
For/IN the/DT important/JJ case/NN of/IN piecewise/NN stationary/JJ stochastic/JJ bandits/NNS ,/, we/PRP show/VBP a/DT significant/JJ improvement/NN over/IN existing/VBG algorithms/NNS ./.
Our/PRP$ algorithms/NNS are/VBP based/VBN on/IN a/DT non-uniform/JJ sampling/NN policy/NN ,/, which/WDT we/PRP show/VBP is/VBZ essential/JJ to/IN the/DT success/NN of/IN any/DT algorithm/NN in/IN the/DT adversarial/JJ setup/NN ./.
Finally/RB ,/, we/PRP show/VBP some/DT simulation/NN results/VBZ on/IN an/DT ultra-wide/JJ band/NN channel/NN selection/NN inspired/VBD setting/VBG indicating/VBG the/DT applicability/NN of/IN our/PRP$ algorithms/NNS ./.
