Sequence/NN -/HYPH to/IN -/HYPH Sequence/NN (/-LRB- seq2seq/NN )/-RRB- modeling/NN has/VBZ rapidly/RB become/VBN an/DT important/JJ general/JJ -/HYPH purpose/NN NLP/NNP tool/NN that/WDT has/VBZ proven/VBN effective/JJ for/IN many/JJ text/NN -/HYPH generation/NN and/CC sequence/NN -/HYPH labeling/NN tasks/NNS ./.
Seq2seq/NN builds/VBZ on/IN deep/JJ neural/JJ language/NN modeling/NN and/CC inherits/VBZ its/PRP$ remarkable/JJ accuracy/NN in/IN estimating/VBG local/JJ ,/, next/JJ -/HYPH word/NN distributions/NNS ./.
In/IN this/DT work/NN ,/, we/PRP introduce/VBP a/DT model/NN and/CC beam/NN -/HYPH search/NN training/NN scheme/NN ,/, based/VBN on/IN the/DT work/NN of/IN Daume/NNP III/NNP and/CC Marcu/NNP (/-LRB- 2005/CD )/-RRB- ,/, that/WDT extends/VBZ seq2seq/NN to/TO learn/VB global/JJ sequence/NN scores/NNS ./.
This/DT structured/JJ approach/NN avoids/VBZ classical/JJ biases/NNS associated/VBN with/IN local/JJ training/NN and/CC unifies/VBZ the/DT training/NN loss/NN with/IN the/DT test/NN -/HYPH time/NN usage/NN ,/, while/IN preserving/VBG the/DT proven/VBN model/NN architecture/NN of/IN seq2seq/NN and/CC its/PRP$ efficient/JJ training/NN approach/NN ./.
We/PRP show/VBP that/IN our/PRP$ system/NN outperforms/VBZ a/DT highly/RB -/HYPH optimized/VBN attention/NN -/HYPH based/VBN seq2seq/NN system/NN and/CC other/JJ baselines/NNS on/IN three/CD different/JJ sequence/NN to/IN sequence/NN tasks/NNS :/: word/NN ordering/NN ,/, parsing/VBG ,/, and/CC machine/NN translation/NN ./.
