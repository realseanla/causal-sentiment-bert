In/IN recent/JJ year/NN ,/, parallel/JJ implementations/NNS have/VBP been/VBN used/VBN to/TO speed/VB up/RP the/DT training/NN of/IN deep/JJ neural/JJ networks/NNS (/-LRB- DNN/NN )/-RRB- ./.
Typically/RB ,/, the/DT parameters/NNS of/IN the/DT local/JJ models/NNS are/VBP periodically/RB communicated/VBN and/CC averaged/VBN to/TO get/VB a/DT global/JJ model/NN until/IN the/DT training/NN curve/NN converges/VBZ (/-LRB- denoted/VBN as/IN MA/NN -/HYPH DNN/NN )/-RRB- ./.
However/RB ,/, since/IN DNN/NNP is/VBZ a/DT highly/RB non-convex/JJ model/NN ,/, the/DT global/JJ model/NN obtained/VBN by/IN averaging/VBG parameters/NNS does/VBZ not/RB have/VB guarantee/NN on/IN its/PRP$ performance/NN improvement/NN over/IN the/DT local/JJ models/NNS and/CC might/MD even/RB be/VB worse/JJR than/IN the/DT average/JJ performance/NN of/IN the/DT local/JJ models/NNS ,/, which/WDT leads/VBZ to/IN the/DT slow/JJ -/HYPH down/NN of/IN convergence/NN and/CC the/DT decrease/NN of/IN the/DT final/JJ performance/NN ./.
To/TO tackle/VB this/DT problem/NN ,/, we/PRP propose/VBP a/DT new/JJ parallel/JJ training/NN method/NN called/VBN \/SYM emph/FW {/-LRB- Ensemble/NNP -/HYPH Compression/NNP }/-RRB- (/-LRB- denoted/VBN as/IN EC/NNP -/HYPH DNN/NNP )/-RRB- ./.
Specifically/RB ,/, we/PRP propose/VBP to/IN aggregate/NN the/DT local/JJ models/NNS by/IN ensemble/NN ,/, i.e./FW ,/, the/DT outputs/NNS of/IN the/DT local/JJ models/NNS are/VBP averaged/VBN instead/RB of/IN the/DT parameters/NNS ./.
Considering/VBG that/IN the/DT widely/RB used/JJ loss/NN functions/NNS are/VBP convex/NN to/IN the/DT output/NN of/IN the/DT model/NN ,/, the/DT performance/NN of/IN the/DT global/JJ model/NN obtained/VBN in/IN this/DT way/NN is/VBZ guaranteed/VBN to/TO be/VB at/IN least/RBS as/RB good/JJ as/IN the/DT average/JJ performance/NN of/IN local/JJ models/NNS ./.
However/RB ,/, the/DT size/NN of/IN the/DT global/JJ model/NN will/MD increase/VB after/IN each/DT ensemble/NN and/CC may/MD explode/VB after/IN multiple/JJ rounds/NNS of/IN ensembles/NNS ./.
Thus/RB ,/, we/PRP conduct/VBP model/NN compression/NN after/IN each/DT ensemble/NN ,/, to/TO ensure/VB the/DT size/NN of/IN the/DT global/JJ model/NN to/TO be/VB the/DT same/JJ as/IN the/DT local/JJ models/NNS ./.
We/PRP conducted/VBD experiments/NNS on/IN a/DT benchmark/NN dataset/NN ./.
The/DT experimental/JJ results/NNS demonstrate/VBP that/IN our/PRP$ proposed/VBN EC/NNP -/HYPH DNN/NNP can/MD stably/RB achieve/VB better/JJR performance/NN than/IN MA/NN -/HYPH DNN/NN ./.
