In/IN this/DT paper/NN ,/, we/PRP address/VBP the/DT problem/NN of/IN learning/VBG the/DT structure/NN of/IN a/DT pairwise/JJ graphical/JJ model/NN from/IN samples/NNS in/IN a/DT high/JJ -/HYPH dimensional/JJ setting/NN ./.
Our/PRP$ first/JJ main/JJ result/NN studies/NNS the/DT sparsistency/NN ,/, or/CC consistency/NN in/IN sparsity/NN pattern/NN recovery/NN ,/, properties/NNS of/IN a/DT forward/RB -/HYPH backward/JJ greedy/JJ algorithm/NN as/IN applied/VBN to/IN general/JJ statistical/JJ models/NNS ./.
As/IN a/DT special/JJ case/NN ,/, we/PRP then/RB apply/VB this/DT algorithm/NN to/TO learn/VB the/DT structure/NN of/IN a/DT discrete/JJ graphical/JJ model/NN via/IN neighborhood/NN estimation/NN ./.
As/IN a/DT corollary/NN of/IN our/PRP$ general/JJ result/NN ,/, we/PRP derive/VBP sufficient/JJ conditions/NNS on/IN the/DT number/NN of/IN samples/NNS n/NN ,/, the/DT maximum/JJ node/NN -/HYPH degree/NN d/NN and/CC the/DT problem/NN size/NN p/NN ,/, as/RB well/RB as/IN other/JJ conditions/NNS on/IN the/DT model/NN parameters/NNS ,/, so/IN that/IN the/DT algorithm/NN recovers/VBZ all/PDT the/DT edges/NNS with/IN high/JJ probability/NN ./.
Our/PRP$ result/NN guarantees/VBZ graph/NN selection/NN for/IN samples/NNS scaling/VBG as/IN n/NN =/SYM Omega/NN (/-LRB- d/NN ^/SYM 2/CD log/NN (/-LRB- p/NN )/-RRB- )/-RRB- ,/, in/IN contrast/NN to/IN existing/VBG convex/NN -/HYPH optimization/NN based/VBN algorithms/NNS that/WDT require/VBP a/DT sample/NN complexity/NN of/IN \/SYM Omega/NN (/-LRB- d/NN ^/SYM 3/CD log/NN (/-LRB- p/NN )/-RRB- )/-RRB- ./.
Further/RB ,/, the/DT greedy/JJ algorithm/NN only/RB requires/VBZ a/DT restricted/VBN strong/JJ convexity/NN condition/NN which/WDT is/VBZ typically/RB milder/JJR than/IN irrepresentability/NN assumptions/NNS ./.
We/PRP corroborate/VBP these/DT results/NNS using/VBG numerical/JJ simulations/NNS at/IN the/DT end/NN ./.
