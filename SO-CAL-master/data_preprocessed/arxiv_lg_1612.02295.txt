Cross-entropy/JJ loss/NN together/RB with/IN softmax/NN is/VBZ arguably/RB one/CD of/IN the/DT most/RBS common/JJ used/VBN supervision/NN components/NNS in/IN convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- ./.
Despite/IN its/PRP$ simplicity/NN ,/, popularity/NN and/CC excellent/JJ performance/NN ,/, the/DT component/NN does/VBZ not/RB explicitly/RB encourage/VB discriminative/JJ learning/NN of/IN features/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT generalized/VBN large/JJ -/HYPH margin/NN softmax/NN (/-LRB- L/NN -/HYPH Softmax/NN )/-RRB- loss/NN which/WDT explicitly/RB encourages/VBZ intra-class/JJ compactness/NN and/CC inter-class/NN separability/NN between/IN learned/VBN features/NNS ./.
Moreover/RB ,/, L/NN -/HYPH Softmax/NN not/RB only/RB can/MD adjust/VB the/DT desired/VBN margin/NN but/CC also/RB can/MD avoid/VB overfitting/NN ./.
We/PRP also/RB show/VBP that/IN the/DT L/NN -/HYPH Softmax/NN loss/NN can/MD be/VB optimized/VBN by/IN typical/JJ stochastic/JJ gradient/NN descent/NN ./.
Extensive/JJ experiments/NNS on/IN four/CD benchmark/NN datasets/NNS demonstrate/VBP that/IN the/DT deeply/RB -/HYPH learned/VBN features/NNS with/IN L/NN -/HYPH softmax/NN loss/NN become/VBN more/RBR discriminative/JJ ,/, hence/RB significantly/RB boosting/VBG the/DT performance/NN on/IN a/DT variety/NN of/IN visual/JJ classification/NN and/CC verification/NN tasks/NNS ./.
