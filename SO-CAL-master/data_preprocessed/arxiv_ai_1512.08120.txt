Multi-relational/JJ learning/NN has/VBZ received/VBN lots/NNS of/IN attention/NN from/IN researchers/NNS in/IN various/JJ research/NN communities/NNS ./.
Most/JJS existing/VBG methods/NNS either/CC suffer/VB from/IN superlinear/NN per/IN -/HYPH iteration/NN cost/NN ,/, or/CC are/VBP sensitive/JJ to/IN the/DT given/VBN ranks/NNS ./.
To/TO address/VB both/DT issues/NNS ,/, we/PRP propose/VBP a/DT scalable/JJ core/NN tensor/NN trace/NN norm/NN Regularized/VBN Orthogonal/NNP Iteration/NNP Decomposition/NNP (/-LRB- ROID/NNP )/-RRB- method/NN for/IN full/JJ or/CC incomplete/JJ tensor/NN analytics/NNS ,/, which/WDT can/MD be/VB generalized/VBN as/IN a/DT graph/NN Laplacian/NNP regularized/VBD version/NN by/IN using/VBG auxiliary/JJ information/NN or/CC a/DT sparse/JJ higher/JJR -/HYPH order/NN orthogonal/JJ iteration/NN (/-LRB- SHOOI/NN )/-RRB- version/NN ./.
We/PRP first/RB induce/VB the/DT equivalence/NN relation/NN of/IN the/DT Schatten/NNP p/NN -/HYPH norm/NN (/-LRB- 0/CD &lt;/SYM p/NN &lt;/SYM \/SYM infty/SYM )/-RRB- of/IN a/DT low/JJ multi-linear/JJ rank/NN tensor/NN and/CC its/PRP$ core/NN tensor/NN ./.
Then/RB we/PRP achieve/VBP a/DT much/RB smaller/JJR matrix/NN trace/NN norm/NN minimization/NN problem/NN ./.
Finally/RB ,/, we/PRP develop/VBP two/CD efficient/JJ augmented/VBN Lagrange/NNP multiplier/JJR algorithms/NNS to/TO solve/VB our/PRP$ problems/NNS with/IN convergence/NN guarantees/NNS ./.
Extensive/JJ experiments/NNS using/VBG both/CC real/JJ and/CC synthetic/JJ datasets/NNS ,/, even/RB though/IN with/IN only/RB a/DT few/JJ observations/NNS ,/, verified/VBD both/CC the/DT efficiency/NN and/CC effectiveness/NN of/IN our/PRP$ methods/NNS ./.
