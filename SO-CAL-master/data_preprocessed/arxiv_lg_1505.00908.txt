In/IN order/NN to/TO speed/VB -/HYPH up/RP classification/NN models/NNS when/WRB facing/VBG a/DT large/JJ number/NN of/IN categories/NNS ,/, one/CD usual/JJ approach/NN consists/VBZ in/IN organizing/VBG the/DT categories/NNS in/IN a/DT particular/JJ structure/NN ,/, this/DT structure/NN being/VBG then/RB used/VBN as/IN a/DT way/NN to/TO speed/VB -/HYPH up/RP the/DT prediction/NN computation/NN ./.
This/DT is/VBZ for/IN example/NN the/DT case/NN when/WRB using/VBG error/NN -/HYPH correcting/VBG codes/NNS or/CC even/RB hierarchies/NNS of/IN categories/NNS ./.
But/CC in/IN the/DT majority/NN of/IN approaches/NNS ,/, this/DT structure/NN is/VBZ chosen/VBN \/SYM textit/FW {/-LRB- by/IN hand/NN }/-RRB- ,/, or/CC during/IN a/DT preliminary/JJ step/NN ,/, and/CC not/RB integrated/VBN in/IN the/DT learning/NN process/NN ./.
We/PRP propose/VBP a/DT new/JJ model/NN called/VBN Reinforced/NNP Decision/NN Tree/NNP which/WDT simultaneously/RB learns/VBZ how/WRB to/TO organize/VB categories/NNS in/IN a/DT tree/NN structure/NN and/CC how/WRB to/TO classify/VB any/DT input/NN based/VBN on/IN this/DT structure/NN ./.
This/DT approach/NN keeps/VBZ the/DT advantages/NNS of/IN existing/VBG techniques/NNS (/-LRB- low/JJ inference/NN complexity/NN )/-RRB- but/CC allows/VBZ one/CD to/TO build/VB efficient/JJ classifiers/NNS in/IN one/CD learning/NN step/NN ./.
The/DT learning/NN algorithm/NN is/VBZ inspired/VBN by/IN reinforcement/NN learning/NN and/CC policy/NN -/HYPH gradient/NN techniques/NNS which/WDT allows/VBZ us/PRP to/TO integrate/VB the/DT two/CD steps/NNS (/-LRB- building/VBG the/DT tree/NN ,/, and/CC learning/VBG the/DT classifier/NN )/-RRB- in/IN one/CD single/JJ algorithm/NN ./.
