The/DT input/NN to/IN a/DT neural/JJ sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN is/VBZ often/RB determined/VBN by/IN an/DT up/NN -/HYPH stream/NN system/NN ,/, e.g./FW a/DT word/NN segmenter/NN ,/, part/NN of/IN speech/NN tagger/NN ,/, or/CC speech/NN recognizer/NN ./.
These/DT up/IN -/HYPH stream/NN models/NNS are/VBP potentially/RB error/NN -/HYPH prone/JJ ./.
Representing/VBG inputs/NNS through/IN word/NN lattices/NNS allows/VBZ making/VBG this/DT uncertainty/NN explicit/JJ by/IN capturing/VBG alternative/JJ sequences/NNS and/CC their/PRP$ posterior/JJ probabilities/NNS in/IN a/DT compact/JJ form/NN ./.
In/IN this/DT work/NN ,/, we/PRP extend/VBP the/DT TreeLSTM/NNP (/-LRB- Tai/NNP et/FW al./FW ,/, 2015/CD )/-RRB- into/IN a/DT LatticeLSTM/NN that/WDT is/VBZ able/JJ to/TO consume/VB word/NN lattices/NNS ,/, and/CC can/MD be/VB used/VBN as/IN encoder/NN in/IN an/DT attentional/JJ encoder/NN -/HYPH decoder/NN model/NN ./.
We/PRP integrate/VBP lattice/NN posterior/JJ scores/NNS into/IN this/DT architecture/NN by/IN extending/VBG the/DT TreeLSTM/NNP 's/POS child/NN -/HYPH sum/NN and/CC forget/VB gates/NNS and/CC introducing/VBG a/DT bias/NN term/NN into/IN the/DT attention/NN mechanism/NN ./.
We/PRP experiment/VBP with/IN speech/NN translation/NN lattices/NNS and/CC report/NN consistent/JJ improvements/NNS over/IN baselines/NNS that/WDT translate/VBP either/CC the/DT 1/CD -/HYPH best/JJS hypothesis/NN or/CC the/DT lattice/NN without/IN posterior/JJ scores/NNS ./.
