Denoising/NNP autoencoders/NNS (/-LRB- DAs/NNS )/-RRB- are/VBP typically/RB applied/VBN to/IN relatively/RB large/JJ datasets/NNS for/IN unsupervised/JJ learning/NN of/IN representative/JJ data/NNS encodings/NNS ;/: they/PRP rely/VBP on/IN the/DT idea/NN of/IN making/VBG the/DT learned/VBN representations/NNS robust/JJ to/IN partial/JJ corruption/NN of/IN the/DT input/NN pattern/NN ,/, and/CC perform/VB learning/NN using/VBG stochastic/JJ gradient/NN descent/NN with/IN relatively/RB large/JJ datasets/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT fully/RB Bayesian/JJ DA/NN architecture/NN that/WDT allows/VBZ for/IN the/DT application/NN of/IN DAs/NNS even/RB when/WRB data/NNS is/VBZ scarce/JJ ./.
Our/PRP$ novel/JJ approach/NN formulates/VBZ the/DT signal/NN encoding/VBG problem/NN under/IN a/DT nonparametric/JJ Bayesian/JJ regard/NN ,/, considering/VBG a/DT Gaussian/JJ process/NN prior/RB over/IN the/DT latent/JJ input/NN encodings/NNS generated/VBN given/VBN the/DT (/-LRB- corrupt/JJ )/-RRB- input/NN observations/NNS ./.
Subsequently/RB ,/, the/DT decoder/NN modules/NNS of/IN our/PRP$ model/NN are/VBP formulated/VBN as/IN large/JJ -/HYPH margin/NN regression/NN models/NNS ,/, treated/VBN under/IN the/DT Bayesian/JJ inference/NN paradigm/NN ,/, by/IN exploiting/VBG the/DT maximum/JJ entropy/NN discrimination/NN (/-LRB- MED/NN )/-RRB- framework/NN ./.
We/PRP exhibit/VBP the/DT effectiveness/NN of/IN our/PRP$ approach/NN using/VBG several/JJ datasets/NNS ,/, dealing/VBG with/IN both/DT classification/NN and/CC transfer/NN learning/NN applications/NNS ./.
