As/IN a/DT new/JJ way/NN of/IN training/NN generative/NN models/NNS ,/, Generative/JJ Adversarial/JJ Nets/NNS (/-LRB- GAN/NN )/-RRB- that/WDT uses/VBZ a/DT discriminative/JJ model/NN to/TO guide/VB the/DT training/NN of/IN the/DT generative/JJ model/NN has/VBZ enjoyed/VBN considerable/JJ success/NN in/IN generating/VBG real/JJ -/HYPH valued/VBN data/NNS ./.
However/RB ,/, it/PRP has/VBZ limitations/NNS when/WRB the/DT goal/NN is/VBZ for/IN generating/VBG sequences/NNS of/IN discrete/JJ tokens/NNS ./.
A/DT major/JJ reason/NN lies/VBZ in/IN that/IN the/DT discrete/JJ outputs/NNS from/IN the/DT generative/JJ model/NN make/VB it/PRP difficult/JJ to/TO pass/VB the/DT gradient/NN update/NN from/IN the/DT discriminative/JJ model/NN to/IN the/DT generative/JJ model/NN ./.
Also/RB ,/, the/DT discriminative/JJ model/NN can/MD only/RB assess/VB a/DT complete/JJ sequence/NN ,/, while/IN for/IN a/DT partially/RB generated/VBN sequence/NN ,/, it/PRP is/VBZ non-trivial/JJ to/TO balance/VB its/PRP$ current/JJ score/NN and/CC the/DT future/JJ one/CD once/IN the/DT entire/JJ sequence/NN has/VBZ been/VBN generated/VBN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT sequence/NN generation/NN framework/NN ,/, called/VBN SeqGAN/NNP ,/, to/TO solve/VB the/DT problems/NNS ./.
Modeling/VBG the/DT data/NNS generator/NN as/IN a/DT stochastic/JJ policy/NN in/IN reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- ,/, SeqGAN/NN bypasses/VBZ the/DT generator/NN differentiation/NN problem/NN by/IN directly/RB performing/VBG gradient/NN policy/NN update/NN ./.
The/DT RL/NN reward/NN signal/NN comes/VBZ from/IN the/DT GAN/NNP discriminator/NN judged/VBN on/IN a/DT complete/JJ sequence/NN ,/, and/CC is/VBZ passed/VBN back/RP to/IN the/DT intermediate/JJ state/NN -/HYPH action/NN steps/NNS using/VBG Monte/NNP Carlo/NNP search/NN ./.
Extensive/JJ experiments/NNS on/IN synthetic/JJ data/NNS and/CC real/JJ -/HYPH world/NN tasks/NNS demonstrate/VBP significant/JJ improvements/NNS over/IN strong/JJ baselines/NNS ./.
