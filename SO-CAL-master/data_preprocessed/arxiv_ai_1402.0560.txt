In/IN this/DT paper/NN ,/, we/PRP consider/VBP the/DT important/JJ problem/NN of/IN safe/JJ exploration/NN in/IN reinforcement/NN learning/NN ./.
While/IN reinforcement/NN learning/NN is/VBZ well/RB -/HYPH suited/VBN to/IN domains/NNS with/IN complex/JJ transition/NN dynamics/NNS and/CC high/JJ -/HYPH dimensional/JJ state/NN -/HYPH action/NN spaces/NNS ,/, an/DT additional/JJ challenge/NN is/VBZ posed/VBN by/IN the/DT need/NN for/IN safe/JJ and/CC efficient/JJ exploration/NN ./.
Traditional/JJ exploration/NN techniques/NNS are/VBP not/RB particularly/RB useful/JJ for/IN solving/VBG dangerous/JJ tasks/NNS ,/, where/WRB the/DT trial/NN and/CC error/NN process/NN may/MD lead/VB to/IN the/DT selection/NN of/IN actions/NNS whose/WP$ execution/NN in/IN some/DT states/NNS may/MD result/VB in/IN damage/NN to/IN the/DT learning/NN system/NN (/-LRB- or/CC any/DT other/JJ system/NN )/-RRB- ./.
Consequently/RB ,/, when/WRB an/DT agent/NN begins/VBZ an/DT interaction/NN with/IN a/DT dangerous/JJ and/CC high/JJ -/HYPH dimensional/JJ state/NN -/HYPH action/NN space/NN ,/, an/DT important/JJ question/NN arises/VBZ ;/: namely/RB ,/, that/DT of/IN how/WRB to/TO avoid/VB (/-LRB- or/CC at/IN least/RBS minimize/VB )/-RRB- damage/NN caused/VBN by/IN the/DT exploration/NN of/IN the/DT state/NN -/HYPH action/NN space/NN ./.
We/PRP introduce/VBP the/DT PI/NN -/HYPH SRL/NN algorithm/NN which/WDT safely/RB improves/VBZ suboptimal/JJ albeit/IN robust/JJ behaviors/NNS for/IN continuous/JJ state/NN and/CC action/NN control/NN tasks/NNS and/CC which/WDT efficiently/RB learns/VBZ from/IN the/DT experience/NN gained/VBD from/IN the/DT environment/NN ./.
We/PRP evaluate/VBP the/DT proposed/JJ method/NN in/IN four/CD complex/JJ tasks/NNS :/: automatic/JJ car/NN parking/NN ,/, pole/NN -/HYPH balancing/NN ,/, helicopter/NN hovering/VBG ,/, and/CC business/NN management/NN ./.
