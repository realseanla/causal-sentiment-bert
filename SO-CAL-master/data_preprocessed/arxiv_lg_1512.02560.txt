The/DT promising/JJ performance/NN of/IN Deep/NNP Learning/NNP (/-LRB- DL/NN )/-RRB- in/IN speech/NN recognition/NN has/VBZ motivated/VBN the/DT use/NN of/IN DL/NN in/IN other/JJ speech/NN technology/NN applications/NNS such/JJ as/IN speaker/NN recognition/NN ./.
Given/VBN i/LS -/: vectors/NNS as/IN inputs/NNS ,/, the/DT authors/NNS proposed/VBD an/DT impostor/NN selection/NN algorithm/NN and/CC a/DT universal/JJ model/NN adaptation/NN process/NN in/IN a/DT hybrid/NN system/NN based/VBN on/IN Deep/NNP Belief/NNP Networks/NNP (/-LRB- DBN/NNP )/-RRB- and/CC Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNN/NN )/-RRB- to/TO discriminatively/RB model/VB each/DT target/NN speaker/NN ./.
In/IN order/NN to/TO have/VB more/JJR insight/NN into/IN the/DT behavior/NN of/IN DL/NN techniques/NNS in/IN both/DT single/JJ and/CC multi-session/JJ speaker/NN enrollment/NN tasks/NNS ,/, some/DT experiments/NNS have/VBP been/VBN carried/VBN out/RP in/IN this/DT paper/NN in/IN both/CC scenarios/NNS ./.
Additionally/RB ,/, the/DT parameters/NNS of/IN the/DT global/JJ model/NN ,/, referred/VBN to/IN as/IN universal/JJ DBN/NN (/-LRB- UDBN/NNP )/-RRB- ,/, are/VBP normalized/VBN before/IN adaptation/NN ./.
UDBN/NN normalization/NN facilitates/VBZ training/NN DNNs/NNS specifically/RB with/IN more/JJR than/IN one/CD hidden/VBN layer/NN ./.
Experiments/NNS are/VBP performed/VBN on/IN the/DT NIST/NNP SRE/NNP 2006/CD corpus/NN ./.
It/PRP is/VBZ shown/VBN that/IN the/DT proposed/VBN impostor/NN selection/NN algorithm/NN and/CC UDBN/NN adaptation/NN process/NN enhance/VB the/DT performance/NN of/IN conventional/JJ DNNs/NNS 8/CD -/HYPH 20/CD percent/NN and/CC 16/CD -/HYPH 20/CD percent/NN in/IN terms/NNS of/IN EER/NNP for/IN the/DT single/JJ and/CC multi-session/JJ tasks/NNS ,/, respectively/RB ./.
In/IN both/DT scenarios/NNS ,/, the/DT proposed/VBN architectures/NNS outperform/VBP the/DT baseline/NN systems/NNS obtaining/VBG up/RP to/IN 17/CD percent/NN reduction/NN in/IN EER/NNP ./.
