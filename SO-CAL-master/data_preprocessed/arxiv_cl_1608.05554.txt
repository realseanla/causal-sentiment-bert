The/DT sequence/NN to/IN sequence/NN architecture/NN is/VBZ widely/RB used/VBN in/IN the/DT response/NN generation/NN and/CC neural/JJ machine/NN translation/NN to/IN model/NN the/DT potential/JJ relationship/NN between/IN two/CD sentences/NNS ./.
It/PRP typically/RB consists/VBZ of/IN two/CD parts/NNS :/: an/DT encoder/NN that/WDT reads/VBZ from/IN the/DT source/NN sentence/NN and/CC a/DT decoder/NN that/WDT generates/VBZ the/DT target/NN sentence/NN word/NN by/IN word/NN according/VBG to/IN the/DT encoder/NN 's/POS output/NN and/CC the/DT last/JJ generated/VBN word/NN ./.
However/RB ,/, it/PRP faces/VBZ to/IN the/DT cold/JJ start/NN problem/NN when/WRB generating/VBG the/DT first/JJ word/NN as/IN there/EX is/VBZ no/DT previous/JJ word/NN to/TO refer/VB ./.
Existing/VBG work/NN mainly/RB use/VBP a/DT special/JJ start/NN symbol/NN &lt;/SYM //HYPH s/NN &gt;/SYM to/TO generate/VB the/DT first/JJ word/NN ./.
An/DT obvious/JJ drawback/NN of/IN these/DT work/NN is/VBZ that/IN there/EX is/VBZ not/RB a/DT learnable/JJ relationship/NN between/IN words/NNS and/CC the/DT start/NN symbol/NN ./.
Furthermore/RB ,/, it/PRP may/MD lead/VB to/IN the/DT error/NN accumulation/NN for/IN decoding/NN when/WRB the/DT first/JJ word/NN is/VBZ incorrectly/RB generated/VBN ./.
In/IN this/DT paper/NN ,/, we/PRP proposed/VBD a/DT novel/JJ approach/NN to/IN learning/VBG to/TO generate/VB the/DT first/JJ word/NN in/IN the/DT sequence/NN to/IN sequence/NN architecture/NN rather/RB than/IN using/VBG the/DT start/NN symbol/NN ./.
Experimental/JJ results/NNS on/IN the/DT task/NN of/IN response/NN generation/NN of/IN short/JJ text/NN conversation/NN show/VBP that/IN the/DT proposed/VBN approach/NN outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN approach/NN in/IN both/DT of/IN the/DT automatic/JJ and/CC manual/JJ evaluations/NNS ./.
