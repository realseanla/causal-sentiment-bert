Natural/JJ language/NN generation/NN of/IN coherent/JJ long/JJ texts/NNS like/IN paragraphs/NNS or/CC longer/JJR documents/NNS is/VBZ a/DT challenging/JJ problem/NN for/IN recurrent/JJ networks/NNS models/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP explore/VBP an/DT important/JJ step/NN toward/IN this/DT generation/NN task/NN :/: training/VBG an/DT LSTM/NN (/-LRB- Long/JJ -/HYPH short/JJ term/NN memory/NN )/-RRB- auto/NN -/HYPH encoder/NN to/TO preserve/VB and/CC reconstruct/VB multi-sentence/NN paragraphs/NNS ./.
We/PRP introduce/VBP an/DT LSTM/NN model/NN that/WDT hierarchically/RB builds/VBZ an/DT embedding/NN for/IN a/DT paragraph/NN from/IN embeddings/NNS for/IN sentences/NNS and/CC words/NNS ,/, then/RB decodes/VBZ this/DT embedding/NN to/TO reconstruct/VB the/DT original/JJ paragraph/NN ./.
We/PRP evaluate/VBP the/DT reconstructed/JJ paragraph/NN using/VBG standard/JJ metrics/NNS like/IN ROUGE/NNP and/CC Entity/NNP Grid/NNP ,/, showing/VBG that/IN neural/JJ models/NNS are/VBP able/JJ to/TO encode/VB texts/NNS in/IN a/DT way/NN that/WDT preserve/VBP syntactic/JJ ,/, semantic/JJ ,/, and/CC discourse/NN coherence/NN ./.
While/IN only/RB a/DT first/JJ step/NN toward/IN generating/VBG coherent/JJ text/NN units/NNS from/IN neural/JJ models/NNS ,/, our/PRP$ work/NN has/VBZ the/DT potential/JJ to/TO significantly/RB impact/VB natural/JJ language/NN generation/NN and/CC summarization/NN \/SYM footnote/NN {/-LRB- Code/NN for/IN the/DT three/CD models/NNS described/VBN in/IN this/DT paper/NN can/MD be/VB found/VBN at/IN www.stanford.edu/~jiweil//NN ./.
