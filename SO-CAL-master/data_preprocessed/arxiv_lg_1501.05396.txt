In/IN this/DT paper/NN ,/, we/PRP present/VBP methods/NNS in/IN deep/JJ multimodal/JJ learning/NN for/IN fusing/VBG speech/NN and/CC visual/JJ modalities/NNS for/IN Audio/NNP -/HYPH Visual/NNP Automatic/NNP Speech/NNP Recognition/NNP (/-LRB- AV/NN -/HYPH ASR/NN )/-RRB- ./.
First/RB ,/, we/PRP study/VBP an/DT approach/NN where/WRB uni-modal/JJ deep/JJ networks/NNS are/VBP trained/VBN separately/RB and/CC their/PRP$ final/JJ hidden/JJ layers/NNS fused/VBN to/TO obtain/VB a/DT joint/JJ feature/NN space/NN in/IN which/WDT another/DT deep/JJ network/NN is/VBZ built/VBN ./.
While/IN the/DT audio/JJ network/NN alone/JJ achieves/VBZ a/DT phone/NN error/NN rate/NN (/-LRB- PER/NN )/-RRB- of/IN $/$ 41/CD \/SYM percent/NN $/$ under/IN clean/JJ condition/NN on/IN the/DT IBM/NNP large/JJ vocabulary/NN audio/NN -/HYPH visual/JJ studio/NN dataset/NN ,/, this/DT fusion/NN model/NN achieves/VBZ a/DT PER/NN of/IN $/$ 35.83/CD \/SYM percent/NN $/$ demonstrating/VBG the/DT tremendous/JJ value/NN of/IN the/DT visual/JJ channel/NN in/IN phone/NN classification/NN even/RB in/IN audio/JJ with/IN high/JJ signal/NN to/IN noise/NN ratio/NN ./.
Second/RB ,/, we/PRP present/VBP a/DT new/JJ deep/JJ network/NN architecture/NN that/WDT uses/VBZ a/DT bilinear/NN softmax/NN layer/NN to/TO account/VB for/IN class/NN specific/JJ correlations/NNS between/IN modalities/NNS ./.
We/PRP show/VBP that/IN combining/VBG the/DT posteriors/NNS from/IN the/DT bilinear/NN networks/NNS with/IN those/DT from/IN the/DT fused/VBN model/NN mentioned/VBN above/IN results/NNS in/IN a/DT further/RBR significant/JJ phone/NN error/NN rate/NN reduction/NN ,/, yielding/VBG a/DT final/JJ PER/NN of/IN $/$ 34.03/CD \/SYM percent/NN $/$ ./.
