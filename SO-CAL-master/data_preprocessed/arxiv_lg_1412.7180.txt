This/DT paper/NN presents/VBZ novel/JJ Bayesian/JJ optimisation/NN algorithms/NNS for/IN minimum/JJ error/NN rate/NN training/NN of/IN statistical/JJ machine/NN translation/NN systems/NNS ./.
We/PRP explore/VBP two/CD classes/NNS of/IN algorithms/NNS for/IN efficiently/RB exploring/VBG the/DT translation/NN space/NN ,/, with/IN the/DT first/JJ based/VBN on/IN N/NN -/HYPH best/JJS lists/NNS and/CC the/DT second/JJ based/VBN on/IN a/DT hypergraph/NN representation/NN that/WDT compactly/RB represents/VBZ an/DT exponential/JJ number/NN of/IN translation/NN options/NNS ./.
Our/PRP$ algorithms/NNS exhibit/VBP faster/RBR convergence/NN and/CC are/VBP capable/JJ of/IN obtaining/VBG lower/JJR error/NN rates/NNS than/IN the/DT existing/VBG translation/NN model/NN specific/JJ approaches/NNS ,/, all/DT within/IN a/DT generic/JJ Bayesian/JJ optimisation/NN framework/NN ./.
Further/RB more/RBR ,/, we/PRP also/RB introduce/VBP a/DT random/JJ embedding/NN algorithm/NN to/TO scale/VB our/PRP$ approach/NN to/IN sparse/JJ high/JJ dimensional/JJ feature/NN sets/NNS ./.
