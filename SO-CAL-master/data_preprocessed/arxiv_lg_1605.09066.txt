In/IN this/DT paper/NN ,/, we/PRP propose/VBP new/JJ Distributed/VBN Asynchronous/JJ Dual/JJ -/HYPH Free/JJ Coordinate/VB Ascent/NN method/NN (/-LRB- Asy/NN -/HYPH df/NN SDCA/NN )/-RRB- ,/, and/CC provide/VB the/DT proof/NN of/IN convergence/NN rate/NN for/IN two/CD cases/NNS :/: the/DT individual/JJ loss/NN is/VBZ convex/NN and/CC the/DT individual/JJ loss/NN is/VBZ non-convex/JJ but/CC its/PRP$ expected/JJ loss/NN is/VBZ convex/NN ./.
Stochastic/JJ Dual/JJ Coordinate/VB Ascent/NN (/-LRB- SDCA/NN )/-RRB- model/NN is/VBZ a/DT popular/JJ method/NN and/CC often/RB has/VBZ better/JJR performances/NNS than/IN stochastic/JJ gradient/NN descent/NN methods/NNS in/IN solving/VBG regularized/VBN convex/NN loss/NN minimization/NN problems/NNS ./.
Dual/JJ -/HYPH Free/JJ Stochastic/JJ Dual/JJ Coordinate/VB Ascent/NN method/NN is/VBZ a/DT variation/NN of/IN SDCA/NNP ,/, and/CC can/MD be/VB applied/VBN to/IN non-convex/JJ problem/NN when/WRB its/PRP$ dual/JJ problem/NN is/VBZ meaningless/JJ ./.
We/PRP extend/VBP Dual/JJ -/HYPH Free/JJ Stochastic/JJ Dual/JJ Coordinate/VB Ascent/NN method/NN to/IN the/DT distributed/VBN mode/NN with/IN considering/VBG the/DT star/NN network/NN in/IN this/DT paper/NN ./.
