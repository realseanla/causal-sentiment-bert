Statistical/JJ language/NN models/NNS are/VBP central/JJ to/IN many/JJ applications/NNS that/WDT use/VBP semantics/NNS ./.
Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNN/NN )/-RRB- are/VBP known/VBN to/TO produce/VB state/NN of/IN the/DT art/NN results/NNS for/IN language/NN modelling/NN ,/, outperforming/VBG their/PRP$ traditional/JJ n/NN -/HYPH gram/NN counterparts/NNS in/IN many/JJ cases/NNS ./.
To/TO generate/VB a/DT probability/NN distribution/NN across/IN a/DT vocabulary/NN ,/, these/DT models/NNS require/VBP a/DT softmax/JJ output/NN layer/NN that/WDT linearly/RB increases/VBZ in/IN size/NN with/IN the/DT size/NN of/IN the/DT vocabulary/NN ./.
Large/JJ vocabularies/NNS need/VBP a/DT commensurately/RB large/JJ softmax/NN layer/NN and/CC training/VBG them/PRP on/IN typical/JJ laptops/NNS //, PCs/NNS requires/VBZ significant/JJ time/NN and/CC machine/NN resources/NNS ./.
In/IN this/DT paper/NN we/PRP present/VBP a/DT new/JJ technique/NN for/IN implementing/VBG RNN/NN based/VBN large/JJ vocabulary/NN language/NN models/NNS that/WDT substantially/RB speeds/VBZ up/RP computation/NN while/IN optimally/RB using/VBG the/DT limited/JJ memory/NN resources/NNS ./.
Our/PRP$ technique/NN ,/, while/IN building/VBG on/IN the/DT notion/NN of/IN factorizing/VBG the/DT output/NN layer/NN by/IN having/VBG multiple/JJ output/NN layers/NNS ,/, improves/VBZ on/IN the/DT earlier/JJR work/NN by/IN substantially/RB optimizing/VBG on/IN the/DT individual/JJ output/NN layer/NN size/NN and/CC also/RB eliminating/VBG the/DT need/NN for/IN a/DT multistep/JJ prediction/NN process/NN ./.
