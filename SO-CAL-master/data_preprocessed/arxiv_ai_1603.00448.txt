Reinforcement/NN learning/NN can/MD acquire/VB complex/JJ behaviors/NNS from/IN high/JJ -/HYPH level/NN specifications/NNS ./.
However/RB ,/, defining/VBG a/DT cost/NN function/NN that/WDT can/MD be/VB optimized/VBN effectively/RB and/CC encodes/VBZ the/DT correct/JJ task/NN is/VBZ challenging/VBG in/IN practice/NN ./.
We/PRP explore/VBP how/WRB inverse/JJ optimal/JJ control/NN (/-LRB- IOC/NNP )/-RRB- can/MD be/VB used/VBN to/TO learn/VB behaviors/NNS from/IN demonstrations/NNS ,/, with/IN applications/NNS to/IN torque/NN control/NN of/IN high/JJ -/HYPH dimensional/JJ robotic/JJ systems/NNS ./.
Our/PRP$ method/NN addresses/NNS two/CD key/JJ challenges/NNS in/IN inverse/JJ optimal/JJ control/NN :/: first/RB ,/, the/DT need/NN for/IN informative/JJ features/NNS and/CC effective/JJ regularization/NN to/TO impose/VB structure/NN on/IN the/DT cost/NN ,/, and/CC second/RB ,/, the/DT difficulty/NN of/IN learning/VBG the/DT cost/NN function/NN under/IN unknown/JJ dynamics/NNS for/IN high/JJ -/HYPH dimensional/JJ continuous/JJ systems/NNS ./.
To/TO address/VB the/DT former/JJ challenge/NN ,/, we/PRP present/VBP an/DT algorithm/NN capable/JJ of/IN learning/VBG arbitrary/JJ nonlinear/JJ cost/NN functions/NNS ,/, such/JJ as/IN neural/JJ networks/NNS ,/, without/IN meticulous/JJ feature/NN engineering/NN ./.
To/TO address/VB the/DT latter/JJ challenge/NN ,/, we/PRP formulate/VBP an/DT efficient/JJ sample/NN -/HYPH based/VBN approximation/NN for/IN MaxEnt/NNP IOC/NNP ./.
We/PRP evaluate/VBP our/PRP$ method/NN on/IN a/DT series/NN of/IN simulated/JJ tasks/NNS and/CC real/JJ -/HYPH world/NN robotic/JJ manipulation/NN problems/NNS ,/, demonstrating/VBG substantial/JJ improvement/NN over/IN prior/JJ methods/NNS both/CC in/IN terms/NNS of/IN task/NN complexity/NN and/CC sample/NN efficiency/NN ./.
