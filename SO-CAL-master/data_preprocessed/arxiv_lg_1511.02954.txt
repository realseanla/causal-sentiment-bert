This/DT paper/NN presents/VBZ a/DT new/JJ method/NN for/IN pre-training/JJ neural/JJ networks/NNS that/WDT can/MD decrease/VB the/DT total/JJ training/NN time/NN for/IN a/DT neural/JJ network/NN while/IN maintaining/VBG the/DT final/JJ performance/NN ,/, which/WDT motivates/VBZ its/PRP$ use/NN on/IN deep/JJ neural/JJ networks/NNS ./.
By/IN partitioning/VBG the/DT training/NN task/NN in/IN multiple/JJ training/NN subtasks/NNS with/IN sub-models/NNS ,/, which/WDT can/MD be/VB performed/VBN independently/RB and/CC in/IN parallel/NN ,/, it/PRP is/VBZ shown/VBN that/IN the/DT size/NN of/IN the/DT sub-models/NNS reduces/VBZ almost/RB quadratically/RB with/IN the/DT number/NN of/IN subtasks/NNS created/VBN ,/, quickly/RB scaling/VBG down/RP the/DT sub-models/NNS used/VBN for/IN the/DT pre-training/NN ./.
The/DT sub-models/NNS are/VBP then/RB merged/VBN to/TO provide/VB a/DT pre-trained/JJ initial/JJ set/NN of/IN weights/NNS for/IN the/DT original/JJ model/NN ./.
The/DT proposed/JJ method/NN is/VBZ independent/JJ of/IN the/DT other/JJ aspects/NNS of/IN the/DT training/NN ,/, such/JJ as/IN architecture/NN of/IN the/DT neural/JJ network/NN ,/, training/NN method/NN ,/, and/CC objective/NN ,/, making/VBG it/PRP compatible/JJ with/IN a/DT wide/JJ range/NN of/IN existing/VBG approaches/NNS ./.
The/DT speedup/NN without/IN loss/NN of/IN performance/NN is/VBZ validated/VBN experimentally/RB on/IN MNIST/NNP and/CC on/IN CIFAR10/NN data/NNS sets/NNS ,/, also/RB showing/VBG that/IN even/RB performing/VBG the/DT subtasks/NNS sequentially/RB can/MD decrease/VB the/DT training/NN time/NN ./.
Moreover/RB ,/, we/PRP show/VBP that/IN larger/JJR models/NNS may/MD present/VB higher/JJR speedups/NNS and/CC conjecture/NN about/IN the/DT benefits/NNS of/IN the/DT method/NN in/IN distributed/VBN learning/NN systems/NNS ./.
