Matching/VBG natural/JJ language/NN sentences/NNS is/VBZ central/JJ for/IN many/JJ applications/NNS such/JJ as/IN information/NN retrieval/NN and/CC question/NN answering/NN ./.
Existing/VBG deep/JJ models/NNS rely/VBP on/IN a/DT single/JJ sentence/NN representation/NN or/CC multiple/JJ granularity/NN representations/NNS for/IN matching/NN ./.
However/RB ,/, such/JJ methods/NNS can/MD not/RB well/RB capture/VB the/DT contextualized/JJ local/JJ information/NN in/IN the/DT matching/NN process/NN ./.
To/TO tackle/VB this/DT problem/NN ,/, we/PRP present/VBP a/DT new/JJ deep/JJ architecture/NN to/TO match/VB two/CD sentences/NNS with/IN multiple/JJ positional/JJ sentence/NN representations/NNS ./.
Specifically/RB ,/, each/DT positional/JJ sentence/NN representation/NN is/VBZ a/DT sentence/NN representation/NN at/IN this/DT position/NN ,/, generated/VBN by/IN a/DT bidirectional/JJ long/JJ short/JJ term/NN memory/NN (/-LRB- Bi-LSTM/NN )/-RRB- ./.
The/DT matching/JJ score/NN is/VBZ finally/RB produced/VBN by/IN aggregating/VBG interactions/NNS between/IN these/DT different/JJ positional/JJ sentence/NN representations/NNS ,/, through/IN $/$ k/CD $/$ -/HYPH Max/NNP pooling/VBG and/CC a/DT multi-layer/JJ perceptron/NN ./.
Our/PRP$ model/NN has/VBZ several/JJ advantages/NNS :/: (/-LRB- 1/LS )/-RRB- By/IN using/VBG Bi-LSTM/NN ,/, rich/JJ context/NN of/IN the/DT whole/JJ sentence/NN is/VBZ leveraged/VBN to/TO capture/VB the/DT contextualized/JJ local/JJ information/NN in/IN each/DT positional/JJ sentence/NN representation/NN ;/: (/-LRB- 2/LS )/-RRB- By/IN matching/VBG with/IN multiple/JJ positional/JJ sentence/NN representations/NNS ,/, it/PRP is/VBZ flexible/JJ to/IN aggregate/JJ different/JJ important/JJ contextualized/JJ local/JJ information/NN in/IN a/DT sentence/NN to/TO support/VB the/DT matching/NN ;/: (/-LRB- 3/LS )/-RRB- Experiments/NNS on/IN different/JJ tasks/NNS such/JJ as/IN question/NN answering/NN and/CC sentence/NN completion/NN demonstrate/VBP the/DT superiority/NN of/IN our/PRP$ model/NN ./.
