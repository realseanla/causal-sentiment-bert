A/DT lot/NN of/IN prior/JJ work/NN on/IN event/NN extraction/NN has/VBZ exploited/VBN a/DT variety/NN of/IN features/NNS to/TO represent/VB events/NNS ./.
Such/JJ methods/NNS have/VBP several/JJ drawbacks/NNS :/: 1/LS )/-RRB- the/DT features/NNS are/VBP often/RB specific/JJ for/IN a/DT particular/JJ domain/NN and/CC do/VBP not/RB generalize/VB well/RB ;/: 2/LS )/-RRB- the/DT features/NNS are/VBP derived/VBN from/IN various/JJ linguistic/JJ analyses/NNS and/CC are/VBP error/NN -/HYPH prone/JJ ;/: and/CC 3/LS )/-RRB- some/DT features/NNS may/MD be/VB expensive/JJ and/CC require/VBP domain/NN expert/NN ./.
In/IN this/DT paper/NN ,/, we/PRP develop/VBP a/DT Chinese/JJ event/NN extraction/NN system/NN that/WDT uses/VBZ word/NN embedding/NN vectors/NNS to/TO represent/VB language/NN ,/, and/CC deep/JJ neural/JJ networks/NNS to/TO learn/VB the/DT abstract/JJ feature/NN representation/NN in/IN order/NN to/TO greatly/RB reduce/VB the/DT effort/NN of/IN feature/NN engineering/NN ./.
In/IN addition/NN ,/, in/IN this/DT framework/NN ,/, we/PRP leverage/VBP large/JJ amount/NN of/IN unlabeled/JJ data/NNS ,/, which/WDT can/MD address/VB the/DT problem/NN of/IN limited/JJ labeled/VBN corpus/NN for/IN this/DT task/NN ./.
Our/PRP$ experiments/NNS show/VBP that/IN our/PRP$ proposed/JJ method/NN performs/VBZ better/JJR compared/VBN to/IN the/DT system/NN using/VBG rich/JJ language/NN features/NNS ,/, and/CC using/VBG unlabeled/JJ data/NNS benefits/VBZ the/DT word/NN embeddings/NNS ./.
This/DT study/NN suggests/VBZ the/DT potential/NN of/IN DNN/NNP and/CC word/NN embedding/NN for/IN the/DT event/NN extraction/NN task/NN ./.
