We/PRP study/VBP the/DT complexity/NN of/IN functions/NNS computable/JJ by/IN deep/JJ feedforward/NN neural/JJ networks/NNS with/IN piece-wise/JJ linear/JJ activations/NNS in/IN terms/NNS of/IN the/DT number/NN of/IN regions/NNS of/IN linearity/NN that/IN they/PRP have/VBP ./.
Deep/JJ networks/NNS are/VBP able/JJ to/TO sequentially/RB map/VB portions/NNS of/IN each/DT layer/NN 's/POS input/NN space/NN to/IN the/DT same/JJ output/NN ./.
In/IN this/DT way/NN ,/, deep/JJ models/NNS compute/VBP functions/NNS with/IN a/DT compositional/JJ structure/NN that/WDT is/VBZ able/JJ to/TO re-use/VB pieces/NNS of/IN computation/NN exponentially/RB often/RB in/IN terms/NNS of/IN their/PRP$ depth/NN ./.
This/DT note/NN investigates/VBZ the/DT complexity/NN of/IN such/JJ compositional/JJ maps/NNS and/CC contributes/VBZ new/JJ theoretical/JJ results/NNS regarding/VBG the/DT advantage/NN of/IN depth/NN for/IN neural/JJ networks/NNS with/IN piece-wise/JJ linear/JJ activation/NN functions/NNS ./.
