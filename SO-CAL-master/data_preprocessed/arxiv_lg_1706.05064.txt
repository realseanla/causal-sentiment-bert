As/IN a/DT step/NN towards/IN developing/VBG zero/CD -/HYPH shot/NN task/NN generalization/NN capabilities/NNS in/IN reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- ,/, we/PRP introduce/VBP a/DT new/JJ RL/NNP problem/NN where/WRB the/DT agent/NN should/MD learn/VB to/TO execute/VB sequences/NNS of/IN instructions/NNS after/IN learning/VBG useful/JJ skills/NNS that/WDT solve/VBP subtasks/NNS ./.
In/IN this/DT problem/NN ,/, we/PRP consider/VBP two/CD types/NNS of/IN generalizations/NNS :/: to/IN previously/RB unseen/JJ instructions/NNS and/CC to/TO longer/RBR sequences/NNS of/IN instructions/NNS ./.
For/IN generalization/NN over/IN unseen/JJ instructions/NNS ,/, we/PRP propose/VBP a/DT new/JJ objective/NN which/WDT encourages/VBZ learning/VBG correspondences/NNS between/IN similar/JJ subtasks/NNS by/IN making/VBG analogies/NNS ./.
For/IN generalization/NN over/IN sequential/JJ instructions/NNS ,/, we/PRP present/VBP a/DT hierarchical/JJ architecture/NN where/WRB a/DT meta/JJ controller/NN learns/VBZ to/TO use/VB the/DT acquired/VBN skills/NNS for/IN executing/VBG the/DT instructions/NNS ./.
To/TO deal/VB with/IN delayed/VBN reward/NN ,/, we/PRP propose/VBP a/DT new/JJ neural/JJ architecture/NN in/IN the/DT meta/JJ controller/NN that/WDT learns/VBZ when/WRB to/TO update/VB the/DT subtask/NN ,/, which/WDT makes/VBZ learning/VBG more/RBR efficient/JJ ./.
Experimental/JJ results/NNS on/IN a/DT stochastic/JJ 3D/NN domain/NN show/VBP that/IN the/DT proposed/VBN ideas/NNS are/VBP crucial/JJ for/IN generalization/NN to/IN longer/JJR instructions/NNS as/RB well/RB as/IN unseen/JJ instructions/NNS ./.
