Accelerating/VBG the/DT inference/NN of/IN a/DT trained/VBN DNN/NN is/VBZ a/DT well/RB studied/VBN subject/NN ./.
In/IN this/DT paper/NN we/PRP switch/VBP the/DT focus/NN to/IN the/DT training/NN of/IN DNNs/NNS ./.
The/DT training/NN phase/NN is/VBZ compute/VB intensive/JJ ,/, demands/VBZ complicated/JJ data/NNS communication/NN ,/, and/CC contains/VBZ multiple/JJ levels/NNS of/IN data/NNS dependencies/NNS and/CC parallelism/NN ./.
This/DT paper/NN presents/VBZ an/DT algorithm/NN //HYPH architecture/NN space/NN exploration/NN of/IN efficient/JJ accelerators/NNS to/TO achieve/VB better/JJR network/NN convergence/NN rates/NNS and/CC higher/JJR energy/NN efficiency/NN for/IN training/NN DNNs/NNS ./.
We/PRP further/RB demonstrate/VBP that/IN an/DT architecture/NN with/IN hierarchical/JJ support/NN for/IN collective/JJ communication/NN semantics/NNS provides/VBZ flexibility/NN in/IN training/NN various/JJ networks/NNS performing/VBG both/CC stochastic/JJ and/CC batched/JJ gradient/NN descent/NN based/VBN techniques/NNS ./.
Our/PRP$ results/NNS suggest/VBP that/IN smaller/JJR networks/NNS favor/VBP non-batched/JJ techniques/NNS while/IN performance/NN for/IN larger/JJR networks/NNS is/VBZ higher/JJR using/VBG batched/VBN operations/NNS ./.
At/IN 45nm/JJ technology/NN ,/, CATERPILLAR/NNP achieves/VBZ performance/NN efficiencies/NNS of/IN 177/CD GFLOPS/NNS //SYM W/NN at/IN over/RB 80/CD percent/NN utilization/NN for/IN SGD/NNP training/NN on/IN small/JJ networks/NNS and/CC 211/CD GFLOPS/NNS //SYM W/NN at/IN over/IN 90/CD percent/NN utilization/NN for/IN pipelined/JJ SGD/NN //HYPH CP/NN training/NN on/IN larger/JJR networks/NNS using/VBG a/DT total/JJ area/NN of/IN 103.2/CD mm/NN $/$ ^/SYM 2/CD $/$ and/CC 178.9/CD mm/NN $/$ ^/SYM 2/CD $/$ respectively/RB ./.
