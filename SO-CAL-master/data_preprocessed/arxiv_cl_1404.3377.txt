We/PRP introduce/VBP a/DT novel/JJ approach/NN for/IN building/VBG language/NN models/NNS based/VBN on/IN a/DT systematic/JJ ,/, recursive/JJ exploration/NN of/IN skip/VB n/NN -/HYPH gram/NN models/NNS which/WDT are/VBP interpolated/VBN using/VBG modified/VBN Kneser/NNP -/HYPH Ney/NNP smoothing/NN ./.
Our/PRP$ approach/NN generalizes/VBZ language/NN models/NNS as/IN it/PRP contains/VBZ the/DT classical/JJ interpolation/NN with/IN lower/JJR order/NN models/NNS as/IN a/DT special/JJ case/NN ./.
In/IN this/DT paper/NN we/PRP motivate/VBP ,/, formalize/VB and/CC present/VB our/PRP$ approach/NN ./.
In/IN an/DT extensive/JJ empirical/JJ experiment/NN over/IN English/NNP text/NN corpora/NNS we/PRP demonstrate/VBP that/IN our/PRP$ generalized/VBN language/NN models/NNS lead/VBP to/IN a/DT substantial/JJ reduction/NN of/IN perplexity/NN between/IN 3.1/CD percent/NN and/CC 12.7/CD percent/NN in/IN comparison/NN to/IN traditional/JJ language/NN models/NNS using/VBG modified/VBN Kneser/NNP -/HYPH Ney/NNP smoothing/NN ./.
Furthermore/RB ,/, we/PRP investigate/VBP the/DT behaviour/NN over/IN three/CD other/JJ languages/NNS and/CC a/DT domain/NN specific/JJ corpus/NN where/WRB we/PRP observed/VBD consistent/JJ improvements/NNS ./.
Finally/RB ,/, we/PRP also/RB show/VBP that/IN the/DT strength/NN of/IN our/PRP$ approach/NN lies/VBZ in/IN its/PRP$ ability/NN to/TO cope/VB in/IN particular/JJ with/IN sparse/JJ training/NN data/NNS ./.
Using/VBG a/DT very/RB small/JJ training/NN data/NNS set/NN of/IN only/RB 736/CD KB/NNP text/NN we/PRP yield/VBP improvements/NNS of/IN even/RB 25.7/CD percent/NN reduction/NN of/IN perplexity/NN ./.
