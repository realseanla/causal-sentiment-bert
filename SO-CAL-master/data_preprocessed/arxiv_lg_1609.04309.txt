We/PRP propose/VBP an/DT approximate/JJ strategy/NN to/TO efficiently/RB train/VB neural/JJ network/NN based/VBN language/NN models/NNS over/IN very/RB large/JJ vocabularies/NNS ./.
Our/PRP$ approach/NN ,/, called/VBN adaptive/JJ softmax/NN ,/, circumvents/VBZ the/DT linear/JJ dependency/NN on/IN the/DT vocabulary/NN size/NN by/IN exploiting/VBG the/DT unbalanced/JJ word/NN distribution/NN to/TO form/VB clusters/NNS that/WDT explicitly/RB minimize/VB the/DT expectation/NN of/IN computational/JJ complexity/NN ./.
Our/PRP$ approach/NN further/RB reduces/VBZ the/DT computational/JJ cost/NN by/IN exploiting/VBG the/DT specificities/NNS of/IN modern/JJ architectures/NNS and/CC matrix/NN -/HYPH matrix/NN vector/NN operations/NNS ,/, making/VBG it/PRP particularly/RB suited/JJ for/IN graphical/JJ processing/NN units/NNS ./.
Our/PRP$ experiments/NNS carried/VBD out/RP on/IN standard/JJ benchmarks/NNS ,/, such/JJ as/IN EuroParl/NNP and/CC One/CD Billion/CD Word/NNP ,/, show/VBP that/IN our/PRP$ approach/NN brings/VBZ a/DT large/JJ gain/NN in/IN efficiency/NN over/IN standard/JJ approximations/NNS while/IN achieving/VBG an/DT accuracy/NN close/NN to/IN that/DT of/IN the/DT full/JJ softmax/NN ./.
