Softmax/NNP GAN/NNP is/VBZ a/DT novel/JJ variant/NN of/IN Generative/NNP Adversarial/NNP Network/NNP (/-LRB- GAN/NNP )/-RRB- ./.
The/DT key/JJ idea/NN of/IN Softmax/NNP GAN/NNP is/VBZ to/TO replace/VB the/DT classification/NN loss/NN in/IN the/DT original/JJ GAN/NNP with/IN a/DT softmax/JJ cross-entropy/JJ loss/NN in/IN the/DT sample/NN space/NN of/IN one/CD single/JJ batch/NN ./.
In/IN the/DT adversarial/JJ learning/NN of/IN $/$ N$/CD real/JJ training/NN samples/NNS and/CC $/$ M$/$ generated/VBN samples/NNS ,/, the/DT target/NN of/IN discriminator/NN training/NN is/VBZ to/TO distribute/VB all/PDT the/DT probability/NN mass/NN to/IN the/DT real/JJ samples/NNS ,/, each/DT with/IN probability/NN $/$ \/CD frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- M/NN }/-RRB- $/$ ,/, and/CC distribute/VB zero/CD probability/NN to/IN generated/VBN data/NNS ./.
In/IN the/DT generator/NN training/NN phase/NN ,/, the/DT target/NN is/VBZ to/TO assign/VB equal/JJ probability/NN to/IN all/DT data/NNS points/NNS in/IN the/DT batch/NN ,/, each/DT with/IN probability/NN $/$ \/CD frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- M/NN N/NN }/-RRB- $/$ ./.
While/IN the/DT original/JJ GAN/NNP is/VBZ closely/RB related/VBN to/IN Noise/NN Contrastive/JJ Estimation/NN (/-LRB- NCE/NN )/-RRB- ,/, we/PRP show/VBP that/IN Softmax/NNP GAN/NNP is/VBZ the/DT Importance/NNP Sampling/NNP version/NN of/IN GAN/NNP ./.
We/PRP futher/JJR demonstrate/VBP with/IN experiments/NNS that/IN this/DT simple/JJ change/NN stabilizes/VBZ GAN/NNP training/NN ./.
