Long/JJ text/NN brings/VBZ a/DT big/JJ challenge/NN to/IN semantic/JJ matching/NN due/IN to/IN their/PRP$ complicated/JJ semantic/JJ and/CC syntactic/JJ structures/NNS ./.
To/TO tackle/VB the/DT challenge/NN ,/, we/PRP consider/VBP using/VBG prior/JJ knowledge/NN to/TO help/VB identify/VB useful/JJ information/NN and/CC filter/NN out/IN noise/NN to/IN matching/VBG in/IN long/JJ text/NN ./.
To/IN this/DT end/NN ,/, we/PRP propose/VBP a/DT knowledge/NN enhanced/VBN hybrid/NN neural/JJ network/NN (/-LRB- KEHNN/NNP )/-RRB- ./.
The/DT model/NN fuses/VBZ prior/JJ knowledge/NN into/IN word/NN representations/NNS by/IN knowledge/NN gates/NNS and/CC establishes/VBZ three/CD matching/JJ channels/NNS with/IN words/NNS ,/, sequential/JJ structures/NNS of/IN sentences/NNS given/VBN by/IN Gated/VBN Recurrent/JJ Units/NNS (/-LRB- GRU/NNP )/-RRB- ,/, and/CC knowledge/NN enhanced/VBN representations/NNS ./.
The/DT three/CD channels/NNS are/VBP processed/VBN by/IN a/DT convolutional/JJ neural/JJ network/NN to/TO generate/VB high/JJ level/NN features/NNS for/IN matching/NN ,/, and/CC the/DT features/NNS are/VBP synthesized/VBN as/IN a/DT matching/JJ score/NN by/IN a/DT multilayer/JJ perceptron/NN ./.
The/DT model/NN extends/VBZ the/DT existing/VBG methods/NNS by/IN conducting/VBG matching/VBG on/IN words/NNS ,/, local/JJ structures/NNS of/IN sentences/NNS ,/, and/CC global/JJ context/NN of/IN sentences/NNS ./.
Evaluation/NN results/NNS from/IN extensive/JJ experiments/NNS on/IN public/JJ data/NN sets/NNS for/IN question/NN answering/NN and/CC conversation/NN show/VBP that/IN KEHNN/NNP can/MD significantly/RB outperform/VB the/DT -/HYPH state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN matching/NN models/NNS and/CC particularly/RB improve/VB the/DT performance/NN on/IN pairs/NNS with/IN long/JJ text/NN ./.
