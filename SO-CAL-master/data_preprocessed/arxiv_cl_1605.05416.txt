Recent/JJ work/NN in/IN learning/NN vector/NN -/HYPH space/NN embeddings/NNS for/IN multi-relational/JJ data/NNS has/VBZ focused/VBN on/IN combining/VBG relational/JJ information/NN derived/VBN from/IN knowledge/NN bases/NNS with/IN distributional/JJ information/NN derived/VBN from/IN large/JJ text/NN corpora/NNS ./.
We/PRP propose/VBP a/DT simple/JJ approach/NN that/WDT leverages/VBZ the/DT descriptions/NNS of/IN entities/NNS or/CC phrases/NNS available/JJ in/IN lexical/JJ resources/NNS ,/, in/IN conjunction/NN with/IN distributional/JJ semantics/NNS ,/, in/IN order/NN to/TO derive/VB a/DT better/JJR initialization/NN for/IN training/NN relational/JJ models/NNS ./.
Applying/VBG this/DT initialization/NN to/IN the/DT TransE/NN model/NN results/VBZ in/IN significant/JJ new/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performances/NNS on/IN the/DT WordNet/NNP dataset/NN ,/, decreasing/VBG the/DT mean/JJ rank/NN from/IN the/DT previous/JJ best/JJS of/IN 212/CD to/IN 51/CD ./.
It/PRP also/RB results/VBZ in/IN faster/JJR convergence/NN of/IN the/DT entity/NN representations/NNS ./.
We/PRP find/VBP that/IN there/EX is/VBZ a/DT trade/NN -/HYPH off/NN between/IN improving/VBG the/DT mean/JJ rank/NN and/CC the/DT hits@10/NN with/IN this/DT approach/NN ./.
This/DT illustrates/VBZ that/IN much/JJ remains/NNS to/TO be/VB understood/VBN regarding/VBG performance/NN improvements/NNS in/IN relational/JJ models/NNS ./.
