Although/IN a/DT number/NN of/IN auto/NN -/HYPH encoder/NN models/NNS enforce/VBP sparsity/NN explicitly/RB in/IN their/PRP$ learned/VBN representation/NN while/IN others/NNS do/VBP n't/RB ,/, there/EX has/VBZ been/VBN little/JJ formal/JJ analysis/NN on/IN what/WP encourages/VBZ sparsity/NN in/IN these/DT models/NNS in/IN general/JJ ./.
Therefore/RB ,/, our/PRP$ objective/NN here/RB is/VBZ to/TO formally/RB study/VB this/DT general/JJ problem/NN for/IN regularized/VBN auto/NN -/HYPH encoders/NNS ./.
We/PRP show/VBP that/IN both/DT regularization/NN and/CC activation/NN function/NN play/VB an/DT important/JJ role/NN in/IN encouraging/VBG sparsity/NN ./.
We/PRP provide/VBP sufficient/JJ conditions/NNS on/IN both/CC these/DT criteria/NNS and/CC show/VBP that/IN multiple/JJ popular/JJ models/NNS --/: like/IN De-noising/NN and/CC Contractive/JJ auto/NN -/HYPH encoder/NN --/: and/CC activations/NNS --/: like/IN Rectified/VBN Linear/NNP and/CC Sigmoid/NNP --/: satisfy/VBP these/DT conditions/NNS ;/: thus/RB explaining/VBG sparsity/NN in/IN their/PRP$ learned/VBN representation/NN ./.
Our/PRP$ theoretical/JJ and/CC empirical/JJ analysis/NN together/RB ,/, throws/VBZ light/NN on/IN the/DT properties/NNS of/IN regularization/NN //HYPH activation/NN that/WDT are/VBP conducive/JJ to/IN sparsity/NN ./.
As/IN a/DT by/IN -/HYPH product/NN of/IN the/DT insights/NNS gained/VBD from/IN our/PRP$ analysis/NN ,/, we/PRP also/RB propose/VBP a/DT new/JJ activation/NN function/NN that/WDT overcomes/VBZ the/DT individual/JJ drawbacks/NNS of/IN multiple/JJ existing/VBG activations/NNS (/-LRB- in/IN terms/NNS of/IN sparsity/NN )/-RRB- and/CC hence/RB produces/VBZ performance/NN at/IN par/NN (/-LRB- or/CC better/JJR )/-RRB- with/IN the/DT best/JJS performing/VBG activation/NN for/IN all/DT auto/NN -/HYPH encoder/NN models/NNS discussed/VBN ./.
