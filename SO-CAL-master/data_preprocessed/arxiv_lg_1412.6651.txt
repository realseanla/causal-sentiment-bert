We/PRP study/VBP the/DT problem/NN of/IN stochastic/JJ optimization/NN for/IN deep/JJ learning/NN in/IN the/DT parallel/JJ computing/NN environment/NN under/IN communication/NN constraints/NNS ./.
A/DT new/JJ algorithm/NN is/VBZ proposed/VBN in/IN this/DT setting/NN where/WRB the/DT communication/NN and/CC coordination/NN of/IN work/NN among/IN concurrent/JJ processes/NNS (/-LRB- local/JJ workers/NNS )/-RRB- ,/, is/VBZ based/VBN on/IN an/DT elastic/JJ force/NN which/WDT links/VBZ the/DT parameter/NN vectors/NNS they/PRP compute/VBP with/IN a/DT center/NN variable/JJ stored/VBN by/IN the/DT parameter/NN server/NN (/-LRB- master/NN )/-RRB- ./.
The/DT algorithm/NN enables/VBZ the/DT local/JJ workers/NNS to/TO perform/VB more/JJR exploration/NN ,/, i.e./FW the/DT algorithm/NN allows/VBZ the/DT local/JJ variables/NNS to/IN fluctuate/VB further/RB from/IN the/DT center/NN variable/NN by/IN reducing/VBG the/DT amount/NN of/IN communication/NN between/IN local/JJ workers/NNS and/CC the/DT master/NN ./.
We/PRP empirically/RB demonstrate/VBP that/IN in/IN the/DT deep/JJ learning/NN setting/NN ,/, due/IN to/IN the/DT existence/NN of/IN many/JJ local/JJ optima/NN ,/, allowing/VBG more/JJR exploration/NN can/MD lead/VB to/IN the/DT improved/VBN performance/NN ./.
We/PRP propose/VBP synchronous/JJ and/CC asynchronous/JJ variants/NNS of/IN the/DT new/JJ algorithm/NN ./.
We/PRP provide/VBP the/DT theoretical/JJ analysis/NN of/IN the/DT synchronous/JJ variant/NN in/IN the/DT quadratic/JJ case/NN and/CC prove/VB it/PRP achieves/VBZ the/DT highest/JJS possible/JJ asymptotic/JJ rate/NN of/IN convergence/NN for/IN the/DT center/NN variable/NN ./.
We/PRP additionally/RB propose/VBP the/DT momentum/NN -/HYPH based/VBN version/NN of/IN the/DT algorithm/NN that/WDT can/MD be/VB applied/VBN in/IN both/CC synchronous/JJ and/CC asynchronous/JJ settings/NNS ./.
An/DT asynchronous/JJ variant/NN of/IN the/DT algorithm/NN is/VBZ applied/VBN to/TO train/VB convolutional/JJ neural/JJ networks/NNS for/IN image/NN classification/NN on/IN the/DT CIFAR/NNP and/CC ImageNet/NNP datasets/NNS ./.
Experiments/NNS demonstrate/VBP that/IN the/DT new/JJ algorithm/NN accelerates/VBZ the/DT training/NN of/IN deep/JJ architectures/NNS compared/VBN to/IN DOWNPOUR/NN and/CC other/JJ common/JJ baseline/NN approaches/NNS and/CC furthermore/RB is/VBZ very/RB communication/NN efficient/JJ ./.
