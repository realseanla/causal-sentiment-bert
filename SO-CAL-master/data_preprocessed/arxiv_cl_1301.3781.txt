We/PRP propose/VBP two/CD novel/JJ model/NN architectures/NNS for/IN computing/VBG continuous/JJ vector/NN representations/NNS of/IN words/NNS from/IN very/RB large/JJ data/NNS sets/NNS ./.
The/DT quality/NN of/IN these/DT representations/NNS is/VBZ measured/VBN in/IN a/DT word/NN similarity/NN task/NN ,/, and/CC the/DT results/NNS are/VBP compared/VBN to/IN the/DT previously/RB best/JJS performing/VBG techniques/NNS based/VBN on/IN different/JJ types/NNS of/IN neural/JJ networks/NNS ./.
We/PRP observe/VBP large/JJ improvements/NNS in/IN accuracy/NN at/IN much/RB lower/JJR computational/JJ cost/NN ,/, i.e./FW it/PRP takes/VBZ less/JJR than/IN a/DT day/NN and/CC one/CD CPU/NN to/TO derive/VB high/JJ quality/NN 300/CD -/HYPH dimensional/JJ vectors/NNS for/IN one/CD million/CD vocabulary/NN from/IN a/DT 1.6/CD billion/CD words/NNS data/NNS set/NN ./.
Furthermore/RB ,/, we/PRP show/VBP that/IN these/DT vectors/NNS provide/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN our/PRP$ test/NN set/VBN for/IN measuring/VBG various/JJ types/NNS of/IN word/NN similarities/NNS ./.
We/PRP intend/VBP to/TO publish/VB this/DT test/NN set/VBN to/TO be/VB used/VBN by/IN the/DT research/NN community/NN ./.
