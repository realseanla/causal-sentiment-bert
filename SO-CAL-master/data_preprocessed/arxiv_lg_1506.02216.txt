In/IN this/DT paper/NN ,/, we/PRP explore/VBP the/DT inclusion/NN of/IN random/JJ variables/NNS into/IN the/DT dynamic/JJ latent/JJ state/NN of/IN a/DT recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- by/IN combining/VBG elements/NNS of/IN the/DT variational/JJ autoencoder/NN ./.
We/PRP argue/VBP that/IN through/IN the/DT use/NN of/IN high/JJ -/HYPH level/NN latent/JJ random/JJ variables/NNS ,/, our/PRP$ variational/JJ RNN/NN (/-LRB- VRNN/NN )/-RRB- is/VBZ able/JJ to/TO learn/VB to/TO model/VB the/DT kind/NN of/IN variability/NN observed/VBN in/IN highly/RB -/HYPH structured/VBN sequential/JJ data/NNS (/-LRB- such/JJ as/IN speech/NN )/-RRB- ./.
We/PRP empirically/RB evaluate/VB the/DT proposed/VBN model/NN against/IN related/VBN sequential/JJ models/NNS on/IN five/CD sequence/NN datasets/NNS ,/, four/CD of/IN speech/NN and/CC one/CD of/IN handwriting/NN ./.
Our/PRP$ results/NNS show/VBP the/DT importance/NN of/IN the/DT role/NN random/JJ variables/NNS can/MD play/VB in/IN the/DT RNN/NN dynamic/JJ latent/JJ state/NN ./.
