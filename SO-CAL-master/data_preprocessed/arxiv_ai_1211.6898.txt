We/PRP consider/VBP infinite/JJ -/HYPH horizon/NN stationary/JJ $/$ \/SYM gamma/NN $/$ -/HYPH discounted/VBN Markov/NNP Decision/NN Processes/NNS ,/, for/IN which/WDT it/PRP is/VBZ known/VBN that/IN there/EX exists/VBZ a/DT stationary/JJ optimal/JJ policy/NN ./.
Using/VBG Value/NNP and/CC Policy/NNP Iteration/NNP with/IN some/DT error/NN $/$ \/CD epsilon/CD $/$ at/IN each/DT iteration/NN ,/, it/PRP is/VBZ well/RB -/HYPH known/VBN that/IN one/PRP can/MD compute/VB stationary/JJ policies/NNS that/WDT are/VBP $/$ \/CD frac/NN {/-LRB- 2/CD \/SYM gamma/NN }/-RRB- {/-LRB- (/-LRB- 1/CD -/HYPH \/SYM gamma/NN )/-RRB- ^/SYM 2/CD }/-RRB- \/SYM epsilon/NN $/$ -/HYPH optimal/JJ ./.
After/IN arguing/VBG that/IN this/DT guarantee/NN is/VBZ tight/JJ ,/, we/PRP develop/VBP variations/NNS of/IN Value/NNP and/CC Policy/NNP Iteration/NNP for/IN computing/VBG non-stationary/JJ policies/NNS that/WDT can/MD be/VB up/RB to/IN $/$ \/CD frac/NN {/-LRB- 2/CD \/SYM gamma/NN }/-RRB- {/-LRB- 1/CD -/HYPH \/SYM gamma/NN }/-RRB- \/SYM epsilon/NN $/$ -/HYPH optimal/JJ ,/, which/WDT constitutes/VBZ a/DT significant/JJ improvement/NN in/IN the/DT usual/JJ situation/NN when/WRB $/$ \/SYM gamma/NN $/$ is/VBZ close/JJ to/IN 1/CD ./.
Surprisingly/RB ,/, this/DT shows/VBZ that/IN the/DT problem/NN of/IN "/`` computing/VBG near/RB -/HYPH optimal/JJ non-stationary/JJ policies/NNS "/'' is/VBZ much/JJ simpler/JJR than/IN that/DT of/IN "/`` computing/VBG near/RB -/HYPH optimal/JJ stationary/JJ policies/NNS "/'' ./.
