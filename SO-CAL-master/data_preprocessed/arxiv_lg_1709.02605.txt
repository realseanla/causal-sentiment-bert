Kernel/NNP methods/NNS have/VBP recently/RB attracted/VBN resurgent/JJ interest/NN ,/, matching/VBG the/DT performance/NN of/IN deep/JJ neural/JJ networks/NNS in/IN tasks/NNS such/JJ as/IN speech/NN recognition/NN ./.
The/DT random/JJ Fourier/NN features/VBZ map/NN is/VBZ a/DT technique/NN commonly/RB used/VBN to/TO scale/VB up/RP kernel/NN machines/NNS ,/, but/CC employing/VBG the/DT randomized/JJ feature/NN map/NN means/VBZ that/IN $/$ O/UH (/-LRB- \/SYM epsilon/NN ^/SYM {/-LRB- -/HYPH 2/CD }/-RRB- )/-RRB- $/$ samples/NNS are/VBP required/VBN to/TO achieve/VB an/DT approximation/NN error/NN of/IN at/IN most/RBS $/$ \/CD epsilon/CD $/$ ./.
In/IN this/DT paper/NN ,/, we/PRP investigate/VBP some/DT alternative/JJ schemes/NNS for/IN constructing/VBG feature/NN maps/NNS that/WDT are/VBP deterministic/JJ ,/, rather/RB than/IN random/JJ ,/, by/IN approximating/VBG the/DT kernel/NN in/IN the/DT frequency/NN domain/NN using/VBG Gaussian/NNP quadrature/NN ./.
We/PRP show/VBP that/IN deterministic/JJ feature/NN maps/NNS can/MD be/VB constructed/VBN ,/, for/IN any/DT $/$ \/SYM gamma/NN &gt;/SYM 0/CD $/$ ,/, to/TO achieve/VB error/NN $/$ \/CD epsilon/CD $/$ with/IN $/$ O/UH (/-LRB- e/NN ^/SYM {/-LRB- \/SYM gamma/NN }/-RRB- \/SYM epsilon/NN ^/SYM {/-LRB- -/HYPH 1/CD //HYPH \/SYM gamma/NN }/-RRB- )/-RRB- $/$ samples/NNS as/IN $/$ \/CD epsilon/CD $/$ goes/VBZ to/IN 0/CD ./.
We/PRP validate/VBP our/PRP$ methods/NNS on/IN datasets/NNS in/IN different/JJ domains/NNS ,/, such/JJ as/IN MNIST/NNP and/CC TIMIT/NNP ,/, showing/VBG that/IN deterministic/JJ features/NNS are/VBP faster/RBR to/TO generate/VB and/CC achieve/VB comparable/JJ accuracy/NN to/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN kernel/NN methods/NNS based/VBN on/IN random/JJ Fourier/NN features/NNS ./.
