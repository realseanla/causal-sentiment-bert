Gaussian/JJ Process/NN bandit/NN optimization/NN has/VBZ emerged/VBN as/IN a/DT powerful/JJ tool/NN for/IN optimizing/VBG noisy/JJ black/JJ box/NN functions/NNS ./.
One/CD example/NN in/IN machine/NN learning/NN is/VBZ hyper/JJ -/HYPH parameter/NN optimization/NN where/WRB each/DT evaluation/NN of/IN the/DT target/NN function/NN requires/VBZ training/VBG a/DT model/NN which/WDT may/MD involve/VB days/NNS or/CC even/RB weeks/NNS of/IN computation/NN ./.
Most/JJS methods/NNS for/IN this/DT so/RB -/HYPH called/VBN "/`` Bayesian/JJ optimization/NN "/'' only/RB allow/VBP sequential/JJ exploration/NN of/IN the/DT parameter/NN space/NN ./.
However/RB ,/, it/PRP is/VBZ often/RB desirable/JJ to/TO propose/VB batches/NNS or/CC sets/NNS of/IN parameter/NN values/NNS to/TO explore/VB simultaneously/RB ,/, especially/RB when/WRB there/EX are/VBP large/JJ parallel/JJ processing/NN facilities/NNS at/IN our/PRP$ disposal/NN ./.
Batch/NN methods/NNS require/VBP modeling/VBG the/DT interaction/NN between/IN the/DT different/JJ evaluations/NNS in/IN the/DT batch/NN ,/, which/WDT can/MD be/VB expensive/JJ in/IN complex/JJ scenarios/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ approach/NN for/IN parallelizing/VBG Bayesian/JJ optimization/NN by/IN modeling/VBG the/DT diversity/NN of/IN a/DT batch/NN via/IN Determinantal/JJ point/NN processes/NNS (/-LRB- DPPs/NNS )/-RRB- whose/WP$ kernels/NNS are/VBP learned/VBN automatically/RB ./.
This/DT allows/VBZ us/PRP to/TO generalize/VB a/DT previous/JJ result/NN as/RB well/RB as/IN prove/VB better/JJR regret/NN bounds/NNS based/VBN on/IN DPP/NNP sampling/NN ./.
Our/PRP$ experiments/NNS on/IN a/DT variety/NN of/IN synthetic/JJ and/CC real/JJ -/HYPH world/NN robotics/NNS and/CC hyper/JJ -/HYPH parameter/NN optimization/NN tasks/NNS indicate/VBP that/IN our/PRP$ DPP/NNP -/HYPH based/VBN methods/NNS ,/, especially/RB those/DT based/VBN on/IN DPP/NNP sampling/NN ,/, outperform/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS ./.
