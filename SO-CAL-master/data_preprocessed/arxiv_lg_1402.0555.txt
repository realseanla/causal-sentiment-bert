We/PRP present/VBP a/DT new/JJ algorithm/NN for/IN the/DT contextual/JJ bandit/NN learning/NN problem/NN ,/, where/WRB the/DT learner/NN repeatedly/RB takes/VBZ an/DT action/NN in/IN response/NN to/IN the/DT observed/VBN context/NN ,/, observing/VBG the/DT reward/NN only/RB for/IN that/DT action/NN ./.
Our/PRP$ method/NN assumes/VBZ access/NN to/IN an/DT oracle/NN for/IN solving/VBG cost/NN -/HYPH sensitive/JJ classification/NN problems/NNS and/CC achieves/VBZ the/DT statistically/RB optimal/JJ regret/NN guarantee/NN with/IN only/RB $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ oracle/NN calls/VBZ across/IN all/DT $/$ T$/CD rounds/NNS ./.
By/IN doing/VBG so/RB ,/, we/PRP obtain/VBP the/DT most/RBS practical/JJ contextual/JJ bandit/NN learning/VBG algorithm/NN amongst/IN approaches/NNS that/WDT work/VBP for/IN general/JJ policy/NN classes/NNS ./.
We/PRP further/RB conduct/VB a/DT proof/NN -/HYPH of/IN -/HYPH concept/NN experiment/NN which/WDT demonstrates/VBZ the/DT excellent/JJ computational/JJ and/CC prediction/NN performance/NN of/IN (/-LRB- an/DT online/JJ variant/NN of/IN )/-RRB- our/PRP$ algorithm/NN relative/JJ to/IN several/JJ baselines/NNS ./.
