Consider/VB learning/VBG a/DT policy/NN from/IN example/NN expert/NN behavior/NN ,/, without/IN interaction/NN with/IN the/DT expert/NN or/CC access/NN to/IN reinforcement/NN signal/NN ./.
One/CD approach/NN is/VBZ to/TO recover/VB the/DT expert/NN 's/POS cost/NN function/NN with/IN inverse/JJ reinforcement/NN learning/NN ,/, then/RB extract/VB a/DT policy/NN from/IN that/DT cost/NN function/NN with/IN reinforcement/NN learning/NN ./.
This/DT approach/NN is/VBZ indirect/JJ and/CC can/MD be/VB slow/JJ ./.
We/PRP propose/VBP a/DT new/JJ general/JJ framework/NN for/IN directly/RB extracting/VBG a/DT policy/NN from/IN data/NNS ,/, as/IN if/IN it/PRP were/VBD obtained/VBN by/IN reinforcement/NN learning/VBG following/VBG inverse/JJ reinforcement/NN learning/NN ./.
We/PRP show/VBP that/IN a/DT certain/JJ instantiation/NN of/IN our/PRP$ framework/NN draws/VBZ an/DT analogy/NN between/IN imitation/NN learning/NN and/CC generative/JJ adversarial/JJ networks/NNS ,/, from/IN which/WDT we/PRP derive/VBP a/DT model/NN -/HYPH free/JJ imitation/NN learning/NN algorithm/NN that/WDT obtains/VBZ significant/JJ performance/NN gains/NNS over/IN existing/VBG model/NN -/HYPH free/JJ methods/NNS in/IN imitating/VBG complex/JJ behaviors/NNS in/IN large/JJ ,/, high/JJ -/HYPH dimensional/JJ environments/NNS ./.
