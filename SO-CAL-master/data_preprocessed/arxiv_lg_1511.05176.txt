Deep/JJ neural/JJ networks/NNS are/VBP powerful/JJ parametric/JJ models/NNS that/WDT can/MD be/VB trained/VBN efficiently/RB using/VBG the/DT backpropagation/NN algorithm/NN ./.
Stochastic/JJ neural/JJ networks/NNS combine/VBP the/DT power/NN of/IN large/JJ parametric/JJ functions/NNS with/IN that/DT of/IN graphical/JJ models/NNS ,/, which/WDT makes/VBZ it/PRP possible/JJ to/TO learn/VB very/RB complex/JJ distributions/NNS ./.
However/RB ,/, as/IN backpropagation/NN is/VBZ not/RB directly/RB applicable/JJ to/IN stochastic/JJ networks/NNS that/WDT include/VBP discrete/JJ sampling/NN operations/NNS within/IN their/PRP$ computational/JJ graph/NN ,/, training/VBG such/JJ networks/NNS remains/VBZ difficult/JJ ./.
We/PRP present/VBP MuProp/NNP ,/, an/DT unbiased/JJ gradient/NN estimator/NN for/IN stochastic/JJ networks/NNS ,/, designed/VBN to/TO make/VB this/DT task/NN easier/JJR ./.
MuProp/NNP improves/VBZ on/IN the/DT likelihood/NN -/HYPH ratio/NN estimator/NN by/IN reducing/VBG its/PRP$ variance/NN using/VBG a/DT control/NN variate/NN based/VBN on/IN the/DT first/JJ -/HYPH order/NN Taylor/NNP expansion/NN of/IN a/DT mean/NN -/HYPH field/NN network/NN ./.
Crucially/RB ,/, unlike/IN prior/JJ attempts/NNS at/IN using/VBG backpropagation/NN for/IN training/NN stochastic/JJ networks/NNS ,/, the/DT resulting/VBG estimator/NN is/VBZ unbiased/JJ and/CC well/RB behaved/VBD ./.
Our/PRP$ experiments/NNS on/IN structured/JJ output/NN prediction/NN and/CC discrete/JJ latent/JJ variable/JJ modeling/NN demonstrate/VBP that/IN MuProp/NNP yields/VBZ consistently/RB good/JJ performance/NN across/IN a/DT range/NN of/IN difficult/JJ tasks/NNS ./.
