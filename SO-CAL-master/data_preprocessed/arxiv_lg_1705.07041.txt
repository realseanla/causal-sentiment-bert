We/PRP present/VBP an/DT algorithm/NN based/VBN on/IN posterior/JJ sampling/NN (/-LRB- aka/RB Thompson/NNP sampling/NN )/-RRB- that/WDT achieves/VBZ near/IN -/HYPH optimal/JJ worst/JJS -/HYPH case/NN regret/NN bounds/NNS when/WRB the/DT underlying/VBG Markov/NNP Decision/NN Process/NN (/-LRB- MDP/NN )/-RRB- is/VBZ communicating/VBG with/IN a/DT finite/NN ,/, though/RB unknown/JJ ,/, diameter/NN ./.
Our/PRP$ main/JJ result/NN is/VBZ a/DT high/JJ probability/NN regret/NN upper/JJ bound/VBN of/IN $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- D/NN \/SYM sqrt/NN {/-LRB- SAT/NN }/-RRB- )/-RRB- $/$ for/IN any/DT communicating/VBG MDP/NN with/IN $/$ S$/CD states/NNS ,/, $/$ A$/$ actions/NNS and/CC diameter/NN $/$ D$/CD ,/, when/WRB $/$ T/NN \/SYM ge/NN S/NN ^/SYM 5A/NN $/$ ./.
Here/RB ,/, regret/NN compares/VBZ the/DT total/JJ reward/NN achieved/VBN by/IN the/DT algorithm/NN to/IN the/DT total/NN expected/VBN reward/NN of/IN an/DT optimal/JJ infinite/JJ -/HYPH horizon/NN undiscounted/JJ average/JJ reward/NN policy/NN ,/, in/IN time/NN horizon/NN $/$ T$/CD ./.
This/DT result/NN improves/VBZ over/IN the/DT best/JJS previously/RB known/VBN upper/JJ bound/VBN of/IN $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- DS/NNP \/SYM sqrt/NNP {/-LRB- AT/NN }/-RRB- )/-RRB- $/$ achieved/VBN by/IN any/DT algorithm/NN in/IN this/DT setting/NN ,/, and/CC matches/VBZ the/DT dependence/NN on/IN $/$ S$/CD in/IN the/DT established/VBN lower/JJR bound/VBN of/IN $/$ \/SYM Omega/NN (/-LRB- \/SYM sqrt/NN {/-LRB- DSAT/NN }/-RRB- )/-RRB- $/$ for/IN this/DT problem/NN ./.
Our/PRP$ techniques/NNS involve/VBP proving/VBG some/DT novel/JJ results/NNS about/IN the/DT anti-concentration/NN of/IN Dirichlet/NNP distribution/NN ,/, which/WDT may/MD be/VB of/IN independent/JJ interest/NN ./.
