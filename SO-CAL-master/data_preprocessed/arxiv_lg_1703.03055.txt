This/DT paper/NN develops/VBZ a/DT general/JJ framework/NN for/IN learning/VBG interpretable/JJ data/NNS representation/NN via/IN Long/NNP Short/NNP -/HYPH Term/NNP Memory/NN (/-LRB- LSTM/NN )/-RRB- recurrent/JJ neural/JJ networks/NNS over/IN hierarchal/JJ graph/NN structures/NNS ./.
Instead/RB of/IN learning/VBG LSTM/NNP models/NNS over/IN the/DT pre-fixed/JJ structures/NNS ,/, we/PRP propose/VBP to/TO further/RB learn/VB the/DT intermediate/JJ interpretable/JJ multi-level/JJ graph/NN structures/NNS in/IN a/DT progressive/JJ and/CC stochastic/JJ way/NN from/IN data/NNS during/IN the/DT LSTM/NNP network/NN optimization/NN ./.
We/PRP thus/RB call/VB this/DT model/NN the/DT structure/NN -/HYPH evolving/VBG LSTM/NNP ./.
In/IN particular/JJ ,/, starting/VBG with/IN an/DT initial/JJ element/NN -/HYPH level/NN graph/NN representation/NN where/WRB each/DT node/NN is/VBZ a/DT small/JJ data/NNS element/NN ,/, the/DT structure/NN -/HYPH evolving/VBG LSTM/NNP gradually/RB evolves/VBZ the/DT multi-level/JJ graph/NN representations/NNS by/IN stochastically/RB merging/VBG the/DT graph/NN nodes/NNS with/IN high/JJ compatibilities/NNS along/IN the/DT stacked/VBN LSTM/NNP layers/NNS ./.
In/IN each/DT LSTM/NNP layer/NN ,/, we/PRP estimate/VBP the/DT compatibility/NN of/IN two/CD connected/JJ nodes/NNS from/IN their/PRP$ corresponding/VBG LSTM/NNP gate/NN outputs/NNS ,/, which/WDT is/VBZ used/VBN to/TO generate/VB a/DT merging/VBG probability/NN ./.
The/DT candidate/NN graph/NN structures/NNS are/VBP accordingly/RB generated/VBN where/WRB the/DT nodes/NNS are/VBP grouped/VBN into/IN cliques/NNS with/IN their/PRP$ merging/VBG probabilities/NNS ./.
We/PRP then/RB produce/VBP the/DT new/JJ graph/NN structure/NN with/IN a/DT Metropolis/NNP -/HYPH Hasting/NNP algorithm/NN ,/, which/WDT alleviates/VBZ the/DT risk/NN of/IN getting/VBG stuck/VBN in/IN local/JJ optimums/NNS by/IN stochastic/JJ sampling/NN with/IN an/DT acceptance/NN probability/NN ./.
Once/IN a/DT graph/NN structure/NN is/VBZ accepted/VBN ,/, a/DT higher/JJR -/HYPH level/NN graph/NN is/VBZ then/RB constructed/VBN by/IN taking/VBG the/DT partitioned/VBN cliques/NNS as/IN its/PRP$ nodes/NNS ./.
During/IN the/DT evolving/VBG process/NN ,/, representation/NN becomes/VBZ more/JJR abstracted/JJ in/IN higher/JJR -/HYPH levels/NNS where/WRB redundant/JJ information/NN is/VBZ filtered/VBN out/RP ,/, allowing/VBG more/RBR efficient/JJ propagation/NN of/IN long/JJ -/HYPH range/NN data/NNS dependencies/NNS ./.
We/PRP evaluate/VBP the/DT effectiveness/NN of/IN structure/NN -/HYPH evolving/VBG LSTM/NNP in/IN the/DT application/NN of/IN semantic/JJ object/NN parsing/VBG and/CC demonstrate/VB its/PRP$ advantage/NN over/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN LSTM/NN models/NNS on/IN standard/JJ benchmarks/NNS ./.
