Recent/JJ studies/NNS have/VBP shown/VBN that/IN embedding/VBG textual/JJ relations/NNS using/VBG deep/JJ neural/JJ networks/NNS greatly/RB helps/VBZ relation/NN extraction/NN ./.
However/RB ,/, many/JJ existing/VBG studies/NNS rely/VBP on/IN supervised/JJ learning/NN ;/: their/PRP$ performance/NN is/VBZ dramatically/RB limited/VBN by/IN the/DT availability/NN of/IN training/NN data/NNS ./.
In/IN this/DT work/NN ,/, we/PRP generalize/VBP textual/JJ relation/NN embedding/NN to/IN the/DT distant/JJ supervision/NN setting/NN ,/, where/WRB much/RB larger/JJR -/HYPH scale/NN but/CC noisy/JJ training/NN data/NNS is/VBZ available/JJ ./.
We/PRP propose/VBP leveraging/VBG global/JJ statistics/NNS of/IN relations/NNS ,/, i.e./FW ,/, the/DT co-occurrence/NN statistics/NNS of/IN textual/JJ and/CC knowledge/NN base/NN relations/NNS collected/VBN from/IN the/DT entire/JJ corpus/NN ,/, to/TO embed/VB textual/JJ relations/NNS ./.
This/DT approach/NN turns/VBZ out/RP to/TO be/VB more/RBR robust/JJ to/IN the/DT training/NN noise/NN introduced/VBN by/IN distant/JJ supervision/NN ./.
On/IN a/DT popular/JJ relation/NN extraction/NN dataset/NN ,/, we/PRP show/VBP that/IN the/DT learned/VBN textual/JJ relation/NN embeddings/NNS can/MD be/VB used/VBN to/TO augment/VB existing/VBG relation/NN extraction/NN models/NNS and/CC significantly/RB improve/VB their/PRP$ performance/NN ./.
Most/JJS remarkably/RB ,/, for/IN the/DT top/JJ 1,000/CD relational/JJ facts/NNS discovered/VBN by/IN the/DT best/JJS existing/VBG model/NN ,/, the/DT precision/NN can/MD be/VB improved/VBN from/IN 83.9/CD percent/NN to/IN 89.3/CD percent/NN ./.
