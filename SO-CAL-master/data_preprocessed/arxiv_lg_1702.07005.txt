In/IN this/DT work/NN we/PRP propose/VBP an/DT accelerated/VBN stochastic/JJ learning/NN system/NN for/IN very/RB large/JJ -/HYPH scale/NN applications/NNS ./.
Acceleration/NN is/VBZ achieved/VBN by/IN mapping/VBG the/DT training/NN algorithm/NN onto/IN massively/RB parallel/JJ processors/NNS :/: we/PRP demonstrate/VBP a/DT parallel/NN ,/, asynchronous/JJ GPU/NNP implementation/NN of/IN the/DT widely/RB used/VBN stochastic/JJ coordinate/NN descent/NN //HYPH ascent/NN algorithm/NN that/WDT can/MD provide/VB up/RP to/IN 35x/CD speed/NN -/HYPH up/NN over/IN a/DT sequential/JJ CPU/NN implementation/NN ./.
In/IN order/NN to/TO train/VB on/IN very/RB large/JJ datasets/NNS that/WDT do/VBP not/RB fit/VB inside/IN the/DT memory/NN of/IN a/DT single/JJ GPU/NNP ,/, we/PRP then/RB consider/VBP techniques/NNS for/IN distributed/VBN stochastic/JJ learning/NN ./.
We/PRP propose/VBP a/DT novel/JJ method/NN for/IN optimally/RB aggregating/VBG model/NN updates/NNS from/IN worker/NN nodes/NNS when/WRB the/DT training/NN data/NNS is/VBZ distributed/VBN either/CC by/IN example/NN or/CC by/IN feature/NN ./.
Using/VBG this/DT technique/NN ,/, we/PRP demonstrate/VBP that/IN one/PRP can/MD scale/VB out/RP stochastic/JJ learning/NN across/IN up/IN to/TO 8/CD worker/NN nodes/NNS without/IN any/DT significant/JJ loss/NN of/IN training/NN time/NN ./.
Finally/RB ,/, we/PRP combine/VBP GPU/NNP acceleration/NN with/IN the/DT optimized/VBN distributed/VBN method/NN to/TO train/VB on/IN a/DT dataset/NN consisting/VBG of/IN 200/CD million/CD training/NN examples/NNS and/CC 75/CD million/CD features/NNS ./.
We/PRP show/VBP by/IN scaling/VBG out/RP across/IN 4/CD GPUs/NNS ,/, one/PRP can/MD attain/VB a/DT high/JJ degree/NN of/IN training/NN accuracy/NN in/IN around/RB 4/CD seconds/NNS :/: a/DT 20x/NN speed/NN -/HYPH up/NN in/IN training/NN time/NN compared/VBN to/IN a/DT multi-threaded/JJ ,/, distributed/VBN implementation/NN across/IN 4/CD CPUs/NNS ./.
