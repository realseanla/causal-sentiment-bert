The/DT graduated/VBD optimization/NN approach/NN ,/, also/RB known/VBN as/IN the/DT continuation/NN method/NN ,/, is/VBZ a/DT popular/JJ heuristic/NN to/IN solving/VBG non-convex/NN problems/NNS that/WDT has/VBZ received/VBN renewed/VBN interest/NN over/IN the/DT last/JJ decade/NN ./.
Despite/IN its/PRP$ popularity/NN ,/, very/RB little/RB is/VBZ known/VBN in/IN terms/NNS of/IN theoretical/JJ convergence/NN analysis/NN ./.
In/IN this/DT paper/NN we/PRP describe/VBP a/DT new/JJ first/JJ -/HYPH order/NN algorithm/NN based/VBN on/IN graduated/VBN optimiza/NN -/HYPH tion/NN and/CC analyze/VB its/PRP$ performance/NN ./.
We/PRP characterize/VBP a/DT parameterized/JJ family/NN of/IN non/AFX -/HYPH convex/NN functions/NNS for/IN which/WDT this/DT algorithm/NN provably/RB converges/VBZ to/IN a/DT global/JJ optimum/JJ ./.
In/IN particular/JJ ,/, we/PRP prove/VBP that/IN the/DT algorithm/NN converges/VBZ to/IN an/DT {/-LRB- \/SYM epsilon/NN }/-RRB- -/HYPH approximate/JJ solution/NN within/IN O/NN (/-LRB- 1/CD //SYM \/SYM epsilon/SYM ^/SYM 2/CD )/-RRB- gradient/NN -/HYPH based/VBN steps/NNS ./.
We/PRP extend/VBP our/PRP$ algorithm/NN and/CC analysis/NN to/IN the/DT setting/NN of/IN stochastic/JJ non-convex/JJ optimization/NN with/IN noisy/JJ gradient/NN feedback/NN ,/, attaining/VBG the/DT same/JJ convergence/NN rate/NN ./.
Additionally/RB ,/, we/PRP discuss/VBP the/DT setting/NN of/IN zero/CD -/HYPH order/NN optimization/NN ,/, and/CC devise/VB a/DT a/DT variant/NN of/IN our/PRP$ algorithm/NN which/WDT converges/VBZ at/IN rate/NN of/IN O/NN (/-LRB- d/NN ^/SYM 2/CD //SYM \/SYM epsilon/SYM ^/SYM 4/CD )/-RRB- ./.
