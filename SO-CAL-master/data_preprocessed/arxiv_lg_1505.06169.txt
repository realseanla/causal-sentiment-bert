We/PRP present/VBP paired/VBN learning/NN and/CC inference/NN algorithms/NNS for/IN significantly/RB reducing/VBG computation/NN and/CC increasing/VBG speed/NN of/IN the/DT vector/NN dot/NN products/NNS in/IN the/DT classifiers/NNS that/WDT are/VBP at/IN the/DT heart/NN of/IN many/JJ NLP/NN components/NNS ./.
This/DT is/VBZ accomplished/VBN by/IN partitioning/VBG the/DT features/NNS into/IN a/DT sequence/NN of/IN templates/NNS which/WDT are/VBP ordered/VBN such/JJ that/IN high/JJ confidence/NN can/MD often/RB be/VB reached/VBN using/VBG only/RB a/DT small/JJ fraction/NN of/IN all/DT features/NNS ./.
Parameter/NN estimation/NN is/VBZ arranged/VBN to/TO maximize/VB accuracy/NN and/CC early/JJ confidence/NN in/IN this/DT sequence/NN ./.
Our/PRP$ approach/NN is/VBZ simpler/JJR and/CC better/JJR suited/JJ to/IN NLP/NN than/IN other/JJ related/JJ cascade/NN methods/NNS ./.
We/PRP present/VBP experiments/NNS in/IN left/JJ -/HYPH to/TO -/HYPH right/JJ part/NN -/HYPH of/IN -/HYPH speech/NN tagging/NN ,/, named/VBN entity/NN recognition/NN ,/, and/CC transition/NN -/HYPH based/VBN dependency/NN parsing/VBG ./.
On/IN the/DT typical/JJ benchmarking/NN datasets/NNS we/PRP can/MD preserve/VB POS/NN tagging/VBG accuracy/NN above/IN 97/CD percent/NN and/CC parsing/VBG LAS/NN above/IN 88.5/CD percent/NN both/CC with/IN over/IN a/DT five-fold/RB reduction/NN in/IN run/NN -/HYPH time/NN ,/, and/CC NER/NN F1/NN above/IN 88/CD with/IN more/JJR than/IN 2x/NN increase/NN in/IN speed/NN ./.
