We/PRP present/VBP a/DT sequential/JJ model/NN for/IN temporal/JJ relation/NN classification/NN between/IN intra-sentence/JJ events/NNS ./.
The/DT key/JJ observation/NN is/VBZ that/IN the/DT overall/JJ syntactic/JJ structure/NN and/CC compositional/JJ meanings/NNS of/IN the/DT multi-word/JJ context/NN between/IN events/NNS are/VBP important/JJ for/IN distinguishing/VBG among/IN fine/JJ -/HYPH grained/JJ temporal/JJ relations/NNS ./.
Specifically/RB ,/, our/PRP$ approach/NN first/JJ extracts/NNS a/DT sequence/NN of/IN context/NN words/NNS that/WDT indicates/VBZ the/DT temporal/JJ relation/NN between/IN two/CD events/NNS ,/, which/WDT well/RB align/VB with/IN the/DT dependency/NN path/NN between/IN two/CD event/NN mentions/VBZ ./.
The/DT context/NN word/NN sequence/NN ,/, together/RB with/IN a/DT parts/NNS -/HYPH of/IN -/HYPH speech/NN tag/NN sequence/NN and/CC a/DT dependency/NN relation/NN sequence/NN that/WDT are/VBP generated/VBN corresponding/VBG to/IN the/DT word/NN sequence/NN ,/, are/VBP then/RB provided/VBN as/IN input/NN to/IN bidirectional/JJ recurrent/JJ neural/JJ network/NN (/-LRB- LSTM/NN )/-RRB- models/NNS ./.
The/DT neural/JJ nets/NNS learn/VBP compositional/JJ syntactic/JJ and/CC semantic/JJ representations/NNS of/IN contexts/NNS surrounding/VBG the/DT two/CD events/NNS and/CC predict/VB the/DT temporal/JJ relation/NN between/IN them/PRP ./.
Evaluation/NN of/IN the/DT proposed/VBN approach/NN on/IN TimeBank/NNP corpus/NN shows/VBZ that/IN sequential/JJ modeling/NN is/VBZ capable/JJ of/IN accurately/RB recognizing/VBG temporal/JJ relations/NNS between/IN events/NNS ,/, which/WDT outperforms/VBZ a/DT neural/JJ net/NN model/NN using/VBG various/JJ discrete/JJ features/NNS as/IN input/NN that/WDT imitates/VBZ previous/JJ feature/NN based/VBN models/NNS ./.
