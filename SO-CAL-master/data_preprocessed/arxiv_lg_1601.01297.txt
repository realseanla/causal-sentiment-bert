We/PRP train/VBP a/DT reinforcement/NN learner/NN to/TO play/VB a/DT simplified/JJ version/NN of/IN the/DT game/NN Angry/NNP Birds/NNP ./.
The/DT learner/NN is/VBZ provided/VBN with/IN a/DT game/NN state/NN in/IN a/DT manner/NN similar/JJ to/IN the/DT output/NN that/WDT could/MD be/VB produced/VBN by/IN computer/NN vision/NN algorithms/NNS ./.
We/PRP improve/VBP on/IN the/DT efficiency/NN of/IN regular/JJ {/-LRB- \/SYM epsilon/NN }/-RRB- -/HYPH greedy/JJ Q/NN -/HYPH Learning/VBG with/IN linear/JJ function/NN approximation/NN through/IN more/JJR systematic/JJ exploration/NN in/IN Randomized/VBN Least/JJS Squares/NNPS Value/NNP Iteration/NNP (/-LRB- RLSVI/NN )/-RRB- ,/, an/DT algorithm/NN that/WDT samples/VBZ its/PRP$ policy/NN from/IN a/DT posterior/JJ distribution/NN on/IN optimal/JJ policies/NNS ./.
With/IN larger/JJR state/NN -/HYPH action/NN spaces/NNS ,/, efficient/JJ exploration/NN becomes/VBZ increasingly/RB important/JJ ,/, as/IN evidenced/VBN by/IN the/DT faster/RBR learning/VBG in/IN RLSVI/NN ./.
