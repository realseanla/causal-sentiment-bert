Recurrent/JJ neural/JJ networks/NNS are/VBP powerful/JJ models/NNS for/IN processing/VBG sequential/JJ data/NNS ,/, but/CC they/PRP are/VBP generally/RB plagued/VBN by/IN vanishing/VBG and/CC exploding/VBG gradient/NN problems/NNS ./.
Unitary/NNP recurrent/JJ neural/JJ networks/NNS (/-LRB- uRNNs/NNS )/-RRB- ,/, which/WDT use/VBP unitary/JJ recurrence/NN matrices/NNS ,/, have/VBP recently/RB been/VBN proposed/VBN as/IN a/DT means/NN to/TO avoid/VB these/DT issues/NNS ./.
However/RB ,/, in/IN previous/JJ experiments/NNS ,/, the/DT recurrence/NN matrices/NNS were/VBD restricted/VBN to/TO be/VB a/DT product/NN of/IN parameterized/JJ unitary/JJ matrices/NNS ,/, and/CC an/DT open/JJ question/NN remains/VBZ :/: when/WRB does/VBZ such/PDT a/DT parameterization/NN fail/VBP to/TO represent/VB all/DT unitary/JJ matrices/NNS ,/, and/CC how/WRB does/VBZ this/DT restricted/JJ representational/JJ capacity/NN limit/VB what/WP can/MD be/VB learned/VBN ?/.
To/TO address/VB this/DT question/NN ,/, we/PRP propose/VBP full/JJ -/HYPH capacity/NN uRNNs/NNS that/WDT optimize/VBP their/PRP$ recurrence/NN matrix/NN over/IN all/DT unitary/JJ matrices/NNS ,/, leading/VBG to/IN significantly/RB improved/VBN performance/NN over/IN uRNNs/NNS that/WDT use/VBP a/DT restricted/VBN -/HYPH capacity/NN recurrence/NN matrix/NN ./.
Our/PRP$ contribution/NN consists/VBZ of/IN two/CD main/JJ components/NNS ./.
First/RB ,/, we/PRP provide/VBP a/DT theoretical/JJ argument/NN to/TO determine/VB if/IN a/DT unitary/JJ parameterization/NN has/VBZ restricted/VBN capacity/NN ./.
Using/VBG this/DT argument/NN ,/, we/PRP show/VBP that/IN a/DT recently/RB proposed/VBN unitary/JJ parameterization/NN has/VBZ restricted/VBN capacity/NN for/IN hidden/JJ state/NN dimension/NN greater/JJR than/IN 7/CD ./.
Second/RB ,/, we/PRP show/VBP how/WRB a/DT complete/JJ ,/, full/JJ -/HYPH capacity/NN unitary/JJ recurrence/NN matrix/NN can/MD be/VB optimized/VBN over/IN the/DT differentiable/JJ manifold/NN of/IN unitary/JJ matrices/NNS ./.
The/DT resulting/VBG multiplicative/JJ gradient/NN step/NN is/VBZ very/RB simple/JJ and/CC does/VBZ not/RB require/VB gradient/NN clipping/VBG or/CC learning/VBG rate/NN adaptation/NN ./.
We/PRP confirm/VBP the/DT utility/NN of/IN our/PRP$ claims/NNS by/IN empirically/RB evaluating/VBG our/PRP$ new/JJ full/JJ -/HYPH capacity/NN uRNNs/NNS on/IN both/DT synthetic/JJ and/CC natural/JJ data/NNS ,/, achieving/VBG superior/JJ performance/NN compared/VBN to/IN both/DT LSTMs/NNS and/CC the/DT original/JJ restricted/JJ -/HYPH capacity/NN uRNNs/NNS ./.
