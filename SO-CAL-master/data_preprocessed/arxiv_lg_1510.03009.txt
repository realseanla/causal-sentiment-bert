For/IN most/JJS deep/JJ learning/NN algorithms/NNS training/NN is/VBZ notoriously/RB time/NN consuming/VBG ./.
Since/IN most/JJS of/IN the/DT computation/NN in/IN training/NN neural/JJ networks/NNS is/VBZ typically/RB spent/VBN on/IN floating/VBG point/NN multiplications/NNS ,/, we/PRP investigate/VBP an/DT approach/NN to/IN training/NN that/WDT eliminates/VBZ the/DT need/NN for/IN most/JJS of/IN these/DT ./.
Our/PRP$ method/NN consists/VBZ of/IN two/CD parts/NNS :/: First/RB we/PRP stochastically/RB binarize/VBP weights/NNS to/TO convert/VB multiplications/NNS involved/VBN in/IN computing/VBG hidden/JJ states/NNS to/TO sign/VB changes/NNS ./.
Second/RB ,/, while/IN back/RB -/HYPH propagating/VBG error/NN derivatives/NNS ,/, in/IN addition/NN to/IN binarizing/VBG the/DT weights/NNS ,/, we/PRP quantize/VBP the/DT representations/NNS at/IN each/DT layer/NN to/TO convert/VB the/DT remaining/VBG multiplications/NNS into/IN binary/JJ shifts/NNS ./.
Experimental/JJ results/NNS across/IN 3/CD popular/JJ datasets/NNS (/-LRB- MNIST/NNP ,/, CIFAR10/NN ,/, SVHN/NN )/-RRB- show/VBP that/IN this/DT approach/NN not/RB only/RB does/VBZ not/RB hurt/VB classification/NN performance/NN but/CC can/MD result/VB in/IN even/RB better/JJR performance/NN than/IN standard/JJ stochastic/JJ gradient/NN descent/NN training/NN ,/, paving/VBG the/DT way/NN to/TO fast/VB ,/, hardware/NN -/HYPH friendly/JJ training/NN of/IN neural/JJ networks/NNS ./.
