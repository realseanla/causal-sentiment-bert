In/IN regularized/VBN risk/NN minimization/NN ,/, the/DT associated/VBN optimization/NN problem/NN becomes/VBZ particularly/RB difficult/JJ when/WRB both/CC the/DT loss/NN and/CC regularizer/NN are/VBP nonsmooth/JJ ./.
Existing/VBG approaches/NNS either/CC have/VBP slow/JJ or/CC unclear/JJ convergence/NN properties/NNS ,/, are/VBP restricted/VBN to/IN limited/JJ problem/NN subclasses/NNS ,/, or/CC require/VBP careful/JJ setting/NN of/IN a/DT smoothing/NN parameter/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT continuation/NN algorithm/NN that/WDT is/VBZ applicable/JJ to/IN a/DT large/JJ class/NN of/IN nonsmooth/JJ regularized/VBN risk/NN minimization/NN problems/NNS ,/, can/MD be/VB flexibly/RB used/VBN with/IN a/DT number/NN of/IN existing/VBG solvers/NNS for/IN the/DT underlying/VBG smoothed/VBN subproblem/NN ,/, and/CC with/IN convergence/NN results/NNS on/IN the/DT whole/JJ algorithm/NN rather/RB than/IN just/RB one/CD of/IN its/PRP$ subproblems/NNS ./.
In/IN particular/JJ ,/, when/WRB accelerated/VBN solvers/NNS are/VBP used/VBN ,/, the/DT proposed/VBN algorithm/NN achieves/VBZ the/DT fastest/RBS known/JJ rates/NNS of/IN $/$ O/UH (/-LRB- 1/CD //SYM T/NN ^/SYM 2/CD )/-RRB- $/$ on/IN strongly/RB convex/JJ problems/NNS ,/, and/CC $/$ O/UH (/-LRB- 1/CD //SYM T/NN )/-RRB- $/$ on/IN general/JJ convex/NN problems/NNS ./.
Experiments/NNS on/IN nonsmooth/JJ classification/NN and/CC regression/NN tasks/NNS demonstrate/VBP that/IN the/DT proposed/VBN algorithm/NN outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
