We/PRP consider/VBP online/JJ similarity/NN prediction/NN problems/NNS over/IN networked/JJ data/NNS ./.
We/PRP begin/VBP by/IN relating/VBG this/DT task/NN to/IN the/DT more/RBR standard/JJ class/NN prediction/NN problem/NN ,/, showing/VBG that/IN ,/, given/VBN an/DT arbitrary/JJ algorithm/NN for/IN class/NN prediction/NN ,/, we/PRP can/MD construct/VB an/DT algorithm/NN for/IN similarity/NN prediction/NN with/IN "/`` nearly/RB "/'' the/DT same/JJ mistake/NN bound/VBN ,/, and/CC vice/RB versa/RB ./.
After/IN noticing/VBG that/IN this/DT general/JJ construction/NN is/VBZ computationally/RB infeasible/JJ ,/, we/PRP target/VBP our/PRP$ study/NN to/IN {/-LRB- \/SYM em/PRP feasible/JJ }/-RRB- similarity/NN prediction/NN algorithms/NNS on/IN networked/JJ data/NNS ./.
We/PRP initially/RB assume/VBP that/IN the/DT network/NN structure/NN is/VBZ {/-LRB- \/NFP em/PRP known/VBN }/-RRB- to/IN the/DT learner/NN ./.
Here/RB we/PRP observe/VBP that/IN Matrix/NNP Winnow/NNP \/SYM citep/NNP {/-LRB- w07/NN }/-RRB- has/VBZ a/DT near/JJ -/HYPH optimal/JJ mistake/NN guarantee/NN ,/, at/IN the/DT price/NN of/IN cubic/JJ prediction/NN time/NN per/IN round/NN ./.
This/DT motivates/VBZ our/PRP$ effort/NN for/IN an/DT efficient/JJ implementation/NN of/IN a/DT Perceptron/NNP algorithm/NN with/IN a/DT weaker/JJR mistake/NN guarantee/NN but/CC with/IN only/JJ poly/NN -/HYPH logarithmic/JJ prediction/NN time/NN ./.
Our/PRP$ focus/NN then/RB turns/VBZ to/IN the/DT challenging/JJ case/NN of/IN networks/NNS whose/WP$ structure/NN is/VBZ initially/RB {/-LRB- \/SYM em/PRP unknown/JJ }/-RRB- to/IN the/DT learner/NN ./.
In/IN this/DT novel/JJ setting/NN ,/, where/WRB the/DT network/NN structure/NN is/VBZ only/RB incrementally/RB revealed/VBN ,/, we/PRP obtain/VBP a/DT mistake/NN -/HYPH bounded/VBN algorithm/NN with/IN a/DT quadratic/JJ prediction/NN time/NN per/IN round/NN ./.
