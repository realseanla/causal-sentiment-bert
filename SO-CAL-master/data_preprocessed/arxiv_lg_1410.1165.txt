Recently/RB proposed/VBN neural/JJ network/NN activation/NN functions/NNS such/JJ as/IN rectified/VBN linear/JJ ,/, maxout/NN ,/, and/CC local/JJ winner/NN -/HYPH take/NN -/HYPH all/DT have/VBP allowed/VBN for/IN faster/RBR and/CC more/RBR effective/JJ training/NN of/IN deep/JJ neural/JJ architectures/NNS on/IN large/JJ and/CC complex/JJ datasets/NNS ./.
The/DT common/JJ trait/NN among/IN these/DT functions/NNS is/VBZ that/IN they/PRP implement/VBP local/JJ competition/NN between/IN small/JJ groups/NNS of/IN units/NNS within/IN a/DT layer/NN ,/, so/IN that/IN only/RB part/NN of/IN the/DT network/NN is/VBZ activated/VBN for/IN any/DT given/VBN input/NN pattern/NN ./.
In/IN this/DT paper/NN ,/, we/PRP attempt/VBP to/TO visualize/VB and/CC understand/VB this/DT self/NN -/HYPH modularization/NN ,/, and/CC suggest/VBP a/DT unified/VBN explanation/NN for/IN the/DT beneficial/JJ properties/NNS of/IN such/JJ networks/NNS ./.
We/PRP also/RB show/VBP how/WRB our/PRP$ insights/NNS can/MD be/VB directly/RB useful/JJ for/IN efficiently/RB performing/VBG retrieval/NN over/IN large/JJ datasets/NNS using/VBG neural/JJ networks/NNS ./.
