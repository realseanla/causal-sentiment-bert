Low/JJ -/HYPH rank/NN matrix/NN decomposition/NN has/VBZ gained/VBN great/JJ popularity/NN recently/RB in/IN scaling/VBG up/RP kernel/NN methods/NNS to/IN large/JJ amounts/NNS of/IN data/NNS ./.
However/RB ,/, some/DT limitations/NNS could/MD prevent/VB them/PRP from/IN working/VBG effectively/RB in/IN certain/JJ domains/NNS ./.
For/IN example/NN ,/, many/JJ existing/VBG approaches/NNS are/VBP intrinsically/RB unsupervised/JJ ,/, which/WDT does/VBZ not/RB incorporate/VB side/NN information/NN (/-LRB- e.g./FW ,/, class/NN labels/NNS )/-RRB- to/TO produce/VB task/NN specific/JJ decompositions/NNS ;/: also/RB ,/, they/PRP typically/RB work/VBP "/`` transductively/RB "/'' ,/, i.e./FW ,/, the/DT factorization/NN does/VBZ not/RB generalize/VB to/IN new/JJ samples/NNS ,/, so/IN the/DT complete/JJ factorization/NN needs/VBZ to/TO be/VB recomputed/VBN when/WRB new/JJ samples/NNS become/VBP available/JJ ./.
To/TO solve/VB these/DT problems/NNS ,/, in/IN this/DT paper/NN we/PRP propose/VBP an/DT "/`` inductive/JJ "/'' -/, flavored/VBN method/NN for/IN low/JJ -/HYPH rank/NN kernel/NN decomposition/NN with/IN priors/NNS ./.
We/PRP achieve/VBP this/DT by/IN generalizing/VBG the/DT Nystr/NNP \/SYM "/`` om/NN method/NN in/IN a/DT novel/JJ way/NN ./.
On/IN the/DT one/CD hand/NN ,/, our/PRP$ approach/NN employs/VBZ a/DT highly/RB flexible/JJ ,/, nonparametric/JJ structure/NN that/WDT allows/VBZ us/PRP to/TO generalize/VB the/DT low/JJ -/HYPH rank/NN factors/NNS to/IN arbitrarily/RB new/JJ samples/NNS ;/: on/IN the/DT other/JJ hand/NN ,/, it/PRP has/VBZ linear/JJ time/NN and/CC space/NN complexities/NNS ,/, which/WDT can/MD be/VB orders/NNS of/IN magnitudes/NNS faster/RBR than/IN existing/VBG approaches/NNS and/CC renders/VBZ great/JJ efficiency/NN in/IN learning/VBG a/DT low/JJ -/HYPH rank/NN kernel/NN decomposition/NN ./.
Empirical/JJ results/NNS demonstrate/VBP the/DT efficacy/NN and/CC efficiency/NN of/IN the/DT proposed/JJ method/NN ./.
