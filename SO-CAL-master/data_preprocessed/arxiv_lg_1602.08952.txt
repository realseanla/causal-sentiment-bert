We/PRP present/VBP novel/JJ methods/NNS for/IN analysing/VBG the/DT activation/NN patterns/NNS of/IN RNNs/NNS and/CC identifying/VBG the/DT types/NNS of/IN linguistic/JJ structure/NN they/PRP learn/VBP ./.
As/IN a/DT case/NN study/NN ,/, we/PRP use/VBP a/DT multi-task/VB gated/VBN recurrent/JJ network/NN model/NN consisting/VBG of/IN two/CD parallel/JJ pathways/NNS with/IN shared/VBN word/NN embeddings/NNS trained/VBN on/IN predicting/VBG the/DT representations/NNS of/IN the/DT visual/JJ scene/NN corresponding/VBG to/IN an/DT input/NN sentence/NN ,/, and/CC predicting/VBG the/DT next/JJ word/NN in/IN the/DT same/JJ sentence/NN ./.
We/PRP show/VBP that/IN the/DT image/NN prediction/NN pathway/NN is/VBZ sensitive/JJ to/IN the/DT information/NN structure/NN of/IN the/DT sentence/NN ,/, and/CC pays/VBZ selective/JJ attention/NN to/IN lexical/JJ categories/NNS and/CC grammatical/JJ functions/NNS that/WDT carry/VBP semantic/JJ information/NN ./.
It/PRP also/RB learns/VBZ to/TO treat/VB the/DT same/JJ input/NN token/NN differently/RB depending/VBG on/IN its/PRP$ grammatical/JJ functions/NNS in/IN the/DT sentence/NN ./.
The/DT language/NN model/NN is/VBZ comparatively/RB more/RBR sensitive/JJ to/IN words/NNS with/IN a/DT syntactic/JJ function/NN ./.
Our/PRP$ analysis/NN of/IN the/DT function/NN of/IN individual/JJ hidden/JJ units/NNS shows/VBZ that/IN each/DT pathway/NN contains/VBZ specialized/VBN units/NNS tuned/VBN to/IN patterns/NNS informative/JJ for/IN the/DT task/NN ,/, some/DT of/IN which/WDT can/MD carry/VB activations/NNS to/IN later/JJ time/NN steps/NNS to/TO encode/VB long/RB -/HYPH term/NN dependencies/NNS ./.
