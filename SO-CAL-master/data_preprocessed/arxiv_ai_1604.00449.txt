Inspired/VBN by/IN the/DT recent/JJ success/NN of/IN methods/NNS that/WDT employ/VBP shape/NN priors/NNS to/TO achieve/VB robust/JJ 3D/NN reconstructions/NNS ,/, we/PRP propose/VBP a/DT novel/JJ recurrent/JJ neural/JJ network/NN architecture/NN that/WDT we/PRP call/VBP the/DT 3D/NN Recurrent/NNP Reconstruction/NNP Neural/JJ Network/NN (/-LRB- 3D/NN -/HYPH R2N2/NN )/-RRB- ./.
The/DT network/NN learns/VBZ a/DT mapping/NN from/IN images/NNS of/IN objects/NNS to/IN their/PRP$ underlying/VBG 3D/NN shapes/NNS from/IN a/DT large/JJ collection/NN of/IN synthetic/JJ data/NNS ./.
Our/PRP$ network/NN takes/VBZ in/IN one/CD or/CC more/JJR images/NNS of/IN an/DT object/NN instance/NN from/IN arbitrary/JJ viewpoints/NNS and/CC outputs/NNS a/DT reconstruction/NN of/IN the/DT object/NN in/IN the/DT form/NN of/IN a/DT 3D/NN occupancy/NN grid/NN ./.
Unlike/IN most/JJS of/IN the/DT previous/JJ works/NNS ,/, our/PRP$ network/NN does/VBZ not/RB require/VB any/DT image/NN annotations/NNS or/CC object/NN class/NN labels/NNS for/IN training/NN or/CC testing/NN ./.
Our/PRP$ extensive/JJ experimental/JJ analysis/NN shows/VBZ that/IN our/PRP$ reconstruction/NN framework/NN i/NN )/-RRB- outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS for/IN single/JJ view/NN reconstruction/NN ,/, and/CC ii/LS )/-RRB- enables/VBZ the/DT 3D/NN reconstruction/NN of/IN objects/NNS in/IN situations/NNS when/WRB traditional/JJ SFM/NN //HYPH SLAM/NN methods/NNS fail/VBP (/-LRB- because/IN of/IN lack/NN of/IN texture/NN and/CC //HYPH or/CC wide/JJ baseline/NN )/-RRB- ./.
