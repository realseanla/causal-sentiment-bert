Recurrent/JJ neural/JJ networks/NNS are/VBP increasing/VBG popular/JJ models/NNS for/IN sequential/JJ learning/NN ./.
Unfortunately/RB ,/, although/IN the/DT most/RBS effective/JJ RNN/NN architectures/NNS are/VBP perhaps/RB excessively/RB complicated/JJ ,/, extensive/JJ searches/NNS have/VBP not/RB found/VBN simpler/JJR alternatives/NNS ./.
This/DT paper/NN imports/NNS ideas/NNS from/IN physics/NN and/CC functional/JJ programming/NN into/IN RNN/NNP design/NN to/TO provide/VB guiding/VBG principles/NNS ./.
From/IN physics/NN we/PRP introduce/VBP type/NN constraints/NNS ,/, analogous/JJ to/IN the/DT constraints/NNS that/WDT disqualify/VBP adding/VBG meters/NNS to/IN seconds/NNS in/IN physics/NN ./.
From/IN functional/JJ programming/NN ,/, we/PRP require/VBP that/IN strongly/RB -/HYPH typed/VBN architectures/NNS factorize/VBP into/IN stateless/JJ learnware/NN and/CC state/NN -/HYPH dependent/JJ firmware/NN ,/, thereby/RB ameliorating/VBG the/DT impact/NN of/IN side/NN -/HYPH effects/NNS ./.
The/DT features/NNS learned/VBN by/IN strongly/RB -/HYPH typed/VBN nets/NNS have/VBP a/DT simple/JJ semantic/JJ interpretation/NN via/IN dynamic/JJ average/JJ -/HYPH pooling/VBG on/IN one/CD -/HYPH dimensional/JJ convolutions/NNS ./.
We/PRP also/RB show/VBP that/IN strongly/RB -/HYPH typed/VBN gradients/NNS are/VBP better/JJR behaved/VBD than/IN in/IN classical/JJ architectures/NNS ,/, and/CC characterize/VB the/DT representational/JJ power/NN of/IN strongly/RB -/HYPH typed/VBN nets/NNS ./.
Finally/RB ,/, experiments/NNS show/VBP that/IN ,/, despite/IN being/VBG more/RBR constrained/JJ ,/, strongly/RB -/HYPH typed/VBN architectures/NNS achieve/VBP lower/JJR training/NN error/NN and/CC comparable/JJ generalization/NN error/NN to/IN classical/JJ architectures/NNS ./.
