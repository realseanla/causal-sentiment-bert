We/PRP propose/VBP a/DT family/NN of/IN trust/NN region/NN policy/NN optimization/NN (/-LRB- TRPO/NN )/-RRB- algorithms/NNS for/IN learning/VBG control/NN policies/NNS ./.
We/PRP first/RB develop/VB a/DT policy/NN update/NN scheme/NN with/IN guaranteed/VBN monotonic/JJ improvement/NN ,/, and/CC then/RB we/PRP describe/VBP a/DT finite/NN -/HYPH sample/NN approximation/NN to/IN this/DT scheme/NN that/WDT is/VBZ practical/JJ for/IN large/JJ -/HYPH scale/NN problems/NNS ./.
In/IN our/PRP$ experiments/NNS ,/, we/PRP evaluate/VBP the/DT method/NN on/IN two/CD different/JJ and/CC very/RB challenging/JJ sets/NNS of/IN tasks/NNS :/: learning/VBG simulated/VBN robotic/JJ swimming/NN ,/, hopping/VBG ,/, and/CC walking/VBG gaits/NNS ,/, and/CC playing/VBG Atari/NNP games/NNS using/VBG images/NNS of/IN the/DT screen/NN as/IN input/NN ./.
For/IN these/DT tasks/NNS ,/, the/DT policies/NNS are/VBP neural/JJ networks/NNS with/IN tens/NNS of/IN thousands/NNS of/IN parameters/NNS ,/, mapping/VBG from/IN observations/NNS to/IN actions/NNS ./.
