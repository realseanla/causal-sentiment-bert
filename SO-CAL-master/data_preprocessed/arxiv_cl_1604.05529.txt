Bi-directional/JJ long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- networks/NNS have/VBP recently/RB proven/VBN successful/JJ for/IN various/JJ NLP/NN sequence/NN modeling/NN tasks/NNS ,/, but/CC little/JJ is/VBZ known/VBN about/IN their/PRP$ reliance/NN to/IN input/NN representations/NNS ,/, target/NN languages/NNS ,/, data/NNS set/VBN size/NN ,/, and/CC label/NN noise/NN ./.
We/PRP address/VBP these/DT issues/NNS and/CC evaluate/VB bi-LSTMs/NNS with/IN word/NN ,/, character/NN ,/, and/CC unicode/JJ byte/NN embeddings/NNS for/IN POS/NN tagging/NN ./.
We/PRP compare/VBP bi-LSTMs/NNS to/IN traditional/JJ POS/NN taggers/NNS across/IN languages/NNS and/CC data/NNS sizes/NNS ./.
We/PRP also/RB present/VBP a/DT novel/JJ bi-LSTM/NN model/NN ,/, which/WDT combines/VBZ the/DT POS/NN tagging/NN loss/NN function/NN with/IN an/DT auxiliary/JJ loss/NN function/NN that/WDT accounts/VBZ for/IN rare/JJ words/NNS ./.
The/DT new/JJ model/NN obtains/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN across/IN 22/CD languages/NNS ,/, and/CC works/VBZ especially/RB well/RB for/IN morphologically/RB complex/JJ languages/NNS ./.
Our/PRP$ analysis/NN suggests/VBZ that/IN bi-LSTMs/NNS are/VBP less/RBR sensitive/JJ to/IN training/NN data/NNS size/NN and/CC label/NN corruptions/NNS (/-LRB- at/IN small/JJ noise/NN levels/NNS )/-RRB- than/IN previously/RB assumed/VBN ./.
