For/IN computer/NN vision/NN applications/NNS ,/, prior/JJ works/NNS have/VBP shown/VBN the/DT efficacy/NN of/IN reducing/VBG the/DT numeric/JJ precision/NN of/IN model/NN parameters/NNS (/-LRB- network/NN weights/NNS )/-RRB- in/IN deep/JJ neural/JJ networks/NNS but/CC also/RB that/DT reducing/VBG the/DT precision/NN of/IN activations/NNS hurts/VBZ model/NN accuracy/NN much/RB more/JJR than/IN reducing/VBG the/DT precision/NN of/IN model/NN parameters/NNS ./.
We/PRP study/VBP schemes/NNS to/TO train/VB networks/NNS from/IN scratch/NN using/VBG reduced/VBN -/HYPH precision/NN activations/NNS without/IN hurting/VBG the/DT model/NN accuracy/NN ./.
We/PRP reduce/VBP the/DT precision/NN of/IN activation/NN maps/NNS (/-LRB- along/IN with/IN model/NN parameters/NNS )/-RRB- using/VBG a/DT novel/JJ quantization/NN scheme/NN and/CC increase/VB the/DT number/NN of/IN filter/NN maps/NNS in/IN a/DT layer/NN ,/, and/CC find/VB that/IN this/DT scheme/NN compensates/VBZ or/CC surpasses/VBZ the/DT accuracy/NN of/IN the/DT baseline/NN full/JJ -/HYPH precision/NN network/NN ./.
As/IN a/DT result/NN ,/, one/PRP can/MD significantly/RB reduce/VB the/DT dynamic/JJ memory/NN footprint/NN ,/, memory/NN bandwidth/NN ,/, computational/JJ energy/NN and/CC speed/VB up/RP the/DT training/NN and/CC inference/NN process/NN with/IN appropriate/JJ hardware/NN support/NN ./.
We/PRP call/VBP our/PRP$ scheme/NN WRPN/NNP -/HYPH wide/JJ reduced/VBN -/HYPH precision/NN networks/NNS ./.
We/PRP report/VBP results/NNS using/VBG our/PRP$ proposed/VBN schemes/NNS and/CC show/VBP that/IN our/PRP$ results/NNS are/VBP better/JJR than/IN previously/RB reported/VBN accuracies/NNS on/IN ILSVRC/NNP -/HYPH 12/CD dataset/NN while/IN being/VBG computationally/RB less/RBR expensive/JJ compared/VBN to/IN previously/RB reported/VBN reduced/VBN -/HYPH precision/NN networks/NNS ./.
