Scalability/NN properties/NNS of/IN deep/JJ neural/JJ networks/NNS raise/VBP key/JJ research/NN questions/NNS ,/, particularly/RB as/IN the/DT problems/NNS considered/VBN become/VBP larger/JJR and/CC more/JJR challenging/JJ ./.
This/DT paper/NN expands/VBZ on/IN the/DT idea/NN of/IN conditional/JJ computation/NN introduced/VBN by/IN Bengio/NNP ,/, et/FW ./.
al./FW ,/, where/WRB the/DT nodes/NNS of/IN a/DT deep/JJ network/NN are/VBP augmented/VBN by/IN a/DT set/NN of/IN gating/NN units/NNS that/WDT determine/VBP when/WRB a/DT node/NN should/MD be/VB calculated/VBN ./.
By/IN factorizing/VBG the/DT weight/NN matrix/NN into/IN a/DT low/JJ -/HYPH rank/NN approximation/NN ,/, an/DT estimation/NN of/IN the/DT sign/NN of/IN the/DT pre-nonlinearity/NN activation/NN can/MD be/VB efficiently/RB obtained/VBN ./.
For/IN networks/NNS using/VBG rectified/VBN -/HYPH linear/JJ hidden/JJ units/NNS ,/, this/DT implies/VBZ that/IN the/DT computation/NN of/IN a/DT hidden/JJ unit/NN with/IN an/DT estimated/VBN negative/JJ pre-nonlinearity/NN can/MD be/VB ommitted/VBN altogether/RB ,/, as/IN its/PRP$ value/NN will/MD become/VB zero/CD when/WRB nonlinearity/NN is/VBZ applied/VBN ./.
For/IN sparse/JJ neural/JJ networks/NNS ,/, this/DT can/MD result/VB in/IN considerable/JJ speed/NN gains/NNS ./.
Experimental/JJ results/NNS using/VBG the/DT MNIST/NNP and/CC SVHN/NNP data/NNS sets/NNS with/IN a/DT fully/RB -/HYPH connected/VBN deep/JJ neural/JJ network/NN demonstrate/VBP the/DT performance/NN robustness/NN of/IN the/DT proposed/VBN scheme/NN with/IN respect/NN to/IN the/DT error/NN introduced/VBN by/IN the/DT conditional/JJ computation/NN process/NN ./.
