We/PRP consider/VBP the/DT most/RBS common/JJ variants/NNS of/IN linear/JJ regression/NN ,/, including/VBG Ridge/NNP ,/, Lasso/NNP and/CC Support/NN -/HYPH vector/NN regression/NN ,/, in/IN a/DT setting/NN where/WRB the/DT learner/NN is/VBZ allowed/VBN to/TO observe/VB only/RB a/DT fixed/VBN number/NN of/IN attributes/NNS of/IN each/DT example/NN at/IN training/NN time/NN ./.
We/PRP present/VBP simple/JJ and/CC efficient/JJ algorithms/NNS for/IN these/DT problems/NNS :/: for/IN Lasso/NNP and/CC Ridge/NNP regression/NN they/PRP need/VBP the/DT same/JJ total/JJ number/NN of/IN attributes/NNS (/-LRB- up/RP to/IN constants/NNS )/-RRB- as/IN do/VBP full/JJ -/HYPH information/NN algorithms/NNS ,/, for/IN reaching/VBG a/DT certain/JJ accuracy/NN ./.
For/IN Support/NN -/HYPH vector/NN regression/NN ,/, we/PRP require/VBP exponentially/RB less/JJR attributes/NNS compared/VBN to/IN the/DT state/NN of/IN the/DT art/NN ./.
By/IN that/DT ,/, we/PRP resolve/VBP an/DT open/JJ problem/NN recently/RB posed/VBN by/IN Cesa/NNP -/HYPH Bianchi/NNP et/FW al./FW (/-LRB- 2010/CD )/-RRB- ./.
Experiments/NNS show/VBP the/DT theoretical/JJ bounds/NNS to/TO be/VB justified/VBN by/IN superior/JJ performance/NN compared/VBN to/IN the/DT state/NN of/IN the/DT art/NN ./.
