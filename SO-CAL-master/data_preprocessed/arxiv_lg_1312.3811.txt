Policy/NN Gradient/NN methods/NNS that/WDT explore/VBP directly/RB in/IN parameter/NN space/NN are/VBP among/IN the/DT most/RBS effective/JJ and/CC robust/JJ direct/JJ policy/NN search/NN methods/NNS and/CC have/VBP drawn/VBN a/DT lot/NN of/IN attention/NN lately/RB ./.
The/DT basic/JJ method/NN from/IN this/DT field/NN ,/, Policy/NN Gradients/NNS with/IN Parameter/NN -/HYPH based/VBN Exploration/NNP ,/, uses/VBZ two/CD samples/NNS that/WDT are/VBP symmetric/JJ around/IN the/DT current/JJ hypothesis/NN to/TO circumvent/VB misleading/JJ reward/NN in/IN \/NN emph/NN {/-LRB- asymmetrical/JJ }/-RRB- reward/NN distributed/VBN problems/NNS gathered/VBD with/IN the/DT usual/JJ baseline/NN approach/NN ./.
The/DT exploration/NN parameters/NNS are/VBP still/RB updated/VBN by/IN a/DT baseline/NN approach/NN -/, leaving/VBG the/DT exploration/NN prone/JJ to/IN asymmetric/JJ reward/NN distributions/NNS ./.
In/IN this/DT paper/NN we/PRP will/MD show/VB how/WRB the/DT exploration/NN parameters/NNS can/MD be/VB sampled/VBN quasi/JJ symmetric/JJ despite/IN having/VBG limited/VBN instead/RB of/IN free/JJ parameters/NNS for/IN exploration/NN ./.
We/PRP give/VBP a/DT transformation/NN approximation/NN to/TO get/VB quasi/JJ symmetric/JJ samples/NNS with/IN respect/NN to/IN the/DT exploration/NN without/IN changing/VBG the/DT overall/JJ sampling/NN distribution/NN ./.
Finally/RB we/PRP will/MD demonstrate/VB that/IN sampling/NN symmetrically/RB also/RB for/IN the/DT exploration/NN parameters/NNS is/VBZ superior/JJ in/IN needs/NNS of/IN samples/NNS and/CC robustness/NN than/IN the/DT original/JJ sampling/NN approach/NN ./.
