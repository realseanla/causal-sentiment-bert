This/DT paper/NN proposes/VBZ a/DT new/JJ method/NN for/IN the/DT K/NN -/HYPH armed/VBN dueling/VBG bandit/NN problem/NN ,/, a/DT variation/NN on/IN the/DT regular/JJ K/NN -/HYPH armed/JJ bandit/NN problem/NN that/WDT offers/VBZ only/RB relative/JJ feedback/NN about/IN pairs/NNS of/IN arms/NNS ./.
Our/PRP$ approach/NN extends/VBZ the/DT Upper/NNP Confidence/NN Bound/VBN algorithm/NN to/IN the/DT relative/JJ setting/NN by/IN using/VBG estimates/NNS of/IN the/DT pairwise/JJ probabilities/NNS to/TO select/VB a/DT promising/JJ arm/NN and/CC applying/VBG Upper/NNP Confidence/NN Bound/VBN with/IN the/DT winner/NN as/IN a/DT benchmark/NN ./.
We/PRP prove/VBP a/DT finite/NN -/HYPH time/NN regret/NN bound/VBN of/IN order/NN O/NN (/-LRB- log/NN t/NN )/-RRB- ./.
In/IN addition/NN ,/, our/PRP$ empirical/JJ results/NNS using/VBG real/JJ data/NNS from/IN an/DT information/NN retrieval/NN application/NN show/VBP that/IN it/PRP greatly/RB outperforms/VBZ the/DT state/NN of/IN the/DT art/NN ./.
