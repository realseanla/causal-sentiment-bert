The/DT performance/NN of/IN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- depends/VBZ critically/RB on/IN how/WRB learning/NN rates/NNS are/VBP tuned/VBN and/CC decreased/VBN over/IN time/NN ./.
We/PRP propose/VBP a/DT method/NN to/TO automatically/RB adjust/VB multiple/JJ learning/NN rates/NNS so/RB as/IN to/TO minimize/VB the/DT expected/VBN error/NN at/IN any/DT one/CD time/NN ./.
The/DT method/NN relies/VBZ on/IN local/JJ gradient/NN variations/NNS across/IN samples/NNS ./.
Using/VBG a/DT number/NN of/IN convex/NN and/CC non-convex/JJ learning/NN tasks/NNS ,/, we/PRP show/VBP that/IN the/DT resulting/VBG algorithm/NN matches/VBZ the/DT performance/NN of/IN the/DT best/JJS settings/NNS obtained/VBN through/IN systematic/JJ search/NN ,/, and/CC effectively/RB removes/VBZ the/DT need/NN for/IN learning/VBG rate/NN tuning/NN ./.
