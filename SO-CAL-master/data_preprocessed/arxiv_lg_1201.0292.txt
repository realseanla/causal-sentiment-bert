Traditional/JJ Reinforcement/NN Learning/NN (/-LRB- RL/NN )/-RRB- has/VBZ focused/VBN on/IN problems/NNS involving/VBG many/JJ states/NNS and/CC few/JJ actions/NNS ,/, such/JJ as/IN simple/JJ grid/NN worlds/NNS ./.
Most/JJS real/JJ world/NN problems/NNS ,/, however/RB ,/, are/VBP of/IN the/DT opposite/JJ type/NN ,/, Involving/NNP Few/JJ relevant/JJ states/NNS and/CC many/JJ actions/NNS ./.
For/IN example/NN ,/, to/TO return/VB home/RB from/IN a/DT conference/NN ,/, humans/NNS identify/VBP only/RB few/JJ subgoal/JJ states/NNS such/JJ as/IN lobby/NN ,/, taxi/NN ,/, airport/NN etc/FW ./.
Each/DT valid/JJ behavior/NN connecting/VBG two/CD such/JJ states/NNS can/MD be/VB viewed/VBN as/IN an/DT action/NN ,/, and/CC there/EX are/VBP trillions/NNS of/IN them/PRP ./.
Assuming/VBG the/DT subgoal/JJ identification/NN problem/NN is/VBZ already/RB solved/VBN ,/, the/DT quality/NN of/IN any/DT RL/NN method/NN ---/, in/IN real/JJ -/HYPH world/NN settings/NNS ---/, depends/VBZ less/RBR on/IN how/WRB well/RB it/PRP scales/VBZ with/IN the/DT number/NN of/IN states/NNS than/IN on/IN how/WRB well/RB it/PRP scales/VBZ with/IN the/DT number/NN of/IN actions/NNS ./.
This/DT is/VBZ where/WRB our/PRP$ new/JJ method/NN T/NN -/HYPH Learning/NN excels/VBZ ,/, by/IN evaluating/VBG the/DT relatively/RB few/JJ possible/JJ transits/NNS from/IN one/CD state/NN to/IN another/DT in/IN a/DT policy/NN -/HYPH independent/JJ way/NN ,/, rather/RB than/IN a/DT huge/JJ number/NN of/IN state/NN -/HYPH action/NN pairs/NNS ,/, or/CC states/NNS in/IN traditional/JJ policy/NN -/HYPH dependent/JJ ways/NNS ./.
Illustrative/JJ experiments/NNS demonstrate/VBP that/IN performance/NN improvements/NNS of/IN T/NN -/HYPH Learning/VBG over/IN Q/NN -/HYPH learning/NN can/MD be/VB arbitrarily/RB large/JJ ./.
