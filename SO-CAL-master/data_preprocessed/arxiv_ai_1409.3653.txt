This/DT paper/NN studies/NNS the/DT off/NN -/HYPH policy/NN evaluation/NN problem/NN ,/, where/WRB one/CD aims/VBZ to/TO estimate/VB the/DT value/NN of/IN a/DT target/NN policy/NN based/VBN on/IN a/DT sample/NN of/IN observations/NNS collected/VBN by/IN another/DT policy/NN ./.
We/PRP first/RB consider/VB the/DT multi-armed/JJ bandit/NN case/NN ,/, establish/VB a/DT minimax/NN risk/NN lower/JJR bound/JJ ,/, and/CC analyze/VB the/DT risk/NN of/IN two/CD standard/JJ estimators/NNS ./.
It/PRP is/VBZ shown/VBN ,/, and/CC verified/VBD in/IN simulation/NN ,/, that/IN one/CD is/VBZ minimax/JJ optimal/JJ up/IN to/IN a/DT constant/JJ ,/, while/IN another/DT can/MD be/VB arbitrarily/RB worse/JJR ,/, despite/IN its/PRP$ empirical/JJ success/NN and/CC popularity/NN ./.
The/DT results/NNS are/VBP applied/VBN to/IN related/JJ problems/NNS in/IN contextual/JJ bandits/NNS and/CC fixed/VBN -/HYPH horizon/NN Markov/NNP decision/NN processes/NNS ,/, and/CC are/VBP also/RB related/VBN to/IN semi-supervised/VBN learning/NN ./.
