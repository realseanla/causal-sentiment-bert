There/EX is/VBZ rising/VBG interest/NN in/IN vector/NN -/HYPH space/NN word/NN embeddings/NNS and/CC their/PRP$ use/NN in/IN NLP/NNP ,/, especially/RB given/VBN recent/JJ methods/NNS for/IN their/PRP$ fast/JJ estimation/NN at/IN very/RB large/JJ scale/NN ./.
Nearly/RB all/PDT this/DT work/NN ,/, however/RB ,/, assumes/VBZ a/DT single/JJ vector/NN per/IN word/NN type/NN ignoring/VBG polysemy/NN and/CC thus/RB jeopardizing/VBG their/PRP$ usefulness/NN for/IN downstream/JJ tasks/NNS ./.
We/PRP present/VBP an/DT extension/NN to/IN the/DT Skip/VB -/HYPH gram/NN model/NN that/WDT efficiently/RB learns/VBZ multiple/JJ embeddings/NNS per/IN word/NN type/NN ./.
It/PRP differs/VBZ from/IN recent/JJ related/JJ work/NN by/IN jointly/RB performing/VBG word/NN sense/NN discrimination/NN and/CC embedding/NN learning/NN ,/, by/IN non-parametrically/RB estimating/VBG the/DT number/NN of/IN senses/NNS per/IN word/NN type/NN ,/, and/CC by/IN its/PRP$ efficiency/NN and/CC scalability/NN ./.
We/PRP present/VBP new/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN the/DT word/NN similarity/NN in/IN context/NN task/NN and/CC demonstrate/VBP its/PRP$ scalability/NN by/IN training/NN with/IN one/CD machine/NN on/IN a/DT corpus/NN of/IN nearly/RB 1/CD billion/CD tokens/NNS in/IN less/JJR than/IN 6/CD hours/NNS ./.
