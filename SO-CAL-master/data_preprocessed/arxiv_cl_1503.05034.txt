We/PRP propose/VBP a/DT novel/JJ convolutional/JJ architecture/NN ,/, named/VBN $/$ gen/CD $/$ CNN/NNP ,/, for/IN word/NN sequence/NN prediction/NN ./.
Different/JJ from/IN previous/JJ work/NN on/IN neural/JJ network/NN -/HYPH based/VBN language/NN modeling/NN and/CC generation/NN (/-LRB- e.g./FW ,/, RNN/NN or/CC LSTM/NN )/-RRB- ,/, we/PRP choose/VBP not/RB to/TO greedily/RB summarize/VB the/DT history/NN of/IN words/NNS as/IN a/DT fixed/VBN length/NN vector/NN ./.
Instead/RB ,/, we/PRP use/VBP a/DT convolutional/JJ neural/JJ network/NN to/TO predict/VB the/DT next/JJ word/NN with/IN the/DT history/NN of/IN words/NNS of/IN variable/JJ length/NN ./.
Also/RB different/JJ from/IN the/DT existing/VBG feedforward/NN networks/NNS for/IN language/NN modeling/NN ,/, our/PRP$ model/NN can/MD effectively/RB fuse/VB the/DT local/JJ correlation/NN and/CC global/JJ correlation/NN in/IN the/DT word/NN sequence/NN ,/, with/IN a/DT convolution/NN -/HYPH gating/NN strategy/NN specifically/RB designed/VBN for/IN the/DT task/NN ./.
We/PRP argue/VBP that/IN our/PRP$ model/NN can/MD give/VB adequate/JJ representation/NN of/IN the/DT history/NN ,/, and/CC therefore/RB can/MD naturally/RB exploit/VB both/PDT the/DT short/JJ and/CC long/JJ range/NN dependencies/NNS ./.
Our/PRP$ model/NN is/VBZ fast/RB ,/, easy/JJ to/TO train/VB ,/, and/CC readily/RB parallelized/VBN ./.
Our/PRP$ extensive/JJ experiments/NNS on/IN text/NN generation/NN and/CC $/$ n/NN $/$ -/HYPH best/JJS re-ranking/NN in/IN machine/NN translation/NN show/NN that/WDT $/$ gen/CD $/$ CNN/NNP outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH arts/NNS with/IN big/JJ margins/NNS ./.
