We/PRP consider/VBP the/DT problem/NN of/IN learning/VBG deep/JJ generative/NN models/NNS from/IN data/NNS ./.
We/PRP formulate/VBP a/DT method/NN that/WDT generates/VBZ an/DT independent/JJ sample/NN via/IN a/DT single/JJ feedforward/NN pass/VB through/IN a/DT multilayer/JJ perceptron/NN ,/, as/IN in/IN the/DT recently/RB proposed/VBN generative/JJ adversarial/JJ networks/NNS (/-LRB- Goodfellow/NNP et/FW al./FW ,/, 2014/CD )/-RRB- ./.
Training/VBG a/DT generative/JJ adversarial/JJ network/NN ,/, however/RB ,/, requires/VBZ careful/JJ optimization/NN of/IN a/DT difficult/JJ minimax/NN program/NN ./.
Instead/RB ,/, we/PRP utilize/VBP a/DT technique/NN from/IN statistical/JJ hypothesis/NN testing/NN known/VBN as/IN maximum/JJ mean/JJ discrepancy/NN (/-LRB- MMD/NN )/-RRB- ,/, which/WDT leads/VBZ to/IN a/DT simple/JJ objective/NN that/WDT can/MD be/VB interpreted/VBN as/IN matching/VBG all/DT orders/NNS of/IN statistics/NNS between/IN a/DT dataset/NN and/CC samples/NNS from/IN the/DT model/NN ,/, and/CC can/MD be/VB trained/VBN by/IN backpropagation/NN ./.
We/PRP further/RB boost/VB the/DT performance/NN of/IN this/DT approach/NN by/IN combining/VBG our/PRP$ generative/JJ network/NN with/IN an/DT auto/NN -/HYPH encoder/NN network/NN ,/, using/VBG MMD/NNP to/TO learn/VB to/TO generate/VB codes/NNS that/WDT can/MD then/RB be/VB decoded/VBN to/TO produce/VB samples/NNS ./.
We/PRP show/VBP that/IN the/DT combination/NN of/IN these/DT techniques/NNS yields/NNS excellent/JJ generative/JJ models/NNS compared/VBN to/IN baseline/NN approaches/NNS as/IN measured/VBN on/IN MNIST/NNP and/CC the/DT Toronto/NNP Face/NNP Database/NNP ./.
