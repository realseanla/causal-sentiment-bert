We/PRP consider/VBP the/DT problem/NN of/IN using/VBG sentence/NN compression/NN techniques/NNS to/TO facilitate/VB query/NN -/HYPH focused/VBN multi-document/JJ summarization/NN ./.
We/PRP present/VBP a/DT sentence/NN -/HYPH compression/NN -/HYPH based/VBN framework/NN for/IN the/DT task/NN ,/, and/CC design/VB a/DT series/NN of/IN learning/NN -/HYPH based/VBN compression/NN models/NNS built/VBN on/IN parse/VB trees/NNS ./.
An/DT innovative/JJ beam/NN search/NN decoder/NN is/VBZ proposed/VBN to/TO efficiently/RB find/VB highly/RB probable/JJ compressions/NNS ./.
Under/IN this/DT framework/NN ,/, we/PRP show/VBP how/WRB to/TO integrate/VB various/JJ indicative/JJ metrics/NNS such/JJ as/IN linguistic/JJ motivation/NN and/CC query/NN relevance/NN into/IN the/DT compression/NN process/NN by/IN deriving/VBG a/DT novel/JJ formulation/NN of/IN a/DT compression/NN scoring/VBG function/NN ./.
Our/PRP$ best/JJS model/NN achieves/VBZ statistically/RB significant/JJ improvement/NN over/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS on/IN several/JJ metrics/NNS (/-LRB- e.g./FW 8.0/CD percent/NN and/CC 5.4/CD percent/NN improvements/NNS in/IN ROUGE/NN -/HYPH 2/CD respectively/RB )/-RRB- for/IN the/DT DUC/NNP 2006/CD and/CC 2007/CD summarization/NN task/NN ./.
