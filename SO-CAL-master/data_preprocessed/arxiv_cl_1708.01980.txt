Phrases/NNS play/VBP an/DT important/JJ role/NN in/IN natural/JJ language/NN understanding/NN and/CC machine/NN translation/NN (/-LRB- Sag/NNP et/FW al./FW ,/, 2002/CD ;/: Villavicencio/NNP et/FW al./FW ,/, 2005/CD )/-RRB- ./.
However/RB ,/, it/PRP is/VBZ difficult/JJ to/TO integrate/VB them/PRP into/IN current/JJ neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- which/WDT reads/VBZ and/CC generates/VBZ sentences/NNS word/NN by/IN word/NN ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT method/NN to/TO translate/VB phrases/NNS in/IN NMT/NN by/IN integrating/VBG a/DT phrase/NN memory/NN storing/VBG target/NN phrases/NNS from/IN a/DT phrase/NN -/HYPH based/VBN statistical/JJ machine/NN translation/NN (/-LRB- SMT/NN )/-RRB- system/NN into/IN the/DT encoder/NN -/HYPH decoder/NN architecture/NN of/IN NMT/NNP ./.
At/IN each/DT decoding/NN step/NN ,/, the/DT phrase/NN memory/NN is/VBZ first/JJ re-written/JJ by/IN the/DT SMT/NN model/NN ,/, which/WDT dynamically/RB generates/VBZ relevant/JJ target/NN phrases/NNS with/IN contextual/JJ information/NN provided/VBN by/IN the/DT NMT/NN model/NN ./.
Then/RB the/DT proposed/VBN model/NN reads/VBZ the/DT phrase/NN memory/NN to/TO make/VB probability/NN estimations/NNS for/IN all/DT phrases/NNS in/IN the/DT phrase/NN memory/NN ./.
If/IN phrase/NN generation/NN is/VBZ carried/VBN on/RP ,/, the/DT NMT/NNP decoder/NN selects/VBZ an/DT appropriate/JJ phrase/NN from/IN the/DT memory/NN to/TO perform/VB phrase/NN translation/NN and/CC updates/NNS its/PRP$ decoding/NN state/NN by/IN consuming/VBG the/DT words/NNS in/IN the/DT selected/VBN phrase/NN ./.
Otherwise/RB ,/, the/DT NMT/NNP decoder/NN generates/VBZ a/DT word/NN from/IN the/DT vocabulary/NN as/IN the/DT general/JJ NMT/NN decoder/NN does/VBZ ./.
Experiment/NN results/NNS on/IN the/DT Chinese/JJ to/IN English/NNP translation/NN show/NN that/IN the/DT proposed/VBN model/NN achieves/VBZ significant/JJ improvements/NNS over/IN the/DT baseline/NN on/IN various/JJ test/NN sets/NNS ./.
