Many/JJ canonical/JJ machine/NN learning/NN problems/NNS boil/VBP down/RP to/IN a/DT convex/NN optimization/NN problem/NN with/IN a/DT finite/JJ sum/NN structure/NN ./.
However/RB ,/, whereas/IN much/JJ progress/NN has/VBZ been/VBN made/VBN in/IN developing/VBG faster/JJR algorithms/NNS for/IN this/DT setting/NN ,/, the/DT inherent/JJ limitations/NNS of/IN these/DT problems/NNS are/VBP not/RB satisfactorily/RB addressed/VBN by/IN existing/VBG lower/JJR bounds/NNS ./.
Indeed/RB ,/, current/JJ bounds/NNS focus/VBP on/IN first/JJ -/HYPH order/NN optimization/NN algorithms/NNS ,/, and/CC only/RB apply/VB in/IN the/DT often/RB unrealistic/JJ regime/NN where/WRB the/DT number/NN of/IN iterations/NNS is/VBZ less/JJR than/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- d/NN //HYPH n/NN )/-RRB- $/$ (/-LRB- where/WRB $/$ d/LS $/$ is/VBZ the/DT dimension/NN and/CC $/$ n/NN $/$ is/VBZ the/DT number/NN of/IN samples/NNS )/-RRB- ./.
In/IN this/DT work/NN ,/, we/PRP extend/VBP the/DT framework/NN of/IN (/-LRB- Arjevani/NNP et/FW al./FW ,/, 2015/CD )/-RRB- to/TO provide/VB new/JJ lower/JJR bounds/NNS ,/, which/WDT are/VBP dimension/NN -/HYPH free/JJ ,/, and/CC go/VB beyond/IN the/DT assumptions/NNS of/IN current/JJ bounds/NNS ,/, thereby/RB covering/VBG standard/JJ finite/JJ sum/NN optimization/NN methods/NNS ,/, e.g./FW ,/, SAG/NNP ,/, SAGA/NNP ,/, SVRG/NNP ,/, SDCA/NNP without/IN duality/NN ,/, as/RB well/RB as/IN stochastic/JJ coordinate/NN -/HYPH descent/NN methods/NNS ,/, such/JJ as/IN SDCA/NNP and/CC accelerated/VBD proximal/JJ SDCA/NN ./.
