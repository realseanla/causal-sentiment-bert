The/DT convolutional/JJ neural/JJ network/NN (/-LRB- ConvNet/NNP or/CC CNN/NNP )/-RRB- is/VBZ a/DT powerful/JJ discriminative/JJ learning/NN machine/NN ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN a/DT generative/JJ random/JJ field/NN model/NN that/WDT we/PRP call/VBP generative/JJ ConvNet/NNP can/MD be/VB derived/VBN from/IN the/DT discriminative/JJ ConvNet/NNP ./.
The/DT probability/NN distribution/NN of/IN the/DT generative/JJ ConvNet/NNP model/NN is/VBZ in/IN the/DT form/NN of/IN exponential/JJ tilting/NN of/IN a/DT reference/NN distribution/NN ./.
Assuming/VBG re-lu/IN non-linearity/JJ and/CC Gaussian/JJ white/JJ noise/NN reference/NN distribution/NN ,/, we/PRP show/VBP that/IN the/DT generative/JJ ConvNet/NNP model/NN contains/VBZ a/DT representational/JJ structure/NN with/IN multiple/JJ layers/NNS of/IN binary/JJ activation/NN variables/NNS ./.
The/DT model/NN is/VBZ non-Gaussian/JJ ,/, or/CC more/RBR precisely/RB ,/, piecewise/RB Gaussian/NNP ,/, where/WRB each/DT piece/NN is/VBZ determined/VBN by/IN an/DT instantiation/NN of/IN the/DT binary/JJ activation/NN variables/NNS that/WDT reconstruct/VBP the/DT mean/NN of/IN the/DT Gaussian/JJ piece/NN ./.
The/DT Langevin/NNP dynamics/NNS for/IN synthesis/NN is/VBZ driven/VBN by/IN the/DT reconstruction/NN error/NN ,/, and/CC the/DT corresponding/VBG gradient/NN descent/NN dynamics/NNS converges/VBZ to/IN a/DT local/JJ energy/NN minimum/NN that/WDT is/VBZ auto/NN -/HYPH encoding/VBG ./.
As/IN for/IN learning/NN ,/, we/PRP show/VBP that/IN the/DT contrastive/JJ divergence/NN learning/NN tends/VBZ to/TO reconstruct/VB the/DT observed/VBN images/NNS ./.
Finally/RB ,/, we/PRP show/VBP that/IN the/DT maximum/JJ likelihood/NN learning/VBG algorithm/NN can/MD generate/VB realistic/JJ natural/JJ images/NNS ./.
