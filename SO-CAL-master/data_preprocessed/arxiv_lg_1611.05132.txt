We/PRP analyze/VBP online/JJ and/CC mini-batch/JJ $/$ k/CD $/$ -/HYPH means/VBZ variants/NNS ./.
Both/DT scale/NN up/RP the/DT widely/RB used/VBN $/$ k/CD $/$ -/HYPH means/VBZ algorithm/NN via/IN stochastic/JJ approximation/NN ,/, and/CC have/VBP become/VBN popular/JJ for/IN large/JJ -/HYPH scale/NN clustering/NN and/CC unsupervised/JJ feature/NN learning/NN ./.
We/PRP show/VBP ,/, for/IN the/DT first/JJ time/NN ,/, that/IN starting/VBG with/IN any/DT initial/JJ solution/NN ,/, they/PRP converge/VBP to/IN a/DT "/`` local/JJ optimum/JJ "/`` at/IN rate/NN $/$ O/UH (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- t/NN }/-RRB- )/-RRB- $/$ (/-LRB- in/IN terms/NNS of/IN the/DT $/$ k/CD $/$ -/HYPH means/NNS objective/NN )/-RRB- under/IN general/JJ conditions/NNS ./.
In/IN addition/NN ,/, we/PRP show/VBP if/IN the/DT dataset/NN is/VBZ clusterable/JJ ,/, when/WRB initialized/VBN with/IN a/DT simple/JJ and/CC scalable/JJ seeding/VBG algorithm/NN ,/, mini-batch/NN $/$ k/CD $/$ -/HYPH means/VBZ converges/VBZ to/IN an/DT optimal/JJ $/$ k/CD $/$ -/HYPH means/NN solution/NN at/IN rate/NN $/$ O/UH (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- t/NN }/-RRB- )/-RRB- $/$ with/IN high/JJ probability/NN ./.
The/DT $/$ k/CD $/$ -/HYPH means/VBZ objective/NN is/VBZ non-convex/JJ and/CC non-differentiable/JJ :/: we/PRP exploit/VBP ideas/NNS from/IN recent/JJ work/NN on/IN stochastic/JJ gradient/NN descent/NN for/IN non-convex/JJ problems/NNS by/IN providing/VBG a/DT novel/JJ characterization/NN of/IN the/DT trajectory/NN of/IN $/$ k/CD $/$ -/HYPH means/VBZ algorithm/NN on/IN its/PRP$ solution/NN space/NN ,/, and/CC circumvent/VB the/DT non-differentiability/JJ problem/NN via/IN geometric/JJ insights/NNS about/RB $/$ k/CD $/$ -/HYPH means/VBZ update/NN ./.
