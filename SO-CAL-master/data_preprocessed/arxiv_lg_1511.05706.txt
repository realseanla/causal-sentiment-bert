The/DT paradigm/NN of/IN multi-task/VB learning/NN is/VBZ that/IN one/PRP can/MD achieve/VB better/JJR generalization/NN by/IN learning/VBG tasks/NNS jointly/RB and/CC thus/RB exploiting/VBG the/DT similarity/NN between/IN the/DT tasks/NNS rather/RB than/IN learning/VBG them/PRP independently/RB of/IN each/DT other/JJ ./.
While/IN previously/RB the/DT relationship/NN between/IN tasks/NNS had/VBD to/TO be/VB user/NN -/HYPH defined/VBN in/IN the/DT form/NN of/IN an/DT output/NN kernel/NN ,/, recent/JJ approaches/NNS jointly/RB learn/VBP the/DT tasks/NNS and/CC the/DT output/NN kernel/NN ./.
As/IN the/DT output/NN kernel/NN is/VBZ a/DT positive/JJ semidefinite/NN matrix/NN ,/, the/DT resulting/VBG optimization/NN problems/NNS are/VBP not/RB scalable/JJ in/IN the/DT number/NN of/IN tasks/NNS as/IN an/DT eigendecomposition/NN is/VBZ required/VBN in/IN each/DT step/NN ./.
\/SYM mbox/NN {/-LRB- Using/VBG }/-RRB- the/DT theory/NN of/IN positive/JJ semidefinite/NN kernels/NNS we/PRP show/VBP in/IN this/DT paper/NN that/WDT for/IN a/DT certain/JJ class/NN of/IN regularizers/NNS on/IN the/DT output/NN kernel/NN ,/, the/DT constraint/NN of/IN being/VBG positive/JJ semidefinite/NN can/MD be/VB dropped/VBN as/IN it/PRP is/VBZ automatically/RB satisfied/JJ for/IN the/DT relaxed/JJ problem/NN ./.
This/DT leads/VBZ to/IN an/DT unconstrained/JJ dual/JJ problem/NN which/WDT can/MD be/VB solved/VBN efficiently/RB ./.
Experiments/NNS on/IN several/JJ multi-task/VB and/CC multi-class/VB data/NNS sets/NNS illustrate/VBP the/DT efficacy/NN of/IN our/PRP$ approach/NN in/IN terms/NNS of/IN computational/JJ efficiency/NN as/RB well/RB as/IN generalization/NN performance/NN ./.
