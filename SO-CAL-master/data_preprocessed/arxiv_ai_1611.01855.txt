Recent/JJ years/NNS have/VBP seen/VBN the/DT proposal/NN of/IN a/DT number/NN of/IN neural/JJ architectures/NNS for/IN the/DT problem/NN of/IN Program/NN Induction/NN ./.
Given/VBN a/DT set/NN of/IN input/NN -/HYPH output/NN examples/NNS ,/, these/DT architectures/NNS are/VBP able/JJ to/TO learn/VB mappings/NNS that/WDT generalize/VBP to/IN new/JJ test/NN inputs/NNS ./.
While/IN achieving/VBG impressive/JJ results/NNS ,/, these/DT approaches/NNS have/VBP a/DT number/NN of/IN important/JJ limitations/NNS :/: (/-LRB- a/LS )/-RRB- they/PRP are/VBP computationally/RB expensive/JJ and/CC hard/JJ to/TO train/VB ,/, (/-LRB- b/LS )/-RRB- a/DT model/NN has/VBZ to/TO be/VB trained/VBN for/IN each/DT task/NN (/-LRB- program/NN )/-RRB- separately/RB ,/, and/CC (/-LRB- c/LS )/-RRB- it/PRP is/VBZ hard/JJ to/TO interpret/VB or/CC verify/VB the/DT correctness/NN of/IN the/DT learnt/VBN mapping/NN (/-LRB- as/IN it/PRP is/VBZ defined/VBN by/IN a/DT neural/JJ network/NN )/-RRB- ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ technique/NN ,/, Neuro/NNP -/HYPH Symbolic/NNP Program/NNP Synthesis/NNP ,/, to/TO overcome/VB the/DT above/JJ -/HYPH mentioned/VBN problems/NNS ./.
Once/IN trained/VBN ,/, our/PRP$ approach/NN can/MD automatically/RB construct/VB computer/NN programs/NNS in/IN a/DT domain/NN -/HYPH specific/JJ language/NN that/WDT are/VBP consistent/JJ with/IN a/DT set/NN of/IN input/NN -/HYPH output/NN examples/NNS provided/VBN at/IN test/NN time/NN ./.
Our/PRP$ method/NN is/VBZ based/VBN on/IN two/CD novel/JJ neural/JJ modules/NNS ./.
The/DT first/JJ module/NN ,/, called/VBD the/DT cross/NN correlation/NN I/NN //HYPH O/NN network/NN ,/, given/VBN a/DT set/NN of/IN input/NN -/HYPH output/NN examples/NNS ,/, produces/VBZ a/DT continuous/JJ representation/NN of/IN the/DT set/NN of/IN I/PRP //SYM O/NN examples/NNS ./.
The/DT second/JJ module/NN ,/, the/DT Recursive/JJ -/HYPH Reverse/JJ -/HYPH Recursive/JJ Neural/JJ Network/NN (/-LRB- R3NN/NN )/-RRB- ,/, given/VBN the/DT continuous/JJ representation/NN of/IN the/DT examples/NNS ,/, synthesizes/VBZ a/DT program/NN by/IN incrementally/RB expanding/VBG partial/JJ programs/NNS ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN of/IN our/PRP$ approach/NN by/IN applying/VBG it/PRP to/IN the/DT rich/JJ and/CC complex/JJ domain/NN of/IN regular/JJ expression/NN based/VBN string/NN transformations/NNS ./.
Experiments/NNS show/VBP that/IN the/DT R3NN/NN model/NN is/VBZ not/RB only/RB able/JJ to/TO construct/VB programs/NNS from/IN new/JJ input/NN -/HYPH output/NN examples/NNS ,/, but/CC it/PRP is/VBZ also/RB able/JJ to/TO construct/VB new/JJ programs/NNS for/IN tasks/NNS that/IN it/PRP had/VBD never/RB observed/VBN before/IN during/IN training/NN ./.
