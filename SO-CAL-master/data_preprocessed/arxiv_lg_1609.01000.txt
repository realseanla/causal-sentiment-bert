We/PRP describe/VBP the/DT class/NN of/IN convexified/VBN convolutional/JJ neural/JJ networks/NNS (/-LRB- CCNNs/NNS )/-RRB- ,/, which/WDT capture/VBP the/DT parameter/NN sharing/NN of/IN convolutional/JJ neural/JJ networks/NNS in/IN a/DT convex/NN manner/NN ./.
By/IN representing/VBG the/DT nonlinear/JJ convolutional/JJ filters/NNS as/IN vectors/NNS in/IN a/DT reproducing/VBG kernel/NN Hilbert/NNP space/NN ,/, the/DT CNN/NNP parameters/NNS can/MD be/VB represented/VBN as/IN a/DT low/JJ -/HYPH rank/NN matrix/NN ,/, which/WDT can/MD be/VB relaxed/JJ to/TO obtain/VB a/DT convex/NN optimization/NN problem/NN ./.
For/IN learning/VBG two/CD -/HYPH layer/NN convolutional/JJ neural/JJ networks/NNS ,/, we/PRP prove/VBP that/IN the/DT generalization/NN error/NN obtained/VBN by/IN a/DT convexified/JJ CNN/NNP converges/VBZ to/IN that/DT of/IN the/DT best/JJS possible/JJ CNN/NNP ./.
For/IN learning/VBG deeper/JJR networks/NNS ,/, we/PRP train/VBP CCNNs/NNS in/IN a/DT layer-wise/JJ manner/NN ./.
Empirically/RB ,/, CCNNs/NNS achieve/VBP performance/NN competitive/JJ with/IN CNNs/NNS trained/VBN by/IN backpropagation/NN ,/, SVMs/NNPS ,/, fully/RB -/HYPH connected/VBN neural/JJ networks/NNS ,/, stacked/VBN denoising/VBG auto/NN -/HYPH encoders/NNS ,/, and/CC other/JJ baseline/NN methods/NNS ./.
