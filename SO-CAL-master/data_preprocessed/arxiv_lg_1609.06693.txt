Deep/JJ neural/JJ networks/NNS are/VBP learning/VBG models/NNS with/IN a/DT very/RB high/JJ capacity/NN and/CC therefore/RB prone/JJ to/IN over-fitting/VBG ./.
Many/JJ regularization/NN techniques/NNS such/JJ as/IN Dropout/NNP ,/, DropCon/NNP -/HYPH nect/NNP ,/, and/CC weight/NN decay/NN all/DT attempt/NN to/TO solve/VB the/DT problem/NN of/IN over-fitting/VBG by/IN reducing/VBG the/DT capacity/NN of/IN their/PRP$ respective/JJ models/NNS (/-LRB- Srivastava/NNP et/FW al./FW ,/, 2014/CD )/-RRB- ,/, (/-LRB- Wan/NNP et/FW al./FW ,/, 2013/CD )/-RRB- ,/, (/-LRB- Krogh/NNP &amp;/CC Hertz/NNP ,/, 1992/CD )/-RRB- ./.
In/IN this/DT paper/NN we/PRP introduce/VBP a/DT new/JJ form/NN of/IN regularization/NN that/WDT guides/VBZ the/DT learning/NN problem/NN in/IN a/DT way/NN that/WDT reduces/VBZ over-fitting/NN without/IN sacrificing/VBG the/DT capacity/NN of/IN the/DT model/NN ./.
The/DT mistakes/NNS that/WDT models/NNS make/VBP in/IN early/JJ stages/NNS of/IN training/NN carry/VBP information/NN about/IN the/DT learning/NN problem/NN ./.
By/IN adjusting/VBG the/DT labels/NNS of/IN the/DT current/JJ epoch/NN of/IN training/NN through/IN a/DT weighted/JJ average/NN of/IN the/DT real/JJ labels/NNS ,/, and/CC an/DT exponential/JJ average/NN of/IN the/DT past/JJ soft/JJ -/HYPH targets/NNS we/PRP achieved/VBD a/DT regularization/NN scheme/NN as/RB powerful/JJ as/IN Dropout/NN without/IN necessarily/RB reducing/VBG the/DT capacity/NN of/IN the/DT model/NN ,/, and/CC simplified/VBD the/DT complexity/NN of/IN the/DT learning/NN problem/NN ./.
SoftTarget/NNP regularization/NN proved/VBD to/TO be/VB an/DT effective/JJ tool/NN in/IN various/JJ neural/JJ network/NN architectures/NNS ./.
