Reinforcement/NN learning/NN has/VBZ significant/JJ applications/NNS for/IN multi-agent/JJ systems/NNS ,/, especially/RB in/IN unknown/JJ dynamic/JJ environments/NNS ./.
However/RB ,/, most/JJS multi-agent/JJ reinforcement/NN learning/NN (/-LRB- MARL/NN )/-RRB- algorithms/NNS suffer/VBP from/IN such/JJ problems/NNS as/IN exponential/JJ computation/NN complexity/NN in/IN the/DT joint/JJ state/NN -/HYPH action/NN space/NN ,/, which/WDT makes/VBZ it/PRP difficult/JJ to/TO scale/VB up/RP to/IN realistic/JJ multi-agent/JJ problems/NNS ./.
In/IN this/DT paper/NN ,/, a/DT novel/JJ algorithm/NN named/VBN negotiation/NN -/HYPH based/VBN MARL/NNP with/IN sparse/JJ interactions/NNS (/-LRB- NegoSI/NN )/-RRB- is/VBZ presented/VBN ./.
In/IN contrast/NN to/IN traditional/JJ sparse/JJ -/HYPH interaction/NN based/VBN MARL/NNP algorithms/NNS ,/, NegoSI/NN adopts/VBZ the/DT equilibrium/NN concept/NN and/CC makes/VBZ it/PRP possible/JJ for/IN agents/NNS to/TO select/VB the/DT non-strict/JJ Equilibrium/NN Dominating/VBG Strategy/NNP Profile/NNP (/-LRB- non-strict/JJ EDSP/NN )/-RRB- or/CC Meta/NN equilibrium/NN for/IN their/PRP$ joint/JJ actions/NNS ./.
The/DT presented/VBN NegoSI/NN algorithm/NN consists/VBZ of/IN four/CD parts/NNS :/: the/DT equilibrium/NN -/HYPH based/VBN framework/NN for/IN sparse/JJ interactions/NNS ,/, the/DT negotiation/NN for/IN the/DT equilibrium/NN set/NN ,/, the/DT minimum/JJ variance/NN method/NN for/IN selecting/VBG one/CD joint/JJ action/NN and/CC the/DT knowledge/NN transfer/NN of/IN local/JJ Q/NN -/HYPH values/NNS ./.
In/IN this/DT integrated/VBN algorithm/NN ,/, three/CD techniques/NNS ,/, i.e./FW ,/, unshared/JJ value/NN functions/NNS ,/, equilibrium/NN solutions/NNS and/CC sparse/JJ interactions/NNS are/VBP adopted/VBN to/TO achieve/VB privacy/NN protection/NN ,/, better/JJR coordination/NN and/CC lower/JJR computational/JJ complexity/NN ,/, respectively/RB ./.
To/TO evaluate/VB the/DT performance/NN of/IN the/DT presented/VBN NegoSI/NN algorithm/NN ,/, two/CD groups/NNS of/IN experiments/NNS are/VBP carried/VBN out/RP regarding/VBG three/CD criteria/NNS :/: steps/NNS of/IN each/DT episode/NN (/-LRB- SEE/VB )/-RRB- ,/, rewards/NNS of/IN each/DT episode/NN (/-LRB- REE/NN )/-RRB- and/CC average/JJ runtime/NN (/-LRB- AR/NNP )/-RRB- ./.
The/DT first/JJ group/NN of/IN experiments/NNS is/VBZ conducted/VBN using/VBG six/CD grid/NN world/NN games/NNS and/CC shows/NNS fast/RB convergence/NN and/CC high/JJ scalability/NN of/IN the/DT presented/VBN algorithm/NN ./.
Then/RB in/IN the/DT second/JJ group/NN of/IN experiments/NNS NegoSI/NNP is/VBZ applied/VBN to/IN an/DT intelligent/JJ warehouse/NN problem/NN and/CC simulated/JJ results/NNS demonstrate/VBP the/DT effectiveness/NN of/IN the/DT presented/VBN NegoSI/NN algorithm/NN compared/VBN with/IN other/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN MARL/NN algorithms/NNS ./.
