We/PRP consider/VBP the/DT use/NN of/IN Frank/NNP -/HYPH Wolfe/NNP optimization/NN algorithms/NNS on/IN the/DT dual/JJ formulation/NN of/IN structural/JJ SVMs/NNS ./.
These/DT yield/VBP simple/JJ algorithms/NNS which/WDT only/RB need/VBP access/NN to/IN an/DT approximate/JJ maximization/NN oracle/NN for/IN the/DT structured/JJ prediction/NN problem/NN and/CC thus/RB have/VBP wide/JJ applicability/NN ./.
This/DT perspective/NN provides/VBZ insights/NNS on/IN previous/JJ popular/JJ algorithms/NNS as/IN we/PRP show/VBP that/IN batch/NN subgradient/NN as/RB well/RB as/IN the/DT cutting/NN plane/NN algorithms/NNS are/VBP equivalent/JJ to/IN versions/NNS of/IN Frank/NNP -/HYPH Wolfe/NNP algorithms/NNS ,/, enabling/VBG us/PRP to/TO improve/VB on/IN their/PRP$ convergence/NN analysis/NN by/IN harvesting/VBG the/DT Frank/NNP -/HYPH Wolfe/NNP literature/NN ./.
Moreover/RB ,/, we/PRP propose/VBP a/DT new/JJ stochastic/JJ coordinate/NN descent/NN version/NN of/IN Frank/NNP -/HYPH Wolfe/NNP which/WDT yields/VBZ a/DT provably/RB convergent/JJ optimization/NN algorithm/NN for/IN structural/JJ SVMs/NNS with/IN total/JJ run/NN -/HYPH time/NN independent/JJ of/IN the/DT number/NN of/IN training/NN examples/NNS ,/, like/IN Pegasos/NNP ,/, but/CC with/IN duality/NN gap/NN certificate/NN guarantees/NNS and/CC step/NN -/HYPH size/NN robustness/NN thanks/NNS to/IN the/DT use/NN of/IN line/NN -/HYPH search/NN ./.
Our/PRP$ experiments/NNS on/IN sequence/NN prediction/NN indicate/VBP that/IN this/DT simple/JJ algorithm/NN outperforms/VBZ all/DT other/JJ optimization/NN algorithms/NNS which/WDT only/RB have/VBP access/NN to/IN the/DT maximization/NN oracle/NN ./.
