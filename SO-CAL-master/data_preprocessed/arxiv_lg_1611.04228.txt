The/DT "/`` fire/NN together/RB ,/, wire/NN together/RB "/`` Hebbian/JJ model/NN is/VBZ a/DT central/JJ principle/NN for/IN learning/VBG in/IN neuroscience/NN ,/, but/CC surprisingly/RB ,/, it/PRP has/VBZ found/VBN limited/JJ applicability/NN in/IN modern/JJ machine/NN learning/NN ./.
In/IN this/DT paper/NN ,/, we/PRP take/VBP a/DT first/JJ step/NN towards/IN bridging/VBG this/DT gap/NN ,/, by/IN developing/VBG flavors/NNS of/IN competitive/JJ Hebbian/JJ learning/NN which/WDT produce/VBP sparse/JJ ,/, distributed/VBN neural/JJ codes/NNS using/VBG online/JJ adaptation/NN with/IN minimal/JJ tuning/NN ./.
We/PRP propose/VBP an/DT unsupervised/JJ algorithm/NN ,/, termed/VBN Adaptive/JJ Hebbian/JJ Learning/NN (/-LRB- AHL/NN )/-RRB- ./.
We/PRP illustrate/VBP the/DT distributed/VBN nature/NN of/IN the/DT learned/VBN representations/NNS via/IN output/NN entropy/NN computations/NNS for/IN synthetic/JJ data/NNS ,/, and/CC demonstrate/VBP superior/JJ performance/NN ,/, compared/VBN to/IN standard/JJ alternatives/NNS such/JJ as/IN autoencoders/NNS ,/, in/IN training/VBG a/DT deep/JJ convolutional/JJ net/NN on/IN standard/JJ image/NN datasets/NNS ./.
