Mixtures/NNS of/IN Experts/NNS combine/VBP the/DT outputs/NNS of/IN several/JJ "/`` expert/NN "/'' networks/NNS ,/, each/DT of/IN which/WDT specializes/VBZ in/IN a/DT different/JJ part/NN of/IN the/DT input/NN space/NN ./.
This/DT is/VBZ achieved/VBN by/IN training/VBG a/DT "/`` gating/NN "/'' network/NN that/WDT maps/VBZ each/DT input/NN to/IN a/DT distribution/NN over/IN the/DT experts/NNS ./.
Such/JJ models/NNS show/VBP promise/NN for/IN building/VBG larger/JJR networks/NNS that/WDT are/VBP still/RB cheap/JJ to/TO compute/VB at/IN test/NN time/NN ,/, and/CC more/RBR parallelizable/JJ at/IN training/NN time/NN ./.
In/IN this/DT this/DT work/NN ,/, we/PRP extend/VBP the/DT Mixture/NN of/IN Experts/NNS to/IN a/DT stacked/VBN model/NN ,/, the/DT Deep/NNP Mixture/NN of/IN Experts/NNS ,/, with/IN multiple/JJ sets/NNS of/IN gating/NN and/CC experts/NNS ./.
This/DT exponentially/RB increases/VBZ the/DT number/NN of/IN effective/JJ experts/NNS by/IN associating/VBG each/DT input/NN with/IN a/DT combination/NN of/IN experts/NNS at/IN each/DT layer/NN ,/, yet/CC maintains/VBZ a/DT modest/JJ model/NN size/NN ./.
On/IN a/DT randomly/RB translated/VBN version/NN of/IN the/DT MNIST/NN dataset/NN ,/, we/PRP find/VBP that/IN the/DT Deep/JJ Mixture/NN of/IN Experts/NNS automatically/RB learns/VBZ to/TO develop/VB location/NN -/HYPH dependent/JJ (/-LRB- "/`` where/WRB "/`` )/-RRB- experts/NNS at/IN the/DT first/JJ layer/NN ,/, and/CC class/NN -/HYPH specific/JJ (/-LRB- "/`` what/WP "/`` )/-RRB- experts/NNS at/IN the/DT second/JJ layer/NN ./.
In/IN addition/NN ,/, we/PRP see/VBP that/IN the/DT different/JJ combinations/NNS are/VBP in/IN use/NN when/WRB the/DT model/NN is/VBZ applied/VBN to/IN a/DT dataset/NN of/IN speech/NN monophones/NNS ./.
These/DT demonstrate/VBP effective/JJ use/NN of/IN all/DT expert/JJ combinations/NNS ./.
