Speaker/NNP identification/NN refers/VBZ to/IN the/DT task/NN of/IN localizing/VBG the/DT face/NN of/IN a/DT person/NN who/WP has/VBZ the/DT same/JJ identity/NN as/IN the/DT ongoing/JJ voice/NN in/IN a/DT video/NN ./.
This/DT task/NN not/RB only/RB requires/VBZ collective/JJ perception/NN over/IN both/DT visual/JJ and/CC auditory/JJ signals/NNS ,/, the/DT robustness/NN to/TO handle/VB severe/JJ quality/NN degradations/NNS and/CC unconstrained/JJ content/NN variations/NNS are/VBP also/RB indispensable/JJ ./.
In/IN this/DT paper/NN ,/, we/PRP describe/VBP a/DT novel/JJ multimodal/JJ Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- architecture/NN which/WDT seamlessly/RB unifies/VBZ both/CC visual/JJ and/CC auditory/JJ modalities/NNS from/IN the/DT beginning/NN of/IN each/DT sequence/NN input/NN ./.
The/DT key/JJ idea/NN is/VBZ to/TO extend/VB the/DT conventional/JJ LSTM/NN by/IN not/RB only/RB sharing/VBG weights/NNS across/IN time/NN steps/NNS ,/, but/CC also/RB sharing/VBG weights/NNS across/IN modalities/NNS ./.
We/PRP show/VBP that/IN modeling/VBG the/DT temporal/JJ dependency/NN across/IN face/NN and/CC voice/NN can/MD significantly/RB improve/VB the/DT robustness/NN to/IN content/JJ quality/NN degradations/NNS and/CC variations/NNS ./.
We/PRP also/RB found/VBD that/IN our/PRP$ multimodal/JJ LSTM/NNP is/VBZ robustness/NN to/IN distractors/NNS ,/, namely/RB the/DT non-speaking/JJ identities/NNS ./.
We/PRP applied/VBD our/PRP$ multimodal/JJ LSTM/NN to/IN The/DT Big/NNP Bang/NNP Theory/NNP dataset/NN and/CC showed/VBD that/IN our/PRP$ system/NN outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS in/IN speaker/NN identification/NN with/IN lower/JJR false/JJ alarm/NN rate/NN and/CC higher/JJR recognition/NN accuracy/NN ./.
