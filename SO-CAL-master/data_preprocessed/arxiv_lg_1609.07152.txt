This/DT paper/NN presents/VBZ the/DT input/NN convex/NN neural/JJ network/NN architecture/NN ./.
These/DT are/VBP scalar/JJ -/HYPH valued/VBN (/-LRB- potentially/RB deep/JJ )/-RRB- neural/JJ networks/NNS with/IN constraints/NNS on/IN the/DT network/NN parameters/NNS such/JJ that/IN the/DT output/NN of/IN the/DT network/NN is/VBZ a/DT convex/NN function/NN of/IN (/-LRB- some/DT of/IN )/-RRB- the/DT inputs/NNS ./.
The/DT networks/NNS allow/VBP for/IN efficient/JJ inference/NN via/IN optimization/NN over/IN some/DT inputs/NNS to/IN the/DT network/NN given/VBN others/NNS ,/, and/CC can/MD be/VB applied/VBN to/IN settings/NNS including/VBG structured/VBN prediction/NN ,/, data/NNS imputation/NN ,/, reinforcement/NN learning/NN ,/, and/CC others/NNS ./.
In/IN this/DT paper/NN we/PRP lay/VBD the/DT basic/JJ groundwork/NN for/IN these/DT models/NNS ,/, proposing/VBG methods/NNS for/IN inference/NN ,/, optimization/NN and/CC learning/NN ,/, and/CC analyze/VB their/PRP$ representational/JJ power/NN ./.
We/PRP show/VBP that/IN many/JJ existing/VBG neural/JJ network/NN architectures/NNS can/MD be/VB made/VBN input/NN -/HYPH convex/NN with/IN only/JJ minor/JJ modification/NN ,/, and/CC develop/VB specialized/JJ optimization/NN algorithms/NNS tailored/VBN to/IN this/DT setting/NN ./.
Finally/RB ,/, we/PRP highlight/VBD the/DT performance/NN of/IN the/DT methods/NNS on/IN multi-label/JJ prediction/NN ,/, image/NN completion/NN ,/, and/CC reinforcement/NN learning/NN problems/NNS ,/, where/WRB we/PRP show/VBP improvement/NN over/IN the/DT existing/VBG state/NN of/IN the/DT art/NN in/IN many/JJ cases/NNS ./.
