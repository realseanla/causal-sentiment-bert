LSTD/NNP is/VBZ a/DT popular/JJ algorithm/NN for/IN value/NN function/NN approximation/NN ./.
Whenever/WRB the/DT number/NN of/IN features/NNS is/VBZ larger/JJR than/IN the/DT number/NN of/IN samples/NNS ,/, it/PRP must/MD be/VB paired/VBN with/IN some/DT form/NN of/IN regularization/NN ./.
In/IN particular/JJ ,/, L1/NN -/HYPH regularization/NN methods/NNS tend/VBP to/TO perform/VB feature/NN selection/NN by/IN promoting/VBG sparsity/NN ,/, and/CC thus/RB ,/, are/VBP well/RB -/HYPH suited/VBN for/IN high/JJ -/HYPH dimensional/JJ problems/NNS ./.
However/RB ,/, since/IN LSTD/NNP is/VBZ not/RB a/DT simple/JJ regression/NN algorithm/NN ,/, but/CC it/PRP solves/VBZ a/DT fixed/VBN --/: point/NN problem/NN ,/, its/PRP$ integration/NN with/IN L1/NN -/HYPH regularization/NN is/VBZ not/RB straightforward/JJ and/CC might/MD come/VB with/IN some/DT drawbacks/NNS (/-LRB- e.g./FW ,/, the/DT P/NN -/HYPH matrix/NN assumption/NN for/IN LASSO/NN -/HYPH TD/NN )/-RRB- ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP a/DT novel/JJ algorithm/NN obtained/VBN by/IN integrating/VBG LSTD/NN with/IN the/DT Dantzig/NNP Selector/NN ./.
We/PRP investigate/VBP the/DT performance/NN of/IN the/DT proposed/VBN algorithm/NN and/CC its/PRP$ relationship/NN with/IN the/DT existing/VBG regularized/VBN approaches/NNS ,/, and/CC show/VB how/WRB it/PRP addresses/VBZ some/DT of/IN their/PRP$ drawbacks/NNS ./.
