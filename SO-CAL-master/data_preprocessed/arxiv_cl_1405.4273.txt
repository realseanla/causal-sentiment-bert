This/DT paper/NN presents/VBZ a/DT scalable/JJ method/NN for/IN integrating/VBG compositional/JJ morphological/JJ representations/NNS into/IN a/DT vector/NN -/HYPH based/VBN probabilistic/JJ language/NN model/NN ./.
Our/PRP$ approach/NN is/VBZ evaluated/VBN in/IN the/DT context/NN of/IN log/NN -/HYPH bilinear/NN language/NN models/NNS ,/, rendered/VBN suitably/RB efficient/JJ for/IN implementation/NN inside/IN a/DT machine/NN translation/NN decoder/NN by/IN factoring/NN the/DT vocabulary/NN ./.
We/PRP perform/VBP both/DT intrinsic/JJ and/CC extrinsic/JJ evaluations/NNS ,/, presenting/VBG results/NNS on/IN a/DT range/NN of/IN languages/NNS which/WDT demonstrate/VBP that/IN our/PRP$ model/NN learns/VBZ morphological/JJ representations/NNS that/IN both/DT perform/VB well/RB on/IN word/NN similarity/NN tasks/NNS and/CC lead/VB to/IN substantial/JJ reductions/NNS in/IN perplexity/NN ./.
When/WRB used/VBN for/IN translation/NN into/IN morphologically/RB rich/JJ languages/NNS with/IN large/JJ vocabularies/NNS ,/, our/PRP$ models/NNS obtain/VB improvements/NNS of/IN up/RB to/IN 1.2/CD BLEU/NN points/NNS relative/JJ to/IN a/DT baseline/NN system/NN using/VBG back/RB -/HYPH off/RB n/NN -/HYPH gram/NN models/NNS ./.
