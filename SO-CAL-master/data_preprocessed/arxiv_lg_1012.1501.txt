We/PRP consider/VBP a/DT class/NN of/IN sparsity/NN -/HYPH inducing/VBG regularization/NN terms/NNS based/VBN on/IN submodular/JJ functions/NNS ./.
While/IN earlier/JJR work/NN has/VBZ focused/VBN on/IN non-decreasing/JJ functions/NNS ,/, we/PRP explore/VBP symmetric/JJ submodular/JJ functions/NNS and/CC their/PRP$ Lovasz/NNP extensions/NNS ./.
We/PRP show/VBP that/IN the/DT Lovasz/NNP extension/NN may/MD be/VB seen/VBN as/IN the/DT convex/NN envelope/NN of/IN a/DT function/NN that/WDT depends/VBZ on/IN level/NN sets/NNS :/: this/DT leads/VBZ to/IN a/DT class/NN of/IN convex/NN structured/VBN regularization/NN terms/NNS that/WDT impose/VBP prior/JJ knowledge/NN on/IN the/DT level/NN sets/NNS ,/, and/CC not/RB on/IN the/DT supports/NNS of/IN the/DT underlying/VBG predictors/NNS ./.
We/PRP provide/VBP a/DT unified/VBN set/NN of/IN optimization/NN algorithms/NNS (/-LRB- such/JJ as/IN proximal/JJ operators/NNS )/-RRB- ,/, and/CC theoretical/JJ guarantees/NNS (/-LRB- allowed/VBN level/NN sets/NNS and/CC recovery/NN conditions/NNS )/-RRB- ./.
By/IN selecting/VBG specific/JJ submodular/JJ functions/NNS ,/, we/PRP give/VBP a/DT new/JJ interpretation/NN to/IN known/VBN norms/NNS ,/, such/JJ as/IN the/DT total/JJ variation/NN ;/: we/PRP also/RB define/VBP new/JJ norms/NNS ,/, in/IN particular/JJ ones/NNS that/WDT are/VBP based/VBN on/IN order/NN statistics/NNS ,/, and/CC on/IN noisy/JJ cuts/NNS in/IN graphs/NNS ./.
