The/DT k-fold/JJ cross-validation/NN is/VBZ commonly/RB used/VBN to/TO evaluate/VB the/DT effectiveness/NN of/IN SVMs/NNS with/IN the/DT selected/VBN hyper/JJ -/HYPH parameters/NNS ./.
It/PRP is/VBZ known/VBN that/IN the/DT SVM/NNP k-fold/RB cross-validation/NN is/VBZ expensive/JJ ,/, since/IN it/PRP requires/VBZ training/NN k/CD SVMs/NNS ./.
However/RB ,/, little/JJ work/NN has/VBZ explored/VBN reusing/VBG the/DT h/NN -/HYPH th/IN SVM/NNP for/IN training/VBG the/DT (/-LRB- h/NN 1/CD )/-RRB- -/HYPH th/IN SVM/NNP for/IN improving/VBG the/DT efficiency/NN of/IN k-fold/JJ cross-validation/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP three/CD algorithms/NNS that/WDT reuse/VBP the/DT h/NN -/HYPH th/IN SVM/NNP for/IN improving/VBG the/DT efficiency/NN of/IN training/VBG the/DT (/-LRB- h/NN 1/CD )/-RRB- -/HYPH th/IN SVM/NNP ./.
Our/PRP$ key/JJ idea/NN is/VBZ to/TO efficiently/RB identify/VB the/DT support/NN vectors/NNS and/CC to/TO accurately/RB estimate/VB their/PRP$ associated/VBN weights/NNS (/-LRB- also/RB called/VBN alpha/NN values/NNS )/-RRB- of/IN the/DT next/JJ SVM/NN by/IN using/VBG the/DT previous/JJ SVM/NNP ./.
Our/PRP$ experimental/JJ results/NNS show/VBP that/IN our/PRP$ algorithms/NNS are/VBP several/JJ times/NNS faster/JJR than/IN the/DT k-fold/JJ cross-validation/NN which/WDT does/VBZ not/RB make/VB use/NN of/IN the/DT previously/RB trained/VBN SVM/NNP ./.
Moreover/RB ,/, our/PRP$ algorithms/NNS produce/VBP the/DT same/JJ results/NNS (/-LRB- hence/RB same/JJ accuracy/NN )/-RRB- as/IN the/DT k-fold/JJ cross-validation/NN which/WDT does/VBZ not/RB make/VB use/NN of/IN the/DT previously/RB trained/VBN SVM/NNP ./.
