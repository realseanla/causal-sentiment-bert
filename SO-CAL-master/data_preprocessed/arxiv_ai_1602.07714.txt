Learning/VBG non-linear/JJ functions/NNS can/MD be/VB hard/JJ when/WRB the/DT magnitude/NN of/IN the/DT target/NN function/NN is/VBZ unknown/JJ beforehand/RB ,/, as/IN most/JJS learning/NN algorithms/NNS are/VBP not/RB scale/VB invariant/JJ ./.
We/PRP propose/VBP an/DT algorithm/NN to/TO adaptively/RB normalize/VB these/DT targets/NNS ./.
This/DT is/VBZ complementary/JJ to/IN recent/JJ advances/NNS in/IN input/NN normalization/NN ./.
Importantly/RB ,/, the/DT proposed/JJ method/NN preserves/VBZ the/DT unnormalized/JJ outputs/NNS whenever/WRB the/DT normalization/NN is/VBZ updated/VBN to/TO avoid/VB instability/NN caused/VBN by/IN non-stationarity/NN ./.
It/PRP can/MD be/VB combined/VBN with/IN any/DT learning/NN algorithm/NN and/CC any/DT non-linear/JJ function/NN approximation/NN ,/, including/VBG the/DT important/JJ special/JJ case/NN of/IN deep/JJ learning/NN ./.
We/PRP empirically/RB validate/VBP the/DT method/NN in/IN supervised/JJ learning/NN and/CC reinforcement/NN learning/NN and/CC apply/VB it/PRP to/IN learning/VBG how/WRB to/TO play/VB Atari/NNP 2600/CD games/NNS ./.
Previous/JJ work/NN on/IN applying/VBG deep/JJ learning/NN to/IN this/DT domain/NN relied/VBD on/IN clipping/VBG the/DT rewards/NNS to/TO make/VB learning/NN in/IN different/JJ games/NNS more/RBR homogeneous/JJ ,/, but/CC this/DT uses/VBZ the/DT domain/NN -/HYPH specific/JJ knowledge/NN that/WDT in/IN these/DT games/NNS counting/VBG rewards/NNS is/VBZ often/RB almost/RB as/IN informative/JJ as/IN summing/VBG these/DT ./.
Using/VBG our/PRP$ adaptive/JJ normalization/NN we/PRP can/MD remove/VB this/DT heuristic/NN without/IN diminishing/VBG overall/JJ performance/NN ,/, and/CC even/RB improve/VB performance/NN on/IN some/DT games/NNS ,/, such/JJ as/IN Ms./NNP Pac/NNP -/HYPH Man/NNP and/CC Centipede/NNP ,/, on/IN which/WDT previous/JJ methods/NNS did/VBD not/RB perform/VB well/RB ./.
