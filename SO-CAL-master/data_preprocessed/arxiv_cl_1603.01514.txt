We/PRP propose/VBP a/DT Bayesian/JJ model/NN of/IN unsupervised/JJ semantic/JJ role/NN induction/NN in/IN multiple/JJ languages/NNS ,/, and/CC use/VB it/PRP to/TO explore/VB the/DT usefulness/NN of/IN parallel/JJ corpora/NNS for/IN this/DT task/NN ./.
Our/PRP$ joint/JJ Bayesian/JJ model/NN consists/VBZ of/IN individual/JJ models/NNS for/IN each/DT language/NN plus/CC additional/JJ latent/JJ variables/NNS that/WDT capture/VBP alignments/NNS between/IN roles/NNS across/IN languages/NNS ./.
Because/IN it/PRP is/VBZ a/DT generative/JJ Bayesian/JJ model/NN ,/, we/PRP can/MD do/VB evaluations/NNS in/IN a/DT variety/NN of/IN scenarios/NNS just/RB by/IN varying/VBG the/DT inference/NN procedure/NN ,/, without/IN changing/VBG the/DT model/NN ,/, thereby/RB comparing/VBG the/DT scenarios/NNS directly/RB ./.
We/PRP compare/VBP using/VBG only/RB monolingual/JJ data/NNS ,/, using/VBG a/DT parallel/JJ corpus/NN ,/, using/VBG a/DT parallel/JJ corpus/NN with/IN annotations/NNS in/IN the/DT other/JJ language/NN ,/, and/CC using/VBG small/JJ amounts/NNS of/IN annotation/NN in/IN the/DT target/NN language/NN ./.
We/PRP find/VBP that/IN the/DT biggest/JJS impact/NN of/IN adding/VBG a/DT parallel/JJ corpus/NN to/IN training/NN is/VBZ actually/RB the/DT increase/NN in/IN mono/NN -/HYPH lingual/NN data/NNS ,/, with/IN the/DT alignments/NNS to/IN another/DT language/NN resulting/VBG in/IN small/JJ improvements/NNS ,/, even/RB with/IN labeled/VBN data/NNS for/IN the/DT other/JJ language/NN ./.
