In/IN this/DT work/NN we/PRP introduce/VBP a/DT binarized/VBN deep/JJ neural/JJ network/NN (/-LRB- BDNN/NN )/-RRB- model/NN ./.
BDNNs/NNS are/VBP trained/VBN using/VBG a/DT novel/JJ binarized/VBN back/RB propagation/NN algorithm/NN (/-LRB- BBP/NN )/-RRB- ,/, which/WDT uses/VBZ binary/JJ weights/NNS and/CC binary/JJ neurons/NNS during/IN the/DT forward/JJ and/CC backward/JJ propagation/NN ,/, while/IN retaining/VBG precision/NN of/IN the/DT stored/VBN weights/NNS in/IN which/WDT gradients/NNS are/VBP accumulated/VBN ./.
At/IN test/NN phase/NN ,/, BDNNs/NNS are/VBP fully/RB binarized/VBN and/CC can/MD be/VB implemented/VBN in/IN hardware/NN with/IN low/JJ circuit/NN complexity/NN ./.
The/DT proposed/VBN binarized/VBN networks/NNS can/MD be/VB implemented/VBN using/VBG binary/JJ convolutions/NNS and/CC proxy/NN matrix/NN multiplications/NNS with/IN only/RB standard/JJ binary/JJ XNOR/NN and/CC population/NN count/NN (/-LRB- popcount/NN )/-RRB- operations/NNS ./.
BBP/NN is/VBZ expected/VBN to/TO reduce/VB energy/NN consumption/NN by/IN at/IN least/RBS two/CD orders/NNS of/IN magnitude/NN when/WRB compared/VBN to/IN the/DT hardware/NN implementation/NN of/IN existing/VBG training/NN algorithms/NNS ./.
We/PRP obtained/VBD near/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS with/IN BDNNs/NNS on/IN the/DT permutation/NN -/HYPH invariant/JJ MNIST/NN ,/, CIFAR/NN -/HYPH 10/CD and/CC SVHN/NN datasets/NNS ./.
