In/IN imitation/NN learning/NN ,/, an/DT agent/NN learns/VBZ how/WRB to/TO behave/VB in/IN an/DT environment/NN with/IN an/DT unknown/JJ cost/NN function/NN by/IN mimicking/VBG expert/JJ demonstrations/NNS ./.
Existing/VBG imitation/NN learning/NN algorithms/NNS typically/RB involve/VBP solving/VBG a/DT sequence/NN of/IN planning/NN or/CC reinforcement/NN learning/NN problems/NNS ./.
Such/JJ algorithms/NNS are/VBP therefore/RB not/RB directly/RB applicable/JJ to/IN large/JJ ,/, high/JJ -/HYPH dimensional/JJ environments/NNS ,/, and/CC their/PRP$ performance/NN can/MD significantly/RB degrade/VB if/IN the/DT planning/NN problems/NNS are/VBP not/RB solved/VBN to/IN optimality/NN ./.
Under/IN the/DT apprenticeship/NN learning/VBG formalism/NN ,/, we/PRP develop/VBP alternative/JJ model/NN -/HYPH free/JJ algorithms/NNS for/IN finding/VBG a/DT parameterized/JJ stochastic/JJ policy/NN that/WDT performs/VBZ at/IN least/JJS as/RB well/RB as/IN an/DT expert/NN policy/NN on/IN an/DT unknown/JJ cost/NN function/NN ,/, based/VBN on/IN sample/NN trajectories/NNS from/IN the/DT expert/NN ./.
Our/PRP$ approach/NN ,/, based/VBN on/IN policy/NN gradients/NNS ,/, scales/NNS to/IN large/JJ continuous/JJ environments/NNS with/IN guaranteed/VBN convergence/NN to/IN local/JJ minima/NN ./.
