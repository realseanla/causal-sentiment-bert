The/DT supervised/JJ learning/NN of/IN the/DT discriminative/JJ convolutional/JJ neural/JJ network/NN (/-LRB- ConvNet/NNP )/-RRB- is/VBZ powered/VBN by/IN back/RB -/HYPH propagation/NN on/IN the/DT parameters/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP show/VBP that/IN the/DT unsupervised/JJ learning/NN of/IN a/DT popular/JJ top/JJ -/HYPH down/JJ generative/JJ ConvNet/NNP model/NN with/IN latent/JJ continuous/JJ factors/NNS can/MD be/VB accomplished/VBN by/IN a/DT learning/NN algorithm/NN that/WDT consists/VBZ of/IN alternatively/RB performing/VBG back/RB -/HYPH propagation/NN on/IN both/CC the/DT latent/JJ factors/NNS and/CC the/DT parameters/NNS ./.
The/DT model/NN is/VBZ a/DT non-linear/JJ generalization/NN of/IN factor/NN analysis/NN ,/, where/WRB the/DT high/JJ -/HYPH dimensional/JJ observed/VBN data/NNS ,/, is/VBZ assumed/VBN to/TO be/VB the/DT noisy/JJ version/NN of/IN a/DT vector/NN generated/VBN by/IN a/DT non-linear/JJ transformation/NN of/IN a/DT low/JJ -/HYPH dimensional/JJ vector/NN of/IN continuous/JJ latent/JJ factors/NNS ./.
Furthermore/RB ,/, it/PRP is/VBZ assumed/VBN that/IN these/DT latent/JJ factors/NNS follow/VBP known/VBN independent/JJ distributions/NNS ,/, such/JJ as/IN standard/JJ normal/JJ distributions/NNS ,/, and/CC the/DT non-linear/JJ transformation/NN is/VBZ assumed/VBN to/TO be/VB parametrized/VBN by/IN a/DT top/JJ -/HYPH down/JJ ConvNet/NNP ,/, which/WDT is/VBZ capable/JJ of/IN approximating/VBG the/DT highly/RB non-linear/JJ mapping/NN from/IN the/DT latent/JJ factors/NNS to/IN the/DT image/NN ./.
We/PRP explore/VBP a/DT simple/JJ and/CC natural/JJ learning/NN algorithm/NN for/IN this/DT model/NN that/WDT alternates/VBZ between/IN the/DT following/VBG two/CD steps/NNS :/: (/-LRB- 1/LS )/-RRB- inferring/VBG the/DT latent/JJ factors/NNS by/IN Langevin/NNP dynamics/NNS or/CC gradient/NN descent/NN ,/, and/CC (/-LRB- 2/LS )/-RRB- updating/VBG the/DT parameters/NNS of/IN the/DT ConvNet/NNP by/IN gradient/NN descent/NN ./.
Step/NN (/-LRB- 1/CD )/-RRB- is/VBZ based/VBN on/IN the/DT gradient/NN of/IN the/DT reconstruction/NN error/NN with/IN respect/NN to/IN the/DT latent/JJ factors/NNS ,/, which/WDT is/VBZ available/JJ by/IN back/RB -/HYPH propagation/NN ./.
We/PRP call/VBP this/DT step/NN inferential/JJ back/RB -/HYPH propagation/NN ./.
Step/NN (/-LRB- 2/CD )/-RRB- is/VBZ based/VBN on/IN the/DT gradient/NN of/IN the/DT reconstruction/NN error/NN with/IN respect/NN to/IN the/DT parameters/NNS ,/, and/CC is/VBZ also/RB obtained/VBN by/IN back/RB -/HYPH propagation/NN ./.
We/PRP refer/VBP to/IN this/DT step/NN as/IN learning/VBG back/RB -/HYPH propagation/NN ./.
The/DT code/NN for/IN inferential/JJ back/RB -/HYPH propagation/NN is/VBZ actually/RB part/NN of/IN the/DT code/NN for/IN learning/VBG back/RB -/HYPH propagation/NN ,/, and/CC thus/RB the/DT inferential/JJ back/NN -/HYPH propagation/NN is/VBZ actually/RB a/DT by/IN -/HYPH product/NN of/IN the/DT learning/NN back/RB -/HYPH propagation/NN ./.
We/PRP show/VBP that/IN our/PRP$ algorithm/NN can/MD learn/VB realistic/JJ generative/JJ models/NNS of/IN images/NNS and/CC sounds/NNS ./.
