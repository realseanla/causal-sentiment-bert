We/PRP present/VBP an/DT algorithm/NN for/IN the/DT statistical/JJ learning/NN setting/VBG with/IN a/DT bounded/VBN exp/NN -/HYPH concave/NN loss/NN in/IN $/$ d/LS $/$ dimensions/NNS that/WDT obtains/VBZ excess/JJ risk/NN $/$ O/UH (/-LRB- d/NN //HYPH n/NN )/-RRB- $/$ with/IN high/JJ probability/NN :/: the/DT dependence/NN on/IN the/DT confidence/NN parameter/NN $/$ \/SYM delta/NN $/$ is/VBZ polylogarithmic/JJ in/IN $/$ 1/CD //HYPH \/SYM delta/NN $/$ ./.
The/DT core/NN technique/NN is/VBZ to/TO boost/VB the/DT confidence/NN of/IN recent/JJ $/NN O/NN (/-LRB- d/NN //HYPH n/NN )/-RRB- $/$ bounds/NNS ,/, without/IN sacrificing/VBG the/DT rate/NN ,/, by/IN leveraging/VBG a/DT Bernstein/NNP -/HYPH type/NN condition/NN which/WDT holds/VBZ due/IN to/IN exp/NN -/HYPH concavity/NN ./.
This/DT Bernstein/NNP -/HYPH type/NN condition/NN implies/VBZ that/IN the/DT variance/NN of/IN excess/JJ loss/NN random/JJ variables/NNS are/VBP controlled/VBN in/IN terms/NNS of/IN their/PRP$ excess/JJ risk/NN ./.
Using/VBG this/DT variance/NN control/NN ,/, we/PRP further/RB show/VBP that/IN a/DT regret/NN bound/VBN for/IN any/DT online/JJ learner/NN in/IN this/DT setting/NN translates/VBZ to/IN a/DT high/JJ probability/NN excess/JJ risk/NN bound/VBN for/IN the/DT corresponding/VBG online/JJ -/HYPH to/IN -/HYPH batch/NN conversion/NN of/IN the/DT online/JJ learner/NN ./.
