We/PRP introduce/VBP word/NN vectors/NNS for/IN the/DT construction/NN domain/NN ./.
Our/PRP$ vectors/NNS were/VBD obtained/VBN by/IN running/VBG word2vec/NN on/IN an/DT 11M/NN -/HYPH word/NN corpus/NN that/WDT we/PRP created/VBD from/IN scratch/NN by/IN leveraging/VBG freely/RB -/HYPH accessible/JJ online/JJ sources/NNS of/IN construction/NN -/HYPH related/VBN text/NN ./.
We/PRP first/RB explore/VB the/DT embedding/NN space/NN and/CC show/VBP that/IN our/PRP$ vectors/NNS capture/VBP meaningful/JJ construction/NN -/HYPH specific/JJ concepts/NNS ./.
We/PRP then/RB evaluate/VB the/DT performance/NN of/IN our/PRP$ vectors/NNS against/IN that/DT of/IN ones/NNS trained/VBN on/IN a/DT 100B/NN -/HYPH word/NN corpus/NN (/-LRB- Google/NNP News/NNP )/-RRB- within/IN the/DT framework/NN of/IN an/DT injury/NN report/NN classification/NN task/NN ./.
Without/IN any/DT parameter/NN tuning/NN ,/, our/PRP$ embeddings/NNS give/VBP competitive/JJ results/NNS ,/, and/CC outperform/VB the/DT Google/NNP News/NNP vectors/NNS in/IN many/JJ cases/NNS ./.
Using/VBG a/DT keyword/NN -/HYPH based/VBN compression/NN of/IN the/DT reports/NNS also/RB leads/VBZ to/IN a/DT significant/JJ speed/NN -/HYPH up/NN with/IN only/RB a/DT limited/JJ loss/NN in/IN performance/NN ./.
We/PRP release/VBP our/PRP$ corpus/NN and/CC the/DT data/NNS set/VBP we/PRP created/VBN for/IN the/DT classification/NN task/NN as/IN publicly/RB available/JJ ,/, in/IN the/DT hope/NN that/IN they/PRP will/MD be/VB used/VBN by/IN future/JJ studies/NNS for/IN benchmarking/NN and/CC building/NN on/IN our/PRP$ work/NN ./.
