In/IN this/DT paper/NN we/PRP consider/VBP learning/VBG in/IN passive/JJ setting/NN but/CC with/IN a/DT slight/JJ modification/NN ./.
We/PRP assume/VBP that/IN the/DT target/NN expected/VBN loss/NN ,/, also/RB referred/VBD to/IN as/IN target/NN risk/NN ,/, is/VBZ provided/VBN in/IN advance/NN for/IN learner/NN as/IN prior/JJ knowledge/NN ./.
Unlike/IN most/JJS studies/NNS in/IN the/DT learning/NN theory/NN that/IN only/RB incorporate/VB the/DT prior/JJ knowledge/NN into/IN the/DT generalization/NN bounds/NNS ,/, we/PRP are/VBP able/JJ to/TO explicitly/RB utilize/VB the/DT target/NN risk/NN in/IN the/DT learning/NN process/NN ./.
Our/PRP$ analysis/NN reveals/VBZ a/DT surprising/JJ result/NN on/IN the/DT sample/NN complexity/NN of/IN learning/NN :/: by/IN exploiting/VBG the/DT target/NN risk/NN in/IN the/DT learning/NN algorithm/NN ,/, we/PRP show/VBP that/IN when/WRB the/DT loss/NN function/NN is/VBZ both/DT strongly/RB convex/JJ and/CC smooth/JJ ,/, the/DT sample/NN complexity/NN reduces/VBZ to/IN $/$ \/SYM O/NN (/-LRB- \/SYM log/NN (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN }/-RRB- )/-RRB- )/-RRB- $/$ ,/, an/DT exponential/JJ improvement/NN compared/VBN to/IN the/DT sample/NN complexity/NN $/$ \/SYM O/NN (/-LRB- \/SYM frac/NN {/-LRB- 1/CD }/-RRB- {/-LRB- \/SYM epsilon/NN }/-RRB- )/-RRB- $/$ for/IN learning/VBG with/IN strongly/RB convex/JJ loss/NN functions/NNS ./.
Furthermore/RB ,/, our/PRP$ proof/NN is/VBZ constructive/JJ and/CC is/VBZ based/VBN on/IN a/DT computationally/RB efficient/JJ stochastic/JJ optimization/NN algorithm/NN for/IN such/JJ settings/NNS which/WDT demonstrate/VBP that/IN the/DT proposed/VBN algorithm/NN is/VBZ practically/RB useful/JJ ./.
