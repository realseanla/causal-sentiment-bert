Aiming/VBG at/IN solving/VBG large/JJ -/HYPH scale/NN learning/NN problems/NNS ,/, this/DT paper/NN studies/NNS distributed/VBN optimization/NN methods/NNS based/VBN on/IN the/DT alternating/VBG direction/NN method/NN of/IN multipliers/NNS (/-LRB- ADMM/NN )/-RRB- ./.
By/IN formulating/VBG the/DT learning/NN problem/NN as/IN a/DT consensus/NN problem/NN ,/, the/DT ADMM/NNP can/MD be/VB used/VBN to/TO solve/VB the/DT consensus/NN problem/NN in/IN a/DT fully/RB parallel/JJ fashion/NN over/IN a/DT computer/NN network/NN with/IN a/DT star/NN topology/NN ./.
However/RB ,/, traditional/JJ synchronized/VBN computation/NN does/VBZ not/RB scale/VB well/RB with/IN the/DT problem/NN size/NN ,/, as/IN the/DT speed/NN of/IN the/DT algorithm/NN is/VBZ limited/VBN by/IN the/DT slowest/JJS workers/NNS ./.
This/DT is/VBZ particularly/RB true/JJ in/IN a/DT heterogeneous/JJ network/NN where/WRB the/DT computing/VBG nodes/NNS experience/VBP different/JJ computation/NN and/CC communication/NN delays/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP an/DT asynchronous/JJ distributed/VBN ADMM/NN (/-LRB- AD/NN -/HYPH AMM/NN )/-RRB- which/WDT can/MD effectively/RB improve/VB the/DT time/NN efficiency/NN of/IN distributed/VBN optimization/NN ./.
Our/PRP$ main/JJ interest/NN lies/VBZ in/IN analyzing/VBG the/DT convergence/NN conditions/NNS of/IN the/DT AD/NN -/HYPH ADMM/NN ,/, under/IN the/DT popular/JJ partially/RB asynchronous/JJ model/NN ,/, which/WDT is/VBZ defined/VBN based/VBN on/IN a/DT maximum/JJ tolerable/JJ delay/NN of/IN the/DT network/NN ./.
Specifically/RB ,/, by/IN considering/VBG general/JJ and/CC possibly/RB non-convex/JJ cost/NN functions/NNS ,/, we/PRP show/VBP that/IN the/DT AD/NN -/HYPH ADMM/NN is/VBZ guaranteed/VBN to/TO converge/VB to/IN the/DT set/NN of/IN Karush/NNP -/HYPH Kuhn/NNP -/HYPH Tucker/NNP (/-LRB- KKT/NNP )/-RRB- points/NNS as/RB long/RB as/IN the/DT algorithm/NN parameters/NNS are/VBP chosen/VBN appropriately/RB according/VBG to/IN the/DT network/NN delay/NN ./.
We/PRP further/RB illustrate/VBP that/IN the/DT asynchrony/NN of/IN the/DT ADMM/NNP has/VBZ to/TO be/VB handled/VBN with/IN care/NN ,/, as/IN slightly/RB modifying/VBG the/DT implementation/NN of/IN the/DT AD/NN -/HYPH ADMM/NN can/MD jeopardize/VB the/DT algorithm/NN convergence/NN ,/, even/RB under/IN a/DT standard/JJ convex/NN setting/NN ./.
