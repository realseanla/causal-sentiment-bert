We/PRP investigate/VBP a/DT general/JJ framework/NN of/IN multiplicative/JJ multitask/JJ feature/NN learning/NN which/WDT decomposes/VBZ each/DT task/NN 's/POS model/NN parameters/NNS into/IN a/DT multiplication/NN of/IN two/CD components/NNS ./.
One/CD of/IN the/DT components/NNS is/VBZ used/VBN across/IN all/DT tasks/NNS and/CC the/DT other/JJ component/NN is/VBZ task/NN -/HYPH specific/JJ ./.
Several/JJ previous/JJ methods/NNS have/VBP been/VBN proposed/VBN as/IN special/JJ cases/NNS of/IN our/PRP$ framework/NN ./.
We/PRP study/VBP the/DT theoretical/JJ properties/NNS of/IN this/DT framework/NN when/WRB different/JJ regularization/NN conditions/NNS are/VBP applied/VBN to/IN the/DT two/CD decomposed/VBN components/NNS ./.
We/PRP prove/VBP that/IN this/DT framework/NN is/VBZ mathematically/RB equivalent/JJ to/IN the/DT widely/RB used/VBN multitask/JJ feature/NN learning/NN methods/NNS that/WDT are/VBP based/VBN on/IN a/DT joint/JJ regularization/NN of/IN all/DT model/NN parameters/NNS ,/, but/CC with/IN a/DT more/RBR general/JJ form/NN of/IN regularizers/NNS ./.
Further/RB ,/, an/DT analytical/JJ formula/NN is/VBZ derived/VBN for/IN the/DT across/IN -/HYPH task/NN component/NN as/IN related/VBN to/IN the/DT task/NN -/HYPH specific/JJ component/NN for/IN all/PDT these/DT regularizers/NNS ,/, leading/VBG to/IN a/DT better/JJR understanding/NN of/IN the/DT shrinkage/NN effect/NN ./.
Study/NN of/IN this/DT framework/NN motivates/VBZ new/JJ multitask/JJ learning/NN algorithms/NNS ./.
We/PRP propose/VBP two/CD new/JJ learning/NN formulations/NNS by/IN varying/VBG the/DT parameters/NNS in/IN the/DT proposed/VBN framework/NN ./.
Empirical/JJ studies/NNS have/VBP revealed/VBN the/DT relative/JJ advantages/NNS of/IN the/DT two/CD new/JJ formulations/NNS by/IN comparing/VBG with/IN the/DT state/NN of/IN the/DT art/NN ,/, which/WDT provides/VBZ instructive/JJ insights/NNS into/IN the/DT feature/NN learning/NN problem/NN with/IN multiple/JJ tasks/NNS ./.
