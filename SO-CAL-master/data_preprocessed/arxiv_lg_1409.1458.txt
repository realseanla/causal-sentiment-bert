Communication/NN remains/VBZ the/DT most/RBS significant/JJ bottleneck/NN in/IN the/DT performance/NN of/IN distributed/VBN optimization/NN algorithms/NNS for/IN large/JJ -/HYPH scale/NN machine/NN learning/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT communication/NN -/HYPH efficient/JJ framework/NN ,/, CoCoA/NN ,/, that/DT uses/VBZ local/JJ computation/NN in/IN a/DT primal/JJ -/HYPH dual/JJ setting/NN to/TO dramatically/RB reduce/VB the/DT amount/NN of/IN necessary/JJ communication/NN ./.
We/PRP provide/VBP a/DT strong/JJ convergence/NN rate/NN analysis/NN for/IN this/DT class/NN of/IN algorithms/NNS ,/, as/RB well/RB as/IN experiments/NNS on/IN real/JJ -/HYPH world/NN distributed/VBN datasets/NNS with/IN implementations/NNS in/IN Spark/NN ./.
In/IN our/PRP$ experiments/NNS ,/, we/PRP find/VBP that/IN as/IN compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN mini-batch/NN versions/NNS of/IN SGD/NNP and/CC SDCA/NNP algorithms/NNS ,/, CoCoA/NN converges/VBZ to/IN the/DT same/JJ .001/NN -/HYPH accurate/JJ solution/NN quality/NN on/IN average/JJ 25x/NN as/RB quickly/RB ./.
