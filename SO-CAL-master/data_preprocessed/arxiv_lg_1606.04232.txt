Large/JJ -/HYPH scale/NN supervised/VBD classification/NN algorithms/NNS ,/, especially/RB those/DT based/VBN on/IN deep/JJ convolutional/JJ neural/JJ networks/NNS (/-LRB- DCNNs/NNP )/-RRB- ,/, require/VBP vast/JJ amounts/NNS of/IN training/NN data/NNS to/TO achieve/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN ./.
Decreasing/VBG this/DT data/NN requirement/NN would/MD significantly/RB speed/VB up/RP the/DT training/NN process/NN and/CC possibly/RB improve/VB generalization/NN ./.
Motivated/VBN by/IN this/DT objective/NN ,/, we/PRP consider/VBP the/DT task/NN of/IN adaptively/RB finding/VBG concise/JJ training/NN subsets/NNS which/WDT will/MD be/VB iteratively/RB presented/VBN to/IN the/DT learner/NN ./.
We/PRP use/VBP convex/JJ optimization/NN methods/NNS ,/, based/VBN on/IN an/DT objective/JJ criterion/NN and/CC feedback/NN from/IN the/DT current/JJ performance/NN of/IN the/DT classifier/NN ,/, to/TO efficiently/RB identify/VB informative/JJ samples/NNS to/TO train/VB on/RP ./.
We/PRP propose/VBP an/DT algorithm/NN to/IN decompose/VB the/DT optimization/NN problem/NN into/IN smaller/JJR per/IN -/HYPH class/NN problems/NNS ,/, which/WDT can/MD be/VB solved/VBN in/IN parallel/NN ./.
We/PRP test/VBP our/PRP$ approach/NN on/IN standard/JJ classification/NN tasks/NNS and/CC demonstrate/VBP its/PRP$ effectiveness/NN in/IN decreasing/VBG the/DT training/NN set/VBN size/NN without/IN compromising/VBG performance/NN ./.
We/PRP also/RB show/VBP that/IN our/PRP$ approach/NN can/MD make/VB the/DT classifier/NN more/RBR robust/JJ in/IN the/DT presence/NN of/IN label/NN noise/NN and/CC class/NN imbalance/NN ./.
