Recurrent/JJ Neural/JJ Network/NN (/-LRB- RNN/NN )/-RRB- and/CC one/CD of/IN its/PRP$ specific/JJ architectures/NNS ,/, Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- ,/, have/VBP been/VBN widely/RB used/VBN for/IN sequence/NN labeling/NN ./.
In/IN this/DT paper/NN ,/, we/PRP first/RB enhance/VB LSTM/NN -/HYPH based/VBN sequence/NN labeling/NN to/IN explicitly/RB model/NN label/NN dependencies/NNS ./.
Then/RB we/PRP propose/VB another/DT enhancement/NN to/TO incorporate/VB the/DT global/JJ information/NN spanning/VBG over/IN the/DT whole/JJ input/NN sequence/NN ./.
The/DT latter/JJ proposed/JJ method/NN ,/, encoder/NN -/HYPH labeler/NN LSTM/NNP ,/, first/JJ encodes/VBZ the/DT whole/JJ input/NN sequence/NN into/IN a/DT fixed/VBN length/NN vector/NN with/IN the/DT encoder/NN LSTM/NNP ,/, and/CC then/RB uses/VBZ this/DT encoded/VBN vector/NN as/IN the/DT initial/JJ state/NN of/IN another/DT LSTM/NNP for/IN sequence/NN labeling/NN ./.
Combining/VBG these/DT methods/NNS ,/, we/PRP can/MD predict/VB the/DT label/NN sequence/NN with/IN considering/VBG label/NN dependencies/NNS and/CC information/NN of/IN whole/JJ input/NN sequence/NN ./.
In/IN the/DT experiments/NNS of/IN a/DT slot/NN filling/VBG task/NN ,/, which/WDT is/VBZ an/DT essential/JJ component/NN of/IN natural/JJ language/NN understanding/NN ,/, with/IN using/VBG the/DT standard/JJ ATIS/NN corpus/NN ,/, we/PRP achieved/VBD the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN F1/NN -/HYPH score/NN of/IN 95.66/CD percent/NN ./.
