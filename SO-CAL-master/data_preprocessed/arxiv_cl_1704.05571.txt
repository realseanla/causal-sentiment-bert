Word/NNP embeddings/NNS have/VBP made/VBN enormous/JJ inroads/NNS in/IN recent/JJ years/NNS in/IN a/DT wide/JJ variety/NN of/IN text/NN mining/NN applications/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP explore/VBP a/DT word/NN embedding/NN -/HYPH based/VBN architecture/NN for/IN predicting/VBG the/DT relevance/NN of/IN a/DT role/NN between/IN two/CD financial/JJ entities/NNS within/IN the/DT context/NN of/IN natural/JJ language/NN sentences/NNS ./.
In/IN this/DT extended/JJ abstract/NN ,/, we/PRP propose/VBP a/DT pooled/VBN approach/NN that/WDT uses/VBZ a/DT collection/NN of/IN sentences/NNS to/TO train/VB word/NN embeddings/NNS using/VBG the/DT skip/VB -/HYPH gram/NN word2vec/NN architecture/NN ./.
We/PRP use/VBP the/DT word/NN embeddings/NNS to/TO obtain/VB context/NN vectors/NNS that/WDT are/VBP assigned/VBN one/CD or/CC more/JJR labels/NNS based/VBN on/IN manual/JJ annotations/NNS ./.
We/PRP train/VBP a/DT machine/NN learning/VBG classifier/NN using/VBG the/DT labeled/VBN context/NN vectors/NNS ,/, and/CC use/VB the/DT trained/VBN classifier/NN to/TO predict/VB contextual/JJ role/NN relevance/NN on/IN test/NN data/NNS ./.
Our/PRP$ approach/NN serves/VBZ as/IN a/DT good/JJ minimal/JJ -/HYPH expertise/NN baseline/NN for/IN the/DT task/NN as/IN it/PRP is/VBZ simple/JJ and/CC intuitive/JJ ,/, uses/VBZ open/JJ -/HYPH source/NN modules/NNS ,/, requires/VBZ little/JJ feature/NN crafting/VBG effort/NN and/CC performs/VBZ well/RB across/IN roles/NNS ./.
