Rapid/NNP progress/NN has/VBZ been/VBN made/VBN towards/IN question/NN answering/NN (/-LRB- QA/NN )/-RRB- systems/NNS that/WDT can/MD extract/VB answers/NNS from/IN text/NN ./.
Existing/VBG neural/JJ approaches/NNS make/VBP use/NN of/IN expensive/JJ bi-directional/JJ attention/NN mechanisms/NNS or/CC score/NN all/DT possible/JJ answer/NN spans/NNS ,/, limiting/VBG scalability/NN ./.
We/PRP propose/VBP instead/RB to/TO cast/VB extractive/JJ QA/NN as/IN an/DT iterative/JJ search/NN problem/NN :/: select/VB the/DT answer/NN 's/POS sentence/NN ,/, start/VB word/NN ,/, and/CC end/VB word/NN ./.
This/DT representation/NN reduces/VBZ the/DT space/NN of/IN each/DT search/NN step/NN and/CC allows/VBZ computation/NN to/TO be/VB conditionally/RB allocated/VBN to/IN promising/VBG search/NN paths/NNS ./.
We/PRP show/VBP that/IN globally/RB normalizing/VBG the/DT decision/NN process/NN and/CC back/RB -/HYPH propagating/VBG through/IN beam/NN search/NN makes/VBZ this/DT representation/NN viable/JJ and/CC learning/NN efficient/JJ ./.
We/PRP empirically/RB demonstrate/VBP the/DT benefits/NNS of/IN this/DT approach/NN using/VBG our/PRP$ model/NN ,/, Globally/RB Normalized/VBN Reader/NN (/-LRB- GNR/NN )/-RRB- ,/, which/WDT achieves/VBZ the/DT second/JJ highest/JJS single/JJ model/NN performance/NN on/IN the/DT Stanford/NNP Question/NN Answering/VBG Dataset/NN (/-LRB- 68.4/CD EM/NN ,/, 76.21/CD F1/NN dev/NN )/-RRB- and/CC is/VBZ 24.7/CD x/SYM faster/JJR than/IN bi-attention-flow/NN ./.
We/PRP also/RB introduce/VBP a/DT data/NN -/HYPH augmentation/NN method/NN to/TO produce/VB semantically/RB valid/JJ examples/NNS by/IN aligning/VBG named/VBN entities/NNS to/IN a/DT knowledge/NN base/NN and/CC swapping/VBG them/PRP with/IN new/JJ entities/NNS of/IN the/DT same/JJ type/NN ./.
This/DT method/NN improves/VBZ the/DT performance/NN of/IN all/DT models/NNS considered/VBN in/IN this/DT work/NN and/CC is/VBZ of/IN independent/JJ interest/NN for/IN a/DT variety/NN of/IN NLP/NN tasks/NNS ./.
