Recently/RB ,/, neural/JJ networks/NNS have/VBP achieved/VBN great/JJ success/NN on/IN sentiment/NN classification/NN due/IN to/IN their/PRP$ ability/NN to/TO alleviate/VB feature/NN engineering/NN ./.
However/RB ,/, one/CD of/IN the/DT remaining/VBG challenges/NNS is/VBZ to/TO model/VB long/JJ texts/NNS in/IN document/NN -/HYPH level/NN sentiment/NN classification/NN under/IN a/DT recurrent/JJ architecture/NN because/IN of/IN the/DT deficiency/NN of/IN the/DT memory/NN unit/NN ./.
To/TO address/VB this/DT problem/NN ,/, we/PRP present/VBP a/DT Cached/NNP Long/NNP Short/NNP -/HYPH Term/NNP Memory/NNP neural/JJ networks/NNS (/-LRB- CLSTM/NNP )/-RRB- to/TO capture/VB the/DT overall/JJ semantic/JJ information/NN in/IN long/JJ texts/NNS ./.
CLSTM/NNP introduces/VBZ a/DT cache/NN mechanism/NN ,/, which/WDT divides/VBZ memory/NN into/IN several/JJ groups/NNS with/IN different/JJ forgetting/VBG rates/NNS and/CC thus/RB enables/VBZ the/DT network/NN to/TO keep/VB sentiment/NN information/NN better/RBR within/IN a/DT recurrent/JJ unit/NN ./.
The/DT proposed/VBN CLSTM/NNP outperforms/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN models/NNS on/IN three/CD publicly/RB available/JJ document/NN -/HYPH level/NN sentiment/NN analysis/NN datasets/NNS ./.
