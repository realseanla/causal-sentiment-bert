In/IN this/DT work/NN we/PRP study/VBP parallelization/NN of/IN online/JJ learning/NN ,/, a/DT core/NN primitive/JJ in/IN machine/NN learning/NN ./.
In/IN a/DT parallel/JJ environment/NN all/DT known/JJ approaches/NNS for/IN parallel/JJ online/JJ learning/NN lead/NN to/IN delayed/VBN updates/NNS ,/, where/WRB the/DT model/NN is/VBZ updated/VBN using/VBG out/RB -/HYPH of/IN -/HYPH date/NN information/NN ./.
In/IN the/DT worst/JJS case/NN ,/, or/CC when/WRB examples/NNS are/VBP temporally/RB correlated/VBN ,/, delay/NN can/MD have/VB a/DT very/RB adverse/JJ effect/NN on/IN the/DT learning/NN algorithm/NN ./.
Here/RB ,/, we/PRP analyze/VBP and/CC present/VBP preliminary/JJ empirical/JJ results/NNS on/IN a/DT set/NN of/IN learning/VBG architectures/NNS based/VBN on/IN a/DT feature/NN sharding/VBG approach/NN that/WDT present/JJ various/JJ tradeoffs/NNS between/IN delay/NN ,/, degree/NN of/IN parallelism/NN ,/, representation/NN power/NN and/CC empirical/JJ performance/NN ./.
