Recently/RB ,/, machine/NN learning/NN methods/NNS have/VBP provided/VBN a/DT broad/JJ spectrum/NN of/IN original/JJ and/CC efficient/JJ algorithms/NNS based/VBN on/IN Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNN/NN )/-RRB- to/TO automatically/RB predict/VB an/DT outcome/NN with/IN respect/NN to/IN a/DT sequence/NN of/IN inputs/NNS ./.
Recurrent/JJ hidden/JJ cells/NNS allow/VBP these/DT DNN/NN -/HYPH based/VBN models/NNS to/TO manage/VB long/RB -/HYPH term/NN dependencies/NNS such/JJ as/IN Recurrent/JJ Neural/JJ Networks/NNS (/-LRB- RNN/NN )/-RRB- and/CC Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- ./.
Nevertheless/RB ,/, these/DT RNNs/NNS process/NN a/DT single/JJ input/NN stream/NN in/IN one/CD (/-LRB- LSTM/NN )/-RRB- or/CC two/CD (/-LRB- Bidirectional/JJ LSTM/NN )/-RRB- directions/NNS ./.
But/CC most/JJS of/IN the/DT information/NN available/JJ nowadays/RB is/VBZ from/IN multistreams/NNS or/CC multimedia/NNS documents/NNS ,/, and/CC require/VBP RNNs/NNS to/TO process/VB these/DT information/NN synchronously/RB during/IN the/DT training/NN ./.
This/DT paper/NN presents/VBZ an/DT original/JJ LSTM/NN -/HYPH based/VBN architecture/NN ,/, named/VBN Parallel/JJ LSTM/NN (/-LRB- PLSTM/NN )/-RRB- ,/, that/WDT carries/VBZ out/RP multiple/JJ parallel/JJ synchronized/VBN input/NN sequences/NNS in/IN order/NN to/TO predict/VB a/DT common/JJ output/NN ./.
The/DT proposed/VBN PLSTM/NN method/NN could/MD be/VB used/VBN for/IN parallel/JJ sequence/NN classification/NN purposes/NNS ./.
The/DT PLSTM/NNP approach/NN is/VBZ evaluated/VBN on/IN an/DT automatic/JJ telecast/NN genre/NN sequences/NNS classification/NN task/NN and/CC compared/VBN with/IN different/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN architectures/NNS ./.
Results/NNS show/VBP that/IN the/DT proposed/VBN PLSTM/NN method/NN outperforms/VBZ the/DT baseline/NN n/NN -/HYPH gram/NN models/NNS as/RB well/RB as/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN LSTM/NN approach/NN ./.
