Generating/NNP captions/NNS for/IN images/NNS is/VBZ a/DT task/NN that/WDT has/VBZ recently/RB received/VBN considerable/JJ attention/NN ./.
In/IN this/DT work/NN we/PRP focus/VBP on/IN caption/NN generation/NN for/IN abstract/JJ scenes/NNS ,/, or/CC object/NN layouts/NNS where/WRB the/DT only/JJ information/NN provided/VBN is/VBZ a/DT set/NN of/IN objects/NNS and/CC their/PRP$ locations/NNS ./.
We/PRP propose/VBP OBJ2TEXT/NNP ,/, a/DT sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN that/WDT encodes/VBZ a/DT set/NN of/IN objects/NNS and/CC their/PRP$ locations/NNS as/IN an/DT input/NN sequence/NN using/VBG an/DT LSTM/NNP network/NN ,/, and/CC decodes/VBZ this/DT representation/NN using/VBG an/DT LSTM/NNP language/NN model/NN ./.
We/PRP show/VBP that/IN our/PRP$ model/NN ,/, despite/IN encoding/VBG object/NN layouts/NNS as/IN a/DT sequence/NN ,/, can/MD represent/VB spatial/JJ relationships/NNS between/IN objects/NNS ,/, and/CC generate/VBP descriptions/NNS that/WDT are/VBP globally/RB coherent/JJ and/CC semantically/RB relevant/JJ ./.
We/PRP test/VBP our/PRP$ approach/NN in/IN a/DT task/NN of/IN object/NN -/HYPH layout/NN captioning/NN by/IN using/VBG only/JJ object/NN annotations/NNS as/IN inputs/NNS ./.
We/PRP additionally/RB show/VBP that/IN our/PRP$ model/NN ,/, combined/VBN with/IN a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN object/NN detector/NN ,/, improves/VBZ an/DT image/NN captioning/NN model/NN from/IN 0.863/CD to/IN 0.950/CD (/-LRB- CIDEr/NN score/NN )/-RRB- in/IN the/DT test/NN benchmark/NN of/IN the/DT standard/JJ MS/NN -/HYPH COCO/NN Captioning/NN task/NN ./.
