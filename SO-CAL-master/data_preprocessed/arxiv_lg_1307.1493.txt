Dropout/NN and/CC other/JJ feature/NN noising/JJ schemes/NNS control/VBP overfitting/VBG by/IN artificially/RB corrupting/VBG the/DT training/NN data/NNS ./.
For/IN generalized/VBN linear/JJ models/NNS ,/, dropout/NN performs/VBZ a/DT form/NN of/IN adaptive/JJ regularization/NN ./.
Using/VBG this/DT viewpoint/NN ,/, we/PRP show/VBP that/IN the/DT dropout/NN regularizer/NN is/VBZ first/JJ -/HYPH order/NN equivalent/JJ to/IN an/DT L2/NN regularizer/NN applied/VBN after/IN scaling/VBG the/DT features/NNS by/IN an/DT estimate/NN of/IN the/DT inverse/JJ diagonal/JJ Fisher/NNP information/NN matrix/NN ./.
We/PRP also/RB establish/VBP a/DT connection/NN to/IN AdaGrad/NNP ,/, an/DT online/JJ learner/NN ,/, and/CC find/VB that/IN a/DT close/JJ relative/NN of/IN AdaGrad/NNP operates/VBZ by/IN repeatedly/RB solving/VBG linear/JJ dropout/NN -/HYPH regularized/VBN problems/NNS ./.
By/IN casting/VBG dropout/NN as/IN regularization/NN ,/, we/PRP develop/VBP a/DT natural/JJ semi-supervised/VBN algorithm/NN that/WDT uses/VBZ unlabeled/JJ data/NNS to/TO create/VB a/DT better/JJR adaptive/JJ regularizer/NN ./.
We/PRP apply/VBP this/DT idea/NN to/TO document/VB classification/NN tasks/NNS ,/, and/CC show/VBP that/IN it/PRP consistently/RB boosts/VBZ the/DT performance/NN of/IN dropout/NN training/NN ,/, improving/VBG on/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN the/DT IMDB/NNP reviews/NNS dataset/NN ./.
