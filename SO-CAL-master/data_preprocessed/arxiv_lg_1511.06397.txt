Recent/JJ methods/NNS for/IN learning/NN vector/NN space/NN representations/NNS of/IN words/NNS have/VBP succeeded/VBN in/IN capturing/VBG fine/JJ -/HYPH grained/JJ semantic/JJ and/CC syntactic/JJ regularities/NNS using/VBG vector/NN arithmetic/NN ./.
However/RB ,/, these/DT vector/NN space/NN representations/NNS (/-LRB- created/VBN through/IN large/JJ -/HYPH scale/NN text/NN analysis/NN )/-RRB- are/VBP typically/RB stored/VBN verbatim/JJ ,/, since/IN their/PRP$ internal/JJ structure/NN is/VBZ opaque/JJ ./.
Using/VBG word/NN -/HYPH analogy/NN tests/NNS to/TO monitor/VB the/DT level/NN of/IN detail/NN stored/VBN in/IN compressed/VBN re-representations/NNS of/IN the/DT same/JJ vector/NN space/NN ,/, the/DT trade/NN -/HYPH offs/NNS between/IN the/DT reduction/NN in/IN memory/NN usage/NN and/CC expressiveness/NN are/VBP investigated/VBN ./.
A/DT simple/JJ scheme/NN is/VBZ outlined/VBN that/DT can/MD reduce/VB the/DT memory/NN footprint/NN of/IN a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN embedding/NN by/IN a/DT factor/NN of/IN 10/CD ,/, with/IN only/RB minimal/JJ impact/NN on/IN performance/NN ./.
Then/RB ,/, using/VBG the/DT same/JJ '/`` bit/NN budget/NN '/'' ,/, a/DT binary/JJ (/-LRB- approximate/JJ )/-RRB- factorisation/NN of/IN the/DT same/JJ space/NN is/VBZ also/RB explored/VBN ,/, with/IN the/DT aim/NN of/IN creating/VBG an/DT equivalent/JJ representation/NN with/IN better/JJR interpretability/NN ./.
