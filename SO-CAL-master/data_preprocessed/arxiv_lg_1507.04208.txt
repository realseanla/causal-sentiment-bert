We/PRP consider/VBP learning/VBG to/TO maximize/VB reward/NN in/IN combinatorial/JJ cascading/VBG bandits/NNS ,/, a/DT new/JJ learning/NN setting/VBG that/IN unifies/VBZ cascading/VBG and/CC combinatorial/JJ bandits/NNS ./.
The/DT unification/NN of/IN these/DT frameworks/NNS presents/VBZ unique/JJ challenges/NNS in/IN the/DT analysis/NN but/CC allows/VBZ for/IN modeling/VBG a/DT rich/JJ set/NN of/IN partial/JJ monitoring/NN problems/NNS ,/, such/JJ as/IN learning/VBG to/IN route/NN in/IN a/DT communication/NN network/NN to/TO minimize/VB the/DT probability/NN of/IN losing/VBG routed/VBN packets/NNS and/CC recommending/VBG diverse/JJ items/NNS ./.
We/PRP propose/VBP CombCascade/NNP ,/, a/DT computationally/RB -/HYPH efficient/JJ UCB/NNP -/HYPH like/JJ algorithm/NN for/IN solving/VBG our/PRP$ problem/NN ;/: and/CC derive/VBP gap/NN -/HYPH dependent/JJ and/CC gap/NN -/HYPH free/JJ upper/JJ bounds/NNS on/IN its/PRP$ regret/NN ./.
Our/PRP$ analysis/NN builds/VBZ on/IN recent/JJ results/NNS in/IN stochastic/JJ combinatorial/JJ semi-bandits/NNS but/CC also/RB addresses/VBZ two/CD novel/JJ challenges/NNS of/IN our/PRP$ learning/NN setting/NN ,/, a/DT non-linear/JJ objective/NN and/CC partial/JJ observability/NN ./.
We/PRP evaluate/VBP CombCascade/NNP on/IN two/CD real/JJ -/HYPH world/NN problems/NNS and/CC demonstrate/VBP that/IN it/PRP performs/VBZ well/RB even/RB when/WRB our/PRP$ modeling/NN assumptions/NNS are/VBP violated/VBN ./.
We/PRP also/RB demonstrate/VBP that/IN our/PRP$ setting/NN requires/VBZ new/JJ learning/NN algorithms/NNS ./.
