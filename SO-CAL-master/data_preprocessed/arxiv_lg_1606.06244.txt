We/PRP show/VBP that/IN learning/VBG algorithms/NNS satisfying/VBG a/DT $/$ \/SYM textit/FW {/-LRB- low/JJ approximate/JJ regret/NN }/-RRB- $/$ property/NN experience/NN fast/RB convergence/NN to/TO approximate/VB optimality/NN in/IN a/DT large/JJ class/NN of/IN repeated/VBN games/NNS ./.
Our/PRP$ property/NN ,/, which/WDT simply/RB requires/VBZ that/IN each/DT learner/NN has/VBZ small/JJ regret/NN compared/VBN to/IN a/DT $/$ (/-LRB- 1/CD \/SYM epsilon/NN )/-RRB- $/$ -/HYPH multiplicative/JJ approximation/NN to/IN the/DT best/JJS action/NN in/IN hindsight/NN ,/, is/VBZ ubiquitous/JJ among/IN learning/VBG algorithms/NNS -/, it/PRP is/VBZ satisfied/JJ even/RB by/IN the/DT vanilla/NN Hedge/NN forecaster/NN ./.
Our/PRP$ results/NNS improve/VB upon/IN recent/JJ work/NN of/IN Syrgkanis/NNP et/FW al./FW [/-LRB- SALS15/NNP ]/-RRB- in/IN a/DT number/NN of/IN ways/NNS ./.
We/PRP improve/VBP upon/IN the/DT speed/NN of/IN convergence/NN by/IN a/DT factor/NN of/IN n/NN ,/, the/DT number/NN of/IN players/NNS ,/, and/CC require/VBP only/RB that/IN the/DT players/NNS observe/VBP payoffs/NNS under/IN other/JJ players/NNS '/POS realized/VBN actions/NNS ,/, as/IN opposed/VBN to/IN expected/VBN payoffs/NNS ./.
We/PRP further/RB show/VBP that/IN convergence/NN occurs/VBZ with/IN high/JJ probability/NN ,/, and/CC under/IN certain/JJ conditions/NNS show/VBP convergence/NN under/IN bandit/NN feedback/NN ./.
Both/CC the/DT scope/NN of/IN settings/NNS and/CC the/DT class/NN of/IN algorithms/NNS for/IN which/WDT our/PRP$ analysis/NN provides/VBZ fast/JJ convergence/NN are/VBP considerably/RB broader/JJR than/IN in/IN previous/JJ work/NN ./.
