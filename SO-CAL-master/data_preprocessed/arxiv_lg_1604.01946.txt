As/IN recurrent/JJ neural/JJ networks/NNS become/VBP larger/JJR and/CC deeper/JJR ,/, training/NN times/NNS for/IN single/JJ networks/NNS are/VBP rising/VBG into/IN weeks/NNS or/CC even/RB months/NNS ./.
As/IN such/JJ there/EX is/VBZ a/DT significant/JJ incentive/NN to/TO improve/VB the/DT performance/NN and/CC scalability/NN of/IN these/DT networks/NNS ./.
While/IN GPUs/NNS have/VBP become/VBN the/DT hardware/NN of/IN choice/NN for/IN training/NN and/CC deploying/VBG recurrent/JJ models/NNS ,/, the/DT implementations/NNS employed/VBN often/RB make/VBP use/NN of/IN only/RB basic/JJ optimizations/NNS for/IN these/DT architectures/NNS ./.
In/IN this/DT article/NN we/PRP demonstrate/VBP that/IN by/IN exposing/VBG parallelism/NN between/IN operations/NNS within/IN the/DT network/NN ,/, an/DT order/NN of/IN magnitude/NN speedup/NN across/IN a/DT range/NN of/IN network/NN sizes/NNS can/MD be/VB achieved/VBN over/IN a/DT naive/JJ implementation/NN ./.
We/PRP describe/VBP three/CD stages/NNS of/IN optimization/NN that/WDT have/VBP been/VBN incorporated/VBN into/IN the/DT fifth/JJ release/NN of/IN NVIDIA/NNP 's/POS cuDNN/NN :/: firstly/RB optimizing/VBG a/DT single/JJ cell/NN ,/, secondly/RB a/DT single/JJ layer/NN ,/, and/CC thirdly/RB the/DT entire/JJ network/NN ./.
