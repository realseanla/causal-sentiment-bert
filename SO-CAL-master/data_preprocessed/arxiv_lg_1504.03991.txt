In/IN this/DT paper/NN ,/, we/PRP study/VBP randomized/JJ reduction/NN methods/NNS ,/, which/WDT reduce/VBP high/JJ -/HYPH dimensional/JJ features/NNS into/IN low/JJ -/HYPH dimensional/JJ space/NN by/IN randomized/JJ methods/NNS (/-LRB- e.g./FW ,/, random/JJ projection/NN ,/, random/JJ hashing/VBG )/-RRB- ,/, for/IN large/JJ -/HYPH scale/NN high/JJ -/HYPH dimensional/JJ classification/NN ./.
Previous/JJ theoretical/JJ results/NNS on/IN randomized/JJ reduction/NN methods/NNS hinge/VBP on/IN strong/JJ assumptions/NNS about/IN the/DT data/NNS ,/, e.g./FW ,/, low/JJ rank/NN of/IN the/DT data/NNS matrix/NN or/CC a/DT large/JJ separable/JJ margin/NN of/IN classification/NN ,/, which/WDT hinder/VBP their/PRP$ applications/NNS in/IN broad/JJ domains/NNS ./.
To/TO address/VB these/DT limitations/NNS ,/, we/PRP propose/VBP dual/JJ -/HYPH sparse/JJ regularized/VBN randomized/JJ reduction/NN methods/NNS that/WDT introduce/VBP a/DT sparse/JJ regularizer/NN into/IN the/DT reduced/VBN dual/JJ problem/NN ./.
Under/IN a/DT mild/JJ condition/NN that/IN the/DT original/JJ dual/JJ solution/NN is/VBZ a/DT (/-LRB- nearly/RB )/-RRB- sparse/JJ vector/NN ,/, we/PRP show/VBP that/IN the/DT resulting/VBG dual/JJ solution/NN is/VBZ close/JJ to/IN the/DT original/JJ dual/JJ solution/NN and/CC concentrates/VBZ on/IN its/PRP$ support/NN set/NN ./.
In/IN numerical/JJ experiments/NNS ,/, we/PRP present/VBP an/DT empirical/JJ study/NN to/TO support/VB the/DT analysis/NN and/CC we/PRP also/RB present/VBP a/DT novel/JJ application/NN of/IN the/DT dual/JJ -/HYPH sparse/JJ regularized/VBN randomized/JJ reduction/NN methods/NNS to/IN reducing/VBG the/DT communication/NN cost/NN of/IN distributed/VBN learning/NN from/IN large/JJ -/HYPH scale/NN high/JJ -/HYPH dimensional/JJ data/NNS ./.
