In/IN this/DT paper/NN ,/, we/PRP improve/VBP the/DT previously/RB best/RB known/VBN regret/NN bound/VBN to/TO achieve/VB $/$ \/CD epsilon/CD $/$ -/HYPH differential/JJ privacy/NN in/IN oblivious/JJ adversarial/JJ bandits/NNS from/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- {/-LRB- (/-LRB- T/NNP ^/SYM {/-LRB- 2/3/CD }/-RRB- //HYPH \/SYM epsilon/NN )/-RRB- }/-RRB- $/$ to/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- {/-LRB- (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- \/SYM ln/NN T/NN //HYPH \/SYM epsilon/NN )/-RRB- }/-RRB- $/$ ./.
This/DT is/VBZ achieved/VBN by/IN combining/VBG a/DT Laplace/NNP Mechanism/NN with/IN EXP3/NN ./.
We/PRP show/VBP that/IN though/IN EXP3/NN is/VBZ already/RB differentially/RB private/JJ ,/, it/PRP leaks/VBZ a/DT linear/JJ amount/NN of/IN information/NN in/IN $/$ T$/CD ./.
However/RB ,/, we/PRP can/MD improve/VB this/DT privacy/NN by/IN relying/VBG on/IN its/PRP$ intrinsic/JJ exponential/JJ mechanism/NN for/IN selecting/VBG actions/NNS ./.
This/DT allows/VBZ us/PRP to/TO reach/VB $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- {/-LRB- (/-LRB- \/SYM sqrt/NN {/-LRB- \/SYM ln/NN T/NN }/-RRB- )/-RRB- }/-RRB- $/$ -/HYPH DP/NN ,/, with/IN a/DT regret/NN of/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- {/-LRB- (/-LRB- T/NNP ^/SYM {/-LRB- 2/3/CD }/-RRB- )/-RRB- }/-RRB- $/$ that/WDT holds/VBZ against/IN an/DT adaptive/JJ adversary/NN ,/, an/DT improvement/NN from/IN the/DT best/RBS known/VBN of/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- {/-LRB- (/-LRB- T/NNP ^/SYM {/-LRB- 3/4/CD }/-RRB- )/-RRB- }/-RRB- $/$ ./.
This/DT is/VBZ done/VBN by/IN using/VBG an/DT algorithm/NN that/WDT run/VBP EXP3/NN in/IN a/DT mini-batch/NN loop/NN ./.
Finally/RB ,/, we/PRP run/VBP experiments/NNS that/WDT clearly/RB demonstrate/VBP the/DT validity/NN of/IN our/PRP$ theoretical/JJ analysis/NN ./.
