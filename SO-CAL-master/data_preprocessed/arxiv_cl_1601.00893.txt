We/PRP provide/VBP the/DT first/JJ extensive/JJ evaluation/NN of/IN how/WRB using/VBG different/JJ types/NNS of/IN context/NN to/TO learn/VB skip/VB -/HYPH gram/NN word/NN embeddings/NNS affects/VBZ performance/NN on/IN a/DT wide/JJ range/NN of/IN intrinsic/JJ and/CC extrinsic/JJ NLP/NN tasks/NNS ./.
Our/PRP$ results/NNS suggest/VBP that/IN while/IN intrinsic/JJ tasks/NNS tend/VBP to/TO exhibit/VB a/DT clear/JJ preference/NN to/IN particular/JJ types/NNS of/IN contexts/NNS and/CC higher/JJR dimensionality/NN ,/, more/RBR careful/JJ tuning/NN is/VBZ required/VBN for/IN finding/VBG the/DT optimal/JJ settings/NNS for/IN most/JJS of/IN the/DT extrinsic/JJ tasks/NNS that/WDT we/PRP considered/VBD ./.
Furthermore/RB ,/, for/IN these/DT extrinsic/JJ tasks/NNS ,/, we/PRP find/VBP that/IN once/IN the/DT benefit/NN from/IN increasing/VBG the/DT embedding/NN dimensionality/NN is/VBZ mostly/RB exhausted/JJ ,/, simple/JJ concatenation/NN of/IN word/NN embeddings/NNS ,/, learned/VBD with/IN different/JJ context/NN types/NNS ,/, can/MD yield/VB further/JJ performance/NN gains/NNS ./.
As/IN an/DT additional/JJ contribution/NN ,/, we/PRP propose/VBP a/DT new/JJ variant/NN of/IN the/DT skip/VB -/HYPH gram/NN model/NN that/WDT learns/VBZ word/NN embeddings/NNS from/IN weighted/JJ contexts/NNS of/IN substitute/NN words/NNS ./.
