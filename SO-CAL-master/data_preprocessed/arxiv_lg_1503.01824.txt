Deep/JJ neural/JJ networks/NNS have/VBP recently/RB achieved/VBN state/NN of/IN the/DT art/NN performance/NN thanks/NNS to/IN new/JJ training/NN algorithms/NNS for/IN rapid/JJ parameter/NN estimation/NN and/CC new/JJ regularization/NN methods/NNS to/TO reduce/VB overfitting/NN ./.
However/RB ,/, in/IN practice/NN the/DT network/NN architecture/NN has/VBZ to/TO be/VB manually/RB set/VBN by/IN domain/NN experts/NNS ,/, generally/RB by/IN a/DT costly/JJ trial/NN and/CC error/NN procedure/NN ,/, which/WDT often/RB accounts/VBZ for/IN a/DT large/JJ portion/NN of/IN the/DT final/JJ system/NN performance/NN ./.
We/PRP view/VBP this/DT as/IN a/DT limitation/NN and/CC propose/VB a/DT novel/JJ training/NN algorithm/NN that/WDT automatically/RB optimizes/VBZ network/NN architecture/NN ,/, by/IN progressively/RB increasing/VBG model/NN complexity/NN and/CC then/RB eliminating/VBG model/NN redundancy/NN by/IN selectively/RB removing/VBG parameters/NNS at/IN training/NN time/NN ./.
For/IN convolutional/JJ neural/JJ networks/NNS ,/, our/PRP$ method/NN relies/VBZ on/IN iterative/JJ split/NN //HYPH merge/NN clustering/NN of/IN convolutional/JJ kernels/NNS interleaved/VBN by/IN stochastic/JJ gradient/NN descent/NN ./.
We/PRP present/VBP a/DT training/NN algorithm/NN and/CC experimental/JJ results/NNS on/IN three/CD different/JJ vision/NN tasks/NNS ,/, showing/VBG improved/VBN performance/NN compared/VBN to/IN similarly/RB sized/JJ hand/NN -/HYPH crafted/VBN architectures/NNS ./.
