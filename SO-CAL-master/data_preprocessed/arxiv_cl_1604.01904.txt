Automatic/JJ headline/NN generation/NN is/VBZ an/DT important/JJ research/NN area/NN within/IN text/NN summarization/NN and/CC sentence/NN compression/NN ./.
Recently/RB ,/, neural/JJ headline/NN generation/NN models/NNS have/VBP been/VBN proposed/VBN to/TO take/VB advantage/NN of/IN well/RB -/HYPH trained/VBN neural/JJ networks/NNS in/IN learning/VBG sentence/NN representations/NNS and/CC mapping/NN sequence/NN to/IN sequence/NN ./.
Nevertheless/RB ,/, traditional/JJ neural/JJ network/NN encoder/NN utilizes/VBZ maximum/JJ likelihood/NN estimation/NN for/IN parameter/NN optimization/NN ,/, which/WDT essentially/RB constraints/NNS the/DT expected/VBN training/NN objective/NN within/IN word/NN level/NN instead/RB of/IN sentence/NN level/NN ./.
Moreover/RB ,/, the/DT performance/NN of/IN model/NN prediction/NN significantly/RB relies/VBZ on/IN training/NN data/NNS distribution/NN ./.
To/TO overcome/VB these/DT drawbacks/NNS ,/, we/PRP employ/VBP minimum/JJ risk/NN training/NN strategy/NN in/IN this/DT paper/NN ,/, which/WDT directly/RB optimizes/VBZ model/NN parameters/NNS with/IN respect/NN to/IN evaluation/NN metrics/NNS and/CC statistically/RB leads/VBZ to/IN significant/JJ improvements/NNS for/IN headline/NN generation/NN ./.
Experiment/NN results/NNS show/VBP that/IN our/PRP$ approach/NN outperforms/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN systems/NNS on/IN both/DT English/NNP and/CC Chinese/JJ headline/NN generation/NN tasks/NNS ./.
