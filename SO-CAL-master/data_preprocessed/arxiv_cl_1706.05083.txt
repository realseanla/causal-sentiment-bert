This/DT work/NN presents/VBZ a/DT novel/JJ approach/NN to/TO jointly/RB tackling/VBG Automatic/JJ Post-Editing/NN (/-LRB- APE/NN )/-RRB- and/CC Word/NNP -/HYPH Level/NNP Quality/NNP Estimation/NNP (/-LRB- QE/NNP )/-RRB- using/VBG ensembles/NNS of/IN specialized/JJ Neural/JJ Machine/NN Translation/NN (/-LRB- NMT/NN )/-RRB- systems/NNS ./.
Word/NNP -/HYPH level/NN features/NNS which/WDT have/VBP proven/VBN effective/JJ for/IN QE/NNP are/VBP included/VBN as/IN input/NN factors/NNS ,/, expanding/VBG the/DT representation/NN of/IN the/DT original/JJ source/NN and/CC the/DT machine/NN translation/NN hypothesis/NN ,/, which/WDT are/VBP used/VBN to/TO generate/VB an/DT automatically/RB post-edited/JJ hypothesis/NN ./.
We/PRP train/VBP a/DT suite/NN of/IN NMT/NN models/NNS which/WDT use/VBP different/JJ input/NN representations/NNS ,/, but/CC share/VB the/DT same/JJ output/NN space/NN ./.
These/DT models/NNS are/VBP then/RB ensembled/VBN together/RB ,/, and/CC tuned/VBN for/IN both/CC the/DT APE/NN and/CC the/DT QE/NNP task/NN ./.
We/PRP thus/RB attempt/VBP to/TO connect/VB the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN approaches/NNS to/TO APE/NNP and/CC QE/NNP within/IN a/DT single/JJ framework/NN ./.
Our/PRP$ models/NNS achieve/VBP state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN both/DT tasks/NNS ,/, with/IN the/DT only/JJ difference/NN in/IN the/DT tuning/NN step/NN which/WDT learns/VBZ weights/NNS for/IN each/DT component/NN of/IN the/DT ensemble/NN ./.
