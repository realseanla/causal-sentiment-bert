We/PRP introduce/VBP Natural/JJ Neural/JJ Networks/NNS ,/, a/DT novel/JJ family/NN of/IN algorithms/NNS that/WDT speed/VBP up/RP convergence/NN by/IN adapting/VBG their/PRP$ internal/JJ representation/NN during/IN training/NN to/TO improve/VB conditioning/NN of/IN the/DT Fisher/NNP matrix/NN ./.
In/IN particular/JJ ,/, we/PRP show/VBP a/DT specific/JJ example/NN that/WDT employs/VBZ a/DT simple/JJ and/CC efficient/JJ reparametrization/NN of/IN the/DT neural/JJ network/NN weights/NNS by/IN implicitly/RB whitening/VBG the/DT representation/NN obtained/VBN at/IN each/DT layer/NN ,/, while/IN preserving/VBG the/DT feed/NN -/HYPH forward/JJ computation/NN of/IN the/DT network/NN ./.
Such/JJ networks/NNS can/MD be/VB trained/VBN efficiently/RB via/IN the/DT proposed/VBN Projected/VBN Natural/JJ Gradient/NN Descent/JJ algorithm/NN (/-LRB- PRONG/NN )/-RRB- ,/, which/WDT amortizes/VBZ the/DT cost/NN of/IN these/DT reparametrizations/NNS over/IN many/JJ parameter/NN updates/NNS and/CC is/VBZ closely/RB related/VBN to/IN the/DT Mirror/NNP Descent/NNP online/RB learning/VBG algorithm/NN ./.
We/PRP highlight/VBP the/DT benefits/NNS of/IN our/PRP$ method/NN on/IN both/DT unsupervised/JJ and/CC supervised/JJ learning/NN tasks/NNS ,/, and/CC showcase/VB its/PRP$ scalability/NN by/IN training/VBG on/IN the/DT large/JJ -/HYPH scale/NN ImageNet/NNP Challenge/NNP dataset/NN ./.
