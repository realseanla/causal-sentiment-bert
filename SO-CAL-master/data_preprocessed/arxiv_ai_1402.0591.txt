Learning/VBG by/IN observation/NN can/MD be/VB of/IN key/JJ importance/NN whenever/WRB agents/NNS sharing/VBG similar/JJ features/NNS want/VBP to/TO learn/VB from/IN each/DT other/JJ ./.
This/DT paper/NN presents/VBZ an/DT agent/NN architecture/NN that/WDT enables/VBZ software/NN agents/NNS to/TO learn/VB by/IN direct/JJ observation/NN of/IN the/DT actions/NNS executed/VBN by/IN expert/JJ agents/NNS while/IN they/PRP are/VBP performing/VBG a/DT task/NN ./.
This/DT is/VBZ possible/JJ because/IN the/DT proposed/VBN architecture/NN displays/NNS information/NN that/WDT is/VBZ essential/JJ for/IN observation/NN ,/, making/VBG it/PRP possible/JJ for/IN software/NN agents/NNS to/TO observe/VB each/DT other/JJ ./.
The/DT agent/NN architecture/NN supports/VBZ a/DT learning/NN process/NN that/WDT covers/VBZ all/DT aspects/NNS of/IN learning/NN by/IN observation/NN ,/, such/JJ as/IN discovering/VBG and/CC observing/VBG experts/NNS ,/, learning/VBG from/IN the/DT observed/VBN data/NNS ,/, applying/VBG the/DT acquired/VBN knowledge/NN and/CC evaluating/VBG the/DT agents/NNS progress/NN ./.
The/DT evaluation/NN provides/VBZ control/NN over/IN the/DT decision/NN to/TO obtain/VB new/JJ knowledge/NN or/CC apply/VB the/DT acquired/VBN knowledge/NN to/IN new/JJ problems/NNS ./.
We/PRP combine/VBP two/CD methods/NNS for/IN learning/VBG from/IN the/DT observed/VBN information/NN ./.
The/DT first/JJ one/CD ,/, the/DT recall/NN method/NN ,/, uses/VBZ the/DT sequence/NN on/IN which/WDT the/DT actions/NNS were/VBD observed/VBN to/TO solve/VB new/JJ problems/NNS ./.
The/DT second/JJ one/NN ,/, the/DT classification/NN method/NN ,/, categorizes/VBZ the/DT information/NN in/IN the/DT observed/VBN data/NNS and/CC determines/VBZ to/TO which/WDT set/NN of/IN categories/NNS the/DT new/JJ problems/NNS belong/VBP ./.
Results/NNS show/VBP that/IN agents/NNS are/VBP able/JJ to/TO learn/VB in/IN conditions/NNS where/WRB common/JJ supervised/VBD learning/NN algorithms/NNS fail/VBP ,/, such/JJ as/IN when/WRB agents/NNS do/VBP not/RB know/VB the/DT results/NNS of/IN their/PRP$ actions/NNS a/FW priori/FW or/CC when/WRB not/RB all/PDT the/DT effects/NNS of/IN the/DT actions/NNS are/VBP visible/JJ ./.
The/DT results/NNS also/RB show/VBP that/IN our/PRP$ approach/NN provides/VBZ better/JJR results/NNS than/IN other/JJ learning/NN methods/NNS since/IN it/PRP requires/VBZ shorter/JJR learning/NN periods/NNS ./.
