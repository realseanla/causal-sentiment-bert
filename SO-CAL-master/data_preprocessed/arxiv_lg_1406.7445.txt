Structure/NN learning/NN of/IN Conditional/JJ Random/NNP Fields/NNPS (/-LRB- CRFs/NNS )/-RRB- can/MD be/VB cast/VBN into/IN an/DT L1/NN -/HYPH regularized/VBN optimization/NN problem/NN ./.
To/TO avoid/VB optimizing/VBG over/IN a/DT fully/RB linked/VBN model/NN ,/, gain/NN -/HYPH based/VBN or/CC gradient/NN -/HYPH based/VBN feature/NN selection/NN methods/NNS start/VBP from/IN an/DT empty/JJ model/NN and/CC incrementally/RB add/VB top/JJ ranked/VBD features/NNS to/IN it/PRP ./.
However/RB ,/, for/IN high/JJ -/HYPH dimensional/JJ problems/NNS like/IN statistical/JJ relational/JJ learning/NN ,/, training/NN time/NN of/IN these/DT incremental/JJ methods/NNS can/MD be/VB dominated/VBN by/IN the/DT cost/NN of/IN evaluating/VBG the/DT gain/NN or/CC gradient/NN of/IN a/DT large/JJ collection/NN of/IN candidate/NN features/NNS ./.
In/IN this/DT study/NN we/PRP propose/VBP a/DT fast/JJ feature/NN evaluation/NN algorithm/NN called/VBN Contrastive/NNP Feature/NN Induction/NN (/-LRB- CFI/NNP )/-RRB- ,/, which/WDT only/RB evaluates/VBZ a/DT subset/NN of/IN features/NNS that/WDT involve/VBP both/DT variables/NNS with/IN high/JJ signals/NNS (/-LRB- deviation/NN from/IN mean/NN )/-RRB- and/CC variables/NNS with/IN high/JJ errors/NNS (/-LRB- residue/NN )/-RRB- ./.
We/PRP prove/VBP that/IN the/DT gradient/NN of/IN candidate/NN features/NNS can/MD be/VB represented/VBN solely/RB as/IN a/DT function/NN of/IN signals/NNS and/CC errors/NNS ,/, and/CC that/IN CFI/NNP is/VBZ an/DT efficient/JJ approximation/NN of/IN gradient/NN -/HYPH based/VBN evaluation/NN methods/NNS ./.
Experiments/NNS on/IN synthetic/JJ and/CC real/JJ data/NNS sets/NNS show/VBP competitive/JJ learning/NN speed/NN and/CC accuracy/NN of/IN CFI/NNP on/IN pairwise/JJ CRFs/NNS ,/, compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN structure/NN learning/NN methods/NNS such/JJ as/IN full/JJ optimization/NN over/IN all/DT features/NNS ,/, and/CC Grafting/NNP ./.
