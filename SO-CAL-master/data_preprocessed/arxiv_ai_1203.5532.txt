We/PRP consider/VBP infinite/JJ -/HYPH horizon/NN discounted/VBN Markov/NNP Decision/NN Processes/NNS ,/, for/IN which/WDT it/PRP is/VBZ known/VBN that/IN there/EX exists/VBZ a/DT stationary/JJ optimal/JJ policy/NN ./.
We/PRP consider/VBP the/DT algorithm/NN Value/NN Iteration/NN and/CC the/DT sequence/NN of/IN policies/NNS $/$ \/CD pi_1/CD ,/, .../: ,/, \/SYM pi_k/FW $/$ it/PRP gen/IN erates/NNS until/IN some/DT iteration/NN $/$ k/CD $/$ ./.
We/PRP provide/VBP performance/NN bounds/NNS for/IN non-stationary/JJ policies/NNS involving/VBG the/DT last/JJ $/$ m/CD $/$ generated/VBN policies/NNS that/WDT reduce/VBP the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN bound/VBN for/IN the/DT last/JJ stationary/JJ policy/NN $/$ \/CD pi_k/CD $/$ by/IN a/DT factor/NN $/$ \/CD frac/NN {/-LRB- 1/CD -/HYPH \/SYM gamma/NN }/-RRB- {/-LRB- 1/CD -/HYPH \/SYM gamma/NN ^/SYM m/NN }/-RRB- $/$ ./.
In/IN other/JJ words/NNS ,/, and/CC contrary/JJ to/IN a/DT common/JJ intuition/NN ,/, we/PRP show/VBP that/IN it/PRP may/MD be/VB much/RB easier/JJR to/TO find/VB a/DT non-stationary/JJ approximately/RB -/HYPH optimal/JJ policy/NN than/IN a/DT stationary/JJ one/NN ./.
