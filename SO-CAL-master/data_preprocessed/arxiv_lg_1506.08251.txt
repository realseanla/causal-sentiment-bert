We/PRP present/VBP a/DT complimentary/JJ objective/NN for/IN training/NN recurrent/JJ neural/JJ networks/NNS (/-LRB- RNN/NN )/-RRB- with/IN gating/NN units/NNS that/WDT helps/VBZ with/IN regularization/NN and/CC interpretability/NN of/IN the/DT trained/VBN model/NN ./.
Attention/NN -/HYPH based/VBN RNN/NN models/NNS have/VBP shown/VBN success/NN in/IN many/JJ difficult/JJ sequence/NN to/IN sequence/NN classification/NN problems/NNS with/IN long/JJ and/CC short/JJ term/NN dependencies/NNS ,/, however/RB these/DT models/NNS are/VBP prone/JJ to/IN overfitting/VBG ./.
In/IN this/DT paper/NN ,/, we/PRP describe/VBP how/WRB to/TO regularize/VB these/DT models/NNS through/IN an/DT L1/NN penalty/NN on/IN the/DT activation/NN of/IN the/DT gating/NN units/NNS ,/, and/CC show/VBP that/IN this/DT technique/NN reduces/VBZ overfitting/VBG on/IN a/DT variety/NN of/IN tasks/NNS while/IN also/RB providing/VBG to/IN us/PRP a/DT human/JJ -/HYPH interpretable/JJ visualization/NN of/IN the/DT inputs/NNS used/VBN by/IN the/DT network/NN ./.
These/DT tasks/NNS include/VBP sentiment/NN analysis/NN ,/, paraphrase/NN recognition/NN ,/, and/CC question/NN answering/NN ./.
