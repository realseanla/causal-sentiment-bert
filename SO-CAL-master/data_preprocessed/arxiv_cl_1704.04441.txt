This/DT paper/NN investigates/VBZ the/DT robustness/NN of/IN NLP/NN against/IN perturbed/VBN word/NN forms/NNS ./.
While/IN neural/JJ approaches/NNS can/MD achieve/VB (/-LRB- almost/RB )/-RRB- human/JJ -/HYPH like/JJ accuracy/NN for/IN certain/JJ tasks/NNS and/CC conditions/NNS ,/, they/PRP often/RB are/VBP sensitive/JJ to/IN small/JJ changes/NNS in/IN the/DT input/NN such/JJ as/IN non-canonical/JJ input/NN (/-LRB- e.g./FW ,/, typos/NNS )/-RRB- ./.
Yet/CC both/DT stability/NN and/CC robustness/NN are/VBP desired/VBN properties/NNS in/IN applications/NNS involving/VBG user/NN -/HYPH generated/VBN content/NN ,/, and/CC the/DT more/RBR as/IN humans/NNS easily/RB cope/VB with/IN such/JJ noisy/JJ or/CC adversary/NN conditions/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP study/VBP the/DT impact/NN of/IN noisy/JJ input/NN ./.
We/PRP consider/VBP different/JJ noise/NN distributions/NNS (/-LRB- one/CD type/NN of/IN noise/NN ,/, combination/NN of/IN noise/NN types/NNS )/-RRB- and/CC mismatched/VBN noise/NN distributions/NNS for/IN training/NN and/CC testing/NN ./.
Moreover/RB ,/, we/PRP empirically/RB evaluate/VB the/DT robustness/NN of/IN different/JJ models/NNS (/-LRB- convolutional/JJ neural/JJ networks/NNS ,/, recurrent/JJ neural/JJ networks/NNS ,/, non-neural/JJ models/NNS )/-RRB- ,/, different/JJ basic/JJ units/NNS (/-LRB- characters/NNS ,/, byte/NN pair/NN encoding/VBG units/NNS )/-RRB- ,/, and/CC different/JJ NLP/NN tasks/NNS (/-LRB- morphological/JJ tagging/NN ,/, machine/NN translation/NN )/-RRB- ./.
