We/PRP study/VBP the/DT computational/JJ complexity/NN of/IN the/DT infinite/JJ -/HYPH horizon/NN discounted/VBN -/HYPH reward/NN Markov/NNP Decision/NN Problem/NN (/-LRB- MDP/NN )/-RRB- with/IN a/DT finite/JJ state/NN space/NN $/$ |/NFP \/SYM mathcal/NN {/-LRB- S/NN }/-RRB- |/NFP $/$ and/CC a/DT finite/JJ action/NN space/NN $/$ |/NFP \/SYM mathcal/JJ {/-LRB- A/DT }/-RRB- |/NFP $/$ ./.
We/PRP show/VBP that/IN any/DT randomized/JJ algorithm/NN needs/VBZ a/DT running/NN time/NN at/IN least/JJS $/$ \/SYM Omega/NN (/-LRB- |/NFP \/SYM mathcal/NN {/-LRB- S/NN }/-RRB- |/NFP ^/SYM 2/CD |/NFP \/SYM mathcal/JJ {/-LRB- A/DT }/-RRB- |/NFP )/-RRB- $/$ to/TO compute/VB an/DT $/$ \/CD epsilon/CD $/$ -/HYPH optimal/JJ policy/NN with/IN high/JJ probability/NN ./.
We/PRP consider/VBP two/CD variants/NNS of/IN the/DT MDP/NN where/WRB the/DT input/NN is/VBZ given/VBN in/IN specific/JJ data/NNS structures/NNS ,/, including/VBG arrays/NNS of/IN cumulative/JJ probabilities/NNS and/CC binary/JJ trees/NNS of/IN transition/NN probabilities/NNS ./.
For/IN these/DT cases/NNS ,/, we/PRP show/VBP that/IN the/DT complexity/NN lower/JJR bound/JJ reduces/VBZ to/IN $/$ \/SYM Omega/NN \/SYM left/NN (/-LRB- \/SYM frac/NN {/-LRB- |/NFP \/SYM mathcal/NN {/-LRB- S/NN }/-RRB- |/NFP |/NFP \/SYM mathcal/JJ {/-LRB- A/DT }/-RRB- |/NFP }/-RRB- {/-LRB- \/SYM epsilon/NN }/-RRB- \/SYM right/NN )/-RRB- $/$ ./.
These/DT results/NNS reveal/VBP a/DT surprising/JJ observation/NN that/IN the/DT computational/JJ complexity/NN of/IN the/DT MDP/NNP depends/VBZ on/IN the/DT data/NNS structure/NN of/IN input/NN ./.
