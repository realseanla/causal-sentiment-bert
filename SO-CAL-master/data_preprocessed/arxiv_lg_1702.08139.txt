Recent/JJ work/NN on/IN generative/JJ modeling/NN of/IN text/NN has/VBZ found/VBN that/IN variational/JJ auto/NN -/HYPH encoders/NNS (/-LRB- VAE/NN )/-RRB- incorporating/VBG LSTM/NN decoders/NNS perform/VBP worse/JJR than/IN simpler/JJR LSTM/NNP language/NN models/NNS (/-LRB- Bowman/NNP et/FW al./FW ,/, 2015/CD )/-RRB- ./.
This/DT negative/JJ result/NN is/VBZ so/RB far/RB poorly/RB understood/VBN ,/, but/CC has/VBZ been/VBN attributed/VBN to/IN the/DT propensity/NN of/IN LSTM/NNP decoders/NNS to/TO ignore/VB conditioning/VBG information/NN from/IN the/DT encoder/NN ./.
In/IN this/DT paper/NN ,/, we/PRP experiment/VBP with/IN a/DT new/JJ type/NN of/IN decoder/NN for/IN VAE/NNP :/: a/DT dilated/JJ CNN/NNP ./.
By/IN changing/VBG the/DT decoder/NN 's/POS dilation/NN architecture/NN ,/, we/PRP control/VBP the/DT effective/JJ context/NN from/IN previously/RB generated/VBN words/NNS ./.
In/IN experiments/NNS ,/, we/PRP find/VBP that/IN there/EX is/VBZ a/DT trade/NN off/RP between/IN the/DT contextual/JJ capacity/NN of/IN the/DT decoder/NN and/CC the/DT amount/NN of/IN encoding/VBG information/NN used/VBN ./.
We/PRP show/VBP that/IN with/IN the/DT right/JJ decoder/NN ,/, VAE/NNP can/MD outperform/VB LSTM/NNP language/NN models/NNS ./.
We/PRP demonstrate/VBP perplexity/NN gains/NNS on/IN two/CD datasets/NNS ,/, representing/VBG the/DT first/JJ positive/JJ experimental/JJ result/NN on/IN the/DT use/NN VAE/NNP for/IN generative/JJ modeling/NN of/IN text/NN ./.
Further/RB ,/, we/PRP conduct/VBP an/DT in/IN -/HYPH depth/NN investigation/NN of/IN the/DT use/NN of/IN VAE/NNP (/-LRB- with/IN our/PRP$ new/JJ decoding/NN architecture/NN )/-RRB- for/IN semi-supervised/JJ and/CC unsupervised/JJ labeling/NN tasks/NNS ,/, demonstrating/VBG gains/NNS over/IN several/JJ strong/JJ baselines/NNS ./.
