Positive/JJ unlabeled/JJ learning/NN (/-LRB- PU/NN learning/NN )/-RRB- refers/VBZ to/IN the/DT task/NN of/IN learning/VBG a/DT binary/JJ classifier/NN from/IN only/RB positive/JJ and/CC unlabeled/JJ data/NNS [/-LRB- 1/CD ]/-RRB- ./.
This/DT problem/NN arises/VBZ in/IN various/JJ practical/JJ applications/NNS ,/, like/IN in/IN multimedia/NN //HYPH information/NN retrieval/NN [/-LRB- 2/CD ]/-RRB- ,/, where/WRB the/DT goal/NN is/VBZ to/TO find/VB samples/NNS in/IN an/DT unlabeled/JJ data/NNS set/VBP that/DT are/VBP similar/JJ to/IN the/DT samples/NNS provided/VBN by/IN a/DT user/NN ,/, as/RB well/RB as/IN for/IN applications/NNS of/IN outlier/NN detection/NN [/-LRB- 3/CD ]/-RRB- or/CC semi-supervised/VBN novelty/NN detection/NN [/-LRB- 4/CD ]/-RRB- ./.
The/DT works/NNS in/IN [/-LRB- 5/CD ]/-RRB- and/CC [/-LRB- 6/CD ]/-RRB- have/VBP recently/RB shown/VBN that/IN PU/NNP learning/NN can/MD be/VB formulated/VBN as/IN a/DT risk/NN minimization/NN problem/NN ./.
In/IN particular/JJ ,/, expressing/VBG the/DT risk/NN with/IN a/DT convex/NN loss/NN function/NN ,/, like/IN the/DT double/JJ Hinge/NNP loss/NN ,/, allows/VBZ to/TO achieve/VB better/JJR classification/NN performance/NN than/IN those/DT ones/NNS obtained/VBN by/IN using/VBG other/JJ loss/NN functions/NNS ./.
Nevertheless/RB ,/, the/DT works/NNS have/VBP only/RB focused/VBN in/IN analysing/VBG the/DT generalization/NN performance/NN obtained/VBN by/IN using/VBG different/JJ loss/NN functions/NNS ,/, without/IN considering/VBG the/DT efficiency/NN of/IN training/NN ./.
In/IN that/DT regard/NN ,/, we/PRP propose/VBP a/DT novel/JJ algorithm/NN ,/, which/WDT optimizes/VBZ efficiently/RB the/DT risk/NN minimization/NN problem/NN stated/VBN in/IN [/-LRB- 6/CD ]/-RRB- ./.
In/IN particular/JJ ,/, we/PRP show/VBP that/IN the/DT storage/NN complexity/NN of/IN our/PRP$ approach/NN scales/VBZ only/RB linearly/RB with/IN the/DT number/NN of/IN training/NN samples/NNS ./.
Concerning/VBG the/DT training/NN time/NN ,/, we/PRP show/VBP experimentally/RB on/IN different/JJ benchmark/NN data/NNS sets/VBZ that/IN our/PRP$ algorithm/NN exhibits/VBZ the/DT same/JJ quadratic/JJ behaviour/NN of/IN existing/VBG optimization/NN algorithms/NNS implemented/VBN in/IN highly/RB -/HYPH efficient/JJ libraries/NNS ./.
The/DT rest/NN of/IN the/DT paper/NN is/VBZ organized/VBN as/IN follows/VBZ ./.
In/IN Section/NN 2/CD we/PRP review/VBP the/DT formulation/NN of/IN the/DT PU/NNP learning/NN problem/NN and/CC we/PRP enunciate/VBP for/IN the/DT first/JJ time/NN the/DT Representer/NNP theorem/NN ./.
In/IN Section/NN 3/CD we/PRP derive/VBP the/DT convex/NN formulation/NN of/IN the/DT problem/NN by/IN using/VBG the/DT double/JJ Hinge/NNP loss/NN function/NN ./.
In/IN Section/NN 4/CD we/PRP propose/VBP an/DT algorithm/NN to/TO solve/VB the/DT optimization/NN problem/NN and/CC we/PRP finally/RB conclude/VBP with/IN the/DT last/JJ section/NN by/IN describing/VBG the/DT experimental/JJ evaluation/NN ./.
