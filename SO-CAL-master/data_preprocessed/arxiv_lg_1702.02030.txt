Although/IN there/EX exist/VBP plentiful/JJ theories/NNS of/IN empirical/JJ risk/NN minimization/NN (/-LRB- ERM/NN )/-RRB- for/IN supervised/JJ learning/NN ,/, current/JJ theoretical/JJ understandings/NNS of/IN ERM/NNP for/IN a/DT related/JJ problem/NN ---/, stochastic/JJ convex/NN optimization/NN (/-LRB- SCO/NNP )/-RRB- ,/, are/VBP limited/VBN ./.
In/IN this/DT work/NN ,/, we/PRP strengthen/VBP the/DT realm/NN of/IN ERM/NNP for/IN SCO/NNP by/IN exploiting/VBG smoothness/NN and/CC strong/JJ convexity/NN conditions/NNS to/TO improve/VB the/DT risk/NN bounds/NNS ./.
First/RB ,/, we/PRP establish/VBP an/DT $/$ \/CD widetilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- d/NN //HYPH n/NN \/SYM sqrt/NN {/-LRB- F/NN _/NFP */SYM //HYPH n/NN }/-RRB- )/-RRB- $/$ risk/NN bound/VBN when/WRB the/DT random/JJ function/NN is/VBZ nonnegative/JJ ,/, convex/NN and/CC smooth/JJ ,/, and/CC the/DT expected/VBN function/NN is/VBZ Lipschitz/NNP continuous/JJ ,/, where/WRB $/$ d/LS $/$ is/VBZ the/DT dimensionality/NN of/IN the/DT problem/NN ,/, $/$ n/NN $/$ is/VBZ the/DT number/NN of/IN samples/NNS ,/, and/CC $/$ F/NNP _/NFP */NFP $/$ is/VBZ the/DT minimal/JJ risk/NN ./.
Thus/RB ,/, when/WRB $/$ F/NNP _/NFP */NFP $/$ is/VBZ small/JJ we/PRP obtain/VBP an/DT $/$ \/CD widetilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- d/NN //HYPH n/NN )/-RRB- $/$ risk/NN bound/VBN ,/, which/WDT is/VBZ analogous/JJ to/IN the/DT $/$ \/CD widetilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- 1/CD //SYM n/NN )/-RRB- $/$ optimistic/JJ rate/NN of/IN ERM/NNP for/IN supervised/JJ learning/NN ./.
Second/RB ,/, if/IN the/DT objective/JJ function/NN is/VBZ also/RB $/$ \/CD lambda/NN $/$ -/HYPH strongly/RB convex/NN ,/, we/PRP prove/VBP an/DT $/$ \/CD widetilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- d/NN //HYPH n/NN \/SYM kappa/NN F/NN _/NFP */SYM //HYPH n/NN )/-RRB- $/$ risk/NN bound/VBN where/WRB $/$ \/SYM kappa/FW $/$ is/VBZ the/DT condition/NN number/NN ,/, and/CC improve/VB it/PRP to/IN $/$ O/UH (/-LRB- 1/CD //SYM [/-LRB- \/SYM lambda/NN n/NN ^/SYM 2/CD ]/-RRB- \/SYM kappa/NN F/NN _/NFP */SYM //HYPH n/NN )/-RRB- $/$ when/WRB $/$ n/NN =/SYM \/SYM widetilde/SYM {/-LRB- \/SYM Omega/NN }/-RRB- (/-LRB- \/SYM kappa/NN d/NN )/-RRB- $/$ ./.
As/IN a/DT result/NN ,/, we/PRP obtain/VBP an/DT $/$ O/UH (/-LRB- \/SYM kappa/NN //SYM n/NN ^/SYM 2/CD )/-RRB- $/$ risk/NN bound/VBN under/IN the/DT condition/NN that/WDT $/$ n/NN $/$ is/VBZ large/JJ and/CC $/$ F/NNP _/NFP */NFP $/$ is/VBZ small/JJ ,/, which/WDT to/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, is/VBZ the/DT first/JJ $/NN O/NN (/-LRB- 1/CD //SYM n/NN ^/SYM 2/CD )/-RRB- $/$ -/HYPH type/NN of/IN risk/NN bound/VBN of/IN ERM/NNP ./.
Third/JJ ,/, we/PRP stress/VBP that/IN the/DT above/JJ results/NNS are/VBP established/VBN in/IN a/DT unified/JJ framework/NN ,/, which/WDT allows/VBZ us/PRP to/TO derive/VB new/JJ risk/NN bounds/NNS under/IN weaker/JJR conditions/NNS ,/, e.g./FW ,/, without/IN convexity/NN of/IN the/DT random/JJ function/NN and/CC Lipschitz/NNP continuity/NN of/IN the/DT expected/VBN function/NN ./.
Finally/RB ,/, we/PRP demonstrate/VBP that/IN to/TO achieve/VB an/DT $/$ O/UH (/-LRB- 1/CD //SYM [/-LRB- \/SYM lambda/NN n/NN ^/SYM 2/CD ]/-RRB- \/SYM kappa/NN F/NN _/NFP */SYM //HYPH n/NN )/-RRB- $/$ risk/NN bound/VBN for/IN supervised/JJ learning/NN ,/, the/DT $/$ \/CD widetilde/NN {/-LRB- \/SYM Omega/NN }/-RRB- (/-LRB- \/SYM kappa/NN d/NN )/-RRB- $/$ requirement/NN on/IN $/$ n/NN $/$ can/MD be/VB replaced/VBN with/IN $/$ \/SYM Omega/NN (/-LRB- \/SYM kappa/NN ^/SYM 2/CD )/-RRB- $/$ ,/, which/WDT is/VBZ dimensionality/NN -/HYPH independent/JJ ./.
