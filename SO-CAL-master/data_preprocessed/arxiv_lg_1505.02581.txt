Deep/JJ neural/JJ networks/NNS have/VBP recently/RB achieved/VBN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS in/IN many/JJ machine/NN learning/NN problems/NNS ,/, e.g./FW ,/, speech/NN recognition/NN or/CC object/NN recognition/NN ./.
Hitherto/RB ,/, work/VB on/IN rectified/VBN linear/JJ units/NNS (/-LRB- ReLU/NN )/-RRB- provides/VBZ empirical/JJ and/CC theoretical/JJ evidence/NN on/IN performance/NN increase/NN of/IN neural/JJ networks/NNS comparing/VBG to/IN typically/RB used/VBN sigmoid/NN activation/NN function/NN ./.
In/IN this/DT paper/NN ,/, we/PRP investigate/VBP a/DT new/JJ manner/NN of/IN improving/VBG neural/JJ networks/NNS by/IN introducing/VBG a/DT bunch/NN of/IN copies/NNS of/IN the/DT same/JJ neuron/NN modeled/VBN by/IN the/DT generalized/VBN Kumaraswamy/NNP distribution/NN ./.
As/IN a/DT result/NN ,/, we/PRP propose/VBP novel/JJ non-linear/JJ activation/NN function/NN which/WDT we/PRP refer/VBP to/IN as/IN Kumaraswamy/NNP unit/NN which/WDT is/VBZ closely/RB related/VBN to/IN ReLU/NNP ./.
In/IN the/DT experimental/JJ study/NN with/IN MNIST/JJ image/NN corpora/NNS we/PRP evaluate/VBP the/DT Kumaraswamy/NNP unit/NN applied/VBD to/IN single/JJ -/HYPH layer/NN (/-LRB- shallow/NN )/-RRB- neural/JJ network/NN and/CC report/VB a/DT significant/JJ drop/NN in/IN test/NN classification/NN error/NN and/CC test/NN cross-entropy/NN in/IN comparison/NN to/IN sigmoid/NN unit/NN ,/, ReLU/NN and/CC Noisy/JJ ReLU/NN ./.
