In/IN structured/JJ prediction/NN problems/NNS where/WRB we/PRP have/VBP indirect/JJ supervision/NN of/IN the/DT output/NN ,/, maximum/JJ marginal/JJ likelihood/NN faces/VBZ two/CD computational/JJ obstacles/NNS :/: non-convexity/NN of/IN the/DT objective/NN and/CC intractability/NN of/IN even/RB a/DT single/JJ gradient/NN computation/NN ./.
In/IN this/DT paper/NN ,/, we/PRP bypass/VBP both/DT obstacles/NNS for/IN a/DT class/NN of/IN what/WP we/PRP call/VBP linear/JJ indirectly/RB -/HYPH supervised/JJ problems/NNS ./.
Our/PRP$ approach/NN is/VBZ simple/JJ :/: we/PRP solve/VB a/DT linear/JJ system/NN to/TO estimate/VB sufficient/JJ statistics/NNS of/IN the/DT model/NN ,/, which/WDT we/PRP then/RB use/VBP to/TO estimate/VB parameters/NNS via/IN convex/NN optimization/NN ./.
We/PRP analyze/VBP the/DT statistical/JJ properties/NNS of/IN our/PRP$ approach/NN and/CC show/NN empirically/RB that/IN it/PRP is/VBZ effective/JJ in/IN two/CD settings/NNS :/: learning/VBG with/IN local/JJ privacy/NN constraints/NNS and/CC learning/VBG from/IN low/JJ -/HYPH cost/NN count/NN -/HYPH based/VBN annotations/NNS ./.
