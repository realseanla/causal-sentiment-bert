In/IN computer/NN architecture/NN ,/, near/IN -/HYPH data/NN processing/NN (/-LRB- NDP/NN )/-RRB- refers/VBZ to/IN augmenting/VBG the/DT memory/NN or/CC the/DT storage/NN with/IN processing/NN power/NN so/IN that/IN it/PRP can/MD process/VB the/DT data/NNS stored/VBN therein/RB ,/, passing/VBG only/RB the/DT processed/VBN data/NNS upwards/RB in/IN the/DT memory/NN hierarchy/NN ./.
By/IN offloading/VBG the/DT computational/JJ burden/NN of/IN CPU/NN and/CC saving/VBG the/DT need/NN for/IN transferring/VBG raw/JJ data/NNS ,/, NDP/NN has/VBZ a/DT great/JJ potential/NN in/IN terms/NNS of/IN accelerating/VBG computation/NN and/CC reducing/VBG power/NN consumption/NN ./.
Despite/IN its/PRP$ potential/NN ,/, NDP/NNP had/VBD only/RB limited/JJ success/NN until/IN recently/RB ,/, mainly/RB due/IN to/IN the/DT performance/NN mismatch/NN in/IN logic/NN and/CC memory/NN process/NN technologies/NNS ./.
Recently/RB ,/, there/EX have/VBP been/VBN two/CD major/JJ changes/NNS in/IN the/DT game/NN ,/, making/VBG NDP/NNP more/RBR appealing/JJ than/IN ever/RB ./.
The/DT first/JJ is/VBZ the/DT success/NN of/IN deep/JJ learning/NN ,/, which/WDT often/RB requires/VBZ frequent/JJ transfers/NNS of/IN big/JJ data/NNS for/IN training/NN ./.
The/DT second/JJ is/VBZ the/DT advent/NN of/IN NAND/NNP flash/NN -/HYPH based/VBN solid/JJ -/HYPH state/NN drives/NNS (/-LRB- SSDs/NNS )/-RRB- containing/VBG multicore/NN CPUs/NNS that/WDT can/MD be/VB used/VBN for/IN data/NNS processing/NN ./.
In/IN this/DT paper/NN ,/, we/PRP evaluate/VBP the/DT potential/NN of/IN NDP/NNP for/IN machine/NN learning/NN using/VBG a/DT new/JJ SSD/NN platform/NN that/WDT allows/VBZ us/PRP to/TO simulate/VB in/IN -/HYPH storage/NN processing/NN (/-LRB- ISP/NN )/-RRB- of/IN machine/NN learning/NN workloads/NNS ./.
Although/IN our/PRP$ platform/NN named/VBN ISPML/NNP can/MD execute/VB various/JJ algorithms/NNS ,/, this/DT paper/NN focuses/VBZ on/IN the/DT stochastic/JJ gradient/NN decent/JJ (/-LRB- SGD/NNP )/-RRB- algorithm/NN ,/, which/WDT is/VBZ the/DT de/FW facto/FW standard/JJ method/NN for/IN training/NN deep/JJ neural/JJ networks/NNS ./.
We/PRP implement/VBP and/CC compare/VBP three/CD variants/NNS of/IN SGD/NNP (/-LRB- synchronous/JJ ,/, downpour/NN ,/, and/CC elastic/JJ averaging/NN )/-RRB- using/VBG the/DT ISP/NN -/HYPH ML/NN platform/NN ,/, in/IN which/WDT we/PRP exploit/VBP the/DT multiple/JJ NAND/NN channels/NNS for/IN implementing/VBG parallel/JJ SGD/NNP ./.
In/IN addition/NN ,/, we/PRP compare/VBP the/DT performance/NN of/IN ISP/NN optimization/NN and/CC that/DT of/IN conventional/JJ in/IN -/HYPH host/NN processing/NN optimization/NN ./.
To/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ one/CD of/IN the/DT first/JJ attempts/NNS to/TO apply/VB NDP/NN to/IN the/DT optimization/NN for/IN machine/NN learning/NN ./.
