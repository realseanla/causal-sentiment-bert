In/IN a/DT traditional/JJ convolutional/JJ layer/NN ,/, the/DT learned/VBN filters/NNS stay/VBP fixed/VBN after/IN training/NN ./.
In/IN contrast/NN ,/, we/PRP introduce/VBP a/DT new/JJ framework/NN ,/, the/DT Dynamic/JJ Filter/NNP Network/NNP ,/, where/WRB filters/NNS are/VBP generated/VBN dynamically/RB conditioned/VBN on/IN an/DT input/NN ./.
We/PRP show/VBP that/IN this/DT architecture/NN is/VBZ a/DT powerful/JJ one/CD ,/, with/IN increased/VBN flexibility/NN thanks/NNS to/IN its/PRP$ adaptive/JJ nature/NN ,/, yet/RB without/IN an/DT excessive/JJ increase/NN in/IN the/DT number/NN of/IN model/NN parameters/NNS ./.
A/DT wide/JJ variety/NN of/IN filtering/VBG operations/NNS can/MD be/VB learned/VBN this/DT way/NN ,/, including/VBG local/JJ spatial/JJ transformations/NNS ,/, but/CC also/RB others/NNS like/IN selective/JJ (/-LRB- de/FW )/-RRB- blurring/VBG or/CC adaptive/JJ feature/NN extraction/NN ./.
Moreover/RB ,/, multiple/JJ such/JJ layers/NNS can/MD be/VB combined/VBN ,/, e.g./FW in/IN a/DT recurrent/JJ architecture/NN ./.
We/PRP demonstrate/VBP the/DT effectiveness/NN of/IN the/DT dynamic/JJ filter/NN network/NN on/IN the/DT tasks/NNS of/IN video/NN and/CC stereo/NN prediction/NN ,/, and/CC reach/VB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN the/DT moving/VBG MNIST/NN dataset/NN with/IN a/DT much/RB smaller/JJR model/NN ./.
By/IN visualizing/VBG the/DT learned/VBN filters/NNS ,/, we/PRP illustrate/VBP that/IN the/DT network/NN has/VBZ picked/VBN up/RP flow/NN information/NN by/IN only/RB looking/VBG at/IN unlabelled/JJ training/NN data/NNS ./.
This/DT suggests/VBZ that/IN the/DT network/NN can/MD be/VB used/VBN to/TO pretrain/VB networks/NNS for/IN various/JJ supervised/JJ tasks/NNS in/IN an/DT unsupervised/JJ way/NN ,/, like/IN optical/JJ flow/NN and/CC depth/NN estimation/NN ./.
