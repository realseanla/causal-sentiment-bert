We/PRP introduce/VBP the/DT first/JJ generic/JJ text/NN representation/NN model/NN that/WDT is/VBZ completely/RB nonsymbolic/JJ ,/, i.e./FW ,/, it/PRP does/VBZ not/RB require/VB the/DT availability/NN of/IN a/DT segmentation/NN or/CC tokenization/NN method/NN that/WDT attempts/VBZ to/TO identify/VB words/NNS or/CC other/JJ symbolic/JJ units/NNS in/IN text/NN ./.
This/DT applies/VBZ to/IN training/VBG the/DT parameters/NNS of/IN the/DT model/NN on/IN a/DT training/NN corpus/NN as/RB well/RB as/IN to/IN applying/VBG it/PRP when/WRB computing/VBG the/DT representation/NN of/IN a/DT new/JJ text/NN ./.
We/PRP show/VBP that/IN our/PRP$ model/NN performs/VBZ better/JJR than/IN prior/JJ work/NN on/IN an/DT information/NN extraction/NN and/CC a/DT text/NN denoising/NN task/NN ./.
