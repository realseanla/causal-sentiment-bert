Connectionist/JJ temporal/JJ classification/NN (/-LRB- CTC/NN )/-RRB- based/VBN supervised/JJ sequence/NN training/NN of/IN recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- has/VBZ shown/VBN great/JJ success/NN in/IN many/JJ machine/NN learning/NN areas/NNS including/VBG end/NN -/HYPH to/IN -/HYPH end/NN speech/NN and/CC handwritten/JJ character/NN recognition/NN ./.
For/IN the/DT CTC/NN training/NN ,/, however/RB ,/, it/PRP is/VBZ required/VBN to/TO unroll/VB the/DT RNN/NN by/IN the/DT length/NN of/IN an/DT input/NN sequence/NN ./.
This/DT unrolling/NN requires/VBZ a/DT lot/NN of/IN memory/NN and/CC hinders/VBZ a/DT small/JJ footprint/NN implementation/NN of/IN online/JJ learning/NN or/CC adaptation/NN ./.
Furthermore/RB ,/, the/DT length/NN of/IN training/NN sequences/NNS is/VBZ usually/RB not/RB uniform/JJ ,/, which/WDT makes/VBZ parallel/JJ training/NN with/IN multiple/JJ sequences/NNS inefficient/JJ on/IN shared/VBN memory/NN models/NNS such/JJ as/IN graphics/NNS processing/VBG units/NNS (/-LRB- GPUs/NNS )/-RRB- ./.
In/IN this/DT work/NN ,/, we/PRP introduce/VBP an/DT expectation/NN -/HYPH maximization/NN (/-LRB- EM/NN )/-RRB- based/VBN online/JJ CTC/NN algorithm/NN that/WDT enables/VBZ unidirectional/JJ RNNs/NNS to/TO learn/VB sequences/NNS that/WDT are/VBP longer/JJR than/IN the/DT amount/NN of/IN unrolling/VBG ./.
The/DT RNNs/NNS can/MD also/RB be/VB trained/VBN to/TO process/VB an/DT infinitely/RB long/JJ input/NN sequence/NN without/IN pre-segmentation/NN or/CC external/JJ reset/NN ./.
Moreover/RB ,/, the/DT proposed/VBN approach/NN allows/VBZ efficient/JJ parallel/JJ training/NN on/IN GPUs/NNS ./.
For/IN evaluation/NN ,/, end/NN -/HYPH to/IN -/HYPH end/NN speech/NN recognition/NN examples/NNS are/VBP presented/VBN on/IN the/DT Wall/NNP Street/NNP Journal/NNP (/-LRB- WSJ/NNP )/-RRB- corpus/NN ./.
