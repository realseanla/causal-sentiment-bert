Stochastic/JJ gradient/NN MCMC/NN (/-LRB- SG/NN -/HYPH MCMC/NN )/-RRB- has/VBZ played/VBN an/DT important/JJ role/NN in/IN large/JJ -/HYPH scale/NN Bayesian/JJ learning/NN ,/, with/IN well/RB -/HYPH developed/VBN theoretical/JJ convergence/NN properties/NNS ./.
In/IN such/JJ applications/NNS of/IN SG/NN -/HYPH MCMC/NN ,/, it/PRP is/VBZ becoming/VBG increasingly/RB popular/JJ to/TO employ/VB distributed/VBN systems/NNS ,/, where/WRB stochastic/JJ gradients/NNS are/VBP computed/VBN based/VBN on/IN some/DT outdated/JJ parameters/NNS ,/, yielding/VBG what/WP are/VBP termed/VBN stale/JJ gradients/NNS ./.
While/IN stale/JJ gradients/NNS could/MD be/VB directly/RB used/VBN in/IN SG/NN -/HYPH MCMC/NN ,/, their/PRP$ impact/NN on/IN convergence/NN properties/NNS has/VBZ not/RB been/VBN well/RB studied/VBN ./.
In/IN this/DT paper/NN we/PRP develop/VBP theory/NN to/TO show/VB that/IN while/IN the/DT bias/NN and/CC MSE/NN of/IN an/DT SG/NN -/HYPH MCMC/NN algorithm/NN depend/VBP on/IN the/DT staleness/NN of/IN stochastic/JJ gradients/NNS ,/, its/PRP$ estimation/NN variance/NN (/-LRB- relative/JJ to/IN the/DT expected/VBN estimate/NN ,/, based/VBN on/IN a/DT prescribed/VBN number/NN of/IN samples/NNS )/-RRB- is/VBZ independent/JJ of/IN it/PRP ./.
In/IN a/DT simple/JJ Bayesian/JJ distributed/VBN system/NN with/IN SG/NN -/HYPH MCMC/NN ,/, where/WRB stale/JJ gradients/NNS are/VBP computed/VBN asynchronously/RB by/IN a/DT set/NN of/IN workers/NNS ,/, our/PRP$ theory/NN indicates/VBZ a/DT linear/JJ speedup/NN on/IN the/DT decrease/NN of/IN estimation/NN variance/NN w.r.t./IN the/DT number/NN of/IN workers/NNS ./.
Experiments/NNS on/IN synthetic/JJ data/NNS and/CC deep/JJ neural/JJ networks/NNS validate/VBP our/PRP$ theory/NN ,/, demonstrating/VBG the/DT effectiveness/NN and/CC scalability/NN of/IN SG/NN -/HYPH MCMC/NN with/IN stale/JJ gradients/NNS ./.
