We/PRP consider/VBP partially/RB observable/JJ Markov/NNP decision/NN processes/NNS (/-LRB- POMDPs/NNS )/-RRB- with/IN a/DT set/NN of/IN target/NN states/NNS and/CC positive/JJ integer/NN costs/NNS associated/VBN with/IN every/DT transition/NN ./.
The/DT traditional/JJ optimization/NN objective/NN (/-LRB- stochastic/JJ shortest/JJS path/NN )/-RRB- asks/VBZ to/TO minimize/VB the/DT expected/VBN total/JJ cost/NN until/IN the/DT target/NN set/NN is/VBZ reached/VBN ./.
We/PRP extend/VBP the/DT traditional/JJ framework/NN of/IN POMDPs/NNS to/TO model/VB energy/NN consumption/NN ,/, which/WDT represents/VBZ a/DT hard/JJ constraint/NN ./.
The/DT energy/NN levels/NNS may/MD increase/VB and/CC decrease/VB with/IN transitions/NNS ,/, and/CC the/DT hard/JJ constraint/NN requires/VBZ that/IN the/DT energy/NN level/NN must/MD remain/VB positive/JJ in/IN all/DT steps/NNS till/IN the/DT target/NN is/VBZ reached/VBN ./.
First/RB ,/, we/PRP present/VBP a/DT novel/JJ algorithm/NN for/IN solving/VBG POMDPs/NNS with/IN energy/NN levels/NNS ,/, developing/VBG on/IN existing/VBG POMDP/NN solvers/NNS and/CC using/VBG RTDP/NN as/IN its/PRP$ main/JJ method/NN ./.
Our/PRP$ second/JJ contribution/NN is/VBZ related/VBN to/IN policy/NN representation/NN ./.
For/IN larger/JJR POMDP/NN instances/NNS the/DT policies/NNS computed/VBN by/IN existing/VBG solvers/NNS are/VBP too/RB large/JJ to/TO be/VB understandable/JJ ./.
We/PRP present/VBP an/DT automated/VBN procedure/NN based/VBN on/IN machine/NN learning/NN techniques/NNS that/WDT automatically/RB extracts/NNS important/JJ decisions/NNS of/IN the/DT policy/NN allowing/VBG us/PRP to/TO compute/VB succinct/JJ human/JJ readable/JJ policies/NNS ./.
Finally/RB ,/, we/PRP show/VBP experimentally/RB that/IN our/PRP$ algorithm/NN performs/VBZ well/RB and/CC computes/VBZ succinct/JJ policies/NNS on/IN a/DT number/NN of/IN POMDP/NN instances/NNS from/IN the/DT literature/NN that/WDT were/VBD naturally/RB enhanced/VBN with/IN energy/NN levels/NNS ./.
