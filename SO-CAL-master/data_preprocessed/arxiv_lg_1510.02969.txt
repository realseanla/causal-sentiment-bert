Despite/IN being/VBG the/DT appearance/NN -/HYPH based/VBN classifier/NN of/IN choice/NN in/IN recent/JJ years/NNS ,/, relatively/RB few/JJ works/NNS have/VBP examined/VBN how/WRB much/JJ convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- can/MD improve/VB performance/NN on/IN accepted/VBN expression/NN recognition/NN benchmarks/NNS and/CC ,/, more/RBR importantly/RB ,/, examine/VB what/WP it/PRP is/VBZ they/PRP actually/RB learn/VBP ./.
In/IN this/DT work/NN ,/, not/RB only/RB do/VBP we/PRP show/VBP that/IN CNNs/NNS can/MD achieve/VB strong/JJ performance/NN ,/, but/CC we/PRP also/RB introduce/VBP an/DT approach/NN to/IN decipher/NN which/WDT portions/NNS of/IN the/DT face/NN influence/NN the/DT CNN/NNP 's/POS predictions/NNS ./.
First/RB ,/, we/PRP train/VBP a/DT zero/CD -/HYPH bias/NN CNN/NNP on/IN facial/JJ expression/NN data/NNS and/CC achieve/VB ,/, to/IN our/PRP$ knowledge/NN ,/, state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN two/CD expression/NN recognition/NN benchmarks/NNS :/: the/DT extended/JJ Cohn/NNP -/HYPH Kanade/NNP (/-LRB- CK/NNP )/-RRB- dataset/NN and/CC the/DT Toronto/NNP Face/NNP Dataset/NN (/-LRB- TFD/NN )/-RRB- ./.
We/PRP then/RB qualitatively/RB analyze/VB the/DT network/NN by/IN visualizing/VBG the/DT spatial/JJ patterns/NNS that/WDT maximally/RB excite/VBP different/JJ neurons/NNS in/IN the/DT convolutional/JJ layers/NNS and/CC show/VB how/WRB they/PRP resemble/VBP Facial/JJ Action/NN Units/NNS (/-LRB- FAUs/NNS )/-RRB- ./.
Finally/RB ,/, we/PRP use/VBP the/DT FAU/NN labels/NNS provided/VBN in/IN the/DT CK/NNP dataset/NN to/TO verify/VB that/IN the/DT FAUs/NNS observed/VBN in/IN our/PRP$ filter/NN visualizations/NNS indeed/RB align/VB with/IN the/DT subject/NN 's/POS facial/JJ movements/NNS ./.
