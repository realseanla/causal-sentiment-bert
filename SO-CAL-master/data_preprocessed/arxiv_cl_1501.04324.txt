We/PRP consider/VBP phrase/NN based/VBN Language/NNP Models/NNPS (/-LRB- LM/NN )/-RRB- ,/, which/WDT generalize/VBP the/DT commonly/RB used/VBN word/NN level/NN models/NNS ./.
Similar/JJ concept/NN on/IN phrase/NN based/VBN LMs/NNPS appears/VBZ in/IN speech/NN recognition/NN ,/, which/WDT is/VBZ rather/RB specialized/JJ and/CC thus/RB less/RBR suitable/JJ for/IN machine/NN translation/NN (/-LRB- MT/NN )/-RRB- ./.
In/IN contrast/NN to/IN the/DT dependency/NN LM/NN ,/, we/PRP first/RB introduce/VB the/DT exhaustive/JJ phrase/NN -/HYPH based/VBN LMs/NNPS tailored/VBN for/IN MT/NN use/NN ./.
Preliminary/JJ experimental/JJ results/NNS show/VBP that/IN our/PRP$ approach/NN outperform/VBP word/NN based/VBN LMs/NNPS with/IN the/DT respect/NN to/IN perplexity/NN and/CC translation/NN quality/NN ./.
