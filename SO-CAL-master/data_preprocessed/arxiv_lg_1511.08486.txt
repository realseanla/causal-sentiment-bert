Matrix/NN -/HYPH parametrized/JJ models/NNS ,/, including/VBG multiclass/NN logistic/JJ regression/NN and/CC sparse/JJ coding/NN ,/, are/VBP used/VBN in/IN machine/NN learning/NN (/-LRB- ML/NN )/-RRB- applications/NNS ranging/VBG from/IN computer/NN vision/NN to/IN computational/JJ biology/NN ./.
When/WRB these/DT models/NNS are/VBP applied/VBN to/IN large/JJ -/HYPH scale/NN ML/NN problems/NNS starting/VBG at/IN millions/NNS of/IN samples/NNS and/CC tens/NNS of/IN thousands/NNS of/IN classes/NNS ,/, their/PRP$ parameter/NN matrix/NN can/MD grow/VB at/IN an/DT unexpected/JJ rate/NN ,/, resulting/VBG in/IN high/JJ parameter/NN synchronization/NN costs/NNS that/WDT greatly/RB slow/RB down/RB distributed/VBN learning/NN ./.
To/TO address/VB this/DT issue/NN ,/, we/PRP propose/VBP a/DT Sufficient/JJ Factor/NN Broadcasting/NNP (/-LRB- SFB/NN )/-RRB- computation/NN model/NN for/IN efficient/JJ distributed/VBN learning/NN of/IN a/DT large/JJ family/NN of/IN matrix/NN -/HYPH parameterized/JJ models/NNS ,/, which/WDT share/VBP the/DT following/VBG property/NN :/: the/DT parameter/NN update/NN computed/VBN on/IN each/DT data/NN sample/NN is/VBZ a/DT rank/NN -/HYPH 1/CD matrix/NN ,/, i.e./FW ,/, the/DT outer/JJ product/NN of/IN two/CD "/'' sufficient/JJ factors/NNS "/'' (/-LRB- SFs/NNS )/-RRB- ./.
By/IN broadcasting/VBG the/DT SFs/NNS among/IN worker/NN machines/NNS and/CC reconstructing/VBG the/DT update/NN matrices/NNS locally/RB at/IN each/DT worker/NN ,/, SFB/NN improves/VBZ communication/NN efficiency/NN ---/, communication/NN costs/NNS are/VBP linear/JJ in/IN the/DT parameter/NN matrix/NN 's/POS dimensions/NNS ,/, rather/RB than/IN quadratic/JJ ---/NFP without/IN affecting/VBG computational/JJ correctness/NN ./.
We/PRP present/VBP a/DT theoretical/JJ convergence/NN analysis/NN of/IN SFB/NN ,/, and/CC empirically/RB corroborate/VBP its/PRP$ efficiency/NN on/IN four/CD different/JJ matrix/NN -/HYPH parametrized/JJ ML/NN models/NNS ./.
