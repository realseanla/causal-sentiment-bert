We/PRP consider/VBP the/DT approximation/NN capability/NN of/IN orthogonal/JJ super/JJ greedy/JJ algorithms/NNS (/-LRB- OSGA/NN )/-RRB- and/CC its/PRP$ applications/NNS in/IN supervised/JJ learning/NN ./.
OSGA/NNP is/VBZ concerned/VBN with/IN selecting/VBG more/JJR than/IN one/CD atoms/NNS in/IN each/DT iteration/NN step/NN ,/, which/WDT ,/, of/IN course/NN ,/, greatly/RB reduces/VBZ the/DT computational/JJ burden/NN when/WRB compared/VBN with/IN the/DT conventional/JJ orthogonal/JJ greedy/JJ algorithm/NN (/-LRB- OGA/NN )/-RRB- ./.
We/PRP prove/VBP that/IN even/RB for/IN function/NN classes/NNS that/WDT are/VBP not/RB the/DT convex/NN hull/NN of/IN the/DT dictionary/NN ,/, OSGA/NN does/VBZ not/RB degrade/VB the/DT approximation/NN capability/NN of/IN OGA/NNP provided/VBD the/DT dictionary/NN is/RB incoherent/JJ ./.
Based/VBN on/IN this/DT ,/, we/PRP deduce/VBP a/DT tight/JJ generalization/NN error/NN bound/VBN for/IN OSGA/NN learning/NN ./.
Our/PRP$ results/NNS show/VBP that/IN in/IN the/DT realm/NN of/IN supervised/JJ learning/NN ,/, OSGA/NN provides/VBZ a/DT possibility/NN to/TO further/RB reduce/VB the/DT computational/JJ burden/NN of/IN OGA/NNP in/IN the/DT premise/NN of/IN maintaining/VBG its/PRP$ prominent/JJ generalization/NN capability/NN ./.
