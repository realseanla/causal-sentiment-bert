It/PRP has/VBZ been/VBN a/DT long/RB -/HYPH standing/VBG goal/NN in/IN machine/NN learning/NN ,/, as/RB well/RB as/IN in/IN AI/NN more/RBR generally/RB ,/, to/TO develop/VB life/NN -/HYPH long/JJ learning/NN systems/NNS that/WDT learn/VBP many/JJ different/JJ tasks/NNS over/IN time/NN ,/, and/CC reuse/VB insights/NNS from/IN tasks/NNS learned/VBD ,/, "/`` learning/VBG to/TO learn/VB "/`` as/IN they/PRP do/VBP so/RB ./.
In/IN this/DT work/NN we/PRP pose/VBP and/CC provide/VBP efficient/JJ algorithms/NNS for/IN several/JJ natural/JJ theoretical/JJ formulations/NNS of/IN this/DT goal/NN ./.
Specifically/RB ,/, we/PRP consider/VBP the/DT problem/NN of/IN learning/VBG many/JJ different/JJ target/NN functions/VBZ over/IN time/NN ,/, that/DT share/NN certain/JJ commonalities/NNS that/WDT are/VBP initially/RB unknown/JJ to/IN the/DT learning/NN algorithm/NN ./.
Our/PRP$ aim/NN is/VBZ to/TO learn/VB new/JJ internal/JJ representations/NNS as/IN the/DT algorithm/NN learns/VBZ new/JJ target/NN functions/NNS ,/, that/DT capture/NN this/DT commonality/NN and/CC allow/VB subsequent/JJ learning/NN tasks/NNS to/TO be/VB solved/VBN more/RBR efficiently/RB and/CC from/IN less/JJR data/NNS ./.
We/PRP develop/VBP efficient/JJ algorithms/NNS for/IN two/CD very/RB different/JJ kinds/NNS of/IN commonalities/NNS that/WDT target/VBP functions/NNS might/MD share/VB :/: one/CD based/VBN on/IN learning/VBG common/JJ low/JJ -/HYPH dimensional/JJ and/CC unions/NNS of/IN low/JJ -/HYPH dimensional/JJ subspaces/NNS and/CC one/CD based/VBN on/IN learning/VBG nonlinear/JJ Boolean/JJ combinations/NNS of/IN features/NNS ./.
Our/PRP$ algorithms/NNS for/IN learning/VBG Boolean/JJ feature/NN combinations/NNS additionally/RB have/VBP a/DT dual/JJ interpretation/NN ,/, and/CC can/MD be/VB viewed/VBN as/IN giving/VBG an/DT efficient/JJ procedure/NN for/IN constructing/VBG near/IN -/HYPH optimal/JJ sparse/JJ Boolean/NNP autoencoders/NNS under/IN a/DT natural/JJ "/`` anchor/NN -/HYPH set/NN "/'' assumption/NN ./.
