This/DT tutorial/NN gives/VBZ a/DT concise/JJ overview/NN of/IN existing/VBG PAC/NN -/HYPH Bayesian/JJ theory/NN focusing/VBG on/IN three/CD generalization/NN bounds/NNS ./.
The/DT first/JJ is/VBZ an/DT Occam/NNP bound/VBD which/WDT handles/VBZ rules/NNS with/IN finite/JJ precision/NN parameters/NNS and/CC which/WDT states/VBZ that/IN generalization/NN loss/NN is/VBZ near/JJ training/NN loss/NN when/WRB the/DT number/NN of/IN bits/NNS needed/VBN to/TO write/VB the/DT rule/NN is/VBZ small/JJ compared/VBN to/IN the/DT sample/NN size/NN ./.
The/DT second/JJ is/VBZ a/DT PAC/NN -/HYPH Bayesian/JJ bound/JJ providing/VBG a/DT generalization/NN guarantee/NN for/IN posterior/JJ distributions/NNS rather/RB than/IN for/IN individual/JJ rules/NNS ./.
The/DT PAC/NN -/HYPH Bayesian/JJ bound/JJ naturally/RB handles/VBZ infinite/JJ precision/NN rule/NN parameters/NNS ,/, $/$ L_2/CD $/$ regularization/CD ,/, {/-LRB- \/SYM em/PRP provides/VBZ a/DT bound/VBN for/IN dropout/NN training/NN }/-RRB- ,/, and/CC defines/VBZ a/DT natural/JJ notion/NN of/IN a/DT single/JJ distinguished/JJ PAC/NN -/HYPH Bayesian/JJ posterior/JJ distribution/NN ./.
The/DT third/JJ bound/VBN is/VBZ a/DT training/NN -/HYPH variance/NN bound/VBN ---/, a/DT kind/NN of/IN bias/NN -/HYPH variance/NN analysis/NN but/CC with/IN bias/NN replaced/VBN by/IN expected/VBN training/NN loss/NN ./.
The/DT training/NN -/HYPH variance/NN bound/VBN dominates/VBZ the/DT other/JJ bounds/NNS but/CC is/VBZ more/RBR difficult/JJ to/TO interpret/VB ./.
It/PRP seems/VBZ to/TO suggest/VB variance/NN reduction/NN methods/NNS such/JJ as/IN bagging/NN and/CC may/MD ultimately/RB provide/VB a/DT more/RBR meaningful/JJ analysis/NN of/IN dropouts/NNS ./.
