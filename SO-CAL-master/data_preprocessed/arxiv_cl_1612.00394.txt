Distributed/VBN representations/NNS of/IN words/NNS have/VBP been/VBN shown/VBN to/TO capture/VB lexical/JJ semantics/NNS ,/, as/IN demonstrated/VBN by/IN their/PRP$ effectiveness/NN in/IN word/NN similarity/NN and/CC analogical/JJ relation/NN tasks/NNS ./.
But/CC ,/, these/DT tasks/NNS only/RB evaluate/VBP lexical/JJ semantics/NNS indirectly/RB ./.
In/IN this/DT paper/NN ,/, we/PRP study/VBP whether/IN it/PRP is/VBZ possible/JJ to/TO utilize/VB distributed/VBN representations/NNS to/TO generate/VB dictionary/JJ definitions/NNS of/IN words/NNS ,/, as/IN a/DT more/RBR direct/JJ and/CC transparent/JJ representation/NN of/IN the/DT embeddings/NNS '/POS semantics/NNS ./.
We/PRP introduce/VBP definition/NN modeling/NN ,/, the/DT task/NN of/IN generating/VBG a/DT definition/NN for/IN a/DT given/VBN word/NN and/CC its/PRP$ embedding/NN ./.
We/PRP present/VBP several/JJ definition/NN model/NN architectures/NNS based/VBN on/IN recurrent/JJ neural/JJ networks/NNS ,/, and/CC experiment/NN with/IN the/DT models/NNS over/IN multiple/JJ data/NNS sets/NNS ./.
Our/PRP$ results/NNS show/VBP that/IN a/DT model/NN that/WDT controls/VBZ dependencies/NNS between/IN the/DT word/NN being/VBG defined/VBN and/CC the/DT definition/NN words/NNS performs/VBZ significantly/RB better/JJR ,/, and/CC that/IN a/DT character/NN -/HYPH level/NN convolution/NN layer/NN designed/VBN to/TO leverage/VB morphology/NN can/MD complement/VB word/NN -/HYPH level/NN embeddings/NNS ./.
Finally/RB ,/, an/DT error/NN analysis/NN suggests/VBZ that/IN the/DT errors/NNS made/VBN by/IN a/DT definition/NN model/NN may/MD provide/VB insight/NN into/IN the/DT shortcomings/NNS of/IN word/NN embeddings/NNS ./.
