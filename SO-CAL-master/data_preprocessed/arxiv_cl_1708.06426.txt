Sequence/NN -/HYPH to/IN -/HYPH sequence/NN (/-LRB- Seq2Seq/NN )/-RRB- models/NNS with/IN attention/NN have/VBP excelled/VBN at/IN tasks/NNS which/WDT involve/VBP generating/VBG natural/JJ language/NN sentences/NNS such/JJ as/IN machine/NN translation/NN ,/, image/NN captioning/NN and/CC speech/NN recognition/NN ./.
Performance/NN has/VBZ further/RB been/VBN improved/VBN by/IN leveraging/VBG unlabeled/JJ data/NNS ,/, often/RB in/IN the/DT form/NN of/IN a/DT language/NN model/NN ./.
In/IN this/DT work/NN ,/, we/PRP present/VBP the/DT Cold/NNP Fusion/NNP method/NN ,/, which/WDT leverages/VBZ a/DT pre-trained/JJ language/NN model/NN during/IN training/NN ,/, and/CC show/VB its/PRP$ effectiveness/NN on/IN the/DT speech/NN recognition/NN task/NN ./.
We/PRP show/VBP that/IN Seq2Seq/NN models/NNS with/IN Cold/NNP Fusion/NNP are/VBP able/JJ to/TO better/RBR utilize/VB language/NN information/NN enjoying/VBG i/LS )/-RRB- faster/RBR convergence/NN and/CC better/JJR generalization/NN ,/, and/CC ii/LS )/-RRB- almost/RB complete/JJ transfer/NN to/IN a/DT new/JJ domain/NN while/IN using/VBG less/JJR than/IN 10/CD percent/NN of/IN the/DT labeled/VBN training/NN data/NNS ./.
