Deep/JJ directed/VBN generative/NN models/NNS have/VBP attracted/VBN much/JJ attention/NN recently/RB due/IN to/IN their/PRP$ expressive/JJ representation/NN power/NN and/CC the/DT ability/NN of/IN ancestral/JJ sampling/NN ./.
One/CD major/JJ difficulty/NN of/IN learning/NN directed/VBN models/NNS with/IN many/JJ latent/JJ variables/NNS is/VBZ the/DT intractable/JJ inference/NN ./.
To/TO address/VB this/DT problem/NN ,/, most/JJS existing/VBG algorithms/NNS make/VBP assumptions/NNS to/TO render/VB the/DT latent/JJ variables/NNS independent/JJ of/IN each/DT other/JJ ,/, either/CC by/IN designing/VBG specific/JJ priors/NNS ,/, or/CC by/IN approximating/VBG the/DT true/JJ posterior/JJ using/VBG a/DT factorized/JJ distribution/NN ./.
We/PRP believe/VBP the/DT correlations/NNS among/IN latent/JJ variables/NNS are/VBP crucial/JJ for/IN faithful/JJ data/NNS representation/NN ./.
Driven/VBN by/IN this/DT idea/NN ,/, we/PRP propose/VBP an/DT inference/NN method/NN based/VBN on/IN the/DT conditional/JJ pseudo-likelihood/NN that/WDT preserves/VBZ the/DT dependencies/NNS among/IN the/DT latent/JJ variables/NNS ./.
For/IN learning/NN ,/, we/PRP propose/VBP to/IN employ/VB the/DT hard/JJ Expectation/NN Maximization/NN (/-LRB- EM/NN )/-RRB- algorithm/NN ,/, which/WDT avoids/VBZ the/DT intractability/NN of/IN the/DT traditional/JJ EM/NN by/IN max/NN -/HYPH out/NN instead/RB of/IN sum/NN -/HYPH out/NN to/IN compute/VB the/DT data/NNS likelihood/NN ./.
Qualitative/JJ and/CC quantitative/JJ evaluations/NNS of/IN our/PRP$ model/NN against/IN state/NN of/IN the/DT art/NN deep/JJ models/NNS on/IN benchmark/NN datasets/NNS demonstrate/VBP the/DT effectiveness/NN of/IN the/DT proposed/VBN algorithm/NN in/IN data/NNS representation/NN and/CC reconstruction/NN ./.
