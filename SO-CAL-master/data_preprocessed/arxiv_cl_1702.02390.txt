In/IN this/DT paper/NN we/PRP explore/VBP the/DT effect/NN of/IN architectural/JJ choices/NNS on/IN learning/VBG a/DT Variational/NNP Autoencoder/NNP (/-LRB- VAE/NNP )/-RRB- for/IN text/NN generation/NN ./.
In/IN contrast/NN to/IN the/DT previously/RB introduced/VBN VAE/NNP model/NN for/IN text/NN where/WRB both/CC the/DT encoder/NN and/CC decoder/NN are/VBP RNNs/NNS ,/, we/PRP propose/VBP a/DT novel/JJ hybrid/NN architecture/NN that/WDT blends/VBZ fully/RB feed/VB -/HYPH forward/RP convolutional/JJ and/CC deconvolutional/JJ components/NNS with/IN a/DT recurrent/JJ language/NN model/NN ./.
Our/PRP$ architecture/NN exhibits/VBZ several/JJ attractive/JJ properties/NNS such/JJ as/IN faster/RBR run/VBN time/NN and/CC convergence/NN ,/, ability/NN to/TO better/RBR handle/VB long/JJ sequences/NNS and/CC ,/, more/RBR importantly/RB ,/, it/PRP helps/VBZ to/TO avoid/VB some/DT of/IN the/DT major/JJ difficulties/NNS posed/VBN by/IN training/VBG VAE/NNP models/NNS on/IN textual/JJ data/NNS ./.
