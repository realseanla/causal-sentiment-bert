Neural/JJ Machine/NN Translation/NN (/-LRB- NMT/NN )/-RRB- ,/, like/IN many/JJ other/JJ deep/JJ learning/NN domains/NNS ,/, typically/RB suffers/VBZ from/IN over-parameterization/NN ,/, resulting/VBG in/IN large/JJ storage/NN sizes/NNS ./.
This/DT paper/NN examines/VBZ three/CD simple/JJ magnitude/NN -/HYPH based/VBN pruning/NN schemes/NNS to/IN compress/VB NMT/NN models/NNS ,/, namely/RB class/NN -/HYPH blind/JJ ,/, class/NN -/HYPH uniform/NN ,/, and/CC class/NN -/HYPH distribution/NN ,/, which/WDT differ/VBP in/IN terms/NNS of/IN how/WRB pruning/NN thresholds/NNS are/VBP computed/VBN for/IN the/DT different/JJ classes/NNS of/IN weights/NNS in/IN the/DT NMT/NNP architecture/NN ./.
We/PRP demonstrate/VBP the/DT efficacy/NN of/IN weight/NN pruning/NN as/IN a/DT compression/NN technique/NN for/IN a/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN NMT/NN system/NN ./.
We/PRP show/VBP that/IN an/DT NMT/NN model/NN with/IN over/IN 200/CD million/CD parameters/NNS can/MD be/VB pruned/VBN by/IN 40/CD percent/NN with/IN very/RB little/JJ performance/NN loss/NN as/IN measured/VBN on/IN the/DT WMT/NNP '14/CD English/NNP -/HYPH German/NNP translation/NN task/NN ./.
This/DT sheds/VBZ light/NN on/IN the/DT distribution/NN of/IN redundancy/NN in/IN the/DT NMT/NNP architecture/NN ./.
Our/PRP$ main/JJ result/NN is/VBZ that/IN with/IN retraining/VBG ,/, we/PRP can/MD recover/VB and/CC even/RB surpass/VB the/DT original/JJ performance/NN with/IN an/DT 80/CD percent/NN -/HYPH pruned/VBN model/NN ./.
