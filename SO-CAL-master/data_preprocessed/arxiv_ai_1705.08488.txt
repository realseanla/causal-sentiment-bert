We/PRP introduce/VBP second/RB -/HYPH order/NN vector/NN representations/NNS of/IN words/NNS ,/, induced/VBN from/IN nearest/JJS neighborhood/NN topological/JJ features/NNS in/IN pre-trained/JJ contextual/JJ word/NN embeddings/NNS ./.
We/PRP then/RB analyze/VB the/DT effects/NNS of/IN using/VBG second/JJ -/HYPH order/NN embeddings/NNS as/IN input/NN features/NNS in/IN two/CD deep/JJ natural/JJ language/NN processing/NN models/NNS ,/, for/IN named/VBN entity/NN recognition/NN and/CC recognizing/VBG textual/JJ entailment/NN ,/, as/RB well/RB as/IN a/DT linear/JJ model/NN for/IN paraphrase/NN recognition/NN ./.
Surprisingly/RB ,/, we/PRP find/VBP that/IN nearest/JJS neighbor/NN information/NN alone/RB is/VBZ sufficient/JJ to/TO capture/VB most/JJS of/IN the/DT performance/NN benefits/NNS derived/VBN from/IN using/VBG pre-trained/JJ word/NN embeddings/NNS ./.
Furthermore/RB ,/, second/JJ -/HYPH order/NN embeddings/NNS are/VBP able/JJ to/TO handle/VB highly/RB heterogeneous/JJ data/NNS better/JJR than/IN first/JJ -/HYPH order/NN representations/NNS ,/, though/RB at/IN the/DT cost/NN of/IN some/DT specificity/NN ./.
Additionally/RB ,/, augmenting/VBG contextual/JJ embeddings/NNS with/IN second/JJ -/HYPH order/NN information/NN further/RB improves/VBZ model/NN performance/NN in/IN some/DT cases/NNS ./.
Due/IN to/IN variance/NN in/IN the/DT random/JJ initializations/NNS of/IN word/NN embeddings/NNS ,/, utilizing/VBG nearest/JJS neighbor/NN features/NNS from/IN multiple/JJ first/JJ -/HYPH order/NN embedding/NN samples/NNS can/MD also/RB contribute/VB to/IN downstream/JJ performance/NN gains/NNS ./.
Finally/RB ,/, we/PRP identify/VBP intriguing/JJ characteristics/NNS of/IN second/JJ -/HYPH order/NN embedding/NN spaces/NNS for/IN further/JJ research/NN ,/, including/VBG much/RB higher/JJR density/NN and/CC different/JJ semantic/JJ interpretations/NNS of/IN cosine/NN similarity/NN ./.
