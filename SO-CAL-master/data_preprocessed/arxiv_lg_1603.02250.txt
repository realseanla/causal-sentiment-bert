We/PRP consider/VBP the/DT online/JJ sparse/JJ linear/JJ regression/NN problem/NN ,/, which/WDT is/VBZ the/DT problem/NN of/IN sequentially/RB making/VBG predictions/NNS observing/VBG only/RB a/DT limited/JJ number/NN of/IN features/NNS in/IN each/DT round/NN ,/, to/TO minimize/VB regret/NN with/IN respect/NN to/IN the/DT best/JJS sparse/JJ linear/JJ regressor/NN ,/, where/WRB prediction/NN accuracy/NN is/VBZ measured/VBN by/IN square/JJ loss/NN ./.
We/PRP give/VBP an/DT inefficient/JJ algorithm/NN that/WDT obtains/VBZ regret/NN bounded/VBN by/IN $/$ \/SYM tilde/NN {/-LRB- O/NN }/-RRB- (/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- )/-RRB- $/$ after/IN $/$ T$/CD prediction/NN rounds/NNS ./.
We/PRP complement/VBP this/DT result/NN by/IN showing/VBG that/IN no/DT algorithm/NN running/VBG in/IN polynomial/JJ time/NN per/IN iteration/NN can/MD achieve/VB regret/NN bounded/VBN by/IN $/$ O/UH (/-LRB- T/NN ^/SYM {/-LRB- 1/CD -/SYM \/SYM delta/NN }/-RRB- )/-RRB- $/$ for/IN any/DT constant/JJ $/$ \/SYM delta/NN &gt;/SYM 0/CD $/$ unless/IN $/$ \/CD text/NN {/-LRB- NP/NNP }/-RRB- \/SYM subseteq/NN \/SYM text/NN {/-LRB- BPP/NN }/-RRB- $/$ ./.
This/DT computational/JJ hardness/NN result/VBP resolves/VBZ an/DT open/JJ problem/NN presented/VBN in/IN COLT/NN 2014/CD (/-LRB- Kale/NNP ,/, 2014/CD )/-RRB- and/CC also/RB posed/VBN by/IN Zolghadr/NNP et/FW al./FW (/-LRB- 2013/CD )/-RRB- ./.
This/DT hardness/NN result/NN holds/VBZ even/RB if/IN the/DT algorithm/NN is/VBZ allowed/VBN to/TO access/VB more/JJR features/NNS than/IN the/DT best/JJS sparse/JJ linear/JJ regressor/NN up/IN to/IN a/DT logarithmic/JJ factor/NN in/IN the/DT dimension/NN ./.
