Convolutional/JJ neural/JJ networks/NNS (/-LRB- CNNs/NNS )/-RRB- are/VBP currently/RB state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN for/IN various/JJ classification/NN tasks/NNS ,/, but/CC are/VBP computationally/RB expensive/JJ ./.
Propagating/VBG through/IN the/DT convolutional/JJ layers/NNS is/VBZ very/RB slow/JJ ,/, as/IN each/DT kernel/NN in/IN each/DT layer/NN must/MD sequentially/RB calculate/VB many/JJ dot/NN products/NNS for/IN a/DT single/JJ forward/NN and/CC backward/JJ propagation/NN which/WDT equates/VBZ to/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- N/NN ^/SYM {/-LRB- 2/CD }/-RRB- n/NN ^/SYM {/-LRB- 2/CD }/-RRB- )/-RRB- $/$ per/IN kernel/NN per/IN layer/NN where/WRB the/DT inputs/NNS are/VBP $/$ N/CD \/SYM times/NNS N$/NN arrays/NNS and/CC the/DT kernels/NNS are/VBP $/$ n/NN \/NN times/NNS n/NN $/$ arrays/NNS ./.
Convolution/NNP can/MD be/VB efficiently/RB performed/VBN as/IN a/DT Hadamard/NNP product/NN in/IN the/DT frequency/NN domain/NN ./.
The/DT bottleneck/NN is/VBZ the/DT transformation/NN which/WDT has/VBZ a/DT cost/NN of/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- N/NN ^/SYM {/-LRB- 2/CD }/-RRB- \/SYM log_2/CD N/NN )/-RRB- $/$ using/VBG the/DT fast/JJ Fourier/NNP transform/VB (/-LRB- FFT/NN )/-RRB- ./.
However/RB ,/, the/DT increase/NN in/IN efficiency/NN is/VBZ less/RBR significant/JJ when/WRB $/$ N/CD \/SYM gg/NNP n/NN $/$ as/RB is/VBZ the/DT case/NN in/IN CNNs/NNS ./.
We/PRP mitigate/VBP this/DT by/IN using/VBG the/DT "/`` overlap/NN -/HYPH and/CC -/HYPH add/VB "/`` technique/NN reducing/VBG the/DT computational/JJ complexity/NN to/IN $/$ \/CD mathcal/NN {/-LRB- O/NN }/-RRB- (/-LRB- N/NN ^/SYM 2/CD \/SYM log_2/CD n/NN )/-RRB- $/$ per/IN kernel/NN ./.
This/DT method/NN increases/VBZ the/DT algorithm/NN 's/POS efficiency/NN in/IN both/CC the/DT forward/JJ and/CC backward/JJ propagation/NN ,/, reducing/VBG the/DT training/NN and/CC testing/NN time/NN for/IN CNNs/NNS ./.
Our/PRP$ empirical/JJ results/NNS show/VBP our/PRP$ method/NN reduces/VBZ computational/JJ time/NN by/IN a/DT factor/NN of/IN up/RB to/IN 16.3/CD times/NNS the/DT traditional/JJ convolution/NN implementation/NN for/IN a/DT 8/CD $/$ \/CD times/NNS $/$ 8/CD kernel/NN and/CC a/DT 224/CD $/$ \/CD times/NNS $/$ 224/CD image/NN ./.
