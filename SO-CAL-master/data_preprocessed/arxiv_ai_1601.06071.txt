Based/VBN on/IN the/DT assumption/NN that/IN there/EX exists/VBZ a/DT neural/JJ network/NN that/WDT efficiently/RB represents/VBZ a/DT set/NN of/IN Boolean/JJ functions/NNS between/IN all/DT binary/JJ inputs/NNS and/CC outputs/NNS ,/, we/PRP propose/VBP a/DT process/NN for/IN developing/VBG and/CC deploying/VBG neural/JJ networks/NNS whose/WP$ weight/NN parameters/NNS ,/, bias/NN terms/NNS ,/, input/NN ,/, and/CC intermediate/JJ hidden/JJ layer/NN output/NN signals/NNS ,/, are/VBP all/DT binary/JJ -/HYPH valued/VBN ,/, and/CC require/VBP only/RB basic/JJ bit/NN logic/NN for/IN the/DT feedforward/NN pass/NN ./.
The/DT proposed/VBN Bitwise/NNP Neural/JJ Network/NN (/-LRB- BNN/NN )/-RRB- is/VBZ especially/RB suitable/JJ for/IN resource/NN -/HYPH constrained/VBN environments/NNS ,/, since/IN it/PRP replaces/VBZ either/CC floating/VBG or/CC fixed/VBN -/HYPH point/NN arithmetic/NN with/IN significantly/RB more/RBR efficient/JJ bitwise/JJ operations/NNS ./.
Hence/RB ,/, the/DT BNN/NNP requires/VBZ for/IN less/JJR spatial/JJ complexity/NN ,/, less/JJR memory/NN bandwidth/NN ,/, and/CC less/JJR power/NN consumption/NN in/IN hardware/NN ./.
In/IN order/NN to/TO design/VB such/JJ networks/NNS ,/, we/PRP propose/VBP to/TO add/VB a/DT few/JJ training/NN schemes/NNS ,/, such/JJ as/IN weight/NN compression/NN and/CC noisy/JJ backpropagation/NN ,/, which/WDT result/VBP in/IN a/DT bitwise/JJ network/NN that/WDT performs/VBZ almost/RB as/RB well/RB as/IN its/PRP$ corresponding/VBG real/JJ -/HYPH valued/VBN network/NN ./.
We/PRP test/VBP the/DT proposed/VBN network/NN on/IN the/DT MNIST/NN dataset/NN ,/, represented/VBN using/VBG binary/JJ features/NNS ,/, and/CC show/VBP that/IN BNNs/NNS result/VBP in/IN competitive/JJ performance/NN while/IN offering/VBG dramatic/JJ computational/JJ savings/NNS ./.
