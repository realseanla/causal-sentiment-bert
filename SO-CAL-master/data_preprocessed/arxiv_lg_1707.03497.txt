This/DT paper/NN proposes/VBZ a/DT novel/JJ deep/JJ reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- architecture/NN ,/, called/VBN Value/NNP Prediction/NNP Network/NNP (/-LRB- VPN/NNP )/-RRB- ,/, which/WDT integrates/VBZ model/NN -/HYPH free/JJ and/CC model/NN -/HYPH based/VBN RL/NN methods/NNS into/IN a/DT single/JJ neural/JJ network/NN ./.
In/IN contrast/NN to/IN typical/JJ model/NN -/HYPH based/VBN RL/NN methods/NNS ,/, VPN/NNP learns/VBZ a/DT dynamics/NNS model/NN whose/WP$ abstract/JJ states/NNS are/VBP trained/VBN to/TO make/VB option/NN -/HYPH conditional/JJ predictions/NNS of/IN future/NN values/NNS (/-LRB- discounted/VBN sum/NN of/IN rewards/NNS )/-RRB- rather/RB than/IN of/IN future/JJ observations/NNS ./.
Our/PRP$ experimental/JJ results/NNS show/VBP that/IN VPN/NNP has/VBZ several/JJ advantages/NNS over/IN both/DT model/NN -/HYPH free/JJ and/CC model/NN -/HYPH based/VBN baselines/NNS in/IN a/DT stochastic/JJ environment/NN where/WRB careful/JJ planning/NN is/VBZ required/VBN but/CC building/VBG an/DT accurate/JJ observation/NN -/HYPH prediction/NN model/NN is/VBZ difficult/JJ ./.
Furthermore/RB ,/, VPN/NNP outperforms/VBZ Deep/JJ Q/NN -/HYPH Network/NN (/-LRB- DQN/NN )/-RRB- on/IN several/JJ Atari/NNP games/NNS even/RB with/IN short/JJ -/HYPH lookahead/NN planning/NN ,/, demonstrating/VBG its/PRP$ potential/NN as/IN a/DT new/JJ way/NN of/IN learning/VBG a/DT good/JJ state/NN representation/NN ./.
