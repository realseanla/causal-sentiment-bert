We/PRP show/VBP that/IN natural/JJ classes/NNS of/IN regularized/VBN learning/NN algorithms/NNS with/IN a/DT form/NN of/IN recency/NN bias/NN achieve/VBP faster/RBR convergence/NN rates/NNS to/TO approximate/VB efficiency/NN and/CC to/IN correlated/VBN equilibria/NNS in/IN multiplayer/NN normal/JJ form/NN games/NNS ./.
When/WRB each/DT player/NN in/IN a/DT game/NN uses/VBZ an/DT algorithm/NN from/IN our/PRP$ class/NN ,/, their/PRP$ individual/JJ regret/NN decays/VBZ at/IN $/$ O/UH (/-LRB- T/NN ^/SYM {/-LRB- -/HYPH 3/4/CD }/-RRB- )/-RRB- $/$ ,/, while/IN the/DT sum/NN of/IN utilities/NNS converges/VBZ to/IN an/DT approximate/JJ optimum/JJ at/IN $/$ O/UH (/-LRB- T/NN ^/SYM {/-LRB- -/HYPH 1/CD }/-RRB- )/-RRB- $/$ --/: an/DT improvement/NN upon/IN the/DT worst/JJS case/NN $/$ O/UH (/-LRB- T/NN ^/SYM {/-LRB- -/HYPH 1/2/CD }/-RRB- )/-RRB- $/$ rates/NNS ./.
We/PRP show/VBP a/DT black/JJ -/HYPH box/NN reduction/NN for/IN any/DT algorithm/NN in/IN the/DT class/NN to/TO achieve/VB $/$ O/UH (/-LRB- T/NN ^/SYM {/-LRB- -/HYPH 1/2/CD }/-RRB- )/-RRB- $/$ rates/NNS against/IN an/DT adversary/NN ,/, while/IN maintaining/VBG the/DT faster/RBR rates/NNS against/IN algorithms/NNS in/IN the/DT class/NN ./.
Our/PRP$ results/NNS extend/VBP those/DT of/IN [/-LRB- Rakhlin/NNP and/CC Shridharan/NNP 2013/CD ]/-RRB- and/CC [/-LRB- Daskalakis/NNP et/FW al./FW 2014/CD ]/-RRB- ,/, who/WP only/RB analyzed/VBD two/CD -/HYPH player/NN zero/CD -/HYPH sum/NN games/NNS for/IN specific/JJ algorithms/NNS ./.
