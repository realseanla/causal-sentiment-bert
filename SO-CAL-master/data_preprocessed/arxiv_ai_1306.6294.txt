We/PRP consider/VBP the/DT problem/NN of/IN learning/VBG good/JJ trajectories/NNS for/IN manipulation/NN tasks/NNS ./.
This/DT is/VBZ challenging/JJ because/IN the/DT criterion/NN defining/VBG a/DT good/JJ trajectory/NN varies/VBZ with/IN users/NNS ,/, tasks/NNS and/CC environments/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT co-active/JJ online/JJ learning/NN framework/NN for/IN teaching/VBG robots/NNS the/DT preferences/NNS of/IN its/PRP$ users/NNS for/IN object/NN manipulation/NN tasks/NNS ./.
The/DT key/JJ novelty/NN of/IN our/PRP$ approach/NN lies/VBZ in/IN the/DT type/NN of/IN feedback/NN expected/VBN from/IN the/DT user/NN :/: the/DT human/JJ user/NN does/VBZ not/RB need/VB to/TO demonstrate/VB optimal/JJ trajectories/NNS as/IN training/NN data/NNS ,/, but/CC merely/RB needs/VBZ to/TO iteratively/RB provide/VB trajectories/NNS that/WDT slightly/RB improve/VBP over/IN the/DT trajectory/NN currently/RB proposed/VBN by/IN the/DT system/NN ./.
We/PRP argue/VBP that/IN this/DT co-active/JJ preference/NN feedback/NN can/MD be/VB more/RBR easily/RB elicited/VBN from/IN the/DT user/NN than/IN demonstrations/NNS of/IN optimal/JJ trajectories/NNS ,/, while/IN ,/, nevertheless/RB ,/, theoretical/JJ regret/NN bounds/NNS of/IN our/PRP$ algorithm/NN match/NN the/DT asymptotic/JJ rates/NNS of/IN optimal/JJ trajectory/NN algorithms/NNS ./.
We/PRP demonstrate/VBP the/DT generalization/NN ability/NN of/IN our/PRP$ algorithm/NN on/IN a/DT variety/NN of/IN tasks/NNS ,/, for/IN whom/WP ,/, the/DT preferences/NNS were/VBD not/RB only/RB influenced/VBN by/IN the/DT object/NN being/VBG manipulated/VBN but/CC also/RB by/IN the/DT surrounding/VBG environment/NN ./.
