In/IN spite/NN of/IN their/PRP$ superior/JJ performance/NN ,/, neural/JJ probabilistic/JJ language/NN models/NNS (/-LRB- NPLMs/NNS )/-RRB- remain/VBP far/RB less/RBR widely/RB used/VBN than/IN n/NN -/HYPH gram/NN models/NNS due/IN to/IN their/PRP$ notoriously/RB long/JJ training/NN times/NNS ,/, which/WDT are/VBP measured/VBN in/IN weeks/NNS even/RB for/IN moderately/RB -/HYPH sized/JJ datasets/NNS ./.
Training/VBG NPLMs/NNS is/VBZ computationally/RB expensive/JJ because/IN they/PRP are/VBP explicitly/RB normalized/VBN ,/, which/WDT leads/VBZ to/IN having/VBG to/TO consider/VB all/DT words/NNS in/IN the/DT vocabulary/NN when/WRB computing/VBG the/DT log/NN -/HYPH likelihood/NN gradients/NNS ./.
