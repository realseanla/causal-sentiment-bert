We/PRP study/VBP the/DT problem/NN of/IN using/VBG causal/JJ models/NNS to/TO improve/VB the/DT rate/NN at/IN which/WDT good/JJ interventions/NNS can/MD be/VB learned/VBN online/RB in/IN a/DT stochastic/JJ environment/NN ./.
Our/PRP$ formalism/NN combines/VBZ multi-arm/JJ bandits/NNS and/CC causal/JJ inference/NN to/TO model/VB a/DT novel/JJ type/NN of/IN bandit/NN feedback/NN that/WDT is/VBZ not/RB exploited/VBN by/IN existing/VBG approaches/NNS ./.
We/PRP propose/VBP a/DT new/JJ algorithm/NN that/WDT exploits/VBZ the/DT causal/JJ feedback/NN and/CC prove/VB a/DT bound/VBN on/IN its/PRP$ simple/JJ regret/NN that/WDT is/VBZ strictly/RB better/JJR (/-LRB- in/IN all/DT quantities/NNS )/-RRB- than/IN algorithms/NNS that/WDT do/VBP not/RB use/VB the/DT additional/JJ causal/JJ information/NN ./.
