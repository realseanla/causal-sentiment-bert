While/IN neural/JJ networks/NNS have/VBP been/VBN successfully/RB applied/VBN to/IN many/JJ NLP/NN tasks/NNS the/DT resulting/VBG vector/NN -/HYPH based/VBN models/NNS are/VBP very/RB difficult/JJ to/TO interpret/VB ./.
For/IN example/NN it/PRP 's/VBZ not/RB clear/JJ how/WRB they/PRP achieve/VBP {/-LRB- \/SYM em/PRP compositionality/NN }/-RRB- ,/, building/VBG sentence/NN meaning/VBG from/IN the/DT meanings/NNS of/IN words/NNS and/CC phrases/NNS ./.
In/IN this/DT paper/NN we/PRP describe/VBP four/CD strategies/NNS for/IN visualizing/VBG compositionality/NN in/IN neural/JJ models/NNS for/IN NLP/NN ,/, inspired/VBN by/IN similar/JJ work/NN in/IN computer/NN vision/NN ./.
We/PRP first/RB plot/VB unit/NN values/NNS to/TO visualize/VB compositionality/NN of/IN negation/NN ,/, intensification/NN ,/, and/CC concessive/JJ clauses/NNS ,/, allow/VB us/PRP to/TO see/VB well/RB -/HYPH known/VBN markedness/NN asymmetries/NNS in/IN negation/NN ./.
We/PRP then/RB introduce/VBP three/CD simple/JJ and/CC straightforward/JJ methods/NNS for/IN visualizing/VBG a/DT unit/NN 's/POS {/-LRB- \/SYM em/PRP salience/NN }/-RRB- ,/, the/DT amount/NN it/PRP contributes/VBZ to/IN the/DT final/JJ composed/VBN meaning/NN :/: (/-LRB- 1/LS )/-RRB- gradient/NN back/RB -/HYPH propagation/NN ,/, (/-LRB- 2/LS )/-RRB- the/DT variance/NN of/IN a/DT token/NN from/IN the/DT average/JJ word/NN node/NN ,/, (/-LRB- 3/LS )/-RRB- LSTM/NN -/HYPH style/NN gates/NNS that/WDT measure/VBP information/NN flow/NN ./.
We/PRP test/VBP our/PRP$ methods/NNS on/IN sentiment/NN using/VBG simple/JJ recurrent/JJ nets/NNS and/CC LSTMs/NNS ./.
Our/PRP$ general/JJ -/HYPH purpose/NN methods/NNS may/MD have/VB wide/JJ applications/NNS for/IN understanding/VBG compositionality/NN and/CC other/JJ semantic/JJ properties/NNS of/IN deep/JJ networks/NNS ,/, and/CC also/RB shed/VBD light/NN on/IN why/WRB LSTMs/NNS outperform/VBP simple/JJ recurrent/JJ nets/NNS ,/,
