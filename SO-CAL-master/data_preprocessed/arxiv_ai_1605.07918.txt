Previous/JJ studies/NNS in/IN Open/NNP Information/NNP Extraction/NN (/-LRB- Open/NNP IE/NNP )/-RRB- are/VBP mainly/RB based/VBN on/IN extraction/NN patterns/NNS ./.
They/PRP manually/RB define/VBP patterns/NNS or/CC automatically/RB learn/VB them/PRP from/IN a/DT large/JJ corpus/NN ./.
However/RB ,/, these/DT approaches/NNS are/VBP limited/VBN when/WRB grasping/VBG the/DT context/NN of/IN a/DT sentence/NN ,/, and/CC they/PRP fail/VBP to/TO capture/VB implicit/JJ relations/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP address/VBP this/DT problem/NN with/IN the/DT following/VBG methods/NNS ./.
First/RB ,/, we/PRP exploit/VBP long/RB short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- networks/NNS to/TO extract/VB higher/JJR -/HYPH level/NN features/NNS along/IN the/DT shortest/JJS dependency/NN paths/NNS ,/, connecting/VBG headwords/NNS of/IN relations/NNS and/CC arguments/NNS ./.
The/DT path/NN -/HYPH level/NN features/NNS from/IN LSTM/NNP networks/NNS provide/VBP useful/JJ clues/NNS regarding/VBG contextual/JJ information/NN and/CC the/DT validity/NN of/IN arguments/NNS ./.
Second/RB ,/, we/PRP constructed/VBD samples/NNS to/TO train/VB LSTM/NNP networks/NNS without/IN the/DT need/NN for/IN manual/JJ labeling/NN ./.
In/IN particular/JJ ,/, feedback/NN negative/JJ sampling/NN picks/NNS highly/RB negative/JJ samples/NNS among/IN non-positive/JJ samples/NNS through/IN a/DT model/NN trained/VBN with/IN positive/JJ samples/NNS ./.
The/DT experimental/JJ results/NNS show/VBP that/IN our/PRP$ approach/NN produces/VBZ more/RBR precise/JJ and/CC abundant/JJ extractions/NNS than/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN open/JJ IE/NNP systems/NNS ./.
To/IN the/DT best/JJS of/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ work/NN to/TO apply/VB deep/JJ learning/NN to/IN Open/NNP IE/NNP ./.
