We/PRP study/VBP the/DT learning/NN algorithm/NN corresponding/VBG to/IN the/DT incremental/JJ gradient/NN descent/NN defined/VBN by/IN the/DT empirical/JJ risk/NN over/IN an/DT infinite/JJ dimensional/JJ hypotheses/NNS space/NN ./.
We/PRP consider/VBP a/DT statistical/JJ learning/NN setting/NN and/CC show/VBP that/IN ,/, provided/VBN with/IN a/DT universal/JJ step/NN -/HYPH size/NN and/CC a/DT suitable/JJ early/JJ stopping/VBG rule/NN ,/, the/DT learning/NN algorithm/NN thus/RB obtained/VBN is/VBZ universally/RB consistent/JJ and/CC derive/VBP finite/JJ sample/NN bounds/NNS ./.
Our/PRP$ results/NNS provide/VBP a/DT theoretical/JJ foundation/NN for/IN considering/VBG early/JJ stopping/VBG in/IN online/JJ learning/NN algorithms/NNS and/CC shed/VBD light/NN on/IN the/DT effect/NN of/IN allowing/VBG for/IN multiple/JJ passes/NNS over/IN the/DT data/NNS ./.
