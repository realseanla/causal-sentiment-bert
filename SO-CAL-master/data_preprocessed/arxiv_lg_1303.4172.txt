This/DT manuscript/NN shows/VBZ that/IN AdaBoost/NNP and/CC its/PRP$ immediate/JJ variants/NNS can/MD produce/VB approximate/JJ maximum/JJ margin/NN classifiers/NNS simply/RB by/IN scaling/VBG step/NN size/NN choices/NNS with/IN a/DT fixed/VBN small/JJ constant/JJ ./.
In/IN this/DT way/NN ,/, when/WRB the/DT unscaled/JJ step/NN size/NN is/VBZ an/DT optimal/JJ choice/NN ,/, these/DT results/NNS provide/VBP guarantees/NNS for/IN Friedman/NNP 's/POS empirically/RB successful/JJ "/`` shrinkage/NN "/'' procedure/NN for/IN gradient/NN boosting/VBG (/-LRB- Friedman/NNP ,/, 2000/CD )/-RRB- ./.
Guarantees/NNS are/VBP also/RB provided/VBN for/IN a/DT variety/NN of/IN other/JJ step/NN sizes/NNS ,/, affirming/VBG the/DT intuition/NN that/WDT increasingly/RB regularized/VBN line/NN searches/NNS provide/VBP improved/VBN margin/NN guarantees/NNS ./.
The/DT results/NNS hold/VBP for/IN the/DT exponential/JJ loss/NN and/CC similar/JJ losses/NNS ,/, most/RBS notably/RB the/DT logistic/JJ loss/NN ./.
