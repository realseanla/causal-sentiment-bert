The/DT popular/JJ Alternating/VBG Least/JJS Squares/NNS (/-LRB- ALS/NNP )/-RRB- algorithm/NN for/IN tensor/NN decomposition/NN is/VBZ extremely/RB efficient/JJ ,/, but/CC often/RB converges/VBZ to/IN poor/JJ local/JJ optima/NN ,/, particularly/RB when/WRB the/DT weights/NNS of/IN the/DT factors/NNS are/VBP non-uniform/JJ ./.
We/PRP propose/VBP a/DT modification/NN of/IN the/DT ALS/NNP approach/NN that/WDT is/VBZ as/IN efficient/JJ as/IN standard/JJ ALS/NNP ,/, but/CC provably/RB recovers/VBZ the/DT true/JJ factors/NNS with/IN random/JJ initialization/NN under/IN standard/JJ incoherence/NN assumptions/NNS on/IN the/DT factors/NNS of/IN the/DT tensor/NN ./.
We/PRP demonstrate/VBP the/DT significant/JJ practical/JJ superiority/NN of/IN our/PRP$ approach/NN over/IN traditional/JJ ALS/NN (/-LRB- with/IN both/DT random/JJ initialization/NN and/CC SVD/NN -/HYPH based/VBN initialization/NN )/-RRB- for/IN a/DT variety/NN of/IN tasks/NNS on/IN synthetic/JJ data/NNS -/, including/VBG tensor/NN factorization/NN on/IN exact/JJ ,/, noisy/JJ and/CC over-complete/JJ tensors/NNS ,/, as/RB well/RB as/IN tensor/NN completion/NN -/HYPH and/CC for/IN computing/VBG word/NN embeddings/NNS from/IN a/DT third/JJ -/HYPH order/NN word/NN tri-occurrence/NN tensor/NN ./.
