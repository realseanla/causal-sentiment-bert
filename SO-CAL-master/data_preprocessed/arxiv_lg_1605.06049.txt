The/DT question/NN of/IN how/WRB to/TO parallelize/VB the/DT stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- method/NN has/VBZ received/VBN much/JJ attention/NN in/IN the/DT literature/NN ./.
In/IN this/DT paper/NN ,/, we/PRP focus/VBP instead/RB on/IN batch/NN methods/NNS that/WDT use/VBP a/DT sizeable/JJ fraction/NN of/IN the/DT training/NN set/VBN at/IN each/DT iteration/NN to/TO facilitate/VB parallelism/NN ,/, and/CC that/DT employ/VB second/RB -/HYPH order/NN information/NN ./.
In/IN order/NN to/TO improve/VB the/DT learning/NN process/NN ,/, we/PRP follow/VBP a/DT multi-batch/JJ approach/NN in/IN which/WDT the/DT batch/NN changes/NNS at/IN each/DT iteration/NN ./.
This/DT inherently/RB gives/VBZ the/DT algorithm/NN a/DT stochastic/JJ flavor/NN that/WDT can/MD cause/VB instability/NN in/IN L/NN -/HYPH BFGS/NN ,/, a/DT popular/JJ batch/NN method/NN in/IN machine/NN learning/NN ./.
These/DT difficulties/NNS arise/VBP because/IN L/NN -/HYPH BFGS/NN employs/VBZ gradient/NN differences/NNS to/TO update/VB the/DT Hessian/JJ approximations/NNS ;/: when/WRB these/DT gradients/NNS are/VBP computed/VBN using/VBG different/JJ data/NNS points/VBZ the/DT process/NN can/MD be/VB unstable/JJ ./.
This/DT paper/NN shows/VBZ how/WRB to/TO perform/VB stable/JJ quasi-Newton/NN updating/VBG in/IN the/DT multi-batch/JJ setting/NN ,/, illustrates/VBZ the/DT behavior/NN of/IN the/DT algorithm/NN in/IN a/DT distributed/VBN computing/NN platform/NN ,/, and/CC studies/VBZ its/PRP$ convergence/NN properties/NNS for/IN both/CC the/DT convex/NN and/CC nonconvex/NN cases/NNS ./.
