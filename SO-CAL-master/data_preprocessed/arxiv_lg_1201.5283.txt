We/PRP consider/VBP the/DT non-smooth/JJ optimization/NN problems/NNS in/IN machine/NN learning/NN ,/, where/WRB both/PDT the/DT loss/NN function/NN and/CC the/DT regularizer/NN are/VBP non-smooth/JJ functions/NNS ./.
Previous/JJ studies/NNS on/IN efficient/JJ empirical/JJ loss/NN minimization/NN assume/VBP either/CC a/DT smooth/JJ loss/NN function/NN or/CC a/DT strongly/RB convex/JJ regularizer/NN ,/, making/VBG them/PRP unsuitable/JJ for/IN non-smooth/JJ optimization/NN ./.
We/PRP develop/VBP an/DT efficient/JJ method/NN for/IN a/DT family/NN of/IN non-smooth/JJ optimization/NN where/WRB the/DT dual/JJ form/NN of/IN the/DT loss/NN function/NN is/VBZ bilinear/NN in/IN primal/JJ and/CC dual/JJ variables/NNS ./.
We/PRP cast/VBD a/DT non-smooth/JJ optimization/NN problem/NN into/IN a/DT minimax/NN optimization/NN problem/NN ,/, and/CC develop/VB a/DT primal/JJ dual/JJ prox/NN method/NN that/WDT solves/VBZ the/DT minimax/NN optimization/NN problem/NN at/IN a/DT rate/NN of/IN $/$ O/UH (/-LRB- 1/CD //SYM T/NN )/-RRB- $/$ ,/, significantly/RB faster/JJR than/IN a/DT standard/JJ gradient/NN descent/NN method/NN (/-LRB- $/$ O/UH (/-LRB- 1/CD //SYM \/SYM sqrt/SYM {/-LRB- T/NN }/-RRB- )/-RRB- $/$ )/-RRB- ./.
Our/PRP$ empirical/JJ study/NN verifies/VBZ the/DT efficiency/NN of/IN the/DT proposed/JJ method/NN for/IN non-smooth/JJ optimization/NN by/IN comparing/VBG it/PRP to/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN first/JJ order/NN methods/NNS ./.
