Recently/RB ,/, dropout/NN has/VBZ seen/VBN increasing/VBG use/NN in/IN deep/JJ learning/NN ./.
For/IN deep/JJ convolutional/JJ neural/JJ networks/NNS ,/, dropout/NN is/VBZ known/VBN to/TO work/VB well/RB in/IN fully/RB -/HYPH connected/VBN layers/NNS ./.
However/RB ,/, its/PRP$ effect/NN in/IN pooling/VBG layers/NNS is/VBZ still/RB not/RB clear/JJ ./.
This/DT paper/NN demonstrates/VBZ that/IN max/NN -/HYPH pooling/VBG dropout/NN is/VBZ equivalent/JJ to/IN randomly/RB picking/VBG activation/NN based/VBN on/IN a/DT multinomial/JJ distribution/NN at/IN training/NN time/NN ./.
In/IN light/NN of/IN this/DT insight/NN ,/, we/PRP advocate/VBP employing/VBG our/PRP$ proposed/VBN probabilistic/JJ weighted/JJ pooling/VBG ,/, instead/RB of/IN commonly/RB used/VBN max/NN -/HYPH pooling/VBG ,/, to/TO act/VB as/IN model/NN averaging/VBG at/IN test/NN time/NN ./.
Empirical/JJ evidence/NN validates/VBZ the/DT superiority/NN of/IN probabilistic/JJ weighted/JJ pooling/VBG ./.
We/PRP also/RB compare/VBP max/NN -/HYPH pooling/VBG dropout/NN and/CC stochastic/JJ pooling/VBG ,/, both/DT of/IN which/WDT introduce/VBP stochasticity/NN based/VBN on/IN multinomial/JJ distributions/NNS at/IN pooling/VBG stage/NN ./.
