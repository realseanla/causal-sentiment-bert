We/PRP seek/VBP decision/NN rules/NNS for/IN prediction/NN -/HYPH time/NN cost/NN reduction/NN ,/, where/WRB complete/JJ data/NNS is/VBZ available/JJ for/IN training/NN ,/, but/CC during/IN prediction/NN -/HYPH time/NN ,/, each/DT feature/NN can/MD only/RB be/VB acquired/VBN for/IN an/DT additional/JJ cost/NN ./.
We/PRP propose/VBP a/DT novel/JJ random/JJ forest/NN algorithm/NN to/TO minimize/VB prediction/NN error/NN for/IN a/DT user/NN -/HYPH specified/VBN {/-LRB- \/SYM it/PRP average/JJ }/-RRB- feature/NN acquisition/NN budget/NN ./.
While/IN random/JJ forests/NNS yield/VBP strong/JJ generalization/NN performance/NN ,/, they/PRP do/VBP not/RB explicitly/RB account/VBP for/IN feature/NN costs/NNS and/CC furthermore/RB require/VBP low/JJ correlation/NN among/IN trees/NNS ,/, which/WDT amplifies/VBZ costs/NNS ./.
Our/PRP$ random/JJ forest/NN grows/VBZ trees/NNS with/IN low/JJ acquisition/NN cost/NN and/CC high/JJ strength/NN based/VBN on/IN greedy/JJ minimax/NN cost/NN -/HYPH weighted/VBN -/HYPH impurity/NN splits/NNS ./.
Theoretically/RB ,/, we/PRP establish/VBP near/IN -/HYPH optimal/JJ acquisition/NN cost/NN guarantees/NNS for/IN our/PRP$ algorithm/NN ./.
Empirically/RB ,/, on/IN a/DT number/NN of/IN benchmark/NN datasets/NNS we/PRP demonstrate/VBP superior/JJ accuracy/NN -/HYPH cost/NN curves/NNS against/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN prediction/NN -/HYPH time/NN algorithms/NNS ./.
