We/PRP present/VBP a/DT probabilistic/JJ language/NN model/NN for/IN time/NN -/HYPH stamped/VBN text/NN data/NNS which/WDT tracks/VBZ the/DT semantic/JJ evolution/NN of/IN individual/JJ words/NNS over/IN time/NN ./.
The/DT model/NN represents/VBZ words/NNS and/CC contexts/NNS by/IN latent/JJ trajectories/NNS in/IN an/DT embedding/NN space/NN ./.
At/IN each/DT moment/NN in/IN time/NN ,/, the/DT embedding/NN vectors/NNS are/VBP inferred/VBN from/IN a/DT probabilistic/JJ version/NN of/IN word2vec/NN [/-LRB- Mikolov/NNP ,/, 2013/CD ]/-RRB- ./.
These/DT embedding/NN vectors/NNS are/VBP connected/VBN in/IN time/NN through/IN a/DT latent/JJ diffusion/NN process/NN ./.
We/PRP describe/VBP two/CD scalable/JJ variational/JJ inference/NN algorithms/NNS ---/, skip/VB -/HYPH gram/NN smoothing/NN and/CC skip/VB -/HYPH gram/NN filtering/NN ---/: that/IN allow/VB us/PRP to/TO train/VB the/DT model/NN jointly/RB over/RB all/DT times/NNS ;/: thus/RB learning/VBG on/IN all/DT data/NNS while/IN simultaneously/RB allowing/VBG word/NN and/CC context/NN vectors/NNS to/IN drift/NN ./.
Experimental/JJ results/NNS on/IN three/CD different/JJ corpora/NNS demonstrate/VBP that/IN our/PRP$ dynamic/JJ model/NN infers/VBZ word/NN embedding/NN trajectories/NNS that/WDT are/VBP more/RBR interpretable/JJ and/CC lead/VB to/IN higher/JJR predictive/JJ likelihoods/NNS than/IN competing/VBG methods/NNS that/WDT are/VBP based/VBN on/IN static/NN models/NNS trained/VBN separately/RB on/IN time/NN slices/NNS ./.
