We/PRP study/VBP the/DT use/NN of/IN inverse/JJ reinforcement/NN learning/NN (/-LRB- IRL/NN )/-RRB- as/IN a/DT tool/NN for/IN the/DT recognition/NN of/IN agents/NNS '/POS behavior/NN on/IN the/DT basis/NN of/IN observation/NN of/IN their/PRP$ sequential/JJ decision/NN behavior/NN interacting/VBG with/IN the/DT environment/NN ./.
We/PRP model/VBP the/DT problem/NN faced/VBN by/IN the/DT agents/NNS as/IN a/DT Markov/NNP decision/NN process/NN (/-LRB- MDP/NN )/-RRB- and/CC model/NN the/DT observed/VBN behavior/NN of/IN the/DT agents/NNS in/IN terms/NNS of/IN degrees/NNS of/IN rationality/NN with/IN respect/NN to/IN optimal/JJ forward/JJ planning/NN for/IN the/DT MDP/NNP ./.
We/PRP use/VBP IRL/NN to/TO learn/VB reward/NN functions/NNS and/CC then/RB use/VB these/DT reward/NN functions/VBZ as/IN the/DT basis/NN for/IN clustering/NN or/CC classification/NN models/NNS ./.
Experimental/JJ studies/NNS with/IN GridWorld/NNP ,/, a/DT navigation/NN problem/NN ,/, and/CC the/DT secretary/NN problem/NN ,/, an/DT optimal/JJ stopping/NN problem/NN ,/, suggest/VBP reward/NN vectors/NNS found/VBN from/IN IRL/NN can/MD be/VB a/DT good/JJ basis/NN for/IN behavior/NN pattern/NN recognition/NN problems/NNS ./.
Empirical/JJ comparisons/NNS of/IN our/PRP$ method/NN with/IN several/JJ existing/VBG IRL/NN algorithms/NNS and/CC with/IN direct/JJ methods/NNS that/WDT use/VBP feature/NN statistics/NNS observed/VBN in/IN state/NN -/HYPH action/NN space/NN suggest/VBP it/PRP may/MD be/VB superior/JJ for/IN recognition/NN problems/NNS ./.
