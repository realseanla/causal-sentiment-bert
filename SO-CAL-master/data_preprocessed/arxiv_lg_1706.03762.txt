The/DT dominant/JJ sequence/NN transduction/NN models/NNS are/VBP based/VBN on/IN complex/JJ recurrent/JJ or/CC convolutional/JJ neural/JJ networks/NNS in/IN an/DT encoder/NN -/HYPH decoder/NN configuration/NN ./.
The/DT best/JJS performing/VBG models/NNS also/RB connect/VBP the/DT encoder/NN and/CC decoder/NN through/IN an/DT attention/NN mechanism/NN ./.
We/PRP propose/VBP a/DT new/JJ simple/JJ network/NN architecture/NN ,/, the/DT Transformer/NNP ,/, based/VBN solely/RB on/IN attention/NN mechanisms/NNS ,/, dispensing/VBG with/IN recurrence/NN and/CC convolutions/NNS entirely/RB ./.
Experiments/NNS on/IN two/CD machine/NN translation/NN tasks/NNS show/VBP these/DT models/NNS to/TO be/VB superior/JJ in/IN quality/NN while/IN being/VBG more/RBR parallelizable/JJ and/CC requiring/VBG significantly/RB less/JJR time/NN to/TO train/VB ./.
Our/PRP$ model/NN achieves/VBZ 28.4/CD BLEU/NN on/IN the/DT WMT/NN 2014/CD English/NNP -/HYPH to/IN -/HYPH German/JJ translation/NN task/NN ,/, improving/VBG over/IN the/DT existing/VBG best/JJS results/NNS ,/, including/VBG ensembles/NNS by/IN over/IN 2/CD BLEU/NN ./.
On/IN the/DT WMT/NN 2014/CD English/NNP -/HYPH to/IN -/HYPH French/JJ translation/NN task/NN ,/, our/PRP$ model/NN establishes/VBZ a/DT new/JJ single/JJ -/HYPH model/NN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN BLEU/NN score/NN of/IN 41.0/CD after/IN training/VBG for/IN 3.5/CD days/NNS on/IN eight/CD GPUs/NNS ,/, a/DT small/JJ fraction/NN of/IN the/DT training/NN costs/NNS of/IN the/DT best/JJS models/NNS from/IN the/DT literature/NN ./.
We/PRP show/VBP that/IN the/DT Transformer/NNP generalizes/VBZ well/RB to/IN other/JJ tasks/NNS by/IN applying/VBG it/PRP successfully/RB to/IN English/NNP constituency/NN parsing/VBG both/CC with/IN large/JJ and/CC limited/JJ training/NN data/NNS ./.
