Consider/VB the/DT binary/JJ classification/NN problem/NN of/IN predicting/VBG a/DT target/NN variable/JJ $/$ Y$/CD from/IN a/DT discrete/JJ feature/NN vector/NN $/$ X/CD =/SYM (/-LRB- X_1/NN ,/, .../NFP ,/, X_d/NN )/-RRB- $/$ ./.
When/WRB the/DT probability/NN distribution/NN $/$ \/CD mathbb/NN {/-LRB- P/NN }/-RRB- (/-LRB- X/NN ,/, Y/NN )/-RRB- $/$ is/VBZ known/VBN ,/, the/DT optimal/JJ classifier/NN ,/, leading/VBG to/IN the/DT minimum/JJ misclassification/NN rate/NN ,/, is/VBZ given/VBN by/IN the/DT Maximum/NNP A-posteriori/NNP Probability/NNP decision/NN rule/NN ./.
However/RB ,/, estimating/VBG the/DT complete/JJ joint/JJ distribution/NN $/$ \/CD mathbb/NN {/-LRB- P/NN }/-RRB- (/-LRB- X/NN ,/, Y/NN )/-RRB- $/$ is/VBZ computationally/RB and/CC statistically/RB impossible/JJ for/IN large/JJ values/NNS of/IN $/$ d/LS $/$ ./.
An/DT alternative/JJ approach/NN is/VBZ to/TO first/RB estimate/VB some/DT low/JJ order/NN marginals/NNS of/IN $/$ \/CD mathbb/NN {/-LRB- P/NN }/-RRB- (/-LRB- X/NN ,/, Y/NN )/-RRB- $/$ and/CC then/RB design/VB the/DT classifier/NN based/VBN on/IN the/DT estimated/VBN low/JJ order/NN marginals/NNS ./.
This/DT approach/NN is/VBZ also/RB helpful/JJ when/WRB the/DT complete/JJ training/NN data/NNS instances/NNS are/VBP not/RB available/JJ due/IN to/IN privacy/NN concerns/NNS ./.
In/IN this/DT work/NN ,/, we/PRP consider/VBP the/DT problem/NN of/IN finding/VBG the/DT optimum/JJ classifier/NN based/VBN on/IN some/DT estimated/VBN low/JJ order/NN marginals/NNS of/IN $/$ (/-LRB- X/NN ,/, Y/NN )/-RRB- $/$ ./.
We/PRP prove/VBP that/IN for/IN a/DT given/VBN set/NN of/IN marginals/NNS ,/, the/DT minimum/JJ Hirschfeld/NNP -/HYPH Gebelein/NNP -/HYPH Renyi/NNP (/-LRB- HGR/NNP )/-RRB- correlation/NN principle/NN introduced/VBN in/IN [/-LRB- 1/CD ]/-RRB- leads/VBZ to/IN a/DT randomized/JJ classification/NN rule/NN which/WDT is/VBZ shown/VBN to/TO have/VB a/DT misclassification/NN rate/NN no/DT larger/JJR than/IN twice/PDT the/DT misclassification/NN rate/NN of/IN the/DT optimal/JJ classifier/NN ./.
Then/RB ,/, under/IN a/DT separability/NN condition/NN ,/, we/PRP show/VBP that/IN the/DT proposed/VBN algorithm/NN is/VBZ equivalent/JJ to/IN a/DT randomized/JJ linear/JJ regression/NN approach/NN ./.
In/IN addition/NN ,/, this/DT method/NN naturally/RB results/VBZ in/IN a/DT robust/JJ feature/NN selection/NN method/NN selecting/VBG a/DT subset/NN of/IN features/NNS having/VBG the/DT maximum/JJ worst/JJS case/NN HGR/NNP correlation/NN with/IN the/DT target/NN variable/NN ./.
Our/PRP$ theoretical/JJ upper/JJ -/HYPH bound/JJ is/VBZ similar/JJ to/IN the/DT recent/JJ Discrete/NNP Chebyshev/NNP Classifier/NNP (/-LRB- DCC/NNP )/-RRB- approach/NN [/-LRB- 2/CD ]/-RRB- ,/, while/IN the/DT proposed/VBN algorithm/NN has/VBZ significant/JJ computational/JJ advantages/NNS since/IN it/PRP only/RB requires/VBZ solving/VBG a/DT least/JJS square/JJ optimization/NN problem/NN ./.
Finally/RB ,/, we/PRP numerically/RB compare/VBP our/PRP$ proposed/VBN algorithm/NN with/IN the/DT DCC/NNP classifier/NN and/CC show/VBP that/IN the/DT proposed/VBN algorithm/NN results/NNS in/IN better/JJR misclassification/NN rate/NN over/IN various/JJ datasets/NNS ./.
