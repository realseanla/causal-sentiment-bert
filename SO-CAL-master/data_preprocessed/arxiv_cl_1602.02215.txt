We/PRP present/VBP Submatrix-wise/JJ Vector/NNP Embedding/NNP Learner/NNP (/-LRB- Swivel/NNP )/-RRB- ,/, a/DT method/NN for/IN generating/VBG low/JJ -/HYPH dimensional/JJ feature/NN embeddings/NNS from/IN a/DT feature/NN co-occurrence/NN matrix/NN ./.
Swivel/NNP performs/VBZ approximate/JJ factorization/NN of/IN the/DT point-wise/JJ mutual/JJ information/NN matrix/NN via/IN stochastic/JJ gradient/NN descent/NN ./.
It/PRP uses/VBZ a/DT piecewise/JJ loss/NN with/IN special/JJ handling/NN for/IN unobserved/JJ co-occurrences/NNS ,/, and/CC thus/RB makes/VBZ use/NN of/IN all/PDT the/DT information/NN in/IN the/DT matrix/NN ./.
While/IN this/DT requires/VBZ computation/NN proportional/JJ to/IN the/DT size/NN of/IN the/DT entire/JJ matrix/NN ,/, we/PRP make/VBP use/NN of/IN vectorized/VBN multiplication/NN to/TO process/VB thousands/NNS of/IN rows/NNS and/CC columns/NNS at/IN once/RB to/IN compute/VB millions/NNS of/IN predicted/VBN values/NNS ./.
Furthermore/RB ,/, we/PRP partition/VBP the/DT matrix/NN into/IN shards/NNS in/IN order/NN to/TO parallelize/VB the/DT computation/NN across/IN many/JJ nodes/NNS ./.
This/DT approach/NN results/VBZ in/IN more/RBR accurate/JJ embeddings/NNS than/IN can/MD be/VB achieved/VBN with/IN methods/NNS that/WDT consider/VBP only/RB observed/VBN co-occurrences/NNS ,/, and/CC can/MD scale/VB to/IN much/RB larger/JJR corpora/NNS than/IN can/MD be/VB handled/VBN with/IN sampling/NN methods/NNS ./.
