In/IN most/JJS common/JJ settings/NNS of/IN Markov/NNP Decision/NN Process/NN (/-LRB- MDP/NN )/-RRB- ,/, an/DT agent/NN evaluate/VB a/DT policy/NN based/VBN on/IN expectation/NN of/IN (/-LRB- discounted/VBN )/-RRB- sum/NN of/IN rewards/NNS ./.
However/RB in/IN many/JJ applications/NNS this/DT criterion/NN might/MD not/RB be/VB suitable/JJ from/IN two/CD perspective/NN :/: first/RB ,/, in/IN risk/NN aversion/NN situation/NN expectation/NN of/IN accumulated/VBN rewards/NNS is/VBZ not/RB robust/JJ enough/RB ,/, this/DT is/VBZ the/DT case/NN when/WRB distribution/NN of/IN accumulated/VBN reward/NN is/VBZ heavily/RB skewed/VBN ;/: another/DT issue/NN is/VBZ that/IN many/JJ applications/NNS naturally/RB take/VBP several/JJ objective/NN into/IN consideration/NN when/WRB evaluating/VBG a/DT policy/NN ,/, for/IN instance/NN in/IN autonomous/JJ driving/VBG an/DT agent/NN needs/VBZ to/TO balance/VB speed/NN and/CC safety/NN when/WRB choosing/VBG appropriate/JJ decision/NN ./.
In/IN this/DT paper/NN ,/, we/PRP consider/VBP evaluating/VBG a/DT policy/NN based/VBN on/IN a/DT sequence/NN of/IN quantiles/NNS it/PRP induces/VBZ on/IN a/DT set/NN of/IN target/NN states/NNS ,/, our/PRP$ idea/NN is/VBZ to/TO reformulate/VB the/DT original/JJ problem/NN into/IN a/DT multi-objective/JJ MDP/NN problem/NN with/IN lexicographic/JJ preference/NN naturally/RB defined/VBN ./.
For/IN computation/NN of/IN finding/VBG an/DT optimal/JJ policy/NN ,/, we/PRP proposed/VBD an/DT algorithm/NN \/SYM textbf/NN {/-LRB- FLMDP/NN }/-RRB- that/WDT could/MD solve/VB general/JJ multi-objective/JJ MDP/NN with/IN lexicographic/JJ reward/NN preference/NN ./.
