Recurrent/JJ neural/JJ networks/NNS (/-LRB- RNNs/NNS )/-RRB- have/VBP achieved/VBN great/JJ success/NN in/IN language/NN modeling/NN ./.
However/RB ,/, since/IN the/DT RNNs/NNS have/VBP fixed/VBN size/NN of/IN memory/NN ,/, their/PRP$ memory/NN can/MD not/RB store/VB all/PDT the/DT information/NN about/IN the/DT words/NNS it/PRP have/VBP seen/VBN before/IN in/IN the/DT sentence/NN ,/, and/CC thus/RB the/DT useful/JJ long/JJ -/HYPH term/NN information/NN may/MD be/VB ignored/VBN when/WRB predicting/VBG the/DT next/JJ words/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP Attention/NN -/HYPH based/VBN Memory/NN Selection/NN Recurrent/NNP Network/NNP (/-LRB- AMSRN/NNP )/-RRB- ,/, in/IN which/WDT the/DT model/NN can/MD review/VB the/DT information/NN stored/VBN in/IN the/DT memory/NN at/IN each/DT previous/JJ time/NN step/NN and/CC select/VB the/DT relevant/JJ information/NN to/TO help/VB generate/VB the/DT outputs/NNS ./.
In/IN AMSRN/NNP ,/, the/DT attention/NN mechanism/NN finds/VBZ the/DT time/NN steps/NNS storing/VBG the/DT relevant/JJ information/NN in/IN the/DT memory/NN ,/, and/CC memory/NN selection/NN determines/VBZ which/WDT dimensions/NNS of/IN the/DT memory/NN are/VBP involved/VBN in/IN computing/VBG the/DT attention/NN weights/NNS and/CC from/IN which/WDT the/DT information/NN is/VBZ extracted.In/ADD the/DT experiments/NNS ,/, AMSRN/NNP outperformed/VBD long/RB short/JJ -/HYPH term/NN memory/NN (/-LRB- LSTM/NN )/-RRB- based/VBN language/NN models/NNS on/IN both/DT English/NNP and/CC Chinese/NNP corpora/NNS ./.
Moreover/RB ,/, we/PRP investigate/VBP using/VBG entropy/NN as/IN a/DT regularizer/NN for/IN attention/NN weights/NNS and/CC visualize/VB how/WRB the/DT attention/NN mechanism/NN helps/VBZ language/NN modeling/NN ./.
