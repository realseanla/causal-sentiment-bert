The/DT online/JJ learning/NN of/IN deep/JJ neural/JJ networks/NNS is/VBZ an/DT interesting/JJ problem/NN of/IN machine/NN learning/NN because/IN ,/, for/IN example/NN ,/, major/JJ IT/NN companies/NNS want/VBP to/TO manage/VB the/DT information/NN of/IN the/DT massive/JJ data/NNS uploaded/VBN on/IN the/DT web/NN daily/RB ,/, and/CC this/DT technology/NN can/MD contribute/VB to/IN the/DT next/JJ generation/NN of/IN lifelong/JJ learning/NN ./.
We/PRP aim/VBP to/TO train/VB deep/JJ models/NNS from/IN new/JJ data/NNS that/WDT consists/VBZ of/IN new/JJ classes/NNS ,/, distributions/NNS ,/, and/CC tasks/NNS at/IN minimal/JJ computational/JJ cost/NN ,/, which/WDT we/PRP call/VBP online/RB deep/JJ learning/NN ./.
Unfortunately/RB ,/, deep/JJ neural/JJ network/NN learning/VBG through/IN classical/JJ online/JJ and/CC incremental/JJ methods/NNS does/VBZ not/RB work/VB well/RB in/IN both/DT theory/NN and/CC practice/NN ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP dual/JJ memory/NN architectures/NNS for/IN online/JJ incremental/JJ deep/JJ learning/NN ./.
The/DT proposed/VBN architecture/NN consists/VBZ of/IN deep/JJ representation/NN learners/NNS and/CC fast/JJ learnable/JJ shallow/JJ kernel/NN networks/NNS ,/, both/DT of/IN which/WDT synergize/VBP to/TO track/VB the/DT information/NN of/IN new/JJ data/NNS ./.
During/IN the/DT training/NN phase/NN ,/, we/PRP use/VBP various/JJ online/RB ,/, incremental/JJ ensemble/NN ,/, and/CC transfer/NN learning/NN techniques/NNS in/IN order/NN to/TO achieve/VB lower/JJR error/NN of/IN the/DT architecture/NN ./.
On/IN the/DT MNIST/NNP ,/, CIFAR/NNP -/HYPH 10/CD ,/, and/CC ImageNet/NNP image/NN recognition/NN tasks/NNS ,/, the/DT proposed/VBN dual/JJ memory/NN architectures/NNS performs/VBZ much/RB better/JJR than/IN the/DT classical/JJ online/NN and/CC incremental/JJ ensemble/NN algorithm/NN ,/, and/CC their/PRP$ accuracies/NNS are/VBP similar/JJ to/IN that/DT of/IN the/DT batch/NN learner/NN ./.
