We/PRP propose/VBP a/DT scalable/JJ approach/NN to/TO learn/VB video/NN -/HYPH based/VBN question/NN answering/NN (/-LRB- QA/NN )/-RRB- :/: answer/VB a/DT "/`` free/JJ -/HYPH form/NN natural/JJ language/NN question/NN "/'' about/IN a/DT video/NN content/NN ./.
Our/PRP$ approach/NN automatically/RB harvests/VBZ a/DT large/JJ number/NN of/IN videos/NNS and/CC descriptions/NNS freely/RB available/JJ online/RB ./.
Then/RB ,/, a/DT large/JJ number/NN of/IN candidate/NN QA/NNP pairs/NNS are/VBP automatically/RB generated/VBN from/IN descriptions/NNS rather/RB than/IN manually/RB annotated/VBN ./.
Next/RB ,/, we/PRP use/VBP these/DT candidate/NN QA/NNP pairs/NNS to/TO train/VB a/DT number/NN of/IN video/NN -/HYPH based/VBN QA/NN methods/NNS extended/VBD fromMN/NN (/-LRB- Sukhbaatar/NNP et/FW al./FW 2015/CD )/-RRB- ,/, VQA/NN (/-LRB- Antol/NNP et/FW al./FW 2015/CD )/-RRB- ,/, SA/NNP (/-LRB- Yao/NNP et/FW al./FW 2015/CD )/-RRB- ,/, SS/NNP (/-LRB- Venugopalan/NNP et/FW al./FW 2015/CD )/-RRB- ./.
In/IN order/NN to/TO handle/VB non-perfect/JJ candidate/NN QA/NNP pairs/NNS ,/, we/PRP propose/VBP a/DT self/NN -/HYPH paced/VBN learning/NN procedure/NN to/IN iteratively/RB identify/VB them/PRP and/CC mitigate/VB their/PRP$ effects/NNS in/IN training/NN ./.
Finally/RB ,/, we/PRP evaluate/VBP performance/NN on/IN manually/RB generated/VBN video/NN -/HYPH based/VBN QA/NNP pairs/NNS ./.
The/DT results/NNS show/VBP that/IN our/PRP$ self/NN -/HYPH paced/VBN learning/NN procedure/NN is/VBZ effective/JJ ,/, and/CC the/DT extended/VBN SS/NN model/NN outperforms/VBZ various/JJ baselines/NNS ./.
