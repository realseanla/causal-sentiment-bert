Deep/JJ reinforcement/NN learning/VBG agents/NNS have/VBP achieved/VBN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS by/IN directly/RB maximising/VBG cumulative/JJ reward/NN ./.
However/RB ,/, environments/NNS contain/VBP a/DT much/JJ wider/JJR variety/NN of/IN possible/JJ training/NN signals/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP introduce/VBP an/DT agent/NN that/WDT also/RB maximises/VBZ many/JJ other/JJ pseudo-reward/NN functions/VBZ simultaneously/RB by/IN reinforcement/NN learning/NN ./.
All/DT of/IN these/DT tasks/NNS share/VBP a/DT common/JJ representation/NN that/WDT ,/, like/IN unsupervised/JJ learning/NN ,/, continues/VBZ to/TO develop/VB in/IN the/DT absence/NN of/IN extrinsic/JJ rewards/NNS ./.
We/PRP also/RB introduce/VBP a/DT novel/JJ mechanism/NN for/IN focusing/VBG this/DT representation/NN upon/IN extrinsic/JJ rewards/NNS ,/, so/IN that/IN learning/NN can/MD rapidly/RB adapt/VB to/IN the/DT most/RBS relevant/JJ aspects/NNS of/IN the/DT actual/JJ task/NN ./.
Our/PRP$ agent/NN significantly/RB outperforms/VBZ the/DT previous/JJ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN on/IN Atari/NNP ,/, averaging/VBG 880/CD \/SYM percent/NN expert/NN human/JJ performance/NN ,/, and/CC a/DT challenging/JJ suite/NN of/IN first/JJ -/HYPH person/NN ,/, three/CD -/HYPH dimensional/JJ \/SYM emph/NN {/-LRB- Labyrinth/NN }/-RRB- tasks/NNS leading/VBG to/IN a/DT mean/JJ speedup/NN in/IN learning/NN of/IN 10/CD $/$ \/CD times/NNS $/$ and/CC averaging/VBG 87/CD \/SYM percent/NN expert/NN human/JJ performance/NN on/IN Labyrinth/NNP ./.
