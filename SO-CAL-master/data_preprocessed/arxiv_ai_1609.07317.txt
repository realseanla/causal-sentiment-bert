In/IN this/DT work/NN we/PRP explore/VBP deep/JJ generative/NN models/NNS of/IN text/NN in/IN which/WDT the/DT latent/JJ representation/NN of/IN a/DT document/NN is/VBZ itself/PRP drawn/VBN from/IN a/DT discrete/JJ language/NN model/NN distribution/NN ./.
We/PRP formulate/VBP a/DT variational/JJ auto/NN -/HYPH encoder/NN for/IN inference/NN in/IN this/DT model/NN and/CC apply/VB it/PRP to/IN the/DT task/NN of/IN compressing/VBG sentences/NNS ./.
In/IN this/DT application/NN the/DT generative/JJ model/NN first/RB draws/VBZ a/DT latent/JJ summary/NN sentence/NN from/IN a/DT background/NN language/NN model/NN ,/, and/CC then/RB subsequently/RB draws/VBZ the/DT observed/VBN sentence/NN conditioned/VBN on/IN this/DT latent/JJ summary/NN ./.
In/IN our/PRP$ empirical/JJ evaluation/NN we/PRP show/VBP that/IN generative/JJ formulations/NNS of/IN both/DT abstractive/JJ and/CC extractive/JJ compression/NN yield/NN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS when/WRB trained/VBN on/IN a/DT large/JJ amount/NN of/IN supervised/JJ data/NNS ./.
Further/RB ,/, we/PRP explore/VBP semi-supervised/JJ compression/NN scenarios/NNS where/WRB we/PRP show/VBP that/IN it/PRP is/VBZ possible/JJ to/TO achieve/VB performance/NN competitive/JJ with/IN previously/RB proposed/VBN supervised/JJ models/NNS while/IN training/NN on/IN a/DT fraction/NN of/IN the/DT supervised/JJ data/NNS ./.
