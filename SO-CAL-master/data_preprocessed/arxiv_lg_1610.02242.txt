In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT simple/JJ and/CC efficient/JJ method/NN for/IN training/NN deep/JJ neural/JJ networks/NNS in/IN a/DT semi-supervised/JJ setting/NN where/WRB only/RB a/DT small/JJ portion/NN of/IN training/NN data/NNS is/VBZ labeled/VBN ./.
We/PRP introduce/VBP temporal/JJ ensembling/NN ,/, where/WRB we/PRP form/VBP a/DT consensus/NN prediction/NN of/IN the/DT unknown/JJ labels/NNS under/IN multiple/JJ instances/NNS of/IN the/DT network/NN -/HYPH in/IN -/HYPH training/NN on/IN different/JJ epochs/NNS ,/, and/CC most/RBS importantly/RB ,/, under/IN different/JJ regularization/NN and/CC input/NN augmentation/NN conditions/NNS ./.
This/DT ensemble/NN prediction/NN can/MD be/VB expected/VBN to/TO be/VB a/DT better/JJR predictor/NN for/IN the/DT unknown/JJ labels/NNS than/IN the/DT output/NN of/IN the/DT network/NN at/IN the/DT most/RBS recent/JJ training/NN epoch/NN ,/, and/CC can/MD thus/RB be/VB used/VBN as/IN a/DT target/NN for/IN training/NN ./.
Using/VBG our/PRP$ method/NN ,/, we/PRP set/VBD new/JJ records/NNS for/IN two/CD standard/JJ semi-supervised/VBN learning/NN benchmarks/NNS ,/, reducing/VBG the/DT classification/NN error/NN rate/NN from/IN 18.63/CD percent/NN to/IN 12.89/CD percent/NN in/IN CIFAR/NN -/HYPH 10/CD with/IN 4000/CD labels/NNS and/CC from/IN 18.44/CD percent/NN to/IN 6.83/CD percent/NN in/IN SVHN/NNP with/IN 500/CD labels/NNS ./.
