Machine/NN comprehension/NN of/IN text/NN is/VBZ an/DT important/JJ problem/NN in/IN natural/JJ language/NN processing/NN ./.
A/DT recently/RB released/VBN dataset/NN ,/, the/DT Stanford/NNP Question/NN Answering/VBG Dataset/NN (/-LRB- SQuAD/NN )/-RRB- ,/, offers/VBZ a/DT large/JJ number/NN of/IN real/JJ questions/NNS and/CC their/PRP$ answers/NNS created/VBN by/IN humans/NNS through/IN crowdsourcing/NN ./.
SQuAD/NN provides/VBZ a/DT challenging/JJ testbed/NN for/IN evaluating/VBG machine/NN comprehension/NN algorithms/NNS ,/, partly/RB because/IN compared/VBN with/IN previous/JJ datasets/NNS ,/, in/IN SQuAD/NNP the/DT answers/NNS do/VBP not/RB come/VB from/IN a/DT small/JJ set/NN of/IN candidate/NN answers/NNS and/CC they/PRP have/VBP variable/JJ lengths/NNS ./.
We/PRP propose/VBP an/DT end/NN -/HYPH to/IN -/HYPH end/NN neural/JJ architecture/NN for/IN the/DT task/NN ./.
The/DT architecture/NN is/VBZ based/VBN on/IN match/NN -/HYPH LSTM/NN ,/, a/DT model/NN we/PRP proposed/VBD previously/RB for/IN textual/JJ entailment/NN ,/, and/CC Pointer/NNP Net/NNP ,/, a/DT sequence/NN -/HYPH to/IN -/HYPH sequence/NN model/NN proposed/VBN by/IN Vinyals/NNP et/FW al./FW (/-LRB- 2015/CD )/-RRB- to/IN constrain/VB the/DT output/NN tokens/NNS to/TO be/VB from/IN the/DT input/NN sequences/NNS ./.
We/PRP propose/VBP two/CD ways/NNS of/IN using/VBG Pointer/NNP Net/NNP for/IN our/PRP$ task/NN ./.
Our/PRP$ experiments/NNS show/VBP that/IN both/DT of/IN our/PRP$ two/CD models/NNS substantially/RB outperform/VBP the/DT best/JJS results/NNS obtained/VBN by/IN Rajpurkar/NNP et/FW al./FW (/-LRB- 2016/CD )/-RRB- using/VBG logistic/JJ regression/NN and/CC manually/RB crafted/VBN features/NNS ./.
