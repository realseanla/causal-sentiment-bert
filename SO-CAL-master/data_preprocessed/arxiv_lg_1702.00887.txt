Attention/NN networks/NNS have/VBP proven/VBN to/TO be/VB an/DT effective/JJ approach/NN for/IN embedding/VBG categorical/JJ inference/NN within/IN a/DT deep/JJ neural/JJ network/NN ./.
However/RB ,/, for/IN many/JJ tasks/NNS we/PRP may/MD want/VB to/TO model/VB richer/JJR structural/JJ dependencies/NNS without/IN abandoning/VBG end/NN -/HYPH to/IN -/HYPH end/NN training/NN ./.
In/IN this/DT work/NN ,/, we/PRP experiment/VBP with/IN incorporating/VBG richer/JJR structural/JJ distributions/NNS ,/, encoded/VBN using/VBG graphical/NN models/NNS ,/, within/IN deep/JJ networks/NNS ./.
We/PRP show/VBP that/IN these/DT structured/JJ attention/NN networks/NNS are/VBP simple/JJ extensions/NNS of/IN the/DT basic/JJ attention/NN procedure/NN ,/, and/CC that/IN they/PRP allow/VBP for/IN extending/VBG attention/NN beyond/IN the/DT standard/JJ soft/JJ -/HYPH selection/NN approach/NN ,/, such/JJ as/IN attending/VBG to/IN partial/JJ segmentations/NNS or/CC to/IN subtrees/NNS ./.
We/PRP experiment/VBP with/IN two/CD different/JJ classes/NNS of/IN structured/JJ attention/NN networks/NNS :/: a/DT linear/JJ -/HYPH chain/NN conditional/JJ random/JJ field/NN and/CC a/DT graph/NN -/HYPH based/VBN parsing/VBG model/NN ,/, and/CC describe/VB how/WRB these/DT models/NNS can/MD be/VB practically/RB implemented/VBN as/IN neural/JJ network/NN layers/NNS ./.
Experiments/NNS show/VBP that/IN this/DT approach/NN is/VBZ effective/JJ for/IN incorporating/VBG structural/JJ biases/NNS ,/, and/CC structured/JJ attention/NN networks/NNS outperform/VBP baseline/JJ attention/NN models/NNS on/IN a/DT variety/NN of/IN synthetic/JJ and/CC real/JJ tasks/NNS :/: tree/NN transduction/NN ,/, neural/JJ machine/NN translation/NN ,/, question/NN answering/NN ,/, and/CC natural/JJ language/NN inference/NN ./.
We/PRP further/RB find/VBP that/IN models/NNS trained/VBN in/IN this/DT way/NN learn/VB interesting/JJ unsupervised/JJ hidden/JJ representations/NNS that/WDT generalize/VBP simple/JJ attention/NN ./.
