The/DT goal/NN of/IN machine/NN learning/NN is/VBZ to/TO develop/VB predictors/NNS that/WDT generalize/VBP well/RB to/TO test/VB data/NNS ./.
Ideally/RB ,/, this/DT is/VBZ achieved/VBN by/IN training/VBG on/IN an/DT almost/RB infinitely/RB large/JJ training/NN data/NNS set/VBP that/DT captures/VBZ all/DT variations/NNS in/IN the/DT data/NNS distribution/NN ./.
In/IN practical/JJ learning/NN settings/NNS ,/, however/RB ,/, we/PRP do/VBP not/RB have/VB infinite/JJ data/NNS and/CC our/PRP$ predictors/NNS may/MD overfit/VB ./.
Overfitting/NN may/MD be/VB combatted/VBN ,/, for/IN example/NN ,/, by/IN adding/VBG a/DT regularizer/NN to/IN the/DT training/NN objective/NN or/CC by/IN defining/VBG a/DT prior/JJ over/IN the/DT model/NN parameters/NNS and/CC performing/VBG Bayesian/JJ inference/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT third/JJ ,/, alternative/JJ approach/NN to/IN combat/NN overfitting/NN :/: we/PRP extend/VBP the/DT training/NN set/VBN with/IN infinitely/RB many/JJ artificial/JJ training/NN examples/NNS that/WDT are/VBP obtained/VBN by/IN corrupting/VBG the/DT original/JJ training/NN data/NNS ./.
We/PRP show/VBP that/IN this/DT approach/NN is/VBZ practical/JJ and/CC efficient/JJ for/IN a/DT range/NN of/IN predictors/NNS and/CC corruption/NN models/NNS ./.
Our/PRP$ approach/NN ,/, called/VBN marginalized/VBN corrupted/JJ features/NNS (/-LRB- MCF/NN )/-RRB- ,/, trains/NNS robust/JJ predictors/NNS by/IN minimizing/VBG the/DT expected/VBN value/NN of/IN the/DT loss/NN function/NN under/IN the/DT corruption/NN model/NN ./.
We/PRP show/VBP empirically/RB on/IN a/DT variety/NN of/IN data/NNS sets/VBZ that/IN MCF/NN classifiers/NNS can/MD be/VB trained/VBN efficiently/RB ,/, may/MD generalize/VB substantially/RB better/JJR to/TO test/VB data/NNS ,/, and/CC are/VBP also/RB more/RBR robust/JJ to/IN feature/NN deletion/NN at/IN test/NN time/NN ./.
