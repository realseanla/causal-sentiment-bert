We/PRP study/VBP the/DT fundamental/JJ limits/NNS to/IN communication/NN -/HYPH efficient/JJ distributed/VBN methods/NNS for/IN convex/NN learning/NN and/CC optimization/NN ,/, under/IN different/JJ assumptions/NNS on/IN the/DT information/NN available/JJ to/IN individual/JJ machines/NNS ,/, and/CC the/DT types/NNS of/IN functions/NNS considered/VBN ./.
We/PRP identify/VBP cases/NNS where/WRB existing/VBG algorithms/NNS are/VBP already/RB worst/RBS -/HYPH case/NN optimal/JJ ,/, as/RB well/RB as/IN cases/NNS where/WRB room/NN for/IN further/JJ improvement/NN is/VBZ still/RB possible/JJ ./.
Among/IN other/JJ things/NNS ,/, our/PRP$ results/NNS indicate/VBP that/IN without/IN similarity/NN between/IN the/DT local/JJ objective/JJ functions/NNS (/-LRB- due/IN to/IN statistical/JJ data/NNS similarity/NN or/CC otherwise/RB )/-RRB- many/JJ communication/NN rounds/NNS may/MD be/VB required/VBN ,/, even/RB if/IN the/DT machines/NNS have/VBP unbounded/JJ computational/JJ power/NN ./.
