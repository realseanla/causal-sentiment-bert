We/PRP consider/VBP an/DT adversarial/JJ online/JJ learning/NN setting/VBG where/WRB a/DT decision/NN maker/NN can/MD choose/VB an/DT action/NN in/IN every/DT stage/NN of/IN the/DT game/NN ./.
In/IN addition/NN to/IN observing/VBG the/DT reward/NN of/IN the/DT chosen/VBN action/NN ,/, the/DT decision/NN maker/NN gets/VBZ side/NN observations/NNS on/IN the/DT reward/NN he/PRP would/MD have/VB obtained/VBN had/VBD he/PRP chosen/VBN some/DT of/IN the/DT other/JJ actions/NNS ./.
The/DT observation/NN structure/NN is/VBZ encoded/VBN as/IN a/DT graph/NN ,/, where/WRB node/NN $/$ i/CD $/$ is/VBZ linked/VBN to/IN node/NN $/NN j/NN $/$ if/IN sampling/VBG $/$ i/CD $/$ provides/VBZ information/NN on/IN the/DT reward/NN of/IN $/$ j/NN $/$ ./.
This/DT setting/NN naturally/RB interpolates/VBZ between/IN the/DT well/NN -/HYPH known/VBN "/`` experts/NNS "/'' setting/VBG ,/, where/WRB the/DT decision/NN maker/NN can/MD view/VB all/DT rewards/NNS ,/, and/CC the/DT multi-armed/JJ bandits/NNS setting/VBG ,/, where/WRB the/DT decision/NN maker/NN can/MD only/RB view/VB the/DT reward/NN of/IN the/DT chosen/VBN action/NN ./.
We/PRP develop/VBP practical/JJ algorithms/NNS with/IN provable/JJ regret/NN guarantees/NNS ,/, as/RB well/RB as/IN partially/RB -/HYPH matching/VBG lower/JJR bounds/NNS ./.
The/DT regret/NN depends/VBZ on/IN non-trivial/JJ graph/NN theoretic/JJ properties/NNS of/IN the/DT information/NN feedback/NN structure/NN ,/, and/CC reveals/VBZ an/DT interesting/JJ trade/NN -/HYPH off/NN between/IN regret/NN optimality/NN and/CC computational/JJ efficiency/NN ./.
