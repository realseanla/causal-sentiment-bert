Boosting/VBG is/VBZ a/DT popular/JJ way/NN to/TO derive/VB powerful/JJ learners/NNS from/IN simpler/JJR hypothesis/NN classes/NNS ./.
Following/VBG previous/JJ work/NN (/-LRB- Mason/NNP et/FW al./FW ,/, 1999/CD ;/: Friedman/NNP ,/, 2000/CD )/-RRB- on/IN general/JJ boosting/VBG frameworks/NNS ,/, we/PRP analyze/VBP gradient/NN -/HYPH based/VBN descent/NN algorithms/NNS for/IN boosting/VBG with/IN respect/NN to/IN any/DT convex/NN objective/NN and/CC introduce/VB a/DT new/JJ measure/NN of/IN weak/JJ learner/NN performance/NN into/IN this/DT setting/NN which/WDT generalizes/VBZ existing/VBG work/NN ./.
We/PRP present/VBP the/DT first/JJ weak/JJ to/IN strong/JJ learning/NN guarantees/NNS for/IN the/DT existing/VBG gradient/NN boosting/VBG work/NN for/IN smooth/JJ convex/NN objectives/NNS ,/, and/CC also/RB demonstrate/VBP that/IN this/DT work/NN fails/VBZ for/IN non-smooth/JJ objectives/NNS ./.
To/TO address/VB this/DT issue/NN ,/, we/PRP present/VBP new/JJ algorithms/NNS which/WDT extend/VBP this/DT boosting/VBG approach/NN to/IN arbitrary/JJ convex/NN loss/NN functions/NNS and/CC give/VB corresponding/VBG weak/JJ to/IN strong/JJ convergence/NN results/NNS ./.
In/IN addition/NN ,/, we/PRP demonstrate/VBP experimental/JJ results/NNS that/WDT support/VBP our/PRP$ analysis/NN and/CC demonstrate/VBP the/DT need/NN for/IN the/DT new/JJ algorithms/NNS we/PRP present/VBP ./.
