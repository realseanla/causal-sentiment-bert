We/PRP consider/VBP learning/VBG representations/NNS (/-LRB- features/NNS )/-RRB- in/IN the/DT setting/NN in/IN which/WDT we/PRP have/VBP access/NN to/IN multiple/JJ unlabeled/JJ views/NNS of/IN the/DT data/NNS for/IN learning/NN while/IN only/RB one/CD view/NN is/VBZ available/JJ for/IN downstream/JJ tasks/NNS ./.
Previous/JJ work/NN on/IN this/DT problem/NN has/VBZ proposed/VBN several/JJ techniques/NNS based/VBN on/IN deep/JJ neural/JJ networks/NNS ,/, typically/RB involving/VBG either/CC autoencoder/NN -/HYPH like/JJ networks/NNS with/IN a/DT reconstruction/NN objective/NN or/CC paired/VBN feedforward/JJ networks/NNS with/IN a/DT batch/NN -/HYPH style/NN correlation/NN -/HYPH based/VBN objective/NN ./.
We/PRP analyze/VBP several/JJ techniques/NNS based/VBN on/IN prior/JJ work/NN ,/, as/RB well/RB as/IN new/JJ variants/NNS ,/, and/CC compare/VB them/PRP empirically/RB on/IN image/NN ,/, speech/NN ,/, and/CC text/NN tasks/NNS ./.
We/PRP find/VBP an/DT advantage/NN for/IN correlation/NN -/HYPH based/VBN representation/NN learning/NN ,/, while/IN the/DT best/JJS results/NNS on/IN most/JJS tasks/NNS are/VBP obtained/VBN with/IN our/PRP$ new/JJ variant/NN ,/, deep/JJ canonically/RB correlated/VBN autoencoders/NNS (/-LRB- DCCAE/NNP )/-RRB- ./.
We/PRP also/RB explore/VB a/DT stochastic/JJ optimization/NN procedure/NN for/IN minibatch/NN correlation/NN -/HYPH based/VBN objectives/NNS and/CC discuss/VB the/DT time/NN //, performance/NN trade/NN -/HYPH offs/NNS for/IN kernel/NN -/HYPH based/VBN and/CC neural/JJ network/NN -/HYPH based/VBN implementations/NNS ./.
