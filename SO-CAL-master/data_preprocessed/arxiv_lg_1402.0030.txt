Highly/RB expressive/JJ directed/VBN latent/JJ variable/JJ models/NNS ,/, such/JJ as/IN sigmoid/NN belief/NN networks/NNS ,/, are/VBP difficult/JJ to/TO train/VB on/IN large/JJ datasets/NNS because/IN exact/JJ inference/NN in/IN them/PRP is/VBZ intractable/JJ and/CC none/NN of/IN the/DT approximate/JJ inference/NN methods/NNS that/WDT have/VBP been/VBN applied/VBN to/IN them/PRP scale/VBP well/RB ./.
We/PRP propose/VBP a/DT fast/JJ non-iterative/JJ approximate/JJ inference/NN method/NN that/WDT uses/VBZ a/DT feedforward/JJ network/NN to/TO implement/VB efficient/JJ exact/JJ sampling/NN from/IN the/DT variational/JJ posterior/JJ ./.
The/DT model/NN and/CC this/DT inference/NN network/NN are/VBP trained/VBN jointly/RB by/IN maximizing/VBG a/DT variational/JJ lower/JJR bound/VBN on/IN the/DT log/NN -/HYPH likelihood/NN ./.
Although/IN the/DT naive/JJ estimator/NN the/DT inference/NN model/NN gradient/NN is/VBZ too/RB high/JJ -/HYPH variance/NN to/TO be/VB useful/JJ ,/, we/PRP make/VBP it/PRP practical/JJ by/IN applying/VBG several/JJ straightforward/JJ model/NN -/HYPH independent/JJ variance/NN reduction/NN techniques/NNS ./.
Applying/VBG our/PRP$ approach/NN to/IN training/NN sigmoid/NN belief/NN networks/NNS and/CC deep/JJ autoregressive/JJ networks/NNS ,/, we/PRP show/VBP that/IN it/PRP outperforms/VBZ the/DT wake/NN -/HYPH sleep/NN algorithm/NN on/IN MNIST/NNP and/CC achieves/VBZ state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN results/NNS on/IN the/DT Reuters/NNP RCV1/NN document/NN dataset/NN ./.
