Recently/RB ,/, end/NN -/HYPH to/IN -/HYPH end/NN memory/NN networks/NNS have/VBP shown/VBN promising/JJ results/NNS on/IN Question/NN Answering/VBG task/NN ,/, which/WDT encode/VBP the/DT past/JJ facts/NNS into/IN an/DT explicit/JJ memory/NN and/CC perform/VB reasoning/NN ability/NN by/IN making/VBG multiple/JJ computational/JJ steps/NNS on/IN the/DT memory/NN ./.
However/RB ,/, memory/NN networks/NNS conduct/VBP the/DT reasoning/NN on/IN sentence/NN -/HYPH level/NN memory/NN to/IN output/NN coarse/JJ semantic/JJ vectors/NNS and/CC do/VBP not/RB further/JJ take/VB any/DT attention/NN mechanism/NN to/TO focus/VB on/IN words/NNS ,/, which/WDT may/MD lead/VB to/IN the/DT model/NN lose/VB some/DT detail/NN information/NN ,/, especially/RB when/WRB the/DT answers/NNS are/VBP rare/JJ or/CC unknown/JJ words/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT novel/JJ Hierarchical/JJ Memory/NN Networks/NNS ,/, dubbed/VBN HMN/NNP ./.
First/RB ,/, we/PRP encode/VBP the/DT past/JJ facts/NNS into/IN sentence/NN -/HYPH level/NN memory/NN and/CC word/NN -/HYPH level/NN memory/NN respectively/RB ./.
Then/RB ,/, (/-LRB- k/NN )/-RRB- -/HYPH max/NN pooling/VBG is/VBZ exploited/VBN following/VBG reasoning/NN module/NN on/IN the/DT sentence/NN -/HYPH level/NN memory/NN to/IN sample/NN the/DT (/-LRB- k/NN )/-RRB- most/RBS relevant/JJ sentences/NNS to/IN a/DT question/NN and/CC feed/VB these/DT sentences/NNS into/IN attention/NN mechanism/NN on/IN the/DT word/NN -/HYPH level/NN memory/NN to/TO focus/VB the/DT words/NNS in/IN the/DT selected/VBN sentences/NNS ./.
Finally/RB ,/, the/DT prediction/NN is/VBZ jointly/RB learned/VBN over/IN the/DT outputs/NNS of/IN the/DT sentence/NN -/HYPH level/NN reasoning/NN module/NN and/CC the/DT word/NN -/HYPH level/NN attention/NN mechanism/NN ./.
The/DT experimental/JJ results/NNS demonstrate/VBP that/IN our/PRP$ approach/NN successfully/RB conducts/VBZ answer/NN selection/NN on/IN unknown/JJ words/NNS and/CC achieves/VBZ a/DT better/JJR performance/NN than/IN memory/NN networks/NNS ./.
