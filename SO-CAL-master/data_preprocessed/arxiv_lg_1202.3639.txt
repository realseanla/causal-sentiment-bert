We/PRP study/VBP the/DT problem/NN of/IN learning/VBG the/DT most/RBS biased/JJ coin/NN among/IN a/DT set/NN of/IN coins/NNS by/IN tossing/VBG the/DT coins/NNS adaptively/RB ./.
The/DT goal/NN is/VBZ to/TO minimize/VB the/DT number/NN of/IN tosses/VBZ to/TO identify/VB a/DT coin/NN i/PRP */NFP such/PDT that/DT prob/NN {/-LRB- coin/NN i/PRP */NFP is/VBZ most/RBS biased/JJ }/-RRB- is/VBZ at/IN least/JJS 1/CD -/HYPH \/SYM delta/NN \/SYM for/IN any/DT given/VBN \/SYM delta/NN &gt;/SYM 0/CD ./.
Under/IN a/DT particular/JJ probabilistic/JJ model/NN ,/, we/PRP give/VBP an/DT optimal/JJ algorithm/NN ,/, i.e./FW ,/, an/DT algorithm/NN that/WDT minimizes/VBZ the/DT expected/VBN number/NN of/IN tosses/VBZ ,/, to/TO learn/VB a/DT most/RBS biased/JJ coin/NN ./.
The/DT problem/NN is/VBZ equivalent/JJ to/IN finding/VBG the/DT best/JJS arm/NN in/IN the/DT multi-armed/JJ bandit/NN problem/NN using/VBG adaptive/JJ strategies/NNS ./.
Dar/NNP et/FW al./FW (/-LRB- 2002/CD )/-RRB- and/CC Mannor/NNP and/CC Tsitsiklis/NNP (/-LRB- 2004/CD )/-RRB- show/VBP upper/JJ and/CC lower/JJR bounds/NNS matching/VBG up/RP to/IN constant/JJ factors/NNS on/IN the/DT number/NN of/IN coin/NN tosses/VBZ for/IN several/JJ underlying/VBG settings/NNS of/IN the/DT bias/NN probabilities/NNS ./.
For/IN a/DT class/NN of/IN such/JJ settings/NNS we/PRP bridge/VBP the/DT constant/JJ factor/NN gap/NN by/IN giving/VBG an/DT optimal/JJ adaptive/JJ strategy/NN --/: a/DT strategy/NN that/WDT performs/VBZ the/DT best/JJS possible/JJ action/NN under/IN any/DT given/JJ history/NN of/IN outcomes/NNS ./.
For/IN any/DT given/JJ history/NN ,/, tossing/VBG the/DT coin/NN chosen/VBN by/IN our/PRP$ strategy/NN minimizes/VBZ the/DT expected/VBN number/NN of/IN tosses/VBZ needed/VBN to/TO learn/VB a/DT most/RBS biased/JJ coin/NN ./.
To/IN our/PRP$ knowledge/NN ,/, this/DT is/VBZ the/DT first/JJ algorithm/NN that/WDT employs/VBZ an/DT optimal/JJ adaptive/JJ strategy/NN under/IN a/DT Bayesian/JJ setting/NN for/IN this/DT problem/NN ./.
