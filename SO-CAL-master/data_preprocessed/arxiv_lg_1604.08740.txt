In/IN online/JJ convex/NN optimization/NN it/PRP is/VBZ well/RB known/VBN that/IN objective/JJ functions/NNS with/IN curvature/NN are/VBP much/RB easier/JJR than/IN arbitrary/JJ convex/NN functions/NNS ./.
Here/RB we/PRP show/VBP that/IN the/DT regret/NN can/MD be/VB significantly/RB reduced/VBN even/RB without/IN curvature/NN ,/, in/IN cases/NNS where/WRB there/EX is/VBZ a/DT stable/JJ optimum/JJ to/TO converge/VB to/IN ./.
More/RBR precisely/RB ,/, the/DT regret/NN of/IN existing/VBG methods/NNS is/VBZ determined/VBN by/IN the/DT norms/NNS of/IN the/DT encountered/VBN gradients/NNS ,/, and/CC matching/VBG worst/JJS -/HYPH case/NN performance/NN lower/JJR bounds/NNS tell/VB us/PRP that/IN this/DT can/MD not/RB be/VB improved/VBN uniformly/RB ./.
Yet/CC we/PRP argue/VBP that/IN this/DT is/VBZ a/DT rather/RB pessimistic/JJ assessment/NN of/IN the/DT complexity/NN of/IN the/DT problem/NN ./.
We/PRP introduce/VBP a/DT new/JJ parameter/NN -/HYPH free/JJ algorithm/NN ,/, called/VBN MetaGrad/NNP ,/, for/IN which/WDT the/DT gradient/NN norms/NNS in/IN the/DT regret/NN are/VBP scaled/VBN down/RP by/IN the/DT distance/NN to/IN the/DT (/-LRB- unknown/JJ )/-RRB- optimum/JJ ./.
So/RB when/WRB the/DT optimum/JJ is/VBZ reasonably/RB stable/JJ over/IN time/NN ,/, making/VBG the/DT algorithm/NN converge/VBP ,/, this/DT new/JJ scaling/NN leads/VBZ to/IN orders/NNS of/IN magnitude/NN smaller/JJR regret/NN even/RB when/WRB the/DT gradients/NNS themselves/PRP do/VBP not/RB vanish/VB ./.
