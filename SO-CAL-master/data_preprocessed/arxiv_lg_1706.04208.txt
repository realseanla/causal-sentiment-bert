One/CD of/IN the/DT main/JJ challenges/NNS in/IN reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- is/VBZ generalisation/NN ./.
In/IN typical/JJ deep/JJ RL/NN methods/NNS this/DT is/VBZ achieved/VBN by/IN approximating/VBG the/DT optimal/JJ value/NN function/NN with/IN a/DT low/JJ -/HYPH dimensional/JJ representation/NN using/VBG a/DT deep/JJ network/NN ./.
While/IN this/DT approach/NN works/VBZ well/RB in/IN many/JJ domains/NNS ,/, in/IN domains/NNS where/WRB the/DT optimal/JJ value/NN function/NN can/MD not/RB easily/RB be/VB reduced/VBN to/IN a/DT low/JJ -/HYPH dimensional/JJ representation/NN ,/, learning/NN can/MD be/VB very/RB slow/JJ and/CC unstable/JJ ./.
This/DT paper/NN contributes/VBZ towards/IN tackling/VBG such/JJ challenging/JJ domains/NNS ,/, by/IN proposing/VBG a/DT new/JJ method/NN ,/, called/VBN Hybrid/NN Reward/NN Architecture/NNP (/-LRB- HRA/NNP )/-RRB- ./.
HRA/NN takes/VBZ as/IN input/NN a/DT decomposed/JJ reward/NN function/NN and/CC learns/VBZ a/DT separate/JJ value/NN function/NN for/IN each/DT component/NN reward/NN function/NN ./.
Because/IN each/DT component/NN typically/RB only/RB depends/VBZ on/IN a/DT subset/NN of/IN all/DT features/NNS ,/, the/DT overall/JJ value/NN function/NN is/VBZ much/JJ smoother/JJR and/CC can/MD be/VB easier/JJR approximated/VBN by/IN a/DT low/JJ -/HYPH dimensional/JJ representation/NN ,/, enabling/VBG more/JJR effective/JJ learning/NN ./.
We/PRP demonstrate/VBP HRA/NNP on/IN a/DT toy/NN -/HYPH problem/NN and/CC the/DT Atari/NNP game/NN Ms./NNP Pac/NNP -/HYPH Man/NNP ,/, where/WRB HRA/NNP achieves/VBZ above/IN -/HYPH human/JJ performance/NN ./.
