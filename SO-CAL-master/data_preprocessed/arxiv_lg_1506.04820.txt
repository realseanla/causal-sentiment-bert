We/PRP extend/VBP the/DT theory/NN of/IN boosting/VBG for/IN regression/NN problems/NNS to/IN the/DT online/JJ learning/NN setting/NN ./.
Generalizing/VBG from/IN the/DT batch/NN setting/VBG for/IN boosting/VBG ,/, the/DT notion/NN of/IN a/DT weak/JJ learning/NN algorithm/NN is/VBZ modeled/VBN as/IN an/DT online/JJ learning/NN algorithm/NN with/IN linear/JJ loss/NN functions/NNS that/WDT competes/VBZ with/IN a/DT base/NN class/NN of/IN regression/NN functions/NNS ,/, while/IN a/DT strong/JJ learning/NN algorithm/NN is/VBZ an/DT online/JJ learning/NN algorithm/NN with/IN convex/NN loss/NN functions/NNS that/WDT competes/VBZ with/IN a/DT larger/JJR class/NN of/IN regression/NN functions/NNS ./.
Our/PRP$ main/JJ result/NN is/VBZ an/DT online/JJ gradient/NN boosting/VBG algorithm/NN which/WDT converts/VBZ a/DT weak/JJ online/JJ learning/NN algorithm/NN into/IN a/DT strong/JJ one/NN where/WRB the/DT larger/JJR class/NN of/IN functions/NNS is/VBZ the/DT linear/JJ span/NN of/IN the/DT base/NN class/NN ./.
We/PRP also/RB give/VBP a/DT simpler/JJR boosting/VBG algorithm/NN that/WDT converts/VBZ a/DT weak/JJ online/JJ learning/NN algorithm/NN into/IN a/DT strong/JJ one/NN where/WRB the/DT larger/JJR class/NN of/IN functions/NNS is/VBZ the/DT convex/NN hull/NN of/IN the/DT base/NN class/NN ,/, and/CC prove/VB its/PRP$ optimality/NN ./.
