This/DT paper/NN investigates/VBZ neural/JJ character/NN -/HYPH based/VBN morphological/JJ tagging/NN for/IN languages/NNS with/IN complex/JJ morphology/NN and/CC large/JJ tag/NN sets/NNS ./.
We/PRP systematically/RB explore/VB a/DT variety/NN of/IN neural/JJ architectures/NNS (/-LRB- DNN/NNP ,/, CNN/NNP ,/, CNNHighway/NNP ,/, LSTM/NNP ,/, BLSTM/NNP )/-RRB- to/TO obtain/VB character/NN -/HYPH based/VBN word/NN vectors/NNS combined/VBN with/IN bidirectional/JJ LSTMs/NNPS to/TO model/VB across/IN -/HYPH word/NN context/NN in/IN an/DT end/NN -/HYPH to/IN -/HYPH end/NN setting/NN ./.
We/PRP explore/VBP supplementary/JJ use/NN of/IN word/NN -/HYPH based/VBN vectors/NNS trained/VBN on/IN large/JJ amounts/NNS of/IN unlabeled/JJ data/NNS ./.
Our/PRP$ experiments/NNS for/IN morphological/JJ tagging/NN suggest/VBP that/IN for/IN "/`` simple/JJ "/'' model/NN configurations/NNS ,/, the/DT choice/NN of/IN the/DT network/NN architecture/NN (/-LRB- CNN/NNP vs./FW CNNHighway/NNP vs./FW LSTM/NNP vs./FW BLSTM/NNP )/-RRB- or/CC the/DT augmentation/NN with/IN pre-trained/JJ word/NN embeddings/NNS can/MD be/VB important/JJ and/CC clearly/RB impact/VB the/DT accuracy/NN ./.
Increasing/VBG the/DT model/NN capacity/NN by/IN adding/VBG depth/NN ,/, for/IN example/NN ,/, and/CC carefully/RB optimizing/VBG the/DT neural/JJ networks/NNS can/MD lead/VB to/IN substantial/JJ improvements/NNS ,/, and/CC the/DT differences/NNS in/IN accuracy/NN (/-LRB- but/CC not/RB training/NN time/NN )/-RRB- become/VBP much/RB smaller/JJR or/CC even/RB negligible/JJ ./.
Overall/RB ,/, our/PRP$ best/JJS morphological/JJ taggers/NNS for/IN German/JJ and/CC Czech/JJ outperform/VBP the/DT best/JJS results/NNS reported/VBN in/IN the/DT literature/NN by/IN a/DT large/JJ margin/NN ./.
