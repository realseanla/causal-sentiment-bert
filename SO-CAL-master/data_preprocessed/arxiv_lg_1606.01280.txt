Conventional/JJ dependency/NN parsers/NNS rely/VBP on/IN a/DT statistical/JJ model/NN and/CC a/DT transition/NN system/NN or/CC graph/NN algorithm/NN to/TO enforce/VB tree/NN -/HYPH structured/VBN outputs/NNS during/IN training/NN and/CC inference/NN ./.
In/IN this/DT work/NN we/PRP formalize/VBP dependency/NN parsing/VBG as/IN the/DT problem/NN of/IN selecting/VBG the/DT head/NN (/-LRB- a.k.a./RB parent/NN )/-RRB- of/IN each/DT word/NN in/IN a/DT sentence/NN ./.
Our/PRP$ model/NN which/WDT we/PRP call/VBP DeNSe/JJ (/-LRB- as/IN shorthand/NN for/IN Dependency/NN Neural/JJ Selection/NN )/-RRB- employs/VBZ bidirectional/JJ recurrent/JJ neural/JJ networks/NNS for/IN the/DT head/NN selection/NN task/NN ./.
Without/IN enforcing/VBG any/DT structural/JJ constraints/NNS during/IN training/NN ,/, DeNSe/NN generates/VBZ (/-LRB- at/IN inference/NN time/NN )/-RRB- trees/NNS for/IN the/DT overwhelming/JJ majority/NN of/IN sentences/NNS (/-LRB- 95/CD percent/NN on/IN an/DT English/NNP dataset/NN )/-RRB- ,/, while/IN remaining/VBG non-tree/JJ outputs/NNS can/MD be/VB adjusted/VBN with/IN a/DT maximum/JJ spanning/VBG tree/NN algorithm/NN ./.
We/PRP evaluate/VBP DeNSe/JJ on/IN four/CD languages/NNS (/-LRB- English/NNP ,/, Chinese/NNP ,/, Czech/NNP ,/, and/CC German/JJ )/-RRB- with/IN varying/VBG degrees/NNS of/IN non-projectivity/JJ ./.
Despite/IN the/DT simplicity/NN of/IN our/PRP$ approach/NN ,/, experiments/NNS show/VBP that/IN the/DT resulting/VBG parsers/NNS are/VBP on/IN par/NN with/IN or/CC outperform/VB the/DT state/NN of/IN the/DT art/NN ./.
