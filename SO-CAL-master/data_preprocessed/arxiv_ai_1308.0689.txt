The/DT Bayesian/JJ approach/NN to/IN machine/NN learning/NN amounts/VBZ to/IN computing/VBG posterior/JJ distributions/NNS of/IN random/JJ variables/NNS from/IN a/DT probabilistic/JJ model/NN of/IN how/WRB the/DT variables/NNS are/VBP related/VBN (/-LRB- that/DT is/VBZ ,/, a/DT prior/JJ distribution/NN )/-RRB- and/CC a/DT set/NN of/IN observations/NNS of/IN variables/NNS ./.
There/EX is/VBZ a/DT trend/NN in/IN machine/NN learning/NN towards/IN expressing/VBG Bayesian/JJ models/NNS as/IN probabilistic/JJ programs/NNS ./.
As/IN a/DT foundation/NN for/IN this/DT kind/NN of/IN programming/NN ,/, we/PRP propose/VBP a/DT core/NN functional/JJ calculus/NN with/IN primitives/NNS for/IN sampling/NN prior/JJ distributions/NNS and/CC observing/VBG variables/NNS ./.
We/PRP define/VBP measure/NN -/HYPH transformer/NN combinators/NNS inspired/VBN by/IN theorems/NNS in/IN measure/NN theory/NN ,/, and/CC use/VB these/DT to/TO give/VB a/DT rigorous/JJ semantics/NNS to/IN our/PRP$ core/NN calculus/NN ./.
The/DT original/JJ features/NNS of/IN our/PRP$ semantics/NNS include/VBP its/PRP$ support/NN for/IN discrete/JJ ,/, continuous/JJ ,/, and/CC hybrid/JJ measures/NNS ,/, and/CC ,/, in/IN particular/JJ ,/, for/IN observations/NNS of/IN zero/CD -/HYPH probability/NN events/NNS ./.
We/PRP compile/VBP our/PRP$ core/JJ language/NN to/IN a/DT small/JJ imperative/JJ language/NN that/WDT is/VBZ processed/VBN by/IN an/DT existing/VBG inference/NN engine/NN for/IN factor/NN graphs/NNS ,/, which/WDT are/VBP data/NNS structures/NNS that/WDT enable/VBP many/JJ efficient/JJ inference/NN algorithms/NNS ./.
This/DT allows/VBZ efficient/JJ approximate/JJ inference/NN of/IN posterior/JJ marginal/JJ distributions/NNS ,/, treating/VBG thousands/NNS of/IN observations/NNS per/IN second/NN for/IN large/JJ instances/NNS of/IN realistic/JJ models/NNS ./.
