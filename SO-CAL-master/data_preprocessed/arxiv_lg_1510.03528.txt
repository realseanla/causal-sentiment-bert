We/PRP study/VBP the/DT improper/JJ learning/NN of/IN multi-layer/JJ neural/JJ networks/NNS ./.
Suppose/VB that/IN the/DT neural/JJ network/NN to/TO be/VB learned/VBN has/VBZ $/$ k/CD $/$ hidden/VBN layers/NNS and/CC that/IN the/DT $/$ \/CD ell_1/CD $/$ -/HYPH norm/NN of/IN the/DT incoming/JJ weights/NNS of/IN any/DT neuron/NN is/VBZ bounded/VBN by/IN $/$ L$/CD ./.
We/PRP present/VBP a/DT kernel/NN -/HYPH based/VBN method/NN ,/, such/JJ that/IN with/IN probability/NN at/IN least/JJS $/$ 1/CD -/HYPH \/SYM delta/NN $/$ ,/, it/PRP learns/VBZ a/DT predictor/NN whose/WP$ generalization/NN error/NN is/VBZ at/IN most/RBS $/$ \/CD epsilon/CD $/$ worse/JJR than/IN that/DT of/IN the/DT neural/JJ network/NN ./.
The/DT sample/NN complexity/NN and/CC the/DT time/NN complexity/NN of/IN the/DT presented/VBN method/NN are/VBP polynomial/JJ in/IN the/DT input/NN dimension/NN and/CC in/IN $/$ (/-LRB- 1/CD //SYM \/SYM epsilon/NN ,/, \/SYM log/NN (/-LRB- 1/CD //SYM \/SYM delta/NN )/-RRB- ,/, F/NN (/-LRB- k/NN ,/, L/NN )/-RRB- )/-RRB- $/$ ,/, where/WRB $/$ F/NN (/-LRB- k/NN ,/, L/NN )/-RRB- $/$ is/VBZ a/DT function/NN depending/VBG on/IN $/$ (/-LRB- k/CD ,/, L/NN )/-RRB- $/$ and/CC on/IN the/DT activation/NN function/NN ,/, independent/JJ of/IN the/DT number/NN of/IN neurons/NNS ./.
The/DT algorithm/NN applies/VBZ to/IN both/DT sigmoid/NN -/HYPH like/JJ activation/NN functions/NNS and/CC ReLU/NN -/HYPH like/JJ activation/NN functions/NNS ./.
It/PRP implies/VBZ that/IN any/DT sufficiently/RB sparse/JJ neural/JJ network/NN is/VBZ learnable/JJ in/IN polynomial/JJ time/NN ./.
