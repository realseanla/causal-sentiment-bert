We/PRP propose/VBP an/DT efficient/JJ method/NN for/IN approximating/VBG natural/JJ gradient/NN descent/NN in/IN neural/JJ networks/NNS which/WDT we/PRP call/VBP Kronecker/NNP -/HYPH factored/VBN Approximate/JJ Curvature/NN (/-LRB- K/NN -/HYPH FAC/NN )/-RRB- ./.
K/NN -/HYPH FAC/NN is/VBZ based/VBN on/IN an/DT efficiently/RB invertible/JJ approximation/NN of/IN a/DT neural/JJ network/NN 's/POS Fisher/NNP information/NN matrix/NN which/WDT is/VBZ neither/CC diagonal/JJ nor/CC low/JJ -/HYPH rank/NN ,/, and/CC in/IN some/DT cases/NNS is/VBZ completely/RB non-sparse/JJ ./.
It/PRP is/VBZ derived/VBN by/IN approximating/VBG various/JJ large/JJ blocks/NNS of/IN the/DT Fisher/NNP (/-LRB- corresponding/VBG to/IN entire/JJ layers/NNS )/-RRB- as/IN factoring/NN as/IN Kronecker/NNP products/NNS between/IN two/CD much/RB smaller/JJR matrices/NNS ./.
While/IN only/RB several/JJ times/NNS more/RBR expensive/JJ to/TO compute/VB than/IN the/DT plain/JJ stochastic/JJ gradient/NN ,/, the/DT updates/NNS produced/VBN by/IN K/NNP -/HYPH FAC/NNP make/VB much/RB more/JJR progress/NN optimizing/VBG the/DT objective/NN ,/, which/WDT results/VBZ in/IN an/DT algorithm/NN that/WDT can/MD be/VB much/JJ faster/JJR than/IN stochastic/JJ gradient/NN descent/NN with/IN momentum/NN in/IN practice/NN ./.
And/CC unlike/IN some/DT previously/RB proposed/VBN approximate/JJ natural/JJ -/HYPH gradient/NN //HYPH Newton/NNP methods/NNS such/JJ as/IN Hessian/JJ -/HYPH free/JJ methods/NNS ,/, K/NNP -/HYPH FAC/NNP works/VBZ very/RB well/RB in/IN highly/RB stochastic/JJ optimization/NN regimes/NNS ./.
