Answer/NN sentence/NN selection/NN is/VBZ the/DT task/NN of/IN identifying/VBG sentences/NNS that/WDT contain/VBP the/DT answer/NN to/IN a/DT given/VBN question/NN ./.
This/DT is/VBZ an/DT important/JJ problem/NN in/IN its/PRP$ own/JJ right/NN as/RB well/RB as/IN in/IN the/DT larger/JJR context/NN of/IN open/JJ domain/NN question/NN answering/VBG ./.
We/PRP propose/VBP a/DT novel/JJ approach/NN to/IN solving/VBG this/DT task/NN via/IN means/NNS of/IN distributed/VBN representations/NNS ,/, and/CC learn/VB to/TO match/VB questions/NNS with/IN answers/NNS by/IN considering/VBG their/PRP$ semantic/JJ encoding/NN ./.
This/DT contrasts/VBZ prior/JJ work/NN on/IN this/DT task/NN ,/, which/WDT typically/RB relies/VBZ on/IN classifiers/NNS with/IN large/JJ numbers/NNS of/IN hand/NN -/HYPH crafted/VBN syntactic/JJ and/CC semantic/JJ features/NNS and/CC various/JJ external/JJ resources/NNS ./.
Our/PRP$ approach/NN does/VBZ not/RB require/VB any/DT feature/NN engineering/NN nor/CC does/VBZ it/PRP involve/VB specialist/NN linguistic/JJ data/NNS ,/, making/VBG this/DT model/NN easily/RB applicable/JJ to/IN a/DT wide/JJ range/NN of/IN domains/NNS and/CC languages/NNS ./.
Experimental/JJ results/NNS on/IN a/DT standard/JJ benchmark/NN dataset/NN from/IN TREC/NNP demonstrate/VBP that/IN ---/, despite/IN its/PRP$ simplicity/NN ---/: our/PRP$ model/NN matches/VBZ state/NN of/IN the/DT art/NN performance/NN on/IN the/DT answer/NN sentence/NN selection/NN task/NN ./.
