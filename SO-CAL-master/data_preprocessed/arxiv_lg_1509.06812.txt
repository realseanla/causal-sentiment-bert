Despite/IN their/PRP$ success/NN ,/, convolutional/JJ neural/JJ networks/NNS are/VBP computationally/RB expensive/JJ because/IN they/PRP must/MD examine/VB all/DT image/NN locations/NNS ./.
Stochastic/JJ attention/NN -/HYPH based/VBN models/NNS have/VBP been/VBN shown/VBN to/TO improve/VB computational/JJ efficiency/NN at/IN test/NN time/NN ,/, but/CC they/PRP remain/VBP difficult/JJ to/TO train/VB because/IN of/IN intractable/JJ posterior/JJ inference/NN and/CC high/JJ variance/NN in/IN the/DT stochastic/JJ gradient/NN estimates/NNS ./.
Borrowing/NN techniques/NNS from/IN the/DT literature/NN on/IN training/NN deep/JJ generative/NN models/NNS ,/, we/PRP present/VBP the/DT Wake/NN -/HYPH Sleep/NN Recurrent/JJ Attention/NN Model/NN ,/, a/DT method/NN for/IN training/NN stochastic/JJ attention/NN networks/NNS which/WDT improves/VBZ posterior/JJ inference/NN and/CC which/WDT reduces/VBZ the/DT variability/NN in/IN the/DT stochastic/JJ gradients/NNS ./.
We/PRP show/VBP that/IN our/PRP$ method/NN can/MD greatly/RB speed/VB up/RP the/DT training/NN time/NN for/IN stochastic/JJ attention/NN networks/NNS in/IN the/DT domains/NNS of/IN image/NN classification/NN and/CC caption/NN generation/NN ./.
