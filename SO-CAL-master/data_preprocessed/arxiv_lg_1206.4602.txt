Four/CD decades/NNS after/IN their/PRP$ invention/NN ,/, quasi-Newton/NN methods/NNS are/VBP still/RB state/NN of/IN the/DT art/NN in/IN unconstrained/JJ numerical/JJ optimization/NN ./.
Although/IN not/RB usually/RB interpreted/VBN thus/RB ,/, these/DT are/VBP learning/VBG algorithms/NNS that/WDT fit/VBP a/DT local/JJ quadratic/JJ approximation/NN to/IN the/DT objective/JJ function/NN ./.
We/PRP show/VBP that/IN many/JJ ,/, including/VBG the/DT most/RBS popular/JJ ,/, quasi-Newton/JJ methods/NNS can/MD be/VB interpreted/VBN as/IN approximations/NNS of/IN Bayesian/JJ linear/JJ regression/NN under/IN varying/VBG prior/JJ assumptions/NNS ./.
This/DT new/JJ notion/NN elucidates/VBZ some/DT shortcomings/NNS of/IN classical/JJ algorithms/NNS ,/, and/CC lights/VBZ the/DT way/NN to/IN a/DT novel/JJ nonparametric/JJ quasi-Newton/NN method/NN ,/, which/WDT is/VBZ able/JJ to/TO make/VB more/RBR efficient/JJ use/NN of/IN available/JJ information/NN at/IN computational/JJ cost/NN similar/JJ to/IN its/PRP$ predecessors/NNS ./.
