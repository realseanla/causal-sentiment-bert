This/DT paper/NN presents/VBZ several/JJ novel/JJ generalization/NN bounds/NNS for/IN the/DT problem/NN of/IN learning/VBG kernels/NNS based/VBN on/IN the/DT analysis/NN of/IN the/DT Rademacher/NNP complexity/NN of/IN the/DT corresponding/VBG hypothesis/NN sets/NNS ./.
Our/PRP$ bound/VBN for/IN learning/VBG kernels/NNS with/IN a/DT convex/NN combination/NN of/IN p/NN base/NN kernels/NNS has/VBZ only/RB a/DT log/NN (/-LRB- p/NN )/-RRB- dependency/NN on/IN the/DT number/NN of/IN kernels/NNS ,/, p/NN ,/, which/WDT is/VBZ considerably/RB more/RBR favorable/JJ than/IN the/DT previous/JJ best/JJS bound/VBN given/VBN for/IN the/DT same/JJ problem/NN ./.
We/PRP also/RB give/VBP a/DT novel/NN bound/VBN for/IN learning/VBG with/IN a/DT linear/JJ combination/NN of/IN p/NN base/NN kernels/NNS with/IN an/DT L_2/NN regularization/NN whose/WP$ dependency/NN on/IN p/NN is/VBZ only/RB in/IN p/NN ^/SYM {/-LRB- 1/4/CD }/-RRB- ./.
