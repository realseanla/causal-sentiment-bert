In/IN recent/JJ years/NNS researchers/NNS have/VBP achieved/VBN considerable/JJ success/NN applying/VBG neural/JJ network/NN methods/NNS to/TO question/VB answering/VBG (/-LRB- QA/NNP )/-RRB- ./.
These/DT approaches/NNS have/VBP achieved/VBN state/NN of/IN the/DT art/NN results/NNS in/IN simplified/VBN closed/JJ -/HYPH domain/NN settings/NNS such/JJ as/IN the/DT SQuAD/NN (/-LRB- Rajpurkar/NNP et/FW al./FW ,/, 2016/CD )/-RRB- dataset/NN ,/, which/WDT provides/VBZ a/DT pre-selected/JJ passage/NN ,/, from/IN which/WDT the/DT answer/NN to/IN a/DT given/VBN question/NN may/MD be/VB extracted/VBN ./.
More/RBR recently/RB ,/, researchers/NNS have/VBP begun/VBN to/TO tackle/VB open/JJ -/HYPH domain/NN QA/NN ,/, in/IN which/WDT the/DT model/NN is/VBZ given/VBN a/DT question/NN and/CC access/NN to/IN a/DT large/JJ corpus/NN (/-LRB- e.g./FW ,/, wikipedia/NNP )/-RRB- instead/RB of/IN a/DT pre-selected/JJ passage/NN (/-LRB- Chen/NNP et/FW al./FW ,/, 2017a/NN )/-RRB- ./.
This/DT setting/NN is/VBZ more/RBR complex/JJ as/IN it/PRP requires/VBZ large/JJ -/HYPH scale/NN search/NN for/IN relevant/JJ passages/NNS by/IN an/DT information/NN retrieval/NN component/NN ,/, combined/VBN with/IN a/DT reading/NN comprehension/NN model/NN that/WDT "/`` reads/VBZ "/'' the/DT passages/NNS to/TO generate/VB an/DT answer/NN to/IN the/DT question/NN ./.
Performance/NN in/IN this/DT setting/NN lags/VBZ considerably/RB behind/IN closed/JJ -/HYPH domain/NN performance/NN ./.
