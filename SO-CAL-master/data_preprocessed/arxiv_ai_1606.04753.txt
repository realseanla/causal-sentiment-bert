In/IN classical/JJ reinforcement/NN learning/NN ,/, when/WRB exploring/VBG an/DT environment/NN ,/, agents/NNS accept/VBP arbitrary/JJ short/JJ term/NN loss/NN for/IN long/JJ term/NN gain/NN ./.
This/DT is/VBZ infeasible/JJ for/IN safety/NN critical/JJ applications/NNS ,/, such/JJ as/IN robotics/NNS ,/, where/WRB even/RB a/DT single/JJ unsafe/JJ action/NN may/MD cause/VB system/NN failure/NN ./.
In/IN this/DT paper/NN ,/, we/PRP address/VBP the/DT problem/NN of/IN safely/RB exploring/VBG finite/JJ Markov/NNP decision/NN processes/NNS (/-LRB- MDP/NN )/-RRB- ./.
We/PRP define/VBP safety/NN in/IN terms/NNS of/IN an/DT ,/, a/FW priori/FW unknown/JJ ,/, safety/NN constraint/NN that/WDT depends/VBZ on/IN states/NNS and/CC actions/NNS ./.
We/PRP aim/VBP to/TO explore/VB the/DT MDP/NN under/IN this/DT constraint/NN ,/, assuming/VBG that/IN the/DT unknown/JJ function/NN satisfies/VBZ regularity/NN conditions/NNS expressed/VBN via/IN a/DT Gaussian/JJ process/NN prior/JJ ./.
We/PRP develop/VBP a/DT novel/JJ algorithm/NN for/IN this/DT task/NN and/CC prove/VB that/IN it/PRP is/VBZ able/JJ to/TO completely/RB explore/VB the/DT safely/RB reachable/JJ part/NN of/IN the/DT MDP/NNP without/IN violating/VBG the/DT safety/NN constraint/NN ./.
To/TO achieve/VB this/DT ,/, it/PRP cautiously/RB explores/VBZ safe/JJ states/NNS and/CC actions/NNS in/IN order/NN to/TO gain/VB statistical/JJ confidence/NN about/IN the/DT safety/NN of/IN unvisited/JJ state/NN -/HYPH action/NN pairs/NNS from/IN noisy/JJ observations/NNS collected/VBN while/IN navigating/VBG the/DT environment/NN ./.
Moreover/RB ,/, the/DT algorithm/NN explicitly/RB considers/VBZ reachability/NN when/WRB exploring/VBG the/DT MDP/NNP ,/, ensuring/VBG that/IN it/PRP does/VBZ not/RB get/VB stuck/VBN in/IN any/DT state/NN with/IN no/DT safe/JJ way/NN out/RB ./.
We/PRP demonstrate/VBP our/PRP$ method/NN on/IN digital/JJ terrain/NN models/NNS for/IN the/DT task/NN of/IN exploring/VBG an/DT unknown/JJ map/NN with/IN a/DT rover/NN ./.
