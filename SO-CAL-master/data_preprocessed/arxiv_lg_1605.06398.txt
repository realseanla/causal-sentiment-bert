We/PRP consider/VBP convex/NN -/HYPH concave/NN saddle/NN -/HYPH point/NN problems/NNS where/WRB the/DT objective/JJ functions/NNS may/MD be/VB split/VBN in/IN many/JJ components/NNS ,/, and/CC extend/VB recent/JJ stochastic/JJ variance/NN reduction/NN methods/NNS (/-LRB- such/JJ as/IN SVRG/NN or/CC SAGA/NN )/-RRB- to/TO provide/VB the/DT first/JJ large/JJ -/HYPH scale/NN linearly/RB convergent/JJ algorithms/NNS for/IN this/DT class/NN of/IN problems/NNS which/WDT is/VBZ common/JJ in/IN machine/NN learning/NN ./.
While/IN the/DT algorithmic/JJ extension/NN is/VBZ straightforward/JJ ,/, it/PRP comes/VBZ with/IN challenges/NNS and/CC opportunities/NNS :/: (/-LRB- a/LS )/-RRB- the/DT convex/NN minimization/NN analysis/NN does/VBZ not/RB apply/VB and/CC we/PRP use/VBP the/DT notion/NN of/IN monotone/JJ operators/NNS to/TO prove/VB convergence/NN ,/, showing/VBG in/IN particular/JJ that/IN the/DT same/JJ algorithm/NN applies/VBZ to/IN a/DT larger/JJR class/NN of/IN problems/NNS ,/, such/JJ as/IN variational/JJ inequalities/NNS ,/, (/-LRB- b/LS )/-RRB- there/EX are/VBP two/CD notions/NNS of/IN splits/NNS ,/, in/IN terms/NNS of/IN functions/NNS ,/, or/CC in/IN terms/NNS of/IN partial/JJ derivatives/NNS ,/, (/-LRB- c/LS )/-RRB- the/DT split/NN does/VBZ need/VB to/TO be/VB done/VBN with/IN convex/NN -/HYPH concave/NN terms/NNS ,/, (/-LRB- d/LS )/-RRB- non-uniform/JJ sampling/NN is/VBZ key/JJ to/IN an/DT efficient/JJ algorithm/NN ,/, both/CC in/IN theory/NN and/CC practice/NN ,/, and/CC (/-LRB- e/LS )/-RRB- these/DT incremental/JJ algorithms/NNS can/MD be/VB easily/RB accelerated/VBN using/VBG a/DT simple/JJ extension/NN of/IN the/DT "/`` catalyst/NN "/'' framework/NN ,/, leading/VBG to/IN an/DT algorithm/NN which/WDT is/VBZ always/RB superior/JJ to/IN accelerated/VBN batch/NN algorithms/NNS ./.
