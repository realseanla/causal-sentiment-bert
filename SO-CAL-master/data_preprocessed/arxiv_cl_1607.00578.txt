We/PRP first/RB observe/VBP a/DT potential/JJ weakness/NN of/IN continuous/JJ vector/NN representations/NNS of/IN symbols/NNS in/IN neural/JJ machine/NN translation/NN ./.
That/DT is/VBZ ,/, the/DT continuous/JJ vector/NN representation/NN ,/, or/CC a/DT word/NN embedding/NN vector/NN ,/, of/IN a/DT symbol/NN encodes/VBZ multiple/JJ dimensions/NNS of/IN similarity/NN ,/, equivalent/JJ to/IN encoding/VBG more/JJR than/IN one/CD meaning/NN of/IN the/DT word/NN ./.
This/DT has/VBZ the/DT consequence/NN that/IN the/DT encoder/NN and/CC decoder/NN recurrent/JJ networks/NNS in/IN neural/JJ machine/NN translation/NN need/NN to/TO spend/VB substantial/JJ amount/NN of/IN their/PRP$ capacity/NN in/IN disambiguating/VBG source/NN and/CC target/NN words/NNS based/VBN on/IN the/DT context/NN which/WDT is/VBZ defined/VBN by/IN a/DT source/NN sentence/NN ./.
Based/VBN on/IN this/DT observation/NN ,/, in/IN this/DT paper/NN we/PRP propose/VBP to/IN contextualize/VB the/DT word/NN embedding/NN vectors/NNS using/VBG a/DT nonlinear/JJ bag/NN -/HYPH of/IN -/HYPH words/NNS representation/NN of/IN the/DT source/NN sentence/NN ./.
Additionally/RB ,/, we/PRP propose/VBP to/TO represent/VB special/JJ tokens/NNS (/-LRB- such/JJ as/IN numbers/NNS ,/, proper/JJ nouns/NNS and/CC acronyms/NNS )/-RRB- with/IN typed/VBN symbols/NNS to/TO facilitate/VB translating/VBG those/DT words/NNS that/WDT are/VBP not/RB well/RB -/HYPH suited/VBN to/TO be/VB translated/VBN via/IN continuous/JJ vectors/NNS ./.
The/DT experiments/NNS on/IN En/NNP -/HYPH Fr/NNP and/CC En/NNP -/HYPH De/NNP reveal/VBP that/IN the/DT proposed/VBN approaches/NNS of/IN contextualization/NN and/CC symbolization/NN improves/VBZ the/DT translation/NN quality/NN of/IN neural/JJ machine/NN translation/NN systems/NNS significantly/RB ./.
