Even/RB though/IN sequence/NN -/HYPH to/IN -/HYPH sequence/NN neural/JJ machine/NN translation/NN (/-LRB- NMT/NN )/-RRB- model/NN have/VBP achieved/VBN state/NN -/HYPH of/IN -/HYPH art/NN performance/NN in/IN the/DT recent/JJ fewer/JJR years/NNS ,/, but/CC it/PRP is/VBZ widely/RB concerned/JJ that/IN the/DT recurrent/JJ neural/JJ network/NN (/-LRB- RNN/NN )/-RRB- units/NNS are/VBP very/RB hard/JJ to/TO capture/VB the/DT long/JJ -/HYPH distance/NN state/NN information/NN ,/, which/WDT means/VBZ RNN/NN can/MD hardly/RB find/VB the/DT feature/NN with/IN long/JJ term/NN dependency/NN as/IN the/DT sequence/NN becomes/VBZ longer/JJR ./.
Similarly/RB ,/, convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- is/VBZ introduced/VBN into/IN NMT/NN for/IN speeding/VBG recently/RB ,/, however/RB ,/, CNN/NNP focus/VB on/IN capturing/VBG the/DT local/JJ feature/NN of/IN the/DT sequence/NN ;/: To/TO relieve/VB this/DT issue/NN ,/, we/PRP incorporate/VBP a/DT relation/NN network/NN into/IN the/DT standard/JJ encoder/NN -/HYPH decoder/NN framework/NN to/TO enhance/VB information/NN -/HYPH propogation/NN in/IN neural/JJ network/NN ,/, ensuring/VBG that/IN the/DT information/NN of/IN the/DT source/NN sentence/NN can/MD flow/VB into/IN the/DT decoder/NN adequately/RB ./.
Experiments/NNS show/VBP that/IN proposed/VBN framework/NN outperforms/VBZ the/DT statistical/JJ MT/NN model/NN and/CC the/DT state/NN -/HYPH of/IN -/HYPH art/NN NMT/NN model/NN significantly/RB on/IN two/CD data/NNS sets/VBZ with/IN different/JJ scales/NNS ./.
