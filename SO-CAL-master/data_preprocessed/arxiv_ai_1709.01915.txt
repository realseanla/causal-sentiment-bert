Building/NN models/NNS that/WDT take/VBP advantage/NN of/IN the/DT hierarchical/JJ structure/NN of/IN language/NN without/IN a/FW priori/FW annotation/NN is/VBZ a/DT longstanding/JJ goal/NN in/IN natural/JJ language/NN processing/NN ./.
We/PRP introduce/VBP such/PDT a/DT model/NN for/IN the/DT task/NN of/IN machine/NN translation/NN ,/, pairing/VBG a/DT recurrent/JJ neural/JJ network/NN grammar/NN encoder/NN with/IN a/DT novel/JJ attentional/JJ RNNG/NN decoder/NN and/CC applying/VBG policy/NN gradient/NN reinforcement/NN learning/VBG to/TO induce/VB unsupervised/JJ tree/NN structures/NNS on/IN both/CC the/DT source/NN and/CC target/NN ./.
When/WRB trained/VBN on/IN character/NN -/HYPH level/NN datasets/NNS with/IN no/DT explicit/JJ segmentation/NN or/CC parse/VB annotation/NN ,/, the/DT model/NN learns/VBZ a/DT plausible/JJ segmentation/NN and/CC shallow/NN parse/VBP ,/, obtaining/VBG performance/NN close/NN to/IN an/DT attentional/JJ baseline/NN ./.
