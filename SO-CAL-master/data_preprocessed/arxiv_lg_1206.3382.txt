We/PRP consider/VBP online/JJ planning/NN in/IN Markov/NNP decision/NN processes/NNS ./.
An/DT algorithm/NN for/IN this/DT problem/NN should/MD explore/VB the/DT set/NN of/IN possible/JJ policies/NNS from/IN the/DT current/JJ state/NN ,/, and/CC ,/, when/WRB interrupted/VBN ,/, recommend/VBP an/DT action/NN to/TO follow/VB based/VBN on/IN the/DT outcome/NN of/IN the/DT exploration/NN ./.
The/DT performance/NN of/IN such/PDT an/DT algorithm/NN is/VBZ assessed/VBN in/IN terms/NNS of/IN its/PRP$ simple/JJ regret/NN ,/, that/DT is/VBZ the/DT loss/NN in/IN performance/NN resulting/VBG from/IN choosing/VBG the/DT recommended/VBN action/NN instead/RB of/IN an/DT optimal/JJ one/CD ,/, and/CC //HYPH or/CC in/IN terms/NNS of/IN probability/NN that/IN the/DT recommended/VBN action/NN is/VBZ not/RB an/DT optimal/JJ one/NN ./.
The/DT best/JJS guarantees/NNS provided/VBN by/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN algorithms/NNS for/IN reduction/NN of/IN these/DT measures/NNS over/IN time/NN are/VBP only/RB polynomial/JJ ./.
We/PRP introduce/VBP a/DT new/JJ algorithm/NN ,/, BRUE/NNP ,/, that/DT achieves/VBZ over/IN time/NN exponential/JJ reduction/NN of/IN these/DT two/CD measures/NNS ./.
The/DT algorithm/NN is/VBZ based/VBN on/IN a/DT simple/JJ yet/CC non-standard/JJ state/NN -/HYPH space/NN sampling/NN scheme/NN in/IN which/WDT different/JJ samples/NNS are/VBP dedicated/VBN to/IN different/JJ objectives/NNS ./.
Our/PRP$ preliminary/JJ empirical/JJ evaluation/NN shows/VBZ that/IN BRUE/NNP not/RB only/RB provides/VBZ superior/JJ performance/NN guarantees/NNS ,/, but/CC is/VBZ also/RB very/RB effective/JJ in/IN practice/NN and/CC favorably/RB compares/VBZ to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
