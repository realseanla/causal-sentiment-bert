Linguistic/JJ structures/NNS exhibit/VBP a/DT rich/JJ array/NN of/IN global/JJ phenomena/NNS ,/, however/RB commonly/RB used/VBN Markov/NNP models/NNS are/VBP unable/JJ to/TO adequately/RB describe/VB these/DT phenomena/NNS due/IN to/IN their/PRP$ strong/JJ locality/NN assumptions/NNS ./.
We/PRP propose/VBP a/DT novel/JJ hierarchical/JJ model/NN for/IN structured/JJ prediction/NN over/IN sequences/NNS and/CC trees/NNS which/WDT exploits/NNS global/JJ context/NN by/IN conditioning/VBG each/DT generation/NN decision/NN on/IN an/DT unbounded/JJ context/NN of/IN prior/JJ decisions/NNS ./.
This/DT builds/VBZ on/IN the/DT success/NN of/IN Markov/NNP models/NNS but/CC without/IN imposing/VBG a/DT fixed/VBN bound/VBN in/IN order/NN to/TO better/RBR represent/VB global/JJ phenomena/NNS ./.
To/TO facilitate/VB learning/NN of/IN this/DT large/JJ and/CC unbounded/JJ model/NN ,/, we/PRP use/VBP a/DT hierarchical/JJ Pitman/NNP -/HYPH Yor/NNP process/NN prior/JJ which/WDT provides/VBZ a/DT recursive/JJ form/NN of/IN smoothing/VBG ./.
We/PRP propose/VBP prediction/NN algorithms/NNS based/VBN on/IN A/DT */NFP and/CC Markov/NNP Chain/NNP Monte/NNP Carlo/NNP sampling/NN ./.
Empirical/JJ results/NNS demonstrate/VBP the/DT potential/NN of/IN our/PRP$ model/NN compared/VBN to/IN baseline/NN finite/NN -/HYPH context/NN Markov/NNP models/NNS on/IN part/NN -/HYPH of/IN -/HYPH speech/NN tagging/NN and/CC syntactic/JJ parsing/VBG ./.
