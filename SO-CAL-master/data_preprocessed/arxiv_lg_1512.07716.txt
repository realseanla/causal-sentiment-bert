As/IN one/CD of/IN the/DT most/RBS popular/JJ classifiers/NNS ,/, linear/JJ SVMs/NNS still/RB have/VBP challenges/NNS in/IN dealing/VBG with/IN very/RB large/JJ -/HYPH scale/NN problems/NNS ,/, even/RB though/IN linear/JJ or/CC sub-linear/JJ algorithms/NNS have/VBP been/VBN developed/VBN recently/RB on/IN single/JJ machines/NNS ./.
Parallel/JJ computing/NN methods/NNS have/VBP been/VBN developed/VBN for/IN learning/VBG large/JJ -/HYPH scale/NN SVMs/NNS ./.
However/RB ,/, existing/VBG methods/NNS rely/VBP on/IN solving/VBG local/JJ sub-optimization/NN problems/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP develop/VBP a/DT novel/JJ parallel/JJ algorithm/NN for/IN learning/VBG large/JJ -/HYPH scale/NN linear/JJ SVM/NN ./.
Our/PRP$ approach/NN is/VBZ based/VBN on/IN a/DT data/NN augmentation/NN equivalent/JJ formulation/NN ,/, which/WDT casts/VBZ the/DT problem/NN of/IN learning/VBG SVM/NNP as/IN a/DT Bayesian/JJ inference/NN problem/NN ,/, for/IN which/WDT we/PRP can/MD develop/VB very/RB efficient/JJ parallel/JJ sampling/NN methods/NNS ./.
We/PRP provide/VBP empirical/JJ results/NNS for/IN this/DT parallel/JJ sampling/NN SVM/NN ,/, and/CC provide/VB extensions/NNS for/IN SVR/NNP ,/, non-linear/JJ kernels/NNS ,/, and/CC provide/VB a/DT parallel/JJ implementation/NN of/IN the/DT Crammer/NNP and/CC Singer/NNP model/NN ./.
This/DT approach/NN is/VBZ very/RB promising/JJ in/IN its/PRP$ own/JJ right/NN ,/, and/CC further/RB is/VBZ a/DT very/RB useful/JJ technique/NN to/IN parallelize/VB a/DT broader/JJR family/NN of/IN general/JJ maximum/NN -/HYPH margin/NN models/NNS ./.
