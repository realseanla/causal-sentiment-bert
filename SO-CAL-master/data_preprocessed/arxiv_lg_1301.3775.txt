We/PRP present/VBP the/DT discriminative/JJ recurrent/JJ sparse/JJ auto/NN -/HYPH encoder/NN model/NN ,/, which/WDT consists/VBZ of/IN an/DT encoder/NN whose/WP$ hidden/JJ layer/NN is/VBZ recurrent/JJ ,/, and/CC two/CD linear/JJ decoders/NNS ,/, one/CD to/TO reconstruct/VB the/DT input/NN ,/, and/CC one/CD to/TO predict/VB the/DT output/NN ./.
The/DT hidden/JJ layer/NN is/VBZ composed/VBN of/IN rectified/VBN linear/JJ units/NNS (/-LRB- ReLU/NN )/-RRB- and/CC is/VBZ subject/JJ to/IN a/DT sparsity/NN penalty/NN ./.
The/DT network/NN is/VBZ first/JJ trained/VBN in/IN unsupervised/JJ mode/NN to/TO reconstruct/VB the/DT input/NN ,/, and/CC subsequently/RB trained/VBN discriminatively/RB to/TO also/RB produce/VB the/DT desired/VBN output/NN ./.
The/DT recurrent/JJ network/NN is/VBZ time/NN -/HYPH unfolded/VBN with/IN a/DT given/VBN number/NN of/IN iterations/NNS ,/, and/CC trained/VBN using/VBG back/RB -/HYPH propagation/NN through/IN time/NN ./.
In/IN its/PRP$ time/NN -/HYPH unfolded/VBN form/NN ,/, the/DT network/NN can/MD be/VB seen/VBN as/IN a/DT very/RB deep/JJ multi-layer/JJ network/NN in/IN which/WDT the/DT weights/NNS are/VBP shared/VBN between/IN the/DT hidden/JJ layers/NNS ./.
The/DT depth/NN allows/VBZ the/DT system/NN to/TO exhibit/VB all/PDT the/DT power/NN of/IN deep/JJ network/NN while/IN substantially/RB reducing/VBG the/DT number/NN of/IN trainable/JJ parameters/NNS ./.
