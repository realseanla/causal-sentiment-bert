We/PRP present/VBP and/CC evaluate/VBP a/DT new/JJ model/NN for/IN Natural/NNP Language/NNP Generation/NNP (/-LRB- NLG/NNP )/-RRB- in/IN Spoken/VBN Dialogue/NN Systems/NNPS ,/, based/VBN on/IN statistical/JJ planning/NN ,/, given/VBN noisy/JJ feedback/NN from/IN the/DT current/JJ generation/NN context/NN (/-LRB- e.g./FW a/DT user/NN and/CC a/DT surface/NN realiser/NN )/-RRB- ./.
We/PRP study/VBP its/PRP$ use/NN in/IN a/DT standard/JJ NLG/NNP problem/NN :/: how/WRB to/TO present/VB information/NN (/-LRB- in/IN this/DT case/NN a/DT set/NN of/IN search/NN results/NNS )/-RRB- to/IN users/NNS ,/, given/VBN the/DT complex/JJ trade/NN -/HYPH offs/NNS between/IN utterance/NN length/NN ,/, amount/NN of/IN information/NN conveyed/VBD ,/, and/CC cognitive/JJ load/NN ./.
We/PRP set/VBD these/DT trade/NN -/HYPH offs/NNS by/IN analysing/VBG existing/VBG MATCH/NNP data/NNS ./.
We/PRP then/RB train/VB a/DT NLG/NN pol/NN -/HYPH icy/NN using/VBG Reinforcement/NN Learning/NN (/-LRB- RL/NN )/-RRB- ,/, which/WDT adapts/VBZ its/PRP$ behaviour/NN to/IN noisy/JJ feed/NN -/HYPH back/NN from/IN the/DT current/JJ generation/NN context/NN ./.
This/DT policy/NN is/VBZ compared/VBN to/IN several/JJ base/NN -/HYPH lines/NNS derived/VBN from/IN previous/JJ work/NN in/IN this/DT area/NN ./.
The/DT learned/VBN policy/NN significantly/RB out/RB -/HYPH performs/VBZ all/PDT the/DT prior/JJ approaches/NNS ./.
