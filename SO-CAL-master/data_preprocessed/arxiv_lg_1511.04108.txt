In/IN this/DT paper/NN ,/, we/PRP apply/VBP a/DT general/JJ deep/JJ learning/NN (/-LRB- DL/NN )/-RRB- framework/NN for/IN the/DT answer/NN selection/NN task/NN ,/, which/WDT does/VBZ not/RB depend/VB on/IN manually/RB defined/VBN features/NNS or/CC linguistic/JJ tools/NNS ./.
The/DT basic/JJ framework/NN is/VBZ to/TO build/VB the/DT embeddings/NNS of/IN questions/NNS and/CC answers/NNS based/VBN on/IN bidirectional/JJ long/JJ short/JJ -/HYPH term/NN memory/NN (/-LRB- biLSTM/NN )/-RRB- models/NNS ,/, and/CC measure/VB their/PRP$ closeness/NN by/IN cosine/NN similarity/NN ./.
We/PRP further/RB extend/VBP this/DT basic/JJ model/NN in/IN two/CD directions/NNS ./.
One/CD is/VBZ to/TO define/VB a/DT more/RBR composite/JJ representation/NN for/IN questions/NNS and/CC answers/NNS by/IN combining/VBG convolutional/JJ neural/JJ network/NN with/IN the/DT basic/JJ framework/NN ,/, the/DT other/JJ is/VBZ to/TO utilize/VB a/DT simple/JJ but/CC efficient/JJ attention/NN mechanism/NN in/IN order/NN to/TO generate/VB the/DT answer/NN representation/NN according/VBG to/IN the/DT question/NN context/NN ./.
Several/JJ variations/NNS of/IN models/NNS are/VBP provided/VBN ./.
Experimental/JJ results/NNS on/IN a/DT public/JJ insurance/NN -/HYPH domain/NN dataset/NN demonstrate/VBP that/IN the/DT extended/JJ models/NNS substantially/RB outperform/VBP two/CD state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN non-DL/JJ baselines/NNS and/CC a/DT strong/JJ DL/NN baseline/NN ./.
