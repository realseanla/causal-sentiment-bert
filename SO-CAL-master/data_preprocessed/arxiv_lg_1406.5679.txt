We/PRP introduce/VBP a/DT model/NN for/IN bidirectional/JJ retrieval/NN of/IN images/NNS and/CC sentences/NNS through/IN a/DT multi-modal/JJ embedding/NN of/IN visual/JJ and/CC natural/JJ language/NN data/NNS ./.
Unlike/IN previous/JJ models/NNS that/WDT directly/RB map/VBP images/NNS or/CC sentences/NNS into/IN a/DT common/JJ embedding/NN space/NN ,/, our/PRP$ model/NN works/VBZ on/IN a/DT finer/JJR level/NN and/CC embeds/VBZ fragments/NNS of/IN images/NNS (/-LRB- objects/NNS )/-RRB- and/CC fragments/NNS of/IN sentences/NNS (/-LRB- typed/VBN dependency/NN tree/NN relations/NNS )/-RRB- into/IN a/DT common/JJ space/NN ./.
In/IN addition/NN to/IN a/DT ranking/VBG objective/NN seen/VBN in/IN previous/JJ work/NN ,/, this/DT allows/VBZ us/PRP to/TO add/VB a/DT new/JJ fragment/NN alignment/NN objective/NN that/WDT learns/VBZ to/TO directly/RB associate/VB these/DT fragments/NNS across/IN modalities/NNS ./.
Extensive/JJ experimental/JJ evaluation/NN shows/VBZ that/IN reasoning/NN on/IN both/CC the/DT global/JJ level/NN of/IN images/NNS and/CC sentences/NNS and/CC the/DT finer/JJR level/NN of/IN their/PRP$ respective/JJ fragments/NNS significantly/RB improves/VBZ performance/NN on/IN image/NN -/HYPH sentence/NN retrieval/NN tasks/NNS ./.
Additionally/RB ,/, our/PRP$ model/NN provides/VBZ interpretable/JJ predictions/NNS since/IN the/DT inferred/VBN inter-modal/JJ fragment/NN alignment/NN is/VBZ explicit/JJ ./.
