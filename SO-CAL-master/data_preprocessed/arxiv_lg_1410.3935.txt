Conditional/JJ random/JJ fields/NNS (/-LRB- CRFs/NNS )/-RRB- are/VBP usually/RB specified/VBN by/IN graphical/JJ models/NNS but/CC in/IN this/DT paper/NN we/PRP propose/VBP to/TO use/VB probabilistic/JJ logic/NN programs/NNS and/CC specify/VB them/PRP generatively/RB ./.
Our/PRP$ intension/NN is/VBZ first/JJ to/TO provide/VB a/DT unified/JJ approach/NN to/IN CRFs/NNS for/IN complex/JJ modeling/NN through/IN the/DT use/NN of/IN a/DT Turing/NNP complete/JJ language/NN and/CC second/JJ to/TO offer/VB a/DT convenient/JJ way/NN of/IN realizing/VBG generative/JJ -/HYPH discriminative/JJ pairs/NNS in/IN machine/NN learning/NN to/TO compare/VB generative/JJ and/CC discriminative/JJ models/NNS and/CC choose/VB the/DT best/JJS model/NN ./.
We/PRP implemented/VBD our/PRP$ approach/NN as/IN the/DT D/NN -/HYPH PRISM/NN language/NN by/IN modifying/VBG PRISM/NN ,/, a/DT logic/NN -/HYPH based/VBN probabilistic/JJ modeling/NN language/NN for/IN generative/JJ modeling/NN ,/, while/IN exploiting/VBG its/PRP$ dynamic/JJ programming/NN mechanism/NN for/IN efficient/JJ probability/NN computation/NN ./.
We/PRP tested/VBD D/NN -/HYPH PRISM/NN with/IN logistic/JJ regression/NN ,/, a/DT linear/JJ -/HYPH chain/NN CRF/NNP and/CC a/DT CRF/NNP -/HYPH CFG/NNP and/CC empirically/RB confirmed/VBD their/PRP$ excellent/JJ discriminative/JJ performance/NN compared/VBN to/IN their/PRP$ generative/JJ counterparts/NNS ,/, i.e./FW \/SYM naive/JJ Bayes/NNP ,/, an/DT HMM/NNP and/CC a/DT PCFG/NN ./.
We/PRP also/RB introduced/VBD new/JJ CRF/NNP models/NNS ,/, CRF/NNP -/HYPH BNCs/NNPS and/CC CRF/NNP -/HYPH LCGs/NNP ./.
They/PRP are/VBP CRF/NNP versions/NNS of/IN Bayesian/JJ network/NN classifiers/NNS and/CC probabilistic/JJ left/JJ -/HYPH corner/NN grammars/NNS respectively/RB and/CC easily/RB implementable/JJ in/IN D/NN -/HYPH PRISM/NN ./.
We/PRP empirically/RB showed/VBD that/IN they/PRP outperform/VBP their/PRP$ generative/JJ counterparts/NNS as/IN expected/VBN ./.
