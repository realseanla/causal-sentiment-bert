We/PRP present/VBP an/DT algorithm/NN for/IN minimizing/VBG a/DT sum/NN of/IN functions/NNS that/WDT combines/VBZ the/DT computational/JJ efficiency/NN of/IN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- with/IN the/DT second/JJ order/NN curvature/NN information/NN accessible/JJ by/IN quasi-Newton/NNP methods/NNS ./.
We/PRP unify/VBP these/DT disparate/JJ approaches/NNS by/IN maintaining/VBG an/DT independent/JJ Hessian/JJ approximation/NN for/IN each/DT contributing/VBG function/NN in/IN the/DT sum/NN ./.
We/PRP maintain/VBP computational/JJ tractability/NN even/RB for/IN high/JJ dimensional/JJ optimization/NN problems/NNS by/IN developing/VBG an/DT adaptive/JJ scheme/NN to/TO store/VB and/CC manipulate/VB these/DT quadratic/JJ approximations/NNS in/IN a/DT shared/VBN ,/, time/NN evolving/VBG low/JJ dimensional/JJ subspace/NN ,/, determined/VBN by/IN the/DT recent/JJ history/NN of/IN gradient/NN evaluations/NNS ./.
This/DT algorithm/NN contrasts/VBZ with/IN earlier/JJR stochastic/JJ second/JJ order/NN techniques/NNS ,/, which/WDT treat/VBP the/DT Hessian/NNP of/IN each/DT contributing/VBG function/NN only/RB as/IN a/DT noisy/JJ approximation/NN to/IN the/DT full/JJ Hessian/JJ ,/, rather/RB than/IN as/IN a/DT target/NN for/IN direct/JJ estimation/NN ./.
Our/PRP$ approach/NN reaps/VBZ the/DT benefits/NNS of/IN both/DT SGD/NNP and/CC quasi-Newton/NNP methods/NNS ;/, each/DT update/NN step/NN requires/VBZ only/RB a/DT single/JJ subfunction/NN evaluation/NN (/-LRB- like/IN SGD/NNP but/CC unlike/IN previous/JJ stochastic/JJ second/JJ order/NN methods/NNS )/-RRB- ,/, while/IN little/JJ to/IN no/DT adjustment/NN of/IN hyperparameters/NNS is/VBZ required/VBN (/-LRB- as/RB is/VBZ typical/JJ for/IN quasi-Newton/JJ methods/NNS but/CC not/RB for/IN SGD/NNP )/-RRB- ./.
For/IN convex/NN problems/NNS the/DT convergence/NN rate/NN of/IN the/DT proposed/VBN technique/NN is/VBZ at/IN least/JJS linear/JJ ./.
We/PRP demonstrate/VBP improved/VBN convergence/NN on/IN five/CD diverse/JJ optimization/NN problems/NNS ./.
