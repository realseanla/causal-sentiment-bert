We/PRP propose/VBP and/CC analyze/VBP a/DT novel/JJ framework/NN for/IN learning/VBG sparse/JJ representations/NNS ,/, based/VBN on/IN two/CD statistical/JJ techniques/NNS :/: kernel/NN smoothing/NN and/CC marginal/JJ regression/NN ./.
The/DT proposed/VBN approach/NN provides/VBZ a/DT flexible/JJ framework/NN for/IN incorporating/VBG feature/NN similarity/NN or/CC temporal/JJ information/NN present/NN in/IN data/NNS sets/NNS ,/, via/IN non-parametric/JJ kernel/NN smoothing/NN ./.
We/PRP provide/VBP generalization/NN bounds/NNS for/IN dictionary/NN learning/NN using/VBG smooth/JJ sparse/JJ coding/NN and/CC show/VB how/WRB the/DT sample/NN complexity/NN depends/VBZ on/IN the/DT L1/NN norm/NN of/IN kernel/NN function/NN used/VBN ./.
Furthermore/RB ,/, we/PRP propose/VBP using/VBG marginal/JJ regression/NN for/IN obtaining/VBG sparse/JJ codes/NNS ,/, which/WDT significantly/RB improves/VBZ the/DT speed/NN and/CC allows/VBZ one/CD to/TO scale/VB to/IN large/JJ dictionary/JJ sizes/NNS easily/RB ./.
We/PRP demonstrate/VBP the/DT advantages/NNS of/IN the/DT proposed/VBN approach/NN ,/, both/CC in/IN terms/NNS of/IN accuracy/NN and/CC speed/NN by/IN extensive/JJ experimentation/NN on/IN several/JJ real/JJ data/NNS sets/NNS ./.
In/IN addition/NN ,/, we/PRP demonstrate/VBP how/WRB the/DT proposed/VBN approach/NN could/MD be/VB used/VBN for/IN improving/VBG semi-supervised/VBN sparse/JJ coding/NN ./.
