We/PRP propose/VBP Novel/JJ Object/NN Captioner/NNP (/-LRB- NOC/NNP )/-RRB- ,/, a/DT deep/JJ visual/JJ semantic/JJ captioning/NN model/NN that/WDT can/MD describe/VB a/DT large/JJ number/NN of/IN object/NN categories/NNS not/RB present/JJ in/IN existing/VBG image/NN -/HYPH caption/NN datasets/NNS ./.
Recent/JJ captioning/NN models/NNS are/VBP limited/VBN in/IN their/PRP$ ability/NN to/TO scale/VB and/CC describe/VB concepts/NNS outside/IN of/IN paired/VBN image/NN -/HYPH text/NN corpora/NNS ./.
Our/PRP$ model/NN takes/VBZ advantage/NN of/IN external/JJ sources/NNS -/HYPH labeled/VBN images/NNS from/IN object/NN recognition/NN datasets/NNS ,/, and/CC semantic/JJ knowledge/NN extracted/VBN from/IN unannotated/JJ text/NN -/HYPH and/CC combines/VBZ them/PRP to/TO generate/VB descriptions/NNS about/IN novel/JJ objects/NNS ./.
We/PRP propose/VBP minimizing/VBG a/DT joint/JJ objective/NN which/WDT can/MD learn/VB from/IN diverse/JJ data/NNS sources/NNS and/CC leverage/NN distributional/JJ semantic/JJ embeddings/NNS ,/, enabling/VBG the/DT model/NN to/TO generalize/VB and/CC describe/VB novel/JJ objects/NNS outside/IN of/IN image/NN -/HYPH caption/NN datasets/NNS ./.
We/PRP demonstrate/VBP that/IN our/PRP$ model/NN exploits/NNS semantic/JJ information/NN to/TO generate/VB captions/NNS for/IN hundreds/NNS of/IN object/NN categories/NNS in/IN the/DT ImageNet/NNP object/NN recognition/NN dataset/NN that/WDT are/VBP not/RB observed/VBN in/IN image/NN -/HYPH caption/NN training/NN data/NNS ,/, as/RB well/RB as/IN many/JJ categories/NNS that/WDT are/VBP observed/VBN very/RB rarely/RB ./.
