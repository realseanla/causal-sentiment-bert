We/PRP consider/VBP the/DT least/RBS -/HYPH square/JJ linear/JJ regression/NN problem/NN with/IN regularization/NN by/IN the/DT l1/NN -/HYPH norm/NN ,/, a/DT problem/NN usually/RB referred/VBN to/IN as/IN the/DT Lasso/NNP ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT detailed/JJ asymptotic/JJ analysis/NN of/IN model/NN consistency/NN of/IN the/DT Lasso/NNP ./.
For/IN various/JJ decays/NNS of/IN the/DT regularization/NN parameter/NN ,/, we/PRP compute/VBP asymptotic/JJ equivalents/NNS of/IN the/DT probability/NN of/IN correct/JJ model/NN selection/NN (/-LRB- i.e./FW ,/, variable/JJ selection/NN )/-RRB- ./.
For/IN a/DT specific/JJ rate/NN decay/NN ,/, we/PRP show/VBP that/IN the/DT Lasso/NNP selects/VBZ all/PDT the/DT variables/NNS that/WDT should/MD enter/VB the/DT model/NN with/IN probability/NN tending/VBG to/IN one/CD exponentially/RB fast/RB ,/, while/IN it/PRP selects/VBZ all/DT other/JJ variables/NNS with/IN strictly/RB positive/JJ probability/NN ./.
We/PRP show/VBP that/IN this/DT property/NN implies/VBZ that/IN if/IN we/PRP run/VBP the/DT Lasso/NNP for/IN several/JJ bootstrapped/VBN replications/NNS of/IN a/DT given/VBN sample/NN ,/, then/RB intersecting/VBG the/DT supports/NNS of/IN the/DT Lasso/NNP bootstrap/NN estimates/VBZ leads/VBZ to/IN consistent/JJ model/NN selection/NN ./.
This/DT novel/JJ variable/JJ selection/NN algorithm/NN ,/, referred/VBN to/IN as/IN the/DT Bolasso/NNP ,/, is/VBZ compared/VBN favorably/RB to/IN other/JJ linear/JJ regression/NN methods/NNS on/IN synthetic/JJ data/NNS and/CC datasets/NNS from/IN the/DT UCI/NNP machine/NN learning/NN repository/NN ./.
