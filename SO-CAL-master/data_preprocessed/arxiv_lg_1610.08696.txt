We/PRP consider/VBP a/DT transfer/NN -/HYPH learning/NN problem/NN by/IN using/VBG the/DT parameter/NN transfer/NN approach/NN ,/, where/WRB a/DT suitable/JJ parameter/NN of/IN feature/NN mapping/NN is/VBZ learned/VBN through/IN one/CD task/NN and/CC applied/VBD to/IN another/DT objective/JJ task/NN ./.
Then/RB ,/, we/PRP introduce/VBP the/DT notion/NN of/IN the/DT local/JJ stability/NN of/IN parametric/JJ feature/NN mapping/NN and/CC parameter/NN transfer/NN learnability/NN ,/, and/CC thereby/RB derive/VBP a/DT learning/NN bound/VBN for/IN parameter/NN transfer/NN algorithms/NNS ./.
As/IN an/DT application/NN of/IN parameter/NN transfer/NN learning/NN ,/, we/PRP discuss/VBP the/DT performance/NN of/IN sparse/JJ coding/NN in/IN self/NN -/HYPH taught/VBN learning/NN ./.
Although/IN self/NN -/HYPH taught/VBN learning/NN algorithms/NNS with/IN plentiful/JJ unlabeled/JJ data/NNS often/RB show/VBP excellent/JJ empirical/JJ performance/NN ,/, their/PRP$ theoretical/JJ analysis/NN has/VBZ not/RB been/VBN studied/VBN ./.
In/IN this/DT paper/NN ,/, we/PRP also/RB provide/VBP the/DT first/JJ theoretical/JJ learning/NN bound/VBN for/IN self/NN -/HYPH taught/VBN learning/NN ./.
