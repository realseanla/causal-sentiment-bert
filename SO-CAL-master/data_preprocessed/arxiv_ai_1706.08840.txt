One/CD major/JJ obstacle/NN towards/IN artificial/JJ intelligence/NN is/VBZ the/DT poor/JJ ability/NN of/IN models/NNS to/TO quickly/RB solve/VB new/JJ problems/NNS ,/, without/IN forgetting/VBG previously/RB acquired/VBN knowledge/NN ./.
To/TO better/RBR understand/VB this/DT issue/NN ,/, we/PRP study/VBP the/DT problem/NN of/IN learning/NN over/IN a/DT continuum/NN of/IN data/NNS ,/, where/WRB the/DT model/NN observes/VBZ ,/, once/RB and/CC one/CD by/IN one/CD ,/, examples/NNS concerning/VBG an/DT ordered/VBN sequence/NN of/IN tasks/NNS ./.
First/RB ,/, we/PRP propose/VBP a/DT set/NN of/IN metrics/NNS to/TO evaluate/VB models/NNS learning/VBG over/IN a/DT continuum/NN of/IN data/NNS ./.
These/DT metrics/NNS characterize/VBP models/NNS not/RB only/RB by/IN their/PRP$ test/NN accuracy/NN ,/, but/CC also/RB in/IN terms/NNS of/IN their/PRP$ ability/NN to/TO transfer/VB knowledge/NN across/IN tasks/NNS ./.
Second/RB ,/, we/PRP propose/VBP a/DT model/NN to/TO learn/VB over/IN continuums/NNS of/IN data/NNS ,/, called/VBN Gradient/NN of/IN Episodic/JJ Memory/NN (/-LRB- GEM/NN )/-RRB- ,/, which/WDT alleviates/VBZ forgetting/VBG while/IN allowing/VBG beneficial/JJ transfer/NN of/IN knowledge/NN to/IN previous/JJ tasks/NNS ./.
Our/PRP$ experiments/NNS on/IN variants/NNS of/IN the/DT MNIST/NN and/CC CIFAR/NN -/HYPH 100/CD datasets/NNS demonstrate/VBP the/DT strong/JJ performance/NN of/IN GEM/NN when/WRB compared/VBN to/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN ./.
