We/PRP study/VBP exploration/NN in/IN Multi-Armed/NNP Bandits/NNPS in/IN a/DT setting/NN where/WRB $/$ k/CD $/$ players/NNS collaborate/VBP in/IN order/NN to/TO identify/VB an/DT $/$ \/CD epsilon/CD $/$ -/HYPH optimal/JJ arm/NN ./.
Our/PRP$ motivation/NN comes/VBZ from/IN recent/JJ employment/NN of/IN bandit/NN algorithms/NNS in/IN computationally/RB intensive/JJ ,/, large/JJ -/HYPH scale/NN applications/NNS ./.
Our/PRP$ results/NNS demonstrate/VBP a/DT non-trivial/JJ tradeoff/NN between/IN the/DT number/NN of/IN arm/NN pulls/VBZ required/VBN by/IN each/DT of/IN the/DT players/NNS ,/, and/CC the/DT amount/NN of/IN communication/NN between/IN them/PRP ./.
In/IN particular/JJ ,/, our/PRP$ main/JJ result/NN shows/VBZ that/IN by/IN allowing/VBG the/DT $/$ k/CD $/$ players/NNS to/TO communicate/VB only/RB once/RB ,/, they/PRP are/VBP able/JJ to/TO learn/VB $/$ \/CD sqrt/CD {/-LRB- k/NN }/-RRB- $/$ times/NNS faster/JJR than/IN a/DT single/JJ player/NN ./.
That/DT is/VBZ ,/, distributing/VBG learning/NN to/IN $/$ k/CD $/$ players/NNS gives/VBZ rise/NN to/IN a/DT factor/NN $/$ \/CD sqrt/CD {/-LRB- k/NN }/-RRB- $/$ parallel/JJ speed/NN -/HYPH up/NN ./.
We/PRP complement/VBP this/DT result/NN with/IN a/DT lower/JJR bound/JJ showing/VBG this/DT is/VBZ in/IN general/JJ the/DT best/JJS possible/JJ ./.
On/IN the/DT other/JJ extreme/JJ ,/, we/PRP present/VBP an/DT algorithm/NN that/WDT achieves/VBZ the/DT ideal/JJ factor/NN $/$ k/CD $/$ speed/NN -/HYPH up/NN in/IN learning/NN performance/NN ,/, with/IN communication/NN only/RB logarithmic/JJ in/IN $/$ 1/CD //SYM \/SYM epsilon/SYM $/$ ./.
