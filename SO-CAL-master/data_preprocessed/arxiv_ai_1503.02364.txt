We/PRP propose/VBP Neural/JJ Responding/VBG Machine/NN (/-LRB- NRM/NNP )/-RRB- ,/, a/DT neural/JJ network/NN -/HYPH based/VBN response/NN generator/NN for/IN Short/JJ -/HYPH Text/VB Conversation/NN ./.
NRM/NNP takes/VBZ the/DT general/JJ encoder/NN -/HYPH decoder/NN framework/NN :/: it/PRP formalizes/VBZ the/DT generation/NN of/IN response/NN as/IN a/DT decoding/NN process/NN based/VBN on/IN the/DT latent/JJ representation/NN of/IN the/DT input/NN text/NN ,/, while/IN both/CC encoding/VBG and/CC decoding/VBG are/VBP realized/VBN with/IN recurrent/JJ neural/JJ networks/NNS (/-LRB- RNN/NN )/-RRB- ./.
The/DT NRM/NNP is/VBZ trained/VBN with/IN a/DT large/JJ amount/NN of/IN one/CD -/HYPH round/NN conversation/NN data/NNS collected/VBN from/IN a/DT microblogging/NN service/NN ./.
Empirical/JJ study/NN shows/VBZ that/IN NRM/NNP can/MD generate/VB grammatically/RB correct/JJ and/CC content-wise/JJ appropriate/JJ responses/NNS to/IN over/IN 75/CD percent/NN of/IN the/DT input/NN text/NN ,/, outperforming/VBG state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH arts/NNS in/IN the/DT same/JJ setting/NN ,/, including/VBG retrieval/NN -/HYPH based/VBN and/CC SMT/NN -/HYPH based/VBN models/NNS ./.
