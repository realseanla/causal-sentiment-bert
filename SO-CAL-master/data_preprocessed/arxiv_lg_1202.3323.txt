We/PRP investigate/VBP extensions/NNS of/IN well/RB -/HYPH known/VBN online/JJ learning/NN algorithms/NNS such/JJ as/IN fixed/VBN -/HYPH share/NN of/IN Herbster/NNP and/CC Warmuth/NNP (/-LRB- 1998/CD )/-RRB- or/CC the/DT methods/NNS proposed/VBN by/IN Bousquet/NNP and/CC Warmuth/NNP (/-LRB- 2002/CD )/-RRB- ./.
These/DT algorithms/NNS use/VBP weight/NN sharing/NN schemes/NNS to/TO perform/VB as/RB well/RB as/IN the/DT best/JJS sequence/NN of/IN experts/NNS with/IN a/DT limited/JJ number/NN of/IN changes/NNS ./.
Here/RB we/PRP show/VBP ,/, with/IN a/DT common/JJ ,/, general/JJ ,/, and/CC simpler/JJR analysis/NN ,/, that/IN weight/NN sharing/NN in/IN fact/NN achieves/VBZ much/RB more/JJR than/IN what/WP it/PRP was/VBD designed/VBN for/IN ./.
We/PRP use/VBP it/PRP to/IN simultaneously/RB prove/VB new/JJ shifting/VBG regret/NN bounds/NNS for/IN online/JJ convex/NN optimization/NN on/IN the/DT simplex/NN in/IN terms/NNS of/IN the/DT total/JJ variation/NN distance/NN as/RB well/RB as/IN new/JJ bounds/NNS for/IN the/DT related/JJ setting/NN of/IN adaptive/JJ regret/NN ./.
Finally/RB ,/, we/PRP exhibit/VBP the/DT first/JJ logarithmic/JJ shifting/NN bounds/NNS for/IN exp/NN -/HYPH concave/NN loss/NN functions/VBZ on/IN the/DT simplex/NN ./.
