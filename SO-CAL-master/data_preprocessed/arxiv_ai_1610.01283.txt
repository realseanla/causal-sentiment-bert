Sample/NN complexity/NN and/CC safety/NN are/VBP major/JJ challenges/NNS when/WRB learning/VBG policies/NNS with/IN reinforcement/NN learning/VBG for/IN real/JJ -/HYPH world/NN tasks/NNS --/: especially/RB when/WRB the/DT policies/NNS are/VBP represented/VBN using/VBG rich/JJ function/NN approximators/NNS like/IN deep/JJ neural/JJ networks/NNS ./.
Model/NN -/HYPH based/VBN methods/NNS where/WRB the/DT real/JJ -/HYPH world/NN target/NN domain/NN is/VBZ approximated/VBN using/VBG a/DT simulated/VBN source/NN domain/NN provide/VBP an/DT avenue/NN to/TO tackle/VB the/DT above/JJ challenges/NNS by/IN augmenting/VBG real/JJ data/NNS with/IN simulated/VBN data/NNS ./.
However/RB ,/, discrepancies/NNS between/IN the/DT simulated/VBN source/NN domain/NN and/CC the/DT target/NN domain/NN pose/VBP a/DT challenge/NN for/IN simulated/JJ training/NN ./.
We/PRP introduce/VBP the/DT EPOpt/JJ algorithm/NN ,/, which/WDT uses/VBZ an/DT ensemble/NN of/IN simulated/VBN source/NN domains/NNS and/CC a/DT form/NN of/IN adversarial/JJ training/NN to/TO learn/VB policies/NNS that/WDT are/VBP robust/JJ and/CC generalize/VB to/IN a/DT broad/JJ range/NN of/IN possible/JJ target/NN domains/NNS ,/, including/VBG to/IN unmodeled/JJ effects/NNS ./.
Further/RB ,/, the/DT probability/NN distribution/NN over/IN source/NN domains/NNS in/IN the/DT ensemble/NN can/MD be/VB adapted/VBN using/VBG data/NNS from/IN target/NN domain/NN and/CC approximate/JJ Bayesian/JJ methods/NNS ,/, to/TO progressively/RB make/VB it/PRP a/DT better/JJR approximation/NN ./.
Thus/RB ,/, learning/VBG on/IN a/DT model/NN ensemble/NN ,/, along/IN with/IN source/NN domain/NN adaptation/NN ,/, provides/VBZ the/DT benefit/NN of/IN both/DT robustness/NN and/CC learning/NN //HYPH adaptation/NN ./.
