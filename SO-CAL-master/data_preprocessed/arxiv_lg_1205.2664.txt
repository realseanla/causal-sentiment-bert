We/PRP present/VBP a/DT modular/JJ approach/NN to/IN reinforcement/NN learning/VBG that/IN uses/VBZ a/DT Bayesian/JJ representation/NN of/IN the/DT uncertainty/NN over/IN models/NNS ./.
The/DT approach/NN ,/, BOSS/NN (/-LRB- Best/JJS of/IN Sampled/NNP Set/NNP )/-RRB- ,/, drives/VBZ exploration/NN by/IN sampling/VBG multiple/JJ models/NNS from/IN the/DT posterior/JJ and/CC selecting/VBG actions/NNS optimistically/RB ./.
It/PRP extends/VBZ previous/JJ work/NN by/IN providing/VBG a/DT rule/NN for/IN deciding/VBG when/WRB to/TO resample/VB and/CC how/WRB to/TO combine/VB the/DT models/NNS ./.
We/PRP show/VBP that/IN our/PRP$ algorithm/NN achieves/VBZ nearoptimal/JJ reward/NN with/IN high/JJ probability/NN with/IN a/DT sample/NN complexity/NN that/WDT is/VBZ low/JJ relative/JJ to/IN the/DT speed/NN at/IN which/WDT the/DT posterior/JJ distribution/NN converges/VBZ during/IN learning/NN ./.
We/PRP demonstrate/VBP that/IN BOSS/NN performs/VBZ quite/RB favorably/RB compared/VBN to/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN reinforcement/NN -/HYPH learning/VBG approaches/NNS and/CC illustrate/VB its/PRP$ flexibility/NN by/IN pairing/VBG it/PRP with/IN a/DT non-parametric/JJ model/NN that/WDT generalizes/VBZ across/IN states/NNS ./.
