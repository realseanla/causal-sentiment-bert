Graphical/JJ models/NNS are/VBP usually/RB learned/VBN without/IN regard/NN to/IN the/DT cost/NN of/IN doing/VBG inference/NN with/IN them/PRP ./.
As/IN a/DT result/NN ,/, even/RB if/IN a/DT good/JJ model/NN is/VBZ learned/VBN ,/, it/PRP may/MD perform/VB poorly/RB at/IN prediction/NN ,/, because/IN it/PRP requires/VBZ approximate/JJ inference/NN ./.
We/PRP propose/VBP an/DT alternative/NN :/: learning/NN models/NNS with/IN a/DT score/NN function/NN that/WDT directly/RB penalizes/VBZ the/DT cost/NN of/IN inference/NN ./.
Specifically/RB ,/, we/PRP learn/VBP arithmetic/NN circuits/NNS with/IN a/DT penalty/NN on/IN the/DT number/NN of/IN edges/NNS in/IN the/DT circuit/NN (/-LRB- in/IN which/WDT the/DT cost/NN of/IN inference/NN is/VBZ linear/JJ )/-RRB- ./.
Our/PRP$ algorithm/NN is/VBZ equivalent/JJ to/IN learning/VBG a/DT Bayesian/JJ network/NN with/IN context/NN -/HYPH specific/JJ independence/NN by/IN greedily/RB splitting/VBG conditional/JJ distributions/NNS ,/, at/IN each/DT step/NN scoring/VBG the/DT candidates/NNS by/IN compiling/VBG the/DT resulting/VBG network/NN into/IN an/DT arithmetic/NN circuit/NN ,/, and/CC using/VBG its/PRP$ size/NN as/IN the/DT penalty/NN ./.
We/PRP show/VBP how/WRB this/DT can/MD be/VB done/VBN efficiently/RB ,/, without/IN compiling/VBG a/DT circuit/NN from/IN scratch/NN for/IN each/DT candidate/NN ./.
Experiments/NNS on/IN several/JJ real/JJ -/HYPH world/NN domains/NNS show/VBP that/IN our/PRP$ algorithm/NN is/VBZ able/JJ to/TO learn/VB tractable/JJ models/NNS with/IN very/RB large/JJ treewidth/NN ,/, and/CC yields/VBZ more/RBR accurate/JJ predictions/NNS than/IN a/DT standard/JJ context/NN -/HYPH specific/JJ Bayesian/JJ network/NN learner/NN ,/, in/IN far/RB less/JJR time/NN ./.
