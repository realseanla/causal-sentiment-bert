Recent/JJ works/NNS using/VBG artificial/JJ neural/JJ networks/NNS based/VBN on/IN word/NN distributed/VBN representation/NN greatly/RB boost/VB the/DT performance/NN of/IN various/JJ natural/JJ language/NN learning/NN tasks/NNS ,/, especially/RB question/VBP answering/VBG ./.
Though/IN ,/, they/PRP also/RB carry/VBP along/RP with/IN some/DT attendant/NN problems/NNS ,/, such/JJ as/IN corpus/NN selection/NN for/IN embedding/VBG learning/NN ,/, dictionary/NN transformation/NN for/IN different/JJ learning/NN tasks/NNS ,/, etc/FW ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/IN straightforwardly/RB model/JJ sentences/NNS by/IN means/NNS of/IN character/NN sequences/NNS ,/, and/CC then/RB utilize/VB convolutional/JJ neural/JJ networks/NNS to/TO integrate/VB character/NN embedding/NN learning/VBG together/RB with/IN point-wise/JJ answer/NN selection/NN training/NN ./.
Compared/VBN with/IN deep/JJ models/NNS pre-trained/VBN on/IN word/NN embedding/NN (/-LRB- WE/PRP )/-RRB- strategy/NN ,/, our/PRP$ character/NN -/HYPH sequential/JJ representation/NN (/-LRB- CSR/NNP )/-RRB- based/VBN method/NN shows/VBZ a/DT much/JJ simpler/JJR procedure/NN and/CC more/RBR stable/JJ performance/NN across/IN different/JJ benchmarks/NNS ./.
Extensive/JJ experiments/NNS on/IN two/CD benchmark/NN answer/NN selection/NN datasets/NNS exhibit/VBP the/DT competitive/JJ performance/NN compared/VBN with/IN the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN methods/NNS ./.
