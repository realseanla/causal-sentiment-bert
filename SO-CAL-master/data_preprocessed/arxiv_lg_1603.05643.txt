We/PRP consider/VBP the/DT fundamental/JJ problem/NN in/IN non-convex/JJ optimization/NN of/IN efficiently/RB reaching/VBG a/DT stationary/JJ point/NN ./.
In/IN contrast/NN to/IN the/DT convex/NN case/NN ,/, in/IN the/DT long/JJ history/NN of/IN this/DT basic/JJ problem/NN ,/, the/DT only/RB known/VBN theoretical/JJ results/NNS on/IN first/JJ -/HYPH order/NN non-convex/JJ optimization/NN remain/VBP to/TO be/VB full/JJ gradient/NN descent/NN that/WDT converges/VBZ in/IN $/$ O/UH (/-LRB- 1/CD //SYM \/SYM varepsilon/SYM )/-RRB- $/$ iterations/NNS for/IN smooth/JJ objectives/NNS ,/, and/CC stochastic/JJ gradient/NN descent/NN that/WDT converges/VBZ in/IN $/$ O/UH (/-LRB- 1/CD //SYM \/SYM varepsilon/SYM ^/SYM 2/CD )/-RRB- $/$ iterations/NNS for/IN objectives/NNS that/WDT are/VBP sum/NN of/IN smooth/JJ functions/NNS ./.
