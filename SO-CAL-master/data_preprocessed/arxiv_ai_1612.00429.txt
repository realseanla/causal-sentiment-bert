Deep/JJ reinforcement/NN learning/NN (/-LRB- RL/NN )/-RRB- can/MD acquire/VB complex/JJ behaviors/NNS from/IN low/JJ -/HYPH level/NN inputs/NNS ,/, such/JJ as/IN images/NNS ./.
However/RB ,/, real/JJ -/HYPH world/NN applications/NNS of/IN such/JJ methods/NNS require/VBP generalizing/VBG to/IN the/DT vast/JJ variability/NN of/IN the/DT real/JJ world/NN ./.
Deep/JJ networks/NNS are/VBP known/VBN to/TO achieve/VB remarkable/JJ generalization/NN when/WRB provided/VBN with/IN massive/JJ amounts/NNS of/IN labeled/VBN data/NNS ,/, but/CC can/MD we/PRP provide/VB this/DT breadth/NN of/IN experience/NN to/IN an/DT RL/NNP agent/NN ,/, such/JJ as/IN a/DT robot/NN ?/.
The/DT robot/NN might/MD continuously/RB learn/VB as/IN it/PRP explores/VBZ the/DT world/NN around/IN it/PRP ,/, even/RB while/IN deployed/VBN ./.
However/RB ,/, this/DT learning/NN requires/VBZ access/NN to/IN a/DT reward/NN function/NN ,/, which/WDT is/VBZ often/RB hard/JJ to/TO measure/VB in/IN real/JJ -/HYPH world/NN domains/NNS ,/, where/WRB the/DT reward/NN could/MD depend/VB on/IN ,/, for/IN example/NN ,/, unknown/JJ positions/NNS of/IN objects/NNS or/CC the/DT emotional/JJ state/NN of/IN the/DT user/NN ./.
Conversely/RB ,/, it/PRP is/VBZ often/RB quite/RB practical/JJ to/TO provide/VB the/DT agent/NN with/IN reward/NN functions/NNS in/IN a/DT limited/JJ set/NN of/IN situations/NNS ,/, such/JJ as/IN when/WRB a/DT human/JJ supervisor/NN is/VBZ present/JJ or/CC in/IN a/DT controlled/VBN setting/NN ./.
Can/MD we/PRP make/VB use/NN of/IN this/DT limited/JJ supervision/NN ,/, and/CC still/RB benefit/VB from/IN the/DT breadth/NN of/IN experience/NN an/DT agent/NN might/MD collect/VB on/IN its/PRP$ own/JJ ?/.
In/IN this/DT paper/NN ,/, we/PRP formalize/VBP this/DT problem/NN as/IN semisupervised/VBN reinforcement/NN learning/NN ,/, where/WRB the/DT reward/NN function/NN can/MD only/RB be/VB evaluated/VBN in/IN a/DT set/NN of/IN "/`` labeled/VBN "/'' MDPs/NNS ,/, and/CC the/DT agent/NN must/MD generalize/VB its/PRP$ behavior/NN to/IN the/DT wide/JJ range/NN of/IN states/NNS it/PRP might/MD encounter/VB in/IN a/DT set/NN of/IN "/`` unlabeled/JJ "/'' MDPs/NNS ,/, by/IN using/VBG experience/NN from/IN both/DT settings/NNS ./.
Our/PRP$ proposed/JJ method/NN infers/VBZ the/DT task/NN objective/NN in/IN the/DT unlabeled/JJ MDPs/NNS through/IN an/DT algorithm/NN that/WDT resembles/VBZ inverse/JJ RL/NN ,/, using/VBG the/DT agent/NN 's/POS own/JJ prior/JJ experience/NN in/IN the/DT labeled/VBN MDPs/NNS as/IN a/DT kind/NN of/IN demonstration/NN of/IN optimal/JJ behavior/NN ./.
We/PRP evaluate/VBP our/PRP$ method/NN on/IN challenging/JJ tasks/NNS that/WDT require/VBP control/NN directly/RB from/IN images/NNS ,/, and/CC show/VBP that/IN our/PRP$ approach/NN can/MD improve/VB the/DT generalization/NN of/IN a/DT learned/VBN deep/JJ neural/JJ network/NN policy/NN by/IN using/VBG experience/NN for/IN which/WDT no/DT reward/NN function/NN is/VBZ available/JJ ./.
We/PRP also/RB show/VBP that/IN our/PRP$ method/NN outperforms/VBZ direct/JJ supervised/JJ learning/NN of/IN the/DT reward/NN ./.
