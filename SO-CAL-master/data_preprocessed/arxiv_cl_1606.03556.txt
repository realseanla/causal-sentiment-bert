We/PRP conduct/VBP large/JJ -/HYPH scale/NN studies/NNS on/IN `/`` human/JJ attention/NN '/'' in/IN Visual/JJ Question/NN Answering/VBG (/-LRB- VQA/NN )/-RRB- to/TO understand/VB where/WRB humans/NNS choose/VBP to/TO look/VB to/IN answer/NN questions/NNS about/IN images/NNS ./.
We/PRP design/NN and/CC test/NN multiple/JJ game/NN -/HYPH inspired/VBN novel/JJ attention/NN -/HYPH annotation/NN interfaces/VBZ that/IN require/VB the/DT subject/NN to/IN sharpen/VB regions/NNS of/IN a/DT blurred/JJ image/NN to/TO answer/VB a/DT question/NN ./.
Thus/RB ,/, we/PRP introduce/VBP the/DT VQA/NN -/HYPH HAT/NN (/-LRB- Human/JJ ATtention/NN )/-RRB- dataset/NN ./.
We/PRP evaluate/VBP attention/NN maps/NNS generated/VBN by/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN VQA/NN models/NNS against/IN human/JJ attention/NN both/CC qualitatively/RB (/-LRB- via/IN visualizations/NNS )/-RRB- and/CC quantitatively/RB (/-LRB- via/IN rank/NN -/HYPH order/NN correlation/NN )/-RRB- ./.
We/PRP find/VBP that/IN depending/VBG on/IN the/DT implementation/NN used/VBN ,/, machine/NN -/HYPH generated/VBN attention/NN maps/NNS are/VBP either/CC \/SYM emph/NN {/-LRB- negatively/RB correlated/JJ }/-RRB- with/IN human/JJ attention/NN or/CC have/VBP positive/JJ correlation/NN worse/JJR than/IN task/NN -/HYPH independent/JJ saliency/NN ./.
Overall/RB ,/, our/PRP$ experiments/NNS paint/VBP a/DT bleak/JJ picture/NN for/IN the/DT current/JJ generation/NN of/IN attention/NN models/NNS in/IN VQA/NNP ./.
