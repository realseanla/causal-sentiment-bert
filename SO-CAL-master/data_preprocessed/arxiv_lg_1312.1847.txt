Convolutional/JJ neural/JJ network/NN models/NNS have/VBP recently/RB been/VBN shown/VBN to/TO achieve/VB excellent/JJ performance/NN on/IN challenging/JJ recognition/NN benchmarks/NNS ./.
However/RB ,/, like/IN many/JJ deep/JJ models/NNS ,/, there/EX is/VBZ little/JJ guidance/NN on/IN how/WRB the/DT architecture/NN of/IN the/DT model/NN should/MD be/VB selected/VBN ./.
Important/JJ hyper/JJ -/HYPH parameters/NNS such/JJ as/IN the/DT degree/NN of/IN parameter/NN sharing/NN ,/, number/NN of/IN layers/NNS ,/, units/NNS per/IN layer/NN ,/, and/CC overall/JJ number/NN of/IN parameters/NNS must/MD be/VB selected/VBN manually/RB through/IN trial/NN -/HYPH and/CC -/HYPH error/NN ./.
To/TO address/VB this/DT ,/, we/PRP introduce/VBP a/DT novel/JJ type/NN of/IN recursive/JJ neural/JJ network/NN that/WDT is/VBZ convolutional/JJ in/IN nature/NN ./.
Its/PRP$ similarity/NN to/IN standard/JJ convolutional/JJ models/NNS allows/VBZ us/PRP to/TO tease/VB apart/RB the/DT important/JJ architectural/JJ factors/NNS that/WDT influence/VBP performance/NN ./.
We/PRP find/VBP that/IN for/IN a/DT given/VBN parameter/NN budget/NN ,/, deeper/JJR models/NNS are/VBP preferred/VBN over/IN shallow/JJ ones/NNS ,/, and/CC models/NNS with/IN more/JJR parameters/NNS are/VBP preferred/VBN to/IN those/DT with/IN fewer/JJR ./.
Surprisingly/RB and/CC perhaps/RB counterintuitively/RB ,/, we/PRP find/VBP that/IN performance/NN is/VBZ independent/JJ of/IN the/DT number/NN of/IN units/NNS ,/, so/RB long/RB as/IN the/DT network/NN depth/NN and/CC number/NN of/IN parameters/NNS is/VBZ held/VBN constant/JJ ./.
This/DT suggests/VBZ that/IN ,/, computational/JJ efficiency/NN considerations/NNS aside/RB ,/, parameter/NN sharing/NN within/IN deep/JJ networks/NNS may/MD not/RB be/VB so/RB beneficial/JJ as/IN previously/RB supposed/VBN ./.
