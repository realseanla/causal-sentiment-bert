In/IN this/DT paper/NN ,/, we/PRP propose/VBP a/DT new/JJ stochastic/JJ alternating/VBG direction/NN method/NN of/IN multipliers/NNS (/-LRB- ADMM/NN )/-RRB- algorithm/NN ,/, which/WDT incrementally/RB approximates/VBZ the/DT full/JJ gradient/NN in/IN the/DT linearized/VBN ADMM/NN formulation/NN ./.
Besides/IN having/VBG a/DT low/JJ per/IN -/HYPH iteration/NN complexity/NN as/IN existing/VBG stochastic/JJ ADMM/NNP algorithms/NNS ,/, the/DT proposed/VBN algorithm/NN improves/VBZ the/DT convergence/NN rate/NN on/IN convex/NN problems/NNS from/IN $/$ O/UH (/-LRB- \/SYM frac/NN 1/CD {/-LRB- \/SYM sqrt/NN {/-LRB- T/NN }/-RRB- }/-RRB- )/-RRB- $/$ to/IN $/$ O/UH (/-LRB- \/SYM frac/NN 1/CD T/NN )/-RRB- $/$ ,/, where/WRB $/$ T$/CD is/VBZ the/DT number/NN of/IN iterations/NNS ./.
This/DT matches/VBZ the/DT convergence/NN rate/NN of/IN the/DT batch/NN ADMM/NN algorithm/NN ,/, but/CC without/IN the/DT need/NN to/TO visit/VB all/PDT the/DT samples/NNS in/IN each/DT iteration/NN ./.
Experiments/NNS on/IN the/DT graph/NN -/HYPH guided/VBN fused/VBN lasso/NN demonstrate/VBP that/IN the/DT new/JJ algorithm/NN is/VBZ significantly/RB faster/JJR than/IN state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN stochastic/JJ and/CC batch/NN ADMM/NN algorithms/NNS ./.
