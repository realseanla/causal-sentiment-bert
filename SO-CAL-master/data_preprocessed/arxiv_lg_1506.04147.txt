Calculation/NN of/IN the/DT log/NN -/HYPH normalizer/NN is/VBZ a/DT major/JJ computational/JJ obstacle/NN in/IN applications/NNS of/IN log/NN -/HYPH linear/JJ models/NNS with/IN large/JJ output/NN spaces/NNS ./.
The/DT problem/NN of/IN fast/JJ normalizer/NN computation/NN has/VBZ therefore/RB attracted/VBN significant/JJ attention/NN in/IN the/DT theoretical/JJ and/CC applied/VBD machine/NN learning/NN literature/NN ./.
In/IN this/DT paper/NN ,/, we/PRP analyze/VBP a/DT recently/RB proposed/VBN technique/NN known/VBN as/IN "/`` self/NN -/HYPH normalization/NN "/'' ,/, which/WDT introduces/VBZ a/DT regularization/NN term/NN in/IN training/NN to/IN penalize/VB log/NN normalizers/NNS for/IN deviating/VBG from/IN zero/CD ./.
This/DT makes/VBZ it/PRP possible/JJ to/TO use/VB unnormalized/JJ model/NN scores/NNS as/IN approximate/JJ probabilities/NNS ./.
Empirical/JJ evidence/NN suggests/VBZ that/IN self/NN -/HYPH normalization/NN is/VBZ extremely/RB effective/JJ ,/, but/CC a/DT theoretical/JJ understanding/NN of/IN why/WRB it/PRP should/MD work/VB ,/, and/CC how/WRB generally/RB it/PRP can/MD be/VB applied/VBN ,/, is/VBZ largely/RB lacking/JJ ./.
We/PRP prove/VBP generalization/NN bounds/NNS on/IN the/DT estimated/VBN variance/NN of/IN normalizers/NNS and/CC upper/JJ bounds/NNS on/IN the/DT loss/NN in/IN accuracy/NN due/IN to/IN self/NN -/HYPH normalization/NN ,/, describe/VB classes/NNS of/IN input/NN distributions/NNS that/WDT self/NN -/HYPH normalize/VB easily/RB ,/, and/CC construct/VB explicit/JJ examples/NNS of/IN high/JJ -/HYPH variance/NN input/NN distributions/NNS ./.
Our/PRP$ theoretical/JJ results/NNS make/VBP predictions/NNS about/IN the/DT difficulty/NN of/IN fitting/JJ self/NN -/HYPH normalized/VBN models/NNS to/IN several/JJ classes/NNS of/IN distributions/NNS ,/, and/CC we/PRP conclude/VBP with/IN empirical/JJ validation/NN of/IN these/DT predictions/NNS ./.
