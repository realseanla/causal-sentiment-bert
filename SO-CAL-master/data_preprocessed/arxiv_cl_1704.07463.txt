We/PRP develop/VBP a/DT streaming/NN (/-LRB- one/CD -/HYPH pass/NN ,/, bounded/VBN -/HYPH memory/NN )/-RRB- word/NN embedding/NN algorithm/NN based/VBN on/IN the/DT canonical/JJ skip/VB -/HYPH gram/NN with/IN negative/JJ sampling/NN algorithm/NN implemented/VBN in/IN word2vec/NN ./.
We/PRP compare/VBP our/PRP$ streaming/NN algorithm/NN to/IN word2vec/NN empirically/RB by/IN measuring/VBG the/DT cosine/NN similarity/NN between/IN word/NN pairs/NNS under/IN each/DT algorithm/NN and/CC by/IN applying/VBG each/DT algorithm/NN in/IN the/DT downstream/JJ task/NN of/IN hashtag/NN prediction/NN on/IN a/DT two/CD -/HYPH month/NN interval/NN of/IN the/DT Twitter/NNP sample/NN stream/NN ./.
We/PRP then/RB discuss/VBP the/DT results/NNS of/IN these/DT experiments/NNS ,/, concluding/VBG they/PRP provide/VBP partial/JJ validation/NN of/IN our/PRP$ approach/NN as/IN a/DT streaming/NN replacement/NN for/IN word2vec/NN ./.
Finally/RB ,/, we/PRP discuss/VBP potential/JJ failure/NN modes/NNS and/CC suggest/VBP directions/NNS for/IN future/JJ work/NN ./.
