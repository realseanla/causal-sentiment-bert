Inspired/VBN by/IN recent/JJ advances/NNS in/IN multimodal/JJ learning/NN and/CC machine/NN translation/NN ,/, we/PRP introduce/VBP an/DT encoder/NN -/HYPH decoder/NN pipeline/NN that/WDT learns/VBZ (/-LRB- a/DT )/-RRB- :/: a/DT multimodal/JJ joint/JJ embedding/NN space/NN with/IN images/NNS and/CC text/NN and/CC (/-LRB- b/LS )/-RRB- :/: a/DT novel/JJ language/NN model/NN for/IN decoding/VBG distributed/VBN representations/NNS from/IN our/PRP$ space/NN ./.
Our/PRP$ pipeline/NN effectively/RB unifies/VBZ joint/JJ image/NN -/HYPH text/NN embedding/NN models/NNS with/IN multimodal/JJ neural/JJ language/NN models/NNS ./.
We/PRP introduce/VBP the/DT structure/NN -/HYPH content/NN neural/JJ language/NN model/NN that/WDT disentangles/VBZ the/DT structure/NN of/IN a/DT sentence/NN to/IN its/PRP$ content/NN ,/, conditioned/VBN on/IN representations/NNS produced/VBN by/IN the/DT encoder/NN ./.
The/DT encoder/NN allows/VBZ one/CD to/IN rank/NN images/NNS and/CC sentences/NNS while/IN the/DT decoder/NN can/MD generate/VB novel/JJ descriptions/NNS from/IN scratch/NN ./.
Using/VBG LSTM/NN to/TO encode/VB sentences/NNS ,/, we/PRP match/VBP the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN Flickr8K/NN and/CC Flickr30K/NN without/IN using/VBG object/NN detections/NNS ./.
We/PRP also/RB set/VBD new/JJ best/JJS results/NNS when/WRB using/VBG the/DT 19/CD -/HYPH layer/NN Oxford/NNP convolutional/JJ network/NN ./.
Furthermore/RB we/PRP show/VBP that/IN with/IN linear/JJ encoders/NNS ,/, the/DT learned/VBN embedding/NN space/NN captures/VBZ multimodal/JJ regularities/NNS in/IN terms/NNS of/IN vector/NN space/NN arithmetic/NN e.g./FW */NFP image/NN of/IN a/DT blue/JJ car/NN */SYM -/HYPH "/`` blue/JJ "/'' "/`` red/JJ "/'' is/VBZ near/JJ images/NNS of/IN red/JJ cars/NNS ./.
Sample/NN captions/NNS generated/VBN for/IN 800/CD images/NNS are/VBP made/VBN available/JJ for/IN comparison/NN ./.
