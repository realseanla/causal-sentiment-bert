Variational/NNP Autoencoders/NNP (/-LRB- VAEs/NNS )/-RRB- are/VBP expressive/JJ latent/JJ variable/JJ models/NNS that/WDT can/MD be/VB used/VBN to/TO learn/VB complex/JJ probability/NN distributions/NNS from/IN training/NN data/NNS ./.
However/RB ,/, the/DT quality/NN of/IN the/DT resulting/VBG model/NN crucially/RB relies/VBZ on/IN the/DT expressiveness/NN of/IN the/DT inference/NN model/NN used/VBN during/IN training/NN ./.
We/PRP introduce/VBP Adversarial/JJ Variational/NNP Bayes/NNP (/-LRB- AVB/NNP )/-RRB- ,/, a/DT technique/NN for/IN training/NN Variational/NNP Autoencoders/NNP with/IN arbitrarily/RB expressive/JJ inference/NN models/NNS ./.
We/PRP achieve/VBP this/DT by/IN introducing/VBG an/DT auxiliary/JJ discriminative/JJ network/NN that/WDT allows/VBZ to/IN rephrase/VB the/DT maximum/NN -/HYPH likelihood/NN -/HYPH problem/NN as/IN a/DT two/CD -/HYPH player/NN game/NN ,/, hence/RB establishing/VBG a/DT principled/JJ connection/NN between/IN VAEs/NNS and/CC Generative/JJ Adversarial/JJ Networks/NNS (/-LRB- GANs/NNS )/-RRB- ./.
We/PRP show/VBP that/IN in/IN the/DT nonparametric/JJ limit/NN our/PRP$ method/NN yields/VBZ an/DT exact/JJ maximum/NN -/HYPH likelihood/NN assignment/NN for/IN the/DT parameters/NNS of/IN the/DT generative/JJ model/NN ,/, as/RB well/RB as/IN the/DT exact/JJ posterior/JJ distribution/NN over/IN the/DT latent/JJ variables/NNS given/VBN an/DT observation/NN ./.
Contrary/JJ to/IN competing/VBG approaches/NNS which/WDT combine/VBP VAEs/NNS with/IN GANs/NNS ,/, our/PRP$ approach/NN has/VBZ a/DT clear/JJ theoretical/JJ justification/NN ,/, retains/VBZ most/RBS advantages/NNS of/IN standard/JJ Variational/NNP Autoencoders/NNPS and/CC is/VBZ easy/JJ to/TO implement/VB ./.
