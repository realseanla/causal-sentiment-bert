One/CD of/IN the/DT major/JJ issues/NNS in/IN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- methods/NNS is/VBZ how/WRB to/TO choose/VB an/DT appropriate/JJ step/NN size/NN while/IN running/VBG the/DT algorithm/NN ./.
Since/IN the/DT traditional/JJ line/NN search/NN technique/NN does/VBZ not/RB apply/VB for/IN stochastic/JJ optimization/NN algorithms/NNS ,/, the/DT common/JJ practice/NN in/IN SGD/NNP is/VBZ either/RB to/TO use/VB a/DT diminishing/VBG step/NN size/NN ,/, or/CC to/TO tune/VB a/DT fixed/VBN step/NN size/NN by/IN hand/NN ./.
Apparently/RB ,/, these/DT two/CD approaches/NNS can/MD be/VB time/NN consuming/VBG in/IN practice/NN ./.
In/IN this/DT paper/NN ,/, we/PRP propose/VBP to/TO use/VB the/DT Barzilai/NNP -/HYPH Borwein/NNP (/-LRB- BB/NNP )/-RRB- method/NN to/TO automatically/RB compute/VB step/NN sizes/NNS for/IN SGD/NNP and/CC its/PRP$ variant/NN :/: stochastic/JJ variance/NN reduced/VBN gradient/NN (/-LRB- SVRG/NN )/-RRB- method/NN ,/, which/WDT leads/VBZ to/IN two/CD algorithms/NNS :/: SGD/NNP -/HYPH BB/NNP and/CC SVRG/NNP -/HYPH BB/NNP ./.
We/PRP prove/VBP that/IN SVRG/NNP -/HYPH BB/NNP converges/VBZ linearly/RB for/IN strongly/RB convex/JJ objective/JJ function/NN ./.
As/IN a/DT by/IN -/HYPH product/NN ,/, we/PRP prove/VBP the/DT linear/JJ convergence/NN result/NN of/IN SVRG/NN with/IN Option/NN I/PRP proposed/VBD in/IN [/-LRB- 10/CD ]/-RRB- ,/, whose/WP$ convergence/NN result/NN has/VBZ been/VBN missing/VBG in/IN the/DT literature/NN ./.
Numerical/NNP experiments/NNS on/IN standard/JJ data/NNS sets/NNS show/VBP that/IN the/DT performance/NN of/IN SGD/NNP -/HYPH BB/NNP and/CC SVRG/NNP -/HYPH BB/NNP is/VBZ comparable/JJ to/IN and/CC sometimes/RB even/RB better/JJR than/IN SGD/NNP and/CC SVRG/NNP with/IN best/JJS -/HYPH tuned/VBN step/NN sizes/NNS ,/, and/CC is/VBZ superior/JJ to/IN some/DT advanced/JJ SGD/NN variants/NNS ./.
