We/PRP introduce/VBP a/DT new/JJ task/NN ,/, visual/JJ sense/NN disambiguation/NN for/IN verbs/NNS :/: given/VBN an/DT image/NN and/CC a/DT verb/VB ,/, assign/VB the/DT correct/JJ sense/NN of/IN the/DT verb/VB ,/, i.e./FW ,/, the/DT one/NN that/WDT describes/VBZ the/DT action/NN depicted/VBN in/IN the/DT image/NN ./.
Just/RB as/IN textual/JJ word/NN sense/NN disambiguation/NN is/VBZ useful/JJ for/IN a/DT wide/JJ range/NN of/IN NLP/NN tasks/NNS ,/, visual/JJ sense/NN disambiguation/NN can/MD be/VB useful/JJ for/IN multimodal/JJ tasks/NNS such/JJ as/IN image/NN retrieval/NN ,/, image/NN description/NN ,/, and/CC text/NN illustration/NN ./.
We/PRP introduce/VBP VerSe/NN ,/, a/DT new/JJ dataset/NN that/WDT augments/VBZ existing/VBG multimodal/JJ datasets/NNS (/-LRB- COCO/NN and/CC TUHOI/NN )/-RRB- with/IN sense/NN labels/NNS ./.
We/PRP propose/VBP an/DT unsupervised/JJ algorithm/NN based/VBN on/IN Lesk/NNP which/WDT performs/VBZ visual/JJ sense/NN disambiguation/NN using/VBG textual/JJ ,/, visual/JJ ,/, or/CC multimodal/JJ embeddings/NNS ./.
We/PRP find/VBP that/IN textual/JJ embeddings/NNS perform/VBP well/RB when/WRB gold/NN -/HYPH standard/JJ textual/JJ annotations/NNS (/-LRB- object/NN labels/NNS and/CC image/NN descriptions/NNS )/-RRB- are/VBP available/JJ ,/, while/IN multimodal/JJ embeddings/NNS perform/VBP well/RB on/IN unannotated/JJ images/NNS ./.
We/PRP also/RB verify/VBP our/PRP$ findings/NNS by/IN using/VBG the/DT textual/JJ and/CC multimodal/JJ embeddings/NNS as/IN features/NNS in/IN a/DT supervised/JJ setting/NN and/CC analyse/VB the/DT performance/NN of/IN visual/JJ sense/NN disambiguation/NN task/NN ./.
VerSe/NN is/VBZ made/VBN publicly/RB available/JJ and/CC can/MD be/VB downloaded/VBN at/IN :/:
