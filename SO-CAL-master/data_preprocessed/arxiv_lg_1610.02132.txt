Parallel/JJ implementations/NNS of/IN stochastic/JJ gradient/NN descent/NN (/-LRB- SGD/NNP )/-RRB- have/VBP received/VBN significant/JJ research/NN attention/NN ,/, thanks/NNS to/IN excellent/JJ scalability/NN properties/NNS of/IN this/DT algorithm/NN ,/, and/CC to/IN its/PRP$ efficiency/NN in/IN the/DT context/NN of/IN training/NN deep/JJ neural/JJ networks/NNS ./.
A/DT fundamental/JJ barrier/NN for/IN parallelizing/VBG large/JJ -/HYPH scale/NN SGD/NNP is/VBZ the/DT fact/NN that/IN the/DT cost/NN of/IN communicating/VBG the/DT gradient/NN updates/NNS between/IN nodes/NNS can/MD be/VB very/RB large/JJ ./.
Consequently/RB ,/, lossy/NN compresion/NN heuristics/NNS have/VBP been/VBN proposed/VBN ,/, by/IN which/WDT nodes/NNS only/RB communicate/VBP quantized/VBN gradients/NNS ./.
Although/IN effective/JJ in/IN practice/NN ,/, these/DT heuristics/NNS do/VBP not/RB always/RB provably/RB converge/VB ,/, and/CC it/PRP is/VBZ not/RB clear/JJ whether/IN they/PRP are/VBP optimal/JJ ./.
