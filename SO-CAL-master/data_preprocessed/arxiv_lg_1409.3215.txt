Deep/JJ Neural/JJ Networks/NNS (/-LRB- DNNs/NNS )/-RRB- are/VBP powerful/JJ models/NNS that/WDT have/VBP achieved/VBN excellent/JJ performance/NN on/IN difficult/JJ learning/NN tasks/NNS ./.
Although/IN DNNs/NNS work/VBP well/RB whenever/WRB large/JJ labeled/VBN training/NN sets/NNS are/VBP available/JJ ,/, they/PRP can/MD not/RB be/VB used/VBN to/TO map/VB sequences/NNS to/IN sequences/NNS ./.
In/IN this/DT paper/NN ,/, we/PRP present/VBP a/DT general/JJ end/NN -/HYPH to/IN -/HYPH end/NN approach/NN to/IN sequence/NN learning/NN that/WDT makes/VBZ minimal/JJ assumptions/NNS on/IN the/DT sequence/NN structure/NN ./.
Our/PRP$ method/NN uses/VBZ a/DT multilayered/JJ Long/JJ Short/JJ -/HYPH Term/NN Memory/NN (/-LRB- LSTM/NN )/-RRB- to/TO map/VB the/DT input/NN sequence/NN to/IN a/DT vector/NN of/IN a/DT fixed/VBN dimensionality/NN ,/, and/CC then/RB another/DT deep/JJ LSTM/NN to/IN decode/VB the/DT target/NN sequence/NN from/IN the/DT vector/NN ./.
Our/PRP$ main/JJ result/NN is/VBZ that/IN on/IN an/DT English/JJ to/IN French/JJ translation/NN task/NN from/IN the/DT WMT/NN -/HYPH 14/CD dataset/NN ,/, the/DT translations/NNS produced/VBN by/IN the/DT LSTM/NNP achieve/VB a/DT BLEU/NN score/NN of/IN 34.7/CD on/IN the/DT entire/JJ test/NN set/VBN ,/, where/WRB the/DT LSTM/NNP 's/POS BLEU/NNP score/NN was/VBD penalized/VBN on/RP out/IN -/HYPH of/IN -/HYPH vocabulary/NN words/NNS ./.
Additionally/RB ,/, the/DT LSTM/NNP did/VBD not/RB have/VB difficulty/NN on/IN long/JJ sentences/NNS ./.
For/IN comparison/NN ,/, a/DT strong/JJ phrase/NN -/HYPH based/VBN SMT/NNP system/NN achieves/VBZ a/DT BLEU/NN score/NN of/IN 33.3/CD on/IN the/DT same/JJ dataset/NN ./.
When/WRB we/PRP used/VBD the/DT LSTM/NNP to/TO rerank/VB the/DT 1000/CD hypotheses/NNS produced/VBN by/IN the/DT aforementioned/JJ SMT/NN system/NN ,/, its/PRP$ BLEU/NNP score/NN increases/VBZ to/IN 36.5/CD ,/, which/WDT beats/VBZ the/DT previous/JJ state/NN of/IN the/DT art/NN ./.
The/DT LSTM/NNP also/RB learned/VBD sensible/JJ phrase/NN and/CC sentence/NN representations/NNS that/WDT are/VBP sensitive/JJ to/IN word/NN order/NN and/CC are/VBP relatively/RB invariant/JJ to/IN the/DT active/JJ and/CC the/DT passive/JJ voice/NN ./.
Finally/RB ,/, we/PRP found/VBD that/IN reversing/VBG the/DT order/NN of/IN the/DT words/NNS in/IN all/DT source/NN sentences/NNS (/-LRB- but/CC not/RB target/NN sentences/NNS )/-RRB- improved/VBD the/DT LSTM/NNP 's/POS performance/NN markedly/RB ,/, because/IN doing/VBG so/RB introduced/VBD many/JJ short/JJ term/NN dependencies/NNS between/IN the/DT source/NN and/CC the/DT target/NN sentence/NN which/WDT made/VBD the/DT optimization/NN problem/NN easier/JJR ./.
