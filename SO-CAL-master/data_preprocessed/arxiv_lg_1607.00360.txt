Bregman/NNP divergences/NNS play/VBP a/DT central/JJ role/NN in/IN the/DT design/NN and/CC analysis/NN of/IN a/DT range/NN of/IN machine/NN learning/NN algorithms/NNS ./.
This/DT paper/NN explores/VBZ the/DT use/NN of/IN Bregman/NNP divergences/NNS to/TO establish/VB reductions/NNS between/IN such/JJ algorithms/NNS and/CC their/PRP$ analyses/NNS ./.
We/PRP present/VBP a/DT new/JJ scaled/VBN isodistortion/NN theorem/NN involving/VBG Bregman/NNP divergences/NNS (/-LRB- scaled/VBN Bregman/NNP theorem/NN for/IN short/JJ )/-RRB- which/WDT shows/VBZ that/IN certain/JJ "/`` Bregman/NNP distortions/NNS '/POS "/`` (/-LRB- employing/VBG a/DT potentially/RB non-convex/JJ generator/NN )/-RRB- may/MD be/VB exactly/RB re-written/VBN as/IN a/DT scaled/VBN Bregman/NNP divergence/NN computed/VBN over/IN transformed/VBN data/NNS ./.
Admissible/JJ distortions/NNS include/VBP geodesic/JJ distances/NNS on/IN curved/JJ manifolds/NNS and/CC projections/NNS or/CC gauge/NN -/HYPH normalisation/NN ,/, while/IN admissible/JJ data/NNS include/VBP scalars/NNS ,/, vectors/NNS and/CC matrices/NNS ./.
