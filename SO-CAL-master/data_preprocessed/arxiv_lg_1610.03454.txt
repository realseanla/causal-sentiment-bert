We/PRP present/VBP deep/JJ variational/JJ canonical/JJ correlation/NN analysis/NN (/-LRB- VCCA/NN )/-RRB- ,/, a/DT deep/JJ multi-view/NN learning/NN model/NN that/WDT extends/VBZ the/DT latent/JJ variable/JJ model/NN interpretation/NN of/IN linear/JJ CCA/NN ~/SYM \/SYM citep/SYM {/-LRB- BachJordan05a/NN }/-RRB- to/TO nonlinear/JJ observation/NN models/NNS parameterized/VBN by/IN deep/JJ neural/JJ networks/NNS (/-LRB- DNNs/NNS )/-RRB- ./.
Marginal/JJ data/NNS likelihood/NN as/IN well/RB as/IN inference/NN are/VBP intractable/JJ under/IN this/DT model/NN ./.
We/PRP derive/VBP a/DT variational/JJ lower/JJR bound/VBN of/IN the/DT data/NNS likelihood/NN by/IN parameterizing/VBG the/DT posterior/JJ density/NN of/IN the/DT latent/JJ variables/NNS with/IN another/DT DNN/NNP ,/, and/CC approximate/VB the/DT lower/JJR bound/JJ via/IN Monte/NNP Carlo/NNP sampling/NN ./.
Interestingly/RB ,/, the/DT resulting/VBG model/NN resembles/VBZ that/DT of/IN multi-view/JJ autoencoders/NNS ~/SYM \/SYM citep/SYM {/-LRB- Ngiam_11b/NN }/-RRB- ,/, with/IN the/DT key/JJ distinction/NN of/IN an/DT additional/JJ sampling/NN procedure/NN at/IN the/DT bottleneck/NN layer/NN ./.
We/PRP also/RB propose/VBP a/DT variant/NN of/IN VCCA/NNP called/VBD VCCA/NNP -/HYPH private/JJ which/WDT can/MD ,/, in/IN addition/NN to/IN the/DT "/`` common/JJ variables/NNS "/'' underlying/VBG both/DT views/NNS ,/, extract/VB the/DT "/`` private/JJ variables/NNS "/'' within/IN each/DT view/NN ./.
We/PRP demonstrate/VBP that/IN VCCA/NN -/HYPH private/JJ is/VBZ able/JJ to/TO disentangle/VB the/DT shared/VBN and/CC private/JJ information/NN for/IN multi-view/JJ data/NNS without/IN hard/JJ supervision/NN ./.
