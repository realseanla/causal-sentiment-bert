In/IN this/DT paper/NN ,/, we/PRP propose/VBP an/DT efficient/JJ transfer/NN leaning/VBG methods/NNS for/IN training/VBG a/DT personalized/JJ language/NN model/NN using/VBG a/DT recurrent/JJ neural/JJ network/NN with/IN long/JJ short/JJ -/HYPH term/NN memory/NN architecture/NN ./.
With/IN our/PRP$ proposed/VBN fast/JJ transfer/NN learning/VBG schemes/NNS ,/, a/DT general/JJ language/NN model/NN is/VBZ updated/VBN to/IN a/DT personalized/JJ language/NN model/NN with/IN a/DT small/JJ amount/NN of/IN user/NN data/NNS and/CC a/DT limited/JJ computing/NN resource/NN ./.
These/DT methods/NNS are/VBP especially/RB useful/JJ for/IN a/DT mobile/JJ device/NN environment/NN while/IN the/DT data/NNS is/VBZ prevented/VBN from/IN transferring/VBG out/IN of/IN the/DT device/NN for/IN privacy/NN purposes/NNS ./.
Through/IN experiments/NNS on/IN dialogue/NN data/NNS in/IN a/DT drama/NN ,/, it/PRP is/VBZ verified/VBN that/IN our/PRP$ transfer/NN learning/NN methods/NNS have/VBP successfully/RB generated/VBN the/DT personalized/JJ language/NN model/NN ,/, whose/WP$ output/NN is/VBZ more/RBR similar/JJ to/IN the/DT personal/JJ language/NN style/NN in/IN both/CC qualitative/JJ and/CC quantitative/JJ aspects/NNS ./.
