Most/JJS conventional/JJ sentence/NN similarity/NN methods/NNS only/RB focus/VBP on/IN similar/JJ parts/NNS of/IN two/CD input/NN sentences/NNS ,/, and/CC simply/RB ignore/VB the/DT dissimilar/JJ parts/NNS ,/, which/WDT usually/RB give/VBP us/PRP some/DT clues/NNS and/CC semantic/JJ meanings/NNS about/IN the/DT sentences/NNS ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP a/DT model/NN to/TO take/VB into/IN account/NN both/CC the/DT similarities/NNS and/CC dissimilarities/NNS by/IN decomposing/VBG and/CC composing/VBG lexical/JJ semantics/NNS over/IN sentences/NNS ./.
The/DT model/NN represents/VBZ each/DT word/NN as/IN a/DT vector/NN ,/, and/CC calculates/VBZ a/DT semantic/JJ matching/NN vector/NN for/IN each/DT word/NN based/VBN on/IN all/DT words/NNS in/IN the/DT other/JJ sentence/NN ./.
Then/RB ,/, each/DT word/NN vector/NN is/VBZ decomposed/VBN into/IN a/DT similar/JJ component/NN and/CC a/DT dissimilar/JJ component/NN based/VBN on/IN the/DT semantic/JJ matching/NN vector/NN ./.
After/IN this/DT ,/, a/DT two/CD -/HYPH channel/NN CNN/NNP model/NN is/VBZ employed/VBN to/TO capture/VB features/NNS by/IN composing/VBG the/DT similar/JJ and/CC dissimilar/JJ components/NNS ./.
Finally/RB ,/, a/DT similarity/NN score/NN is/VBZ estimated/VBN over/IN the/DT composed/VBN feature/NN vectors/NNS ./.
Experimental/JJ results/NNS show/VBP that/IN our/PRP$ model/NN gets/VBZ the/DT state/NN -/HYPH of/IN -/HYPH the/DT -/HYPH art/NN performance/NN on/IN the/DT answer/NN sentence/NN selection/NN task/NN ,/, and/CC achieves/VBZ a/DT comparable/JJ result/NN on/IN the/DT paraphrase/NN identification/NN task/NN ./.
