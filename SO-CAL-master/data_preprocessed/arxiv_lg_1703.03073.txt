Deep/JJ convolutional/JJ neural/JJ network/NN (/-LRB- CNN/NNP )/-RRB- inference/NN requires/VBZ significant/JJ amount/NN of/IN memory/NN and/CC computation/NN ,/, which/WDT limits/VBZ its/PRP$ deployment/NN on/IN embedded/VBN devices/NNS ./.
To/TO alleviate/VB these/DT problems/NNS to/IN some/DT extent/NN ,/, prior/JJ research/NN utilize/VBP low/JJ precision/NN fixed/VBN -/HYPH point/NN numbers/NNS to/TO represent/VB the/DT CNN/NNP weights/NNS and/CC activations/NNS ./.
However/RB ,/, the/DT minimum/NN required/VBN data/NNS precision/NN of/IN fixed/VBN -/HYPH point/NN weights/NNS varies/VBZ across/IN different/JJ networks/NNS and/CC also/RB across/IN different/JJ layers/NNS of/IN the/DT same/JJ network/NN ./.
In/IN this/DT work/NN ,/, we/PRP propose/VBP using/VBG floating/VBG -/HYPH point/NN numbers/NNS for/IN representing/VBG the/DT weights/NNS and/CC fixed/VBN -/HYPH point/NN numbers/NNS for/IN representing/VBG the/DT activations/NNS ./.
We/PRP show/VBP that/IN using/VBG floating/VBG -/HYPH point/NN representation/NN for/IN weights/NNS is/VBZ more/RBR efficient/JJ than/IN fixed/VBN -/HYPH point/NN representation/NN for/IN the/DT same/JJ bit/NN -/HYPH width/NN and/CC demonstrate/VBP it/PRP on/IN popular/JJ large/JJ -/HYPH scale/NN CNNs/NNS such/JJ as/IN AlexNet/NNP ,/, SqueezeNet/NNP ,/, GoogLeNet/NNP and/CC VGG/NNP -/HYPH 16/NNP ./.
We/PRP also/RB show/VBP that/IN such/PDT a/DT representation/NN scheme/NN enables/VBZ compact/JJ hardware/NN multiply/VB -/HYPH and/CC -/HYPH accumulate/VB (/-LRB- MAC/NNP )/-RRB- unit/NN design/NN ./.
Experimental/JJ results/NNS show/VBP that/IN the/DT proposed/VBN scheme/NN reduces/VBZ the/DT weight/NN storage/NN by/IN up/RB to/IN 36/CD percent/NN and/CC power/NN consumption/NN of/IN the/DT hardware/NN multiplier/JJR by/IN up/RB to/IN 50/CD percent/NN ./.
