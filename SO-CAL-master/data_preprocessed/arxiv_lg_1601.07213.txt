We/PRP present/VBP DataGrad/NNP ,/, a/DT general/JJ back/NN -/HYPH propagation/NN style/NN training/NN procedure/NN for/IN deep/JJ neural/JJ architectures/NNS that/WDT uses/VBZ regularization/NN of/IN a/DT deep/JJ Jacobian/NNP -/HYPH based/VBN penalty/NN ./.
It/PRP can/MD be/VB viewed/VBN as/IN a/DT deep/JJ extension/NN of/IN the/DT layerwise/NN contractive/JJ auto/NN -/HYPH encoder/NN penalty/NN ./.
More/RBR importantly/RB ,/, it/PRP unifies/VBZ previous/JJ proposals/NNS for/IN adversarial/JJ training/NN of/IN deep/JJ neural/JJ nets/NNS --/: this/DT list/NN includes/VBZ directly/RB modifying/VBG the/DT gradient/NN ,/, training/NN on/IN a/DT mix/NN of/IN original/JJ and/CC adversarial/JJ examples/NNS ,/, using/VBG contractive/JJ penalties/NNS ,/, and/CC approximately/RB optimizing/VBG constrained/VBN adversarial/JJ objective/JJ functions/NNS ./.
In/IN an/DT experiment/NN using/VBG a/DT Deep/JJ Sparse/JJ Rectifier/NNP Network/NNP ,/, we/PRP find/VBP that/IN the/DT deep/JJ Jacobian/JJ regularization/NN of/IN DataGrad/NNP (/-LRB- which/WDT also/RB has/VBZ L1/NN and/CC L2/NN flavors/NNS of/IN regularization/NN )/-RRB- outperforms/VBZ traditional/JJ L1/NN and/CC L2/NN regularization/NN both/CC on/IN the/DT original/JJ dataset/NN as/RB well/RB as/IN on/IN adversarial/JJ examples/NNS ./.
