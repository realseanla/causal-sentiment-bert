In this work, we study parameter tuning towards the M$^2$ metric, the standard metric for automatic grammar error correction (GEC) tasks.
After implementing M$^2$ as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task.
We notice erratic behavior when optimizing sparse feature weights with M$^2$ and offer partial solutions.
To our surprise, we find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37 percent M$^2$ over previously 40.56 percent, by a neural encoder-decoder model) while being trained on the same data.
Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49 percent M$^2$.
