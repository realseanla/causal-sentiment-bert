Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device.
For example, language models can improve speech recognition and text entry, and image models can automatically select good photos.
However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data-center and training there using conventional approaches.
We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates.
We term this decentralized approach Federated Learning.
