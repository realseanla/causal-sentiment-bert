Advances in natural language processing tasks have gained momentum in recent years due to the increasingly popular neural network methods.
In this paper, we explore deep learning techniques for answering multi-step reasoning questions that operate on semi-structured tables.
Challenges here arise from the level of logical compositionality expressed by questions, as well as the domain openness.
Our approach is weakly supervised, trained on question-answer-table triples without requiring intermediate strong supervision.
It performs two phases: first, machine understandable logical forms (programs) are generated from natural language questions following the work of [Pasupat and Liang, 2015].
Second, paraphrases of logical forms and questions are embedded in a jointly learned vector space using word and character convolutional neural networks.
A neural scoring function is further used to rank and retrieve the most probable logical form (interpretation) of a question.
Our best single model achieves 34.8 percent accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7 percent, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of [Neelakantan et al., 2016].
