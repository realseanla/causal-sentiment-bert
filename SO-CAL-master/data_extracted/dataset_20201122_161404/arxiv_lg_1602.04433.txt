The recent success of deep neural networks relies on massive amounts of labeled data.
For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain.
In this paper, we propose a new approach to domain adaptation in deep networks that can simultaneously learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain.
We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function.
We enable classifier adaptation by plugging several layers into the deep network to explicitly learn the residual function with reference to the target classifier.
We embed features of multiple layers into reproducing kernel Hilbert spaces (RKHSs) and match feature distributions for feature adaptation.
The adaptation behaviors can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently using standard back-propagation.
Empirical evidence exhibits that the approach outperforms state of art methods on standard domain adaptation datasets.
