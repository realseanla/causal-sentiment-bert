We present Deep Speaker, a neural speaker embedding system that maps utterances to a hypersphere where speaker similarity is measured by cosine similarity.
The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering.
We experiment with ResCNN and GRU architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity.
Experiments on three distinct datasets suggest that Deep Speaker outperforms a DNN-based i-vector baseline.
For example, Deep Speaker reduces the verification equal error rate by 50 percent (relatively) and improves the identification accuracy by 60 percent (relatively) on a text-independent dataset.
We also present results that suggest adapting from a model trained with Mandarin can improve accuracy for English speaker recognition.
