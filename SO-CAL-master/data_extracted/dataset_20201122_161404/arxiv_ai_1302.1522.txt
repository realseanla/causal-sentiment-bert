Much recent research in decision theoretic planning has adopted Markov decision processes (MDPs) as the model of choice, and has attempted to make their solution more tractable by exploiting problem structure.
One particular algorithm, structured policy construction achieves this by means of a decision theoretic analog of goal regression using action descriptions based on Bayesian networks with tree-structured conditional probability tables.
The algorithm as presented is not able to deal with actions with correlated effects.
We describe a new decision theoretic regression operator that corrects this weakness.
While conceptually straightforward, this extension requires a somewhat more complicated technical approach.
