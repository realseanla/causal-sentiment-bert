Humans and animals are constantly exposed to a continuous stream of sensory information from different modalities.
At the same time, they form more compressed representations like concepts or symbols.
In species that use language, this process is further structured by this interaction, where a mapping between the sensorimotor concepts and linguistic elements needs to be established.
There is evidence that children might be learning language by simply disambiguating potential meanings based on multiple exposures to utterances in different contexts (cross-situational learning).
In existing models, the mapping between modalities is usually found in a single step by directly using frequencies of referent and meaning co-occurrences.
In this paper, we present an extension of this one-step mapping and introduce a newly proposed sequential mapping algorithm together with a publicly available Matlab implementation.
For demonstration, we have chosen a less typical scenario: instead of learning to associate objects with their names, we focus on body representations.
A humanoid robot is receiving tactile stimulations on its body, while at the same time listening to utterances of the body part names (e.g., hand, forearm and torso).
With the goal at arriving at the correct "body categories", we demonstrate how a sequential mapping algorithm outperforms one-step mapping.
In addition, the effect of data set size and noise in the linguistic input are studied.
