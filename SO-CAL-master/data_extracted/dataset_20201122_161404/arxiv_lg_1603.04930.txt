In this work we present a deep learning framework for video compressive sensing.
The proposed formulation enables recovery of video frames in a few seconds at significantly improved reconstruction quality compared to previous approaches.
Our investigation starts by learning a linear mapping between video sequences and corresponding measured frames which turns out to provide promising results.
We then extend the linear formulation to deep fully-connected networks and explore the performance gains using deeper architectures.
Our analysis is always driven by the applicability of the proposed framework on existing compressive video architectures.
Extensive simulations on several video sequences document the superiority of our approach both quantitatively and qualitatively.
Finally, our analysis offers insights into understanding how dataset sizes and number of layers affect reconstruction performance while raising a few points for future investigation.
