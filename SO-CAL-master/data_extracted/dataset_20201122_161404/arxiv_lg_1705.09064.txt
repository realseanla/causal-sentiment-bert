Deep learning has shown promising results on hard perceptual problems in recent years.
However, deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human.
Such specially crafted perturbations cause deep learning systems to output incorrect decisions, with potentially disastrous consequences.
These vulnerabilities hinder the deployment of deep learning systems where safety or security is important.
Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective.
