How detailed should we make the goals we prescribe to AI agents acting on our behalf in complex environments?
Detailed and low-level specification of goals can be tedious and expensive to create, and abstract and high-level goals could lead to negative surprises as the agent may find behaviors that we would not want it to do, i.e., lead to unsafe AI.
One approach to addressing this dilemma is for the agent to infer human goals by observing human behavior.
This is the Inverse Reinforcement Learning (IRL) problem.
However, IRL is generally ill-posed for there are typically many reward functions for which the observed behavior is optimal.
While the use of heuristics to select from among the set of feasible reward functions has led to successful applications of IRL to learning from demonstration, such heuristics do not address AI safety.
In this paper we introduce a novel repeated IRL problem that captures an aspect of AI safety as follows.
The agent has to act on behalf of a human in a sequence of tasks and wishes to minimize the number of tasks that it surprises the human.
Each time the human is surprised the agent is provided a demonstration of the desired behavior by the human.
We formalize this problem, including how the sequence of tasks is chosen, in a few different ways and provide some foundational results.
