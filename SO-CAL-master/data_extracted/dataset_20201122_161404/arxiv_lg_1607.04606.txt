Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks.
Many popular models to learn such representations ignore the morphology of words, by assigning a distinct vector to each word.
This is a limitation, especially for morphologically rich languages with large vocabularies and many rare words.
In this paper, we propose a new approach based on the skip-gram model, where each word is represented as a bag of character n-grams.
A vector representation is associated to each character n-gram, words being represented as the sum of these representations.
Our method is fast, allowing to train models on large corpus quickly.
We evaluate the obtained word representations on five different languages, on word similarity and analogy tasks.
