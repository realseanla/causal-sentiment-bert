We present a Bayesian approach to adapting parameters of a well-trained context-dependent deep-neural-network hid-den Markov models (CD-DNN-HMMs) to improve automatic speech recognition performance.
Due to an abundance of DNN parameters but with only a limited amount of adaptation data, the posterior probabilities of unseen CD states (senones) are often pushed towards zero during adaptation, and consequently the ability to model these senones can be degraded.
We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output senones and compare it to the feature space maximum a posteriori linear regression previously proposed.
Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework.
In supervised adaptation, the proposed MAP adaptation provides more than 10 percent relative error reduction and consistently outperforms the conventional transformation based methods.
Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.
