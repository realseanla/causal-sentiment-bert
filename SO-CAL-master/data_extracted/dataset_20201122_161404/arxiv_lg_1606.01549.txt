In this paper we study the problem of answering cloze-style questions over short documents.
We introduce a new attention mechanism which uses multiplicative interactions between the query embedding and intermediate states of a recurrent neural network reader.
This enables the reader to build query-specific representations of tokens in the document which are further used for answer selection.
Our model, the Gated-Attention Reader, outperforms all state-of-the-art models on several large-scale benchmark datasets for this task---the CNN \&amp; Dailymail news stories and Children's Book Test.
We also provide a detailed analysis of the performance of our model and several baselines over a subset of questions manually annotated with certain linguistic features.
The analysis sheds light on the strengths and weaknesses of several existing models.
