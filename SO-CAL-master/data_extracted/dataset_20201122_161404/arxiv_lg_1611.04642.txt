Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations.
However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly.
In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory.
Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training.
While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7 percent.
