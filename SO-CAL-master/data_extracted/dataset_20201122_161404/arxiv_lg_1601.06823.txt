The recurrent neural networks (RNN) can be used to solve the sequence to sequence problem, where both the input and the output have sequential structures.
Usually there are some implicit relations between the structures.
However, it is hard for the common RNN model to fully explore the relations between the sequences.
In this survey, we introduce some attention based RNN models which can focus on different parts of the input for each output item, in order to explore and take advantage of the implicit relations between the input and the output items.
The different attention mechanisms are described in detail.
We then introduce some applications in computer vision which apply the attention based RNN models.
The superiority of the attention based RNN model is shown by the experimental results.
At last some future research directions are given.
