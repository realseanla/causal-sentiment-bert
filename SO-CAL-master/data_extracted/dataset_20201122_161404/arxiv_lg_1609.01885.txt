Extracting and understanding affective states of subjects through analysis of face images/videos is of high consequence to advance the levels of interaction in human-computer interfaces.
This paper aims to highlight vision-related tasks focused on understanding "reactions" of subjects to presented content which has not been largely studied by the vision community in comparison to other emotions.
To facilitate future study in this field, we present an effort in collecting DAiSEE, a free to use large-scale dataset using crowd annotation, that not only simulates a real world setting for e-learning environments, but also captures the interpretability issues of such affective states by human annotators.
In addition to the dataset, we present benchmark results based on standard baseline methods and vote aggregation strategies, thus providing a springboard for further research.
