Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information.
Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled.
This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT?
2) Is tight integration of words and syntax better than multitask training?
We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence.
Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English-German, a high-resource pair, and for English-Romanian, a low-resource pair and also several syntactic phenomena including prepositional phrase attachment.
Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.
