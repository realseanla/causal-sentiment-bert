In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, for handling an unbounded action space with applications to language understanding for text-based games.
For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward.
In these games, the best action is typically that which fits the best to the current situation (modeled as a state in the DRRN), also described by text.
Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions.
Therefore, it is very difficult to pre-define the action set as in the deep Q-network (DQN).
To address this challenge, the DRRN extracts high-level embedding vectors from the texts that describe states and actions, respectively, and computes the inner products between the state and action embedding vectors to approximate the Q-function.
We evaluate the DRRN on two popular text games, showing superior performance over the DQN.
