We introduce a simple and accurate neural model for dependency-based semantic role labeling.
Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder.
The semantic role labeler achieves respectable performance on English even without any kind of syntactic information and only using local inference.
However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the CoNLL-2009 dataset.
Syntactic parsers are unreliable on out-of-domain data, so standard (i.e.
syntactically-informed) SRL models are hindered when tested in this setting.
Our syntax-agnostic model appears more robust, resulting in the best reported results on the standard out-of-domain test set.
