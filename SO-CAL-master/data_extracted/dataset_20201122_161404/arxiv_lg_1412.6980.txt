We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions.
The method is straightforward to implement and is based an adaptive estimates of lower-order moments of the gradients.
The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters.
The method is also ap- propriate for non-stationary objectives and problems with very noisy and/or sparse gradients.
The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function.
The hyper-parameters have intuitive interpretations and typically require little tuning.
Some connections to related algorithms, on which Adam was inspired, are discussed.
We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework.
We demonstrate that Adam works well in practice when experimentally compared to other stochastic optimization methods.
