With the rapid growth of knowledge bases (KBs) on the web, how to take full advantage of them becomes increasingly important.
Knowledge base-based question answering (KB-QA) is one of the most promising approaches to access the substantial knowledge.
Meantime, as the neural network-based (NN-based) methods develop, NN-based KB-QA has already achieved impressive results.
However, previous work did not put emphasis on question representation, and the question is converted into a fixed vector regardless of its candidate answers.
This simple representation strategy is unable to express the proper information of the question.
Hence, we present a neural attention-based model to represent the questions dynamically according to the different focuses of various candidate answer aspects.
In addition, we leverage the global knowledge inside the underlying KB, aiming at integrating the rich KB information into the representation of the answers.
And it also alleviates the out of vocabulary (OOV) problem, which helps the attention model to represent the question more precisely.
The experimental results on WEBQUESTIONS demonstrate the effectiveness of the proposed approach.
