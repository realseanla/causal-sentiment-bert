Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim.
CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures.
We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control.
The estimation scheme that we propose uses the empirical distribution in order to estimate the CPT-value of a random variable.
We then use this scheme in the inner loop of policy optimization procedures for a Markov decision process (MDP).
We propose both gradient-based as well as gradient-free policy optimization algorithms.
The former includes both first-order and second-order methods that are based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA), while the latter is based on a reference distribution that concentrates on the global optima.
Using an empirical distribution over the policy space in conjunction with Kullback-Leibler (KL) divergence to the reference distribution, we get a global policy optimization scheme.
We provide theoretical convergence guarantees for all the proposed algorithms.
