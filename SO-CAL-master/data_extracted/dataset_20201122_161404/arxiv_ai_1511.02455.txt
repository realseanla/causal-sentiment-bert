This paper presents a theoretical, idealized model of the thinking process with the following characteristics: 1) the model can produce complex thought sequences and can be generalized to new inputs, 2) it can receive and maintain input information indefinitely for generation of thoughts and later use, and 3) it supports learning while executing.
The crux of the model lies within the concept of internal consistency, or the generated thoughts should always be consistent with the inputs from which they are created.
Its merit, apart from the capability to generate new creative thoughts from internal mechanism, depends on the potential to help training to generalize better.
This is consequently enabled by separating input information into several parts to be handled by different processing components with a focus mechanism to fetch information for each.
This modularized view with focus binds the model with the computationally capable Turing machines.
And as a final remark, this paper constructively shows that the computational complexity of the model is at least, if not surpass, that of a universal Turing machine.
