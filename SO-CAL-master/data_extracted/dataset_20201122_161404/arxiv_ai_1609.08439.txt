Robotic code needs to be verified to ensure its safety and functional correctness, especially when the robot is interacting with people.
Testing the real code in simulation is a viable option.
It reduces the costs of experiments and provides detail that is lost when using formal methods.
However, generating tests that cover interesting scenarios, while executing most of the code, is a challenge amplified by the complexity of the interactions between the environment and the software.
Model-based test generation methods can automate otherwise manual processes and facilitate reaching rare scenarios during testing.
In this paper, we compare the use of Belief-Desire-Intention (BDI) agents as models for test generation, with more conventional, model-based test generation, that exploits automata and model checking techniques, and random test generation methods, in terms of practicality, performance, scalability, and exploration (`coverage').
Simulators and automated testbenches were implemented in Robot Operating System (ROS) and Gazebo, for testing the code of two robots, BERT2 in a cooperative manufacture (table assembly) task, and Tiago as a home care assistant.
The results highlight the clear advantages of using BDI agents for test generation, compared to random and conventional automata-based approaches.
BDI agents naturally emulate the agency present in Human-Robot Interaction (HRI).
They are thus more expressive and scale well in HRI applications.
