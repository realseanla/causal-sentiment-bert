We investigate extensions of well-known online learning algorithms such as fixed-share of Herbster and Warmuth (1998) or the methods proposed by Bousquet and Warmuth (2002).
These algorithms use weight sharing schemes to perform as well as the best sequence of experts with a limited number of changes.
Here we show, with a common, general, and simpler analysis, that weight sharing in fact achieves much more than what it was designed for.
We use it to simultaneously prove new shifting regret bounds for online convex optimization on the simplex in terms of the total variation distance as well as new bounds for the related setting of adaptive regret.
Finally, we exhibit the first logarithmic shifting bounds for exp-concave loss functions on the simplex.
