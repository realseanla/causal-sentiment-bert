In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems.
The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g.
ROC curves.
In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm ||C||.
We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss.
Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid's inequality.
As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly.
To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.
