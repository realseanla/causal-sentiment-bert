Recent studies have shown that embedding textual relations using deep neural networks greatly helps relation extraction.
However, many existing studies rely on supervised learning; their performance is dramatically limited by the availability of training data.
In this work, we generalize textual relation embedding to the distant supervision setting, where much larger-scale but noisy training data is available.
We propose leveraging global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus, to embed textual relations.
This approach turns out to be more robust to the training noise introduced by distant supervision.
On a popular relation extraction dataset, we show that the learned textual relation embeddings can be used to augment existing relation extraction models and significantly improve their performance.
Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9 percent to 89.3 percent.
