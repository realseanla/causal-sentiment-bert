Low-rank learning has attracted much attention recently due to its efficacy in a rich variety of real-world tasks, e.g., subspace segmentation and image categorization.
Most low-rank methods are incapable of capturing low-dimensional subspace for supervised learning tasks, e.g., classification and regression.
This paper aims to learn both the discriminant low-rank representation (LRR) and the robust projecting subspace in a supervised manner.
To achieve this goal, we cast the problem into a constrained rank minimization framework by adopting the least squares regularization.
Naturally, the data label structure tends to resemble that of the corresponding low-dimensional representation, which is derived from the robust subspace projection of clean data by low-rank learning.
Moreover, the low-dimensional representation of original data can be paired with some informative structure by imposing an appropriate constraint, e.g., Laplacian regularizer.
Therefore, we propose a novel constrained LRR method.
The objective function is formulated as a constrained nuclear norm minimization problem, which can be solved by the inexact augmented Lagrange multiplier algorithm.
Extensive experiments on image classification, human pose estimation, and robust face recovery have confirmed the superiority of our method.
