Discourse coherence is strongly associated with text quality, making it important to natural language generation and understanding.
Yet existing models of coherence focus on individual aspects of coherence (lexical overlap, rhetorical structure, entity centering) and are trained on narrow domains.
We introduce algorithms that capture diverse kinds of coherence by learning to distinguish coherent from incoherent discourse from vast amounts of open-domain training data.
We propose two models, one discriminative and one generative, both using LSTMs as the backbone.
The discriminative model treats windows of sentences from original human-generated articles as coherent examples and windows generated by randomly replacing sentences as incoherent examples.
The generative model is a \sts model that estimates the probability of generating a sentence given its contexts.
Our models achieve state-of-the-art performance on multiple coherence evaluations.
Qualitative analysis suggests that our generative model captures many aspects of coherence including lexical, temporal, causal, and entity-based coherence.
