This paper proposes a general framework to combine context and commonsense knowledge for solving the Winograd Schema (WS) and Pronoun Disambiguation Problems (PDP).
In the proposed framework, commonsense knowledge bases (e.g.
cause-effect word pairs) are quantized as knowledge constraints.
The constraints guide us to learn knowledge enhanced embeddings (KEE) from large text corpus.
Based on the pre-trained KEE models, this paper proposes two methods to solve the WS and PDP problems.
The first method is an unsupervised method, which represents all the pronouns and candidate mentions in continuous vector spaces based on their contexts and calculates the semantic similarities between all the possible word pairs.
The pronoun disambiguation procedure could then be implemented by comparing the semantic similarities between the pronoun (to be resolved) and all the candidate mentions.
The second method is a supervised method, which extracts features for all the pronouns and candidate mentions and solves the WS problems by training a typical mention pair classification model.
Similar to the first method, the features used in the second method are also extracted based on the KEE models.
Experiments conducted on the available PDP and WS test sets show that, these two methods both achieve consistent improvements over the baseline systems.
The best performance reaches 62\ percent in accuracy on the PDP test set of the first Winograd Schema Challenge.
