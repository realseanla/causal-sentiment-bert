Mental health forums are online communities where people express their issues and seek help from moderators and other users.
In such forums, there are often posts with severe content indicating that the user is in acute distress and there is a risk of attempted self-harm.
Moderators need to respond to these severe posts in a timely manner to prevent potential self-harm.
However, the large volume of daily posted content makes it difficult for the moderators to locate and respond to these critical posts.
We present a framework for triaging user content into four severity categories which are defined based on indications of self-harm ideation.
Our models are based on a feature-rich classification framework which includes lexical, psycholinguistic, contextual and topic modeling features.
Our approaches improve the state of the art in triaging the content severity in mental health forums by large margins (up to 17 percent improvement over the F-1 scores).
Using the proposed model, we analyze the mental state of users and we show that overall, long-term users of the forum demonstrate a decreased severity of risk over time.
Our analysis on the interaction of the moderators with the users further indicates that without an automatic way to identify critical content, it is indeed challenging for the moderators to provide timely response to the users in need.
