Functional Distributional Semantics is a framework that aims to learn, from text, semantic representations which can be interpreted in terms of truth.
Here we make two contributions to this framework.
The first is to show how a type of logical inference can be performed by evaluating conditional probabilities.
The second is to make these calculations tractable by means of a variational approximation.
This approximation also enables faster convergence during training, allowing us to close the gap with state-of-the-art vector space models when evaluating on semantic similarity.
We demonstrate promising performance on two tasks.
