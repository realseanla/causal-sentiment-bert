Recent work in automatic recognition of conversational telephone speech (CTS) has achieved accuracy levels comparable to human transcribers, although there is some debate how to precisely quantify human performance on this task, using the NIST 2000 CTS evaluation set.
This raises the question what systematic differences, if any, may be found differentiating human from machine transcription errors.
In this paper we approach this question by comparing the output of our most accurate CTS recognition system to that of a standard speech transcription vendor pipeline.
We find that the most frequent substitution, deletion and insertion error types of both outputs show a high degree of overlap.
The only notable exception is that the automatic recognizer tends to confuse filled pauses ("uh") and backchannel acknowledgments ("uhhuh").
Humans tend not to make this error, presumably due to the distinctive and opposing pragmatic functions attached to these words.
Furthermore, we quantify the correlation between human and machine errors at the speaker level, and investigate the effect of speaker overlap between training and test data.
Finally, we report on an informal "Turing test" asking humans to discriminate between automatic and human transcription error cases.
