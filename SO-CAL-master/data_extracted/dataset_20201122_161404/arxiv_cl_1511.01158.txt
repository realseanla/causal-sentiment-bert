This paper is an empirical study of the distributed deep learning for a question answering subtask: answer selection.
Comparison studies of SGD, MSGD, DOWNPOUR and EASGD/EAMSGD algorithms have been presented.
Experimental results show that the message passing interface based distributed framework can accelerate the convergence speed at a sublinear scale.
This paper demonstrates the importance of distributed training: with 120 workers, an 83x speedup is achievable and running time is decreased from 107.9 hours to 1.3 hours, which will benefit the productivity significantly.
