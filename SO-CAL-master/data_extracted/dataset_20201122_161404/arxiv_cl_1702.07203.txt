We investigate the use of pivot languages for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora.
We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models.
In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts.
We also show that using multiple related pivot languages can outperform a direct translation model.
Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus.
Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting.
