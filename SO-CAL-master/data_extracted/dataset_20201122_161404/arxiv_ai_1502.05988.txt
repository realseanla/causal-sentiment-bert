In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time.
Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training.
This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy.
In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time.
For this task we use a deep learning approach with restricted Boltzmann machines.
We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature
