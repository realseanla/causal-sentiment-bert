This paper develops a model that addresses syntactic embedding for machine comprehension, a key task of natural language understanding.
Our proposed model, structural embedding of syntactic trees (SEST), takes each word in a sentence, constructs a sequence of syntactic nodes extracted from syntactic parse trees, and encodes the sequence into a vector representation.
The learned vector is then incorporated into neural attention models, which allows learning the mapping of syntactic structures between question and context pairs.
We evaluate our approach on SQuAD dataset and demonstrate that our model can accurately identify the syntactic boundaries of the sentences and to extract answers that are syntactically coherent over the baseline methods.
