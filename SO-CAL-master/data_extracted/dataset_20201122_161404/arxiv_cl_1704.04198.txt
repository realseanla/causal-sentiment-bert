In recent years we have seen rapid and significant progress in automatic image description but what are the open problems in this area?
Most work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved.
In this paper, we present a detailed error analysis of the descriptions generated by a state-of-the-art attention-based model.
Our analysis operates on two levels: first we check the descriptions for accuracy, and then we categorize the types of errors we observe in the inaccurate descriptions.
We find only 20 percent of the descriptions are free from errors, and surprisingly that 26 percent are unrelated to the image.
Finally, we manually correct the most frequently occurring error types (e.g.
gender identification) to estimate the performance reward for addressing these errors, observing gains of 0.2--1 BLEU point per type.
