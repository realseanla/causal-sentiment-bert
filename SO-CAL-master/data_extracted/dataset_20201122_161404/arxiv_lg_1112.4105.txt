We study the worst case error of kernel density estimates via subset approximation.
A kernel density estimate of a distribution is the convolution of that distribution with a fixed kernel (e.g.
Gaussian kernel).
Given a subset (i.e.
a point set) of the input distribution, we can compare the kernel density estimates of the input distribution with the subset, and bound the worst case error.
If the maximum error is eps, then this subset can be thought of as an eps-sample (aka an eps-approximation) of the range space defined with the input distribution as the ground set and the fixed kernel representing the family of ranges.
Note in this case the ranges are not binary, but have a continuous range (for simplicity we mainly discuss kernels with range of [0,1]); these allow for smoother notions of range spaces, the same way that a kernel density estimates have improved upon histograms.
It turns out, the use of this smoother family of range spaces has an added benefit of greatly decreasing the size required for eps-samples (in the plane) from O((1/eps^{4/3}) log^{2/3}(1/eps)) for disks to O((1/eps) sqrt{log (1/eps)}) for Gaussian kernels and for kernels with bounded slope that only affect a bounded domain.
