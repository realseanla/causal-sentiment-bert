Recent research in the deep learning field has produced a plethora of new architectures.
At the same time, a growing number of groups are applying deep learning to new applications and problems.
Many of these groups might be composed of inexperienced deep learning practitioners who are baffled by the dizzying array of architecture choices and therefore use an older architecture, such as Alexnet.
Here, we are attempting to bridge this gap by mining the collective knowledge contained in recent deep learning research to discover underlying principles for designing neural network architectures.
In addition, we describe several architectural innovations, including Fractal of FractalNet, Stagewise Boosting Networks, and Taylor Series Networks (our Caffe code and prototxt files will be made publicly available).
We hope others are inspired to build on this preliminary work.
