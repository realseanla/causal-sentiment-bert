Although different learning systems are coordinated to afford complex behavior, little is known about how this occurs.
This article describes a theoretical framework that specifies how complex behaviors that might be thought to require error-driven learning might instead be acquired through simple reinforcement.
This framework includes specific assumptions about the mechanisms that contribute to the evolution of (artificial) neural networks to generate topologies that allow the networks to learn large-scale complex problems using only information about the quality of their performance.
The practical and theoretical implications of the framework are discussed, as are possible biological analogs of the approach.
