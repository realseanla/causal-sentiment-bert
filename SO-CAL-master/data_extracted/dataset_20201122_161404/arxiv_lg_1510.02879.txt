The ability to transfer knowledge from learnt source tasks to a new target task can be very useful in speeding up the learning process of a Reinforcement Learning agent.
This has been receiving a lot of attention, but the application of transfer poses two serious challenges which have not been adequately addressed in the past.
First, the agent should be able to avoid negative transfer, which happens when the transfer hampers or slows down the learning instead of speeding it up.
Secondly, the agent should be able to do selective transfer which is the ability to select and transfer from different and multiple source tasks for different parts of the state space of the target task.
We propose ADAAPT: A Deep Architecture for Adaptive Policy Transfer, which addresses these challenges.
We test ADAAPT using two different instantiations: One as ADAAPTive REINFORCE algorithm for direct policy search and another as ADAAPTive Actor-Critic where the actor uses ADAAPT.
Empirical evaluations on simulated domains show that ADAAPT can be effectively used for policy transfer from multiple source MDPs sharing the same state and action space.
