End-to-end neural machine translation has overtaken statistical machine translation in terms of translation quality for some language pairs, specially those with a large amount of parallel data available.
Beside this palpable improvement, neural networks embrace several new properties.
A single system can be trained to translate between many languages at almost no additional cost other than training time.
Furthermore, internal representations learned by the network serve as a new semantic representation of words -or sentences- which, unlike standard word embeddings, are learned in an essentially bilingual or even multilingual context.
In view of these properties, the contribution of the present work is two-fold.
First, we systematically study the context vectors, i.e.
output of the encoder, and their prowess as an interlingua representation of a sentence.
Their quality and effectiveness are assessed by similarity measures across translations, semantically related, and semantically unrelated sentence pairs.
Second, and as extrinsic evaluation of the first point, we identify parallel sentences in comparable corpora, obtaining an F1=98.2 percent on data from a shared task when using only context vectors.
F1 reaches 98.9 percent when complementary similarity measures are used.
