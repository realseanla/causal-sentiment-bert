In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy.
Our method is based on a soft relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training.
We showcase this method for two challenging applications: Image compression and neural network compression.
While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.
