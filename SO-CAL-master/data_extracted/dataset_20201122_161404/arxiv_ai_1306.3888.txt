This article is an overview of the "SP theory of intelligence".
The theory aims to simplify and integrate concepts across artificial intelligence, mainstream computing and human perception and cognition, with information compression as a unifying theme.
It is conceived as a brain-like system that receives 'New' information and stores some or all of it in compressed form as 'Old' information.
It is realised in the form of a computer model - a first version of the SP machine.
The concept of "multiple alignment" is a powerful central idea.
Using heuristic techniques, the system builds multiple alignments that are 'good' in terms of information compression.
For each multiple alignment, probabilities may be calculated.
These provide the basis for calculating the probabilities of inferences.
The system learns new structures from partial matches between patterns.
Using heuristic techniques, the system searches for sets of structures that are 'good' in terms of information compression.
These are normally ones that people judge to be 'natural', in accordance with the 'DONSVIC' principle -- the discovery of natural structures via information compression.
The SP theory may be applied in several areas including 'computing', aspects of mathematics and logic, representation of knowledge, natural language processing, pattern recognition, several kinds of reasoning, information storage and retrieval, planning and problem solving, information compression, neuroscience, and human perception and cognition.
Examples include the parsing and production of language including discontinuous dependencies in syntax, pattern recognition at multiple levels of abstraction and its integration with part-whole relations, nonmonotonic reasoning and reasoning with default values, reasoning in Bayesian networks including 'explaining away', causal diagnosis, and the solving of a geometric analogy problem.
