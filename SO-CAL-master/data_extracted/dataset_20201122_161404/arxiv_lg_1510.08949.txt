We investigate attention as the active pursuit of useful information.
This contrasts with attention as a mechanism for the attenuation of irrelevant information.
We also consider the role of short-term memory, whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends.
We present several simple synthetic tasks, which become considerably more interesting when we impose strong constraints on how a model can interact with its input, and on how long it can take to produce its output.
We develop a model with a different structure from those seen in previous work, and we train it using stochastic variational inference with a learned proposal distribution.
