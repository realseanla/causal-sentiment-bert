Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms.
This paper explores the use of Bregman divergences to establish reductions between such algorithms and their analyses.
We present a new scaled isodistortion theorem involving Bregman divergences (scaled Bregman theorem for short) which shows that certain "Bregman distortions'" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data.
Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation, while admissible data include scalars, vectors and matrices.
