We show that learning algorithms satisfying a $\textit{low approximate regret}$ property experience fast convergence to approximate optimality in a large class of repeated games.
Our property, which simply requires that each learner has small regret compared to a $(1+\epsilon)$-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms - it is satisfied even by the vanilla Hedge forecaster.
Our results improve upon recent work of Syrgkanis et al.
[SALS15] in a number of ways.
We improve upon the speed of convergence by a factor of n, the number of players, and require only that the players observe payoffs under other players' realized actions, as opposed to expected payoffs.
We further show that convergence occurs with high probability, and under certain conditions show convergence under bandit feedback.
Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work.
