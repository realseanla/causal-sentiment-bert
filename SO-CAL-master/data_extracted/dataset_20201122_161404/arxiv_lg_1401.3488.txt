We present a novel Bayesian topic model for learning discourse-level document structure.
Our model leverages insights from discourse theory to constrain latent topic assignments in a way that reflects the underlying organization of document topics.
We propose a global model in which both topic selection and ordering are biased to be similar across a collection of related documents.
We show that this space of orderings can be effectively represented using a distribution over permutations called the Generalized Mallows Model.
We apply our method to three complementary discourse-level tasks: cross-document alignment, document segmentation, and information ordering.
Our experiments show that incorporating our permutation-based model in these applications yields substantial improvements in performance over previously proposed methods.
