We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English&lt;-&gt;Czech, English&lt;-&gt;German, English&lt;-&gt;Romanian and English&lt;-&gt;Russian.
Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary.
We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models.
All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems.
