We introduce a neural network that represents sentences by composing their words according to induced binary parse trees.
We use Tree-LSTM as our composition function, applied along a tree structure found by a fully differentiable natural language chart parser.
Our model simultaneously optimises both the composition function and the parser, thus eliminating the need for externally-provided parse trees which are normally required for Tree-LSTM.
It can therefore be seen as a tree-based RNN that is unsupervised with respect to the parse trees.
As it is fully differentiable, our model is easily trained with an off-the-shelf gradient descent method and backpropagation.
We demonstrate that it achieves better performance compared to various supervised Tree-LSTM architectures on a textual entailment task and a reverse dictionary task.
