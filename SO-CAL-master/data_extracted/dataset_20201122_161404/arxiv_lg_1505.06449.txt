We extend previous work on efficiently training linear models by applying stochastic updates to non-zero features only, lazily bringing weights current as needed.
To date, only the closed form updates for the $\ell_1$, $\ell_{\infty}$, and the rarely used $\ell_2$ norm have been described.
We extend this work by showing the proper closed form updates for the popular $\ell^2_2$ and elastic net regularized models.
We show a dynamic programming algorithm to calculate the proper elastic net update with only one constant-time subproblem computation per update.
Our algorithm handles both fixed and decreasing learning rates and we derive the result for both stochastic gradient descent (SGD) and forward backward splitting (FoBoS).
We empirically validate the algorithm, showing that on a bag-of-words dataset with $260,941$ features and $88$ nonzero features on average per example, our method trains a logistic regression classifier with elastic net regularization $612$ times faster than an otherwise identical implementation with dense updates.
