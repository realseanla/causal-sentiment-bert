Unsupervised feature selection has been always attracting research attention in the communities of machine learning and data mining for decades.
In this paper, we propose an unsupervised feature selection method seeking a feature coefficient matrix to select the most distinctive features.
Specifically, our proposed algorithm integrates the Maximum Margin Criterion with a sparsity-based model into a joint framework, where the class margin and feature correlation are taken into account at the same time.
To maximize the total data separability while preserving minimized within-class scatter simultaneously, we propose to embed Kmeans into the framework generating pseudo class label information in a scenario of unsupervised feature selection.
Meanwhile, a sparsity-based model, ` 2 ,p-norm, is imposed to the regularization term to effectively discover the sparse structures of the feature coefficient matrix.
In this way, noisy and irrelevant features are removed by ruling out those features whose corresponding coefficients are zeros.
To alleviate the local optimum problem that is caused by random initializations of K-means, a convergence guaranteed algorithm with an updating strategy for the clustering indicator matrix, is proposed to iteractively chase the optimal solution.
Performance evaluation is extensively conducted over six benchmark data sets.
From plenty of experimental results, it is demonstrated that our method has superior performance against all other compared approaches.
