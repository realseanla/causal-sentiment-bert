Big data consists of large multidimensional datasets that would often be difficult to analyze if working with the original tensor.
There is a rising interest in the use of tensor decompositions to approximate large tensors in order to reduce their dimensions by selecting important features for classification.
Of particular interest is the Tucker decomposition (TD) that has already been applied in neuroscience, geoscience, signal processing, pattern and image recognition.
However the decomposition itself leads to exponential computational time for high-order tensors.
To circumvent this obstacle we propose an alternative known as the matrix product state (MPS) decomposition for the data representation of big data tensors.
This decomposition has been used extensively in quantum physics within the last decade and its benefit has surprisingly not been seen in other areas of research.
We prove that the MPS decomposition for feature extraction and classification in supervised learning can be implemented efficiently with high classification rates in pattern and image recognition.
