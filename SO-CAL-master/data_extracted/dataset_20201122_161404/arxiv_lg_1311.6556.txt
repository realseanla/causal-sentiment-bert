We consider the problem of building classifiers with the option to reject i.e., not return a prediction on a given test example.
Adding a reject option to a classifier is well-known in practice; traditionally, this has been accomplished in two different ways.
One is the {\em decoupled} method where an optimal base classifier (without the reject option) is build first and then the rejection boundary is optimized, typically in terms of a band around the separating surface.
The {\em coupled} method is based on finding both the classifier as well as the rejection band at the same time.
Existing coupled approaches are based on minimizing risk under an extension of the classical $0-1$ loss function wherein a loss $d \in (0,.5)$ is assigned to a rejected example.
In this paper, we propose a {\bf double ramp loss} function which gives a continuous upper bound for $(0-d-1)$ loss described above.
Our coupled approach is based on minimizing regularized risk under the double ramp loss which is done using difference of convex (DC) programming.
We show the effectiveness of our approach through experiments on synthetic and benchmark datasets.
