This paper makes progress on several open theoretical issues related to Generative Adversarial Networks.
A definition is provided for what it means for the training to generalize, and it is shown that generalization is not guaranteed for the popular distances between distributions such as Jensen-Shannon or Wasserstein.
We introduce a new metric called neural net distance for which generalization does occur.
We also show that an approximate pure equilibrium in the 2-player game exists for a natural training objective (Wasserstein).
Showing such a result has been an open problem (for any training objective).
