We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment.
Specifically, we construct a 'coreset' representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space.
We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection.
Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos.
The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints.
We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average.
This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data.
