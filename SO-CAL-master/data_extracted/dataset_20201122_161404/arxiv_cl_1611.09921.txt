Statistical topic models efficiently facilitate the exploration of large-scale data sets.
Many models have been developed and broadly used to summarize the semantic structure in news, science, social media, and digital humanities.
However, a common and practical objective in data exploration tasks is not to enumerate all existing topics, but to quickly extract representative ones that broadly cover the content of the corpus, i.e., a few topics that serve as a good summary of the data.
Most existing topic models fit exactly the same number of topics as a user specifies, which have imposed an unnecessary burden to the users who have limited prior knowledge.
We instead propose new models that are able to learn fewer but more representative topics for the purpose of data summarization.
We propose a reinforced random walk that allows prominent topics to absorb tokens from similar and smaller topics, thus enhances the diversity among the top topics extracted.
With this reinforced random walk as a general process embedded in classical topic models, we obtain \textit{diverse topic models} that are able to extract the most prominent and diverse topics from data.
The inference procedures of these diverse topic models remain as simple and efficient as the classical models.
Experimental results demonstrate that the diverse topic models not only discover topics that better summarize the data, but also require minimal prior knowledge of the users.
