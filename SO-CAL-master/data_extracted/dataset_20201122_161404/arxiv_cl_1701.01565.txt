Reproducing experiments is an important instrument to validate previous work and build upon existing approaches.
It has been tackled numerous times in different areas of science.
In this paper, we introduce an empirical replicability study of three well-known algorithms for syntactic centric aspect-based opinion mining.
We show that reproducing results continues to be a difficult endeavor, mainly due to the lack of details regarding preprocessing and parameter setting, as well as due to the absence of available implementations that clarify these details.
We consider these are important threats to validity of the research on the field, specifically when compared to other problems in NLP where public datasets and code availability are critical validity components.
We conclude by encouraging code-based research, which we think has a key role in helping researchers to understand the meaning of the state-of-the-art better and to generate continuous advances.
