Due to the recent cases of algorithmic bias in data-driven decision-making, machine learning methods are being put under the microscope in order to understand the root cause of these biases and how to correct them.
Here, we consider a basic algorithmic task that is central in machine learning: subsampling from a large data set.
Subsamples are used both as an end-goal in data summarization (where fairness could either be a legal, political or moral requirement) and to train algorithms (where biases in the samples are often a source of bias in the resulting model).
Consequently, there is a growing effort to modify either the subsampling methods or the algorithms themselves in order to ensure fairness.
However, in doing so, a question that seems to be overlooked is whether it is possible to produce fair subsamples that are also adequately representative of the feature space of the data set - an important and classic requirement in machine learning.
Can diversity and fairness be simultaneously ensured?
We start by noting that, in some applications, guaranteeing one does not necessarily guarantee the other, and a new approach is required.
Subsequently, we present an algorithmic framework which allows us to produce both fair and diverse samples.
Our experimental results on an image summarization task show marked improvements in fairness without compromising feature diversity by much, giving us the best of both the worlds.
