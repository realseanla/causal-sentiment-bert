We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet.
This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer.
The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network .
The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification.
The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s).
Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling.
This eliminates the need for learning to upsample.
The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps.
We compare our proposed architecture with the fully convolutional network (FCN) architecture and its variants.
This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance.
The design of SegNet was primarily motivated by road scene understanding applications.
Hence, it is efficient both in terms of memory and computational time during inference.
It is also significantly smaller in the number of trainable parameters than competing architectures and can be trained end-to-end using stochastic gradient descent.
We also benchmark the performance of SegNet on Pascal VOC12 salient object segmentation and the recent SUN RGB-D indoor scene understanding challenge.
We show that SegNet provides competitive performance although it is significantly smaller than other architectures.
We also provide a Caffe implementation of SegNet and a webdemo at
