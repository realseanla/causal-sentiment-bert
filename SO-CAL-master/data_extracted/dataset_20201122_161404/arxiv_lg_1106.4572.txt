We develop, analyze, and evaluate a novel, supervised, specific-to-general learner for a simple temporal logic and use the resulting algorithm to learn visual event definitions from video sequences.
First, we introduce a simple, propositional, temporal, event-description language called AMA that is sufficiently expressive to represent many events yet sufficiently restrictive to support learning.
We then give algorithms, along with lower and upper complexity bounds, for the subsumption and generalization problems for AMA formulas.
We present a positive-examples--only specific-to-general learning method based on these algorithms.
We also present a polynomial-time--computable ``syntactic'' subsumption test that implies semantic subsumption without being equivalent to it.
A generalization algorithm based on syntactic subsumption can be used in place of semantic generalization to improve the asymptotic complexity of the resulting learning algorithm.
Finally, we apply this algorithm to the task of learning relational event definitions from video and show that it yields definitions that are competitive with hand-coded ones.
