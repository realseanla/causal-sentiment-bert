Learned models composed of probabilistic logical rules are useful for many tasks, such as knowledge base completion.
Unfortunately this learning problem is difficult, since determining the structure of the theory normally requires solving a discrete optimization problem.
In this paper, we propose an alternative approach: a completely differentiable model for learning sets of first-order rules.
The approach is inspired by a recently-developed differentiable logic, i.e.
a subset of first-order logic for which inference tasks can be compiled into sequences of differentiable operations.
Here we describe a neural controller system which learns how to sequentially compose the these primitive differentiable operations to solve reasoning tasks, and in particular, to perform knowledge base completion.
The long-term goal of this work is to develop integrated, end-to-end systems that can learn to perform high-level logical reasoning as well as lower-level perceptual tasks.
