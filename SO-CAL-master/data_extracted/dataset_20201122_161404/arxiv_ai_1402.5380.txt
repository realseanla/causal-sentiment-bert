It is known that benign looking AI objectives may result in powerful AI drives that may pose a risk to the human society.
We examine the alternative scenario of what happens when universal goals that are not human-centric are used for designing AI agents.
We follow a design approach that tries to exclude malevolent motivations from AI's, however, we see that even objectives that seem benevolent at first may pose significant risk to humanity.
We also discuss various solution approaches including selfless goals, hybrid designs, universal constraints, and generalization of robot laws.
