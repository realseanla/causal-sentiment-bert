We present a study on two key characteristics of human syntactic annotations: anchoring and agreement.
Anchoring is a well known cognitive bias in human decision making, where judgments are drawn towards pre-existing values.
We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output.
Our experiments demonstrate a clear anchoring effect and reveal unwanted consequences, including overestimation of parsing performance and lower quality of annotations in comparison with human-based annotations.
Using sentences from the Penn Treebank WSJ, we also report the first systematically obtained inter-annotator agreement estimates for English syntactic parsing.
Our agreement results control for anchoring bias, and are consequential in that they are \emph{on par} with state of the art parsing performance for English.
We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations.
