Deep learning has been popularized by its recent successes on challenging artificial intelligence problems.
One of the reasons for its dominance is also an ongoing challenge: the need for immense amounts of computational power.
Hardware architects have responded by proposing a wide array of promising ideas, but to date, the majority of the work has focused on specific algorithms in somewhat narrow application domains.
While their specificity does not diminish these approaches, there is a clear need for more flexible solutions.
We believe the first step is to examine the characteristics of cutting edge models from across the deep learning community.
