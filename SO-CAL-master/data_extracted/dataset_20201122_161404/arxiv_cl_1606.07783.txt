We investigate the usage of convolutional neural networks (CNNs) for the slot filling task in spoken language understanding.
We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context.
Moreover, it combines the information from the past and the future words for classification.
Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61 percent on the ATIS benchmark dataset without using any additional linguistic knowledge and resources.
