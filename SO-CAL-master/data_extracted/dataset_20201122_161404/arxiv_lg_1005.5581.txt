Many classical bounds on the sample complexity of active learning based on the realizability assumption have been derived, which show that active learning can exponentially improve the sample complexity over passive learning.
However, this realizability assumption could not be met in practice and few results on the exponential improvement in the sample complexity in the non-realizable case has been obtained.
In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case with Tsybakov noise condition under the multi-view setting.
We prove that the sample complexity of active learning with unbounded Tsybakov noise can be $\widetilde{O}(\log \frac{1}{\epsilon})$, contrasting to that polynomial improvement is the best possible achievement with the same noise condition in single-view setting.
We also prove that, contrasting to that in previous polynomial bounds the order of $1/\epsilon$ is related to the Tsybakov noise condition, in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\widetilde{O}(\frac{1}{\epsilon})$, where the order of $1/\epsilon$ is independent of the Tsybakov noise condition.
