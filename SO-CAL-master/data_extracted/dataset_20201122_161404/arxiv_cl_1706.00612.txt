Speech emotion recognition is an important and challenging task in the realm of human-computer interaction.
Prior work proposed a variety of models and feature sets for training a system.
In this work, we conduct extensive experiments using an attentive convolutional neural network with multi-view learning objective function.
We compare system performance using different lengths of the input signal, different types of acoustic features and different types of emotion speech (improvised/scripted).
Our experimental results on the Interactive Emotional Motion Capture (IEMOCAP) database reveal that the recognition performance strongly depends on the type of speech data independent of the choice of input features.
Furthermore, we achieved state-of-the-art results on the improvised speech data of IEMOCAP.
