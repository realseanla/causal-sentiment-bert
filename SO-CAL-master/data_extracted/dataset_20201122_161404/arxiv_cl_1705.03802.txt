Recently, several data-sets associating data to text have been created to train data-to-text surface realisers.
It is unclear however to what extent the surface realisation task exercised by these data-sets is linguistically challenging.
Do these data-sets provide enough variety to encourage the development of generic, high-quality data-to-text surface realisers ?
In this paper, we argue that these data-sets have important drawbacks.
We back up our claim using statistics, metrics and manual evaluation.
We conclude by eliciting a set of criteria for the creation of a data-to-text benchmark which could help better support the development, evaluation and comparison of linguistically sophisticated data-to-text surface realisers.
