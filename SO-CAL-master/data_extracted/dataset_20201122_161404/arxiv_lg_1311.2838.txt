Lifelong learning focuses on the idea of retaining and reusing the knowledge learned from observed tasks for solving a new but related task.
In this work we present a PAC-Bayesian bound on the generalization error in the lifelong learning framework.
Our result gives a theoretical justification of how exploring the data from several related tasks makes it possible to automatically learn prior knowledge and how a prior that performs well on sufficiently many tasks guarantees good learning performance on new tasks from the same environment with high probability.
