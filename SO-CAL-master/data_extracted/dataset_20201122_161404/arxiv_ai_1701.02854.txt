In this work, we propose a novel decoding approach for neural machine translation (NMT) based on continuous optimisation.
The resulting optimisation problem can then be tackled using a whole range of continuous optimisation algorithms which have been developed and used in the literature mainly for training.
Our approach is general and can be applied to other sequence-to-sequence neural models as well.
We make use of this powerful decoding approach to intersect an underlying NMT with a language model, to intersect left-to-right and right-to-left NMT models, and to decode with soft constraints involving coverage and fertility of the source sentence words.
The experimental results show the promise of the proposed framework.
