The mathematical representation of semantics is a key issue for Natural Language Processing (NLP).
A lot of research has been devoted to finding ways of representing the semantics of individual words in vector spaces.
Distributional approaches --- meaning distributed representations that exploit co-occurrence statistics of large corpora --- have proved popular and successful across a number of tasks.
However, natural language usually comes in structures beyond the word level, with meaning arising not only from the individual words but also the structure they are contained in at the phrasal or sentential level.
Modelling the compositional process by which the meaning of an utterance arises from the meaning of its parts is an equally fundamental task of NLP.
