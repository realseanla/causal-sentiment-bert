The temporal-difference methods TD($\lambda$) and Sarsa($\lambda$) form a core part of modern reinforcement learning.
Their appeal comes from their good performance, low computational cost, and their simple interpretation, given by their forward view.
Recently, new versions of these methods were introduced, called true online TD($\lambda$) and true online Sarsa($\lambda$), respectively (van Seijen and Sutton, 2014).
Algorithmically, these true online methods only make two small changes to the update rules of the regular methods, and the extra computational cost is negligible in most cases.
However, they follow the ideas underlying the forward view much more closely.
In particular, they maintain an exact equivalence with the forward view at all times, whereas the traditional versions only approximate it for small step-sizes.
We hypothesize that these true online methods not only have better theoretical properties, but also dominate the regular methods empirically.
In this article, we put this hypothesis to the test by performing an extensive empirical comparison.
Specifically, we compare the performance of true online TD($\lambda$)/Sarsa($\lambda$) with regular TD($\lambda$)/Sarsa($\lambda$) on random MRPs, a real-world myoelectric prosthetic arm, and a domain from the Arcade Learning Environment.
We use linear function approximation with tabular, binary, and non-binary features.
Our results suggest that the true online methods indeed dominate the regular methods.
Across all domains/representations the learning speed of the true online methods are often better, but never worse than that of the regular methods.
An additional advantage is that no choice between traces has to be made for the true online methods.
We show that new true online temporal-difference methods can be derived by making changes to the real-time forward view and then rewriting the update equations.
