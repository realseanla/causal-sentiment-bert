Recently, several works in the domain of natural language processing presented successful methods for word embedding.
Among them, the Skip-gram (SG) with negative sampling, known also as Word2Vec, advanced the state-of-the-art of various linguistics tasks.
In this paper, we propose a scalable Bayesian neural word embedding algorithm that can be beneficial to general item similarity tasks as well.
The algorithm relies on a Variational Bayes solution for the SG objective and a detailed step by step description of the algorithm is provided.
We present experimental results that demonstrate the performance of the proposed algorithm and show it is competitive with the original SG method.
