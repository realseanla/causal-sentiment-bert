We propose Information Theoretic-Learning (ITL) divergence measures for variational regularization of neural networks.
We also explore ITL-regularized autoencoders as an alternative to variational autoencoding bayes, adversarial autoencoders and generative adversarial networks for randomly generating sample data without explicitly defining a partition function.
This paper also formalizes, generative moment matching networks under the ITL framework.
