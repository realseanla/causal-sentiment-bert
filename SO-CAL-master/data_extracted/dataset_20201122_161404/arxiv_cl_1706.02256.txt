Resolving abstract anaphora is an important, but difficult task for text understanding.
With recent advances in representation learning this task becomes a tangible aim.
A central property of abstract anaphora is that it establishes a relation between the anaphor embedded in the anaphoric sentence and its (typically non-nominal) antecedent.
We propose an LSTM-based mention-ranking model that learns how abstract anaphors relate to their antecedents with a Siamese Net.
We overcome the lack of training data by generating artificial anaphoric sentence-antecedent pairs.
Our model outperforms state-of-the-art results on shell noun resolution.
We also report first benchmark results on an abstract anaphora subset of the ARRAU corpus.
This corpus presents a greater challenge due to a greater range of confounders.
Our model is able to select syntactically plausible candidates and - if disregarding syntax - discriminates candidates using deeper features.
Deeper inspection shows that the model is able to learn a relation between the anaphor in the anaphoric sentence and its antecedent.
