We present a novel learning method for word embeddings designed for relation classification.
Our word embeddings are trained by predicting words between noun pairs using lexical relation-specific features on a large unlabeled corpus.
This allows us to explicitly incorporate relation-specific information into the word embeddings.
The learned word embeddings are then used to construct feature vectors for a relation classification model.
On a well-established semantic relation classification task, our method significantly outperforms a baseline based on a previously introduced word embedding method, and compares favorably to previous state-of-the-art models without syntactic information or manually constructed external resources.
Furthermore, when incorporating external resources, our method outperforms the previous state of the art.
