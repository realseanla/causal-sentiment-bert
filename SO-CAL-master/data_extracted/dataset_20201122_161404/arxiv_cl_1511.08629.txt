Distributed word representations have been demonstrated to be effective in capturing se- mantic and syntactic regularities.
Unsuper- vised representation learning from large un- labeled corpora can learn similar representa- tions for those words that present similar co- occurrence statistics.
Besides local occurrence statistics, global topical information is also important knowledge that may help discrimi- nate a word from another.
In this paper, we in- corporate category information of documents in the learning of word representations and to learn the proposed models in a document- wise manner.
Our models outperform several state-of-the-art models in word analogy and word similarity tasks.
Moreover, we evaluate the learned word vectors on sentiment analysis and text classification tasks, which shows the superiority of our learned word vectors.
We also learn high-quality category embeddings that reflect topical meanings.
