Automatically recognizing entailment relations between pairs of natural language sentences has so far been the dominion of classifiers employing hand engineered features derived from natural language processing pipelines.
End-to-end differentiable neural architectures have failed to approach state-of-the-art performance until very recently.
In this paper, we propose a neural model that reads two sentences to determine entailment using long short-term memory units.
We extend this model with a word-by-word neural attention mechanism that encourages reasoning over entailments of pairs of words and phrases.
Furthermore, we present a qualitative analysis of attention weights produced by this model, demonstrating such reasoning capabilities.
On a large entailment dataset this model outperforms the previous best neural model and a classifier with engineered features by a substantial margin.
It is the first generic end-to-end differentiable system that achieves state-of-the-art accuracy on a textual entailment dataset.
