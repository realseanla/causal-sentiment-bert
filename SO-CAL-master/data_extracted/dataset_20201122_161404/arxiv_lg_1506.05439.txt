Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions.
In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance.
The Wasserstein distance provides a natural notion of dissimilarity for probability measures.
Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed.
We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures.
We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index.
The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space.
We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn't use the metric.
