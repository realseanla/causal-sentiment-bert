Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL).
We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering.
It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.
