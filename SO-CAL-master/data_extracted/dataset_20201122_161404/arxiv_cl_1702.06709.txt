Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types.
Distant supervision paradigm is extensively used to generate training data for this task.
However, generated training data assigns same set of labels to every mention of an entity without considering its local context.
Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features.
Our work overcomes both drawbacks.
We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features.
Our model treats training data as noisy and uses non-parametric variant of hinge loss function.
Experiments show that the proposed model outperforms previous state-of-the-art methods on two publicly available datasets, namely FIGER (GOLD) and BBN with an average relative improvement of 2.69 percent in micro-F1 score.
Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems.
These approaches of transferring knowledge further improve the performance of respective models.
