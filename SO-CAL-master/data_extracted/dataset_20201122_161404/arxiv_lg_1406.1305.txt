The Frank-Wolfe method (a.k.a.
conditional gradient algorithm) for smooth optimization has regained much interest in recent years in the context of large scale optimization and machine learning.
A key advantage of the method is that it avoids projections - the computational bottleneck in many applications - replacing it by a linear optimization step.
Despite this advantage, the convergence rates of the FW method fall behind standard gradient methods for most settings of interest.
It is an active line of research to derive faster FW algorithms for various settings of convex optimization.
