Reservoir computing (RC) is a novel approach to time series prediction using recurrent neural networks.
In RC, an input signal perturbs the intrinsic dynamics of a medium called a reservoir.
A readout layer is then trained to reconstruct a target output from the reservoir's state.
The multitude of RC architectures and evaluation metrics poses a challenge to both practitioners and theorists who study the task-solving performance and computational power of RC.
In addition, in contrast to traditional computation models, the reservoir is a dynamical system in which computation and memory are inseparable, and therefore hard to analyze.
Here, we compare echo state networks (ESN), a popular RC architecture, with tapped-delay lines (DL) and nonlinear autoregressive exogenous (NARX) networks, which we use to model systems with limited computation and limited memory respectively.
We compare the performance of the three systems while computing three common benchmark time series: H{\'e}non Map, NARMA10, and NARMA20.
We find that the role of the reservoir in the reservoir computing paradigm goes beyond providing a memory of the past inputs.
The DL and the NARX network have higher memorization capability, but fall short of the generalization power of the ESN.
