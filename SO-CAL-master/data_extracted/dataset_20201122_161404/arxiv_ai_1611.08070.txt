This work presents a multiscale framework to solve an inverse reinforcement learning (IRL) problem for continuous-time/state stochastic systems.
We take advantage of a diffusion wavelet representation of the associated Markov chain to abstract the state space.
This not only allows for effectively handling the large (and geometrically complex) decision space but also provides more interpretable representations of the demonstrated state trajectories and also of the resulting policy of IRL.
In the proposed framework, the problem is divided into the global and local IRL, where the global approximation of the optimal value functions are obtained using coarse features and the local details are quantified using fine local features.
An illustrative numerical example on robot path control in a complex environment is presented to verify the proposed method.
