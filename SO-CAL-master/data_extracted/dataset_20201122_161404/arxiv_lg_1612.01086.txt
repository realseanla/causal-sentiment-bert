We propose a scheme for training a computerized agent to perform complex human tasks such as highway steering.
The scheme resembles natural teaching-learning processes used by humans to teach themselves and each other complex tasks, and consists of the following four stages.
In the first stage the agent learns by itself an informative low-dimensional representations of raw input signals in an unsupervised learning manner.
In the second stage the agent learns to mimic the human instructor using supervised learning so as to reach a basic performance level; the third stage is devoted to learning an instantaneous reward model.
Here, the (human) instructor observes (possibly in real time) the agent performing the task and provides reward feedback.
During this stage the agent monitors both itself and the instructor feedback and learns a reward model using supervised learning.
This stage terminates when the reward model is sufficiently accurate.
In the last stage a reinforcement learning algorithm is deployed to optimize the agent policy.
The guidance reward signal in the reinforcement learning algorithm relies on the previously learned reward model.
As a proof of concept for the proposed scheme, we designed a system consisting of deep convolutional neural networks, and applied it to successfully learn a computerized agent capable of autonomous highway steering over the well-known racing game Assetto Corsa.
