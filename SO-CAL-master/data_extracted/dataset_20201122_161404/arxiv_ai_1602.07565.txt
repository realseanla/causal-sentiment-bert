We consider partially observable Markov decision processes (POMDPs) with a set of target states and positive integer costs associated with every transition.
The traditional optimization objective (stochastic shortest path) asks to minimize the expected total cost until the target set is reached.
We extend the traditional framework of POMDPs to model energy consumption, which represents a hard constraint.
The energy levels may increase and decrease with transitions, and the hard constraint requires that the energy level must remain positive in all steps till the target is reached.
First, we present a novel algorithm for solving POMDPs with energy levels, developing on existing POMDP solvers and using RTDP as its main method.
Our second contribution is related to policy representation.
For larger POMDP instances the policies computed by existing solvers are too large to be understandable.
We present an automated procedure based on machine learning techniques that automatically extracts important decisions of the policy allowing us to compute succinct human readable policies.
Finally, we show experimentally that our algorithm performs well and computes succinct policies on a number of POMDP instances from the literature that were naturally enhanced with energy levels.
