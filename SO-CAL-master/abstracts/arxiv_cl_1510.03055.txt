Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., \textit{I don't know}) regardless of the input.
We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks.
Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models.
Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in \bleu scores on two conversational datasets.
