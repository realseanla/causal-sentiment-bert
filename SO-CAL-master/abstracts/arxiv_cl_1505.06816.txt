NLP tasks differ in the semantic information they require, and at this time no single semantic representation fulfills all requirements.
Logic-based representations characterize sentence structure, but do not capture the graded aspect of meaning.
Distributional models give graded similarity ratings for words and phrases, but do not adequately capture overall sentence structure.
So it has been argued that the two are complementary.
In this paper, we adopt a hybrid approach that combines logic-based and distributional semantics through probabilistic logic inference in Markov Logic Networks (MLNs).
We focus on textual entailment (RTE), a task that can utilize the strengths of both representations.
Our system is three components, 1) parsing and task representation, where input RTE problems are represented in probabilistic logic.
This is quite different from representing them in standard first-order logic.
2) knowledge base construction in the form of weighted inference rules from different sources like WordNet, paraphrase collections, and lexical and phrasal distributional rules generated on the fly.
We use a variant of Robinson resolution to determine the necessary inference rules.
More sources can easily be added by mapping them to logical rules; our system learns a resource-specific weight that counteract scaling differences between resources.
3) inference, where we show how to solve the inference problems efficiently.
In this paper we focus on the SICK dataset, and we achieve a state-of-the-art result.
Our system handles overall sentence structure and phenomena like negation in the logic, then uses our Robinson resolution variant to query distributional systems about words and short phrases.
Therefor, we use our system to evaluate distributional lexical entailment approaches.
We also publish the set of rules queried from the SICK dataset, which can be a good resource to evaluate them.
