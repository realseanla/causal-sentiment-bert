Tensor decomposition is a powerful tool for multiway data analysis.
Many popular tensor decomposition approaches---such as the Tucker decomposition and CANDECOMP/PARAFAC (CP)---conduct multi-linear factorization.
They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g.
missing data and binary data), and (iii) noisy observations and outliers.
To address these issues, we propose a tensor-variate latent $t$ process model, InfTucker, for robust multiway data analysis: it conducts robust Tucker decomposition in an infinite feature space.
Unlike classical tensor decomposition models, it handles both continuous and binary data in a probabilistic framework.
Unlike previous nonparametric Bayesian models on matrices and tensors, our latent $t$-process model focuses on multiway analysis and uses nonlinear covariance functions.
To efficiently learn InfTucker from data, we develop a novel variational inference technique on tensors.
Compared with classical implementation, the new technique reduces both time and space complexities by several orders of magnitude.
This technique can be easily adopted in other contexts (e.g., multitask learning) where we encounter tensor-variate $t$ processes or Gaussian processes.
Our experimental results on chemometrics and social network datasets demonstrate that the new InfTucker model achieves significantly higher prediction accuracy than several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD), Weighted CP, and nonnegative tensor decomposition.
