Supervised recursive neural network models (RNNs) for sentence meaning have been successful in an array of sophisticated language tasks, but it remains an open question whether they can learn compositional semantic grammars that support logical deduction.
We address this question directly by for the first time evaluating whether each of two classes of neural model --- plain RNNs and recursive neural tensor networks (RNTNs) --- can correctly learn relationships such as entailment and contradiction between pairs of sentences, where we have generated controlled data sets of sentences from a logical grammar.
Our first experiment evaluates whether these models can learn the basic algebra of logical relations involved.
Our second and third experiments extend this evaluation to complex recursive structures and sentences involving quantification.
We find that the plain RNN achieves only mixed results on all three experiments, whereas the stronger RNTN model generalizes well in every setting and appears capable of learning suitable representations for natural language logical inference.
