Environmental audio tagging is a newly proposed task to predict the presence or absence of a specific audio event in a chunk.
Deep neural network (DNN) based methods have been successfully adopted for predicting the audio tags in the domestic audio scene.
In this paper, we propose to use a convolutional neural network (CNN) to extract robust features from mel-filter banks (MFBs), spectrograms or even raw waveforms for audio tagging.
Gated recurrent unit (GRU) based recurrent neural networks (RNNs) are then cascaded to model the long-term temporal structure of the audio signal.
To complement the input information, an auxiliary CNN is designed to learn on the spatial features of stereo recordings.
We evaluate our proposed methods on Task 4 (audio tagging) of the Detection and Classification of Acoustic Scenes and Events 2016 (DCASE 2016) challenge.
Compared with our recent DNN-based method, the proposed structure can reduce the equal error rate (EER) from 0.13 to 0.11 on the development set.
The spatial features can further reduce the EER to 0.10.
The performance of the end-to-end learning on raw waveforms is also comparable.
Finally, on the evaluation set, we get the state-of-the-art performance with 0.12 EER while the performance of the best existing system is 0.15 EER.
