Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima.
However, the practical observation is that, at least for the most successful Deep Convolutional Neural Networks (DCNNs) for visual processing, practitioners can always increase the network size to fit the training data (an extreme example would be [1]).
The most successful DCNNs such as VGG and ResNets are best used with a small degree of "overparametrization".
In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs.
We first prove the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations).
The zero-minimizers -- in the case of classification -- have a non-zero margin.
The same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the zero-minimizer with larger margin, as discussed in Theory III (to be released).
We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima.
Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN's empirical loss surface, which might not be as complicated as people commonly believe.
