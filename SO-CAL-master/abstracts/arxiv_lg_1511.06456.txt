Often, the performance on a supervised machine learning task is evaluated with a loss function that cannot be optimized directly.
A common solution to this problem is to simply optimize a surrogate loss function in the hope that the task loss of interest will go down as well.
The task loss function defines the cost for all possible answers and it is arguably the most important element of the problem setup.
In this work, we argue, that it is beneficial to use a surrogate loss that is explicitly aware of the task loss of interest.
We propose a generic method to define such loss-aware optimization criteria for a broad class of structured output problems.
We are particularly interested in the end-to-end training scenario where the system is only allowed to produce one discrete answer, such as in speech recognition or machine translation.
For this purpose, we propose to train an estimator of the task loss function itself and to make predictions by looking for the output with the lowest estimated loss.
We show that the proposed method has certain theoretical guarantees and how it can be applied to the to sequence prediction problems with discrete output symbols.
Finally, we validate the new method experimentally on a speech recognition task without extra text corpora and obtain a significant~13\ percent relative gain in terms of Character Error Rate over cross-entropy training.
