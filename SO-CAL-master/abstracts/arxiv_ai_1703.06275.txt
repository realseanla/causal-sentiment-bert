Most games have, or can be generalised to have, a number of parameters that may be varied in order to provide instances of games that lead to very different player experiences.
The space of possible parameter settings can be seen as a search space, and we can therefore use a Random Mutation Hill Climbing algorithm or other search methods to find the parameter settings that induce the best games.
One of the hardest parts of this approach is defining a suitable fitness function.
In this paper we explore the possibility of using one of a growing set of General Video Game AI agents to perform automatic play-testing.
This enables a very general approach to game evaluation based on estimating the skill-depth of a game.
Agent-based play-testing is computationally expensive, so we compare two simple but efficient optimisation algorithms: the Random Mutation Hill-Climber and the Multi-Armed Bandit Random Mutation Hill-Climber.
For the test game we use a space-battle game in order to provide a suitable balance between simulation speed and potential skill-depth.
Results show that both algorithms are able to rapidly evolve game versions with significant skill-depth, but that choosing a suitable resampling number is essential in order to combat the effects of noise.
