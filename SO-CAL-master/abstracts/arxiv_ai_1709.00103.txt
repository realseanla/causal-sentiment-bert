A significant amount of the world's knowledge is stored in relational databases.
However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL.
We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries.
Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries.
Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss.
In addition, we will publish WikiSQL, a dataset of 87726 hand-annotated examples of questions and SQL queries distributed across 26375 tables from Wikipedia.
This dataset is required to train our model and is an order of magnitude larger than comparable datasets.
By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9 percent to 60.3 percent and logical form accuracy from 23.4 percent to 49.2 percent.
