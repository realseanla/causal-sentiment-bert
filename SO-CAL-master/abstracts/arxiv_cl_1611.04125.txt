Joint representation learning of text and knowledge within a unified semantic space enables us to perform knowledge graph completion more accurately.
In this work, we propose a novel framework to embed words, entities and relations into the same continuous vector space.
In this model, both entity and relation embeddings are learned by taking knowledge graph and plain text into consideration.
In experiments, we evaluate the joint learning model on three tasks including entity prediction, relation prediction and relation classification from text.
The experiment results show that our model can significantly and consistently improve the performance on the three tasks as compared with other baselines.
