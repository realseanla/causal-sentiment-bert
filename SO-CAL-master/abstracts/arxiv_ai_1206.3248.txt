A graphical multiagent model (GMM) represents a joint distribution over the behavior of a set of agents.
One source of knowledge about agents' behavior may come from gametheoretic analysis, as captured by several graphical game representations developed in recent years.
GMMs generalize this approach to express arbitrary distributions, based on game descriptions or other sources of knowledge bearing on beliefs about agent behavior.
To illustrate the flexibility of GMMs, we exhibit game-derived models that allow probabilistic deviation from equilibrium, as well as models based on heuristic action choice.
We investigate three different methods of integrating these models into a single model representing the combined knowledge sources.
To evaluate the predictive performance of the combined model, we treat as actual outcome the behavior produced by a reinforcement learning process.
We find that combining the two knowledge sources, using any of the methods, provides better predictions than either source alone.
Among the combination methods, mixing data outperforms the opinion pool and direct update methods investigated in this empirical trial.
