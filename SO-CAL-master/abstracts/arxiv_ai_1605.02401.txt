Acoustic event detection is essential for content analysis and description of multimedia recordings.
The majority of current literature on the topic learns the detectors through fully-supervised techniques employing strongly labeled data.
However, the labels available for online multimedia data are generally weak and do not provide sufficient detail for such methods to be employed.
In this paper we propose a framework for learning acoustic event detectors using only weakly labeled data based on a Multiple Instance Learning (MIL) framework.
We first show that audio event detection using weak data can be formulated as an MIL problem.
We then suggest two frameworks for solving multiple-instance learning, one based on neural networks, and the second on support vector machines.
The proposed methods can help in removing the time consuming and expensive process of manually annotating data to facilitate fully supervised learning.
Our proposed framework can not only successfully detect events in a recording but can also provide temporal locations of events in the recording.
This is interesting as these information were never known in the first place for weakly labeled data.
