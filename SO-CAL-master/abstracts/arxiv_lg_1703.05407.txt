We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner.
Our scheme pits two versions of the same agent, Alice and Bob, against one another.
Alice proposes a task for Bob to complete; and then Bob attempts to complete the task.
In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will "propose" the task by running a set of actions and then Bob must partially undo, or repeat them, respectively.
Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent.
When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn.
