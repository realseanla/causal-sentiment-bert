In this work, we investigate several neural network architectures for fine-grained entity type classification.
Particularly, we consider extensions to a recently proposed attentive neural architecture and make three key contributions.
Previous work on attentive neural architectures do not consider hand-crafted features, we combine learnt and hand-crafted features and observe that they complement each other.
Additionally, through quantitative analysis we establish that the attention mechanism is capable of learning to attend over syntactic heads and the phrase containing the mention, where both are known strong hand-crafted features for our task.
We enable parameter sharing through a hierarchical label encoding method, that in low-dimensional projections show clear clusters for each type hierarchy.
Lastly, despite using the same evaluation dataset, the literature frequently compare models trained using different data.
We establish that the choice of training data has a drastic impact on performance, with decreases by as much as 9.85 percent loose micro F1 score for a previously proposed method.
Despite this, our best model achieves state-of-the-art results with 75.36 percent loose micro F1 score on the well- established FIGER (GOLD) dataset.
