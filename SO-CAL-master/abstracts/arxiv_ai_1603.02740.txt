As datasets capturing human choices grow in richness and scale, particularly in online domains, there is an increasing need for choice models that explain and predict complex empirical choices that violate traditional choice-theoretic assumptions such as regularity, stochastic transitivity, or Luce's choice axiom.
In this work we introduce a Pairwise Choice Markov Chain (PCMC) model of discrete choice that is free of all those assumptions while still satisfying the attractive foundational axiom of uniform expansion.
Uniform expansion is known to imply Luce's choice axiom in the context of independent random utility models (RUMs), but the PCMC model is not a RUM (let alone an independent RUM).
Inference for the PCMC model is straight-forward, and we thus introduce it as the first inferentially tractable model of discrete choice known to satisfy uniform expansion without the choice axiom, regularity, or strict stochastic transitivity.
It is thus more flexible than even Tversky's Elimination By Aspects model, which assumes regularity and is also known to be inferentially intractable.
We show that our model learns and predicts synthetic non-transitive data well.
Our analysis also synthesizes several recent observations connecting the Multinomial Logit (MNL) model and Markov chains; the PCMC model retains the Multinomial Logit model as a special case.
