Minimizing a convex function over the spectrahedron, i.e., the set of all positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing.
It is also notoriously difficult to solve in large-scale since standard techniques require expensive matrix decompositions.
An alternative, is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting.
The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient.
On the downside, the CG method, in general, converges with an inferior rate.
The error for minimizing a $\beta$-smooth function after $t$ iterations scales like $\beta/t$.
This convergence rate does not improve even if the function is also strongly convex.
