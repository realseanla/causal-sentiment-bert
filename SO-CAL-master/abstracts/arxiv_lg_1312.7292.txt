The aim in this paper is to allocate the `sleep time' of the individual sensors in an intrusion detection application so that the energy consumption from the sensors is reduced, while keeping the tracking error to a minimum.
We propose two novel reinforcement learning (RL) based algorithms - with both infinite horizon discounted and long-run average cost objectives - for solving this problem.
All our algorithms incorporate feature-based representations to handle the curse of dimensionality associated with the underlying partially-observable Markov decision process (POMDP).
Further, the feature selection scheme used in our algorithms intelligently manages the energy cost and tracking cost factors, which in turn assists the search for the optimal sleeping policy.
The first algorithm in either (discounted or average) setting is based on Q-learning, while the second algorithm is a novel two-timescale algorithm that performs on-policy Q-learning.
The latter possesses theoretical convergence guarantees, unlike the former Q-learning based algorithm.
We also extend these algorithms to a setting where the intruder's mobility model is not known by incorporating a stochastic iterative scheme for estimating the mobility model.
The simulation results on a synthetic 2-d network setting suggest that our proposed algorithms result in better tracking accuracy at the cost of a few additional sensors, in comparison to a recent prior work.
We also observe empirically that the proposed model estimation scheme converges to the true model.
