To manage and maintain large-scale cellular networks, operators need to know which sectors underperform at any given time.
For this purpose, they use the so-called hot spot score, which is the result of a combination of multiple network measurements and reflects the instantaneous overall performance of individual sectors.
While operators have a good understanding of the current performance of a network and its overall trend, forecasting the performance of each sector over time is a challenging task, as it is affected by both regular and non-regular events, triggered by human behavior and hardware failures.
In this paper, we study the spatio-temporal patterns of the hot spot score and uncover its regularities.
Based on our observations, we then explore the possibility to use recent measurements' history to predict future hot spots.
To this end, we consider tree-based machine learning models, and study their performance as a function of time, amount of past data, and prediction horizon.
Our results indicate that, compared to the best baseline, tree-based models can deliver up to 14 percent better forecasts for regular hot spots and 153 percent better forecasts for non-regular hot spots.
The latter brings strong evidence that, for moderate horizons, forecasts can be made even for sectors exhibiting isolated, non-regular behavior.
Overall, our work provides insight into the dynamics of cellular sectors and their predictability.
It also paves the way for more proactive network operations with greater forecasting horizons.
