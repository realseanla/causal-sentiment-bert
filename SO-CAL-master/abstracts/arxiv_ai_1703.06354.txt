Research on human self-regulation has shown that people hold many goals simultaneously and have complex self-regulation mechanisms to deal with this goal conflict.
Artificial autonomous systems may also need to find ways to cope with conflicting goals.
Indeed, the intricate interplay among different goals may be critical to the design as well as long-term safety and stability of artificial autonomous systems.
I discuss some of the critical features of the human self-regulation system and how it might be applied to an artificial system.
Furthermore, the implications of goal conflict for the reliability and stability of artificial autonomous systems and ensuring their alignment with human goals and ethics is examined.
