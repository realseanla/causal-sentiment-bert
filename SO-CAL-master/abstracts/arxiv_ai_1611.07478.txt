Understanding why a model made a certain prediction is crucial in many data science fields.
Interpretable predictions engender appropriate trust and provide insight into how the model may be improved.
However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability.
Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features.
Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods.
This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties.
We show how we can leverage these properties to create novel visual explanations of model predictions.
The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.
