In this paper, a method is proposed to detect the emotion of a song based on its lyrical and audio features.
Lyrical features are generated by segmentation of lyrics during the process of data extraction.
ANEW and WordNet knowledge is then incorporated to compute Valence and Arousal values.
In addition to this, linguistic association rules are applied to ensure that the issue of ambiguity is properly addressed.
Audio features are used to supplement the lyrical ones and include attributes like energy, tempo, and danceability.
These features are extracted from The Echo Nest, a widely used music intelligence platform.
Construction of training and test sets is done on the basis of social tags extracted from the last.fm website.
The classification is done by applying feature weighting and stepwise threshold reduction on the k-Nearest Neighbors algorithm to provide fuzziness in the classification.
