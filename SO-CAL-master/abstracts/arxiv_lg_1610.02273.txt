In computer architecture, near-data processing (NDP) refers to augmenting the memory or the storage with processing power so that it can process the data stored therein, passing only the processed data upwards in the memory hierarchy.
By offloading the computational burden of CPU and saving the need for transferring raw data, NDP has a great potential in terms of accelerating computation and reducing power consumption.
Despite its potential, NDP had only limited success until recently, mainly due to the performance mismatch in logic and memory process technologies.
Recently, there have been two major changes in the game, making NDP more appealing than ever.
The first is the success of deep learning, which often requires frequent transfers of big data for training.
The second is the advent of NAND flash-based solid-state drives (SSDs) containing multicore CPUs that can be used for data processing.
In this paper, we evaluate the potential of NDP for machine learning using a new SSD platform that allows us to simulate in-storage processing (ISP) of machine learning workloads.
Although our platform named ISPML can execute various algorithms, this paper focuses on the stochastic gradient decent (SGD) algorithm, which is the de facto standard method for training deep neural networks.
We implement and compare three variants of SGD (synchronous, downpour, and elastic averaging) using the ISP-ML platform, in which we exploit the multiple NAND channels for implementing parallel SGD.
In addition, we compare the performance of ISP optimization and that of conventional in-host processing optimization.
To the best of our knowledge, this is one of the first attempts to apply NDP to the optimization for machine learning.
