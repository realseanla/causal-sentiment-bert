Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications.
In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network.
Using carefully timed external stimulation, the network can be driven towards a desired dynamical state.
We term this principle "Learning by Stimulation Avoidance" (LSA).
We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1].
We examine the mechanism's basic dynamics in a reduced network, and demonstrate how it scales up to a network of 100 neurons.
We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation, and can be used as a learning rule for an embodied application: learning of wall avoidance by a simulated robot.
The surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics: to the authors' knowledge, this is the first work demonstrating sensory-motor learning with random spiking networks through pure Hebbian learning.
