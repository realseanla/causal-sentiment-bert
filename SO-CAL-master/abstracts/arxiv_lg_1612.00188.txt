Recurrent Neural Networks (RNNs) have been successfully used in many applications.
However, the problem of learning long-term dependencies in sequences using these networks is still a major challenge.
Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training, which ensures that its norm is exactly equal to one.
These methods either have limited expressiveness or scale poorly with the size of the network when compared to the simple RNN case, especially in an online learning setting.
Our contributions are as follows.
We first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint.
Therefore, it may not be necessary to work with complex valued matrices.
Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the transition matrix is always orthogonal.
Using our approach, one online gradient step can, in the worst case, be performed in time complexity $\mathcal{O}(T n^2)$, where $T$ and $n$ are the length of the input sequence and the size of the hidden layer respectively.
This time complexity is the same as the simple RNN case.
Finally, we test our new parametrisation on problems with long-term dependencies.
Our results suggest that the orthogonal constraint on the transition matrix has similar benefits to the unitary constraint.
