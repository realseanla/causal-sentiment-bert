Distributed optimization algorithms for large-scale machine learning suffer from a communication bottleneck.
Reducing communication makes the efficient aggregation of partial work from different machines more challenging.
In this paper we present a novel generalization of the recent communication efficient primal-dual coordinate ascent framework (CoCoA).
Our framework, CoCoA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allowed conservative averaging.
We give stronger (primal-dual) convergence rate guarantees for both CoCoA as well as our new variants, and generalize the theory for both methods to also cover non-smooth convex loss functions.
We provide an extensive experimental comparison on several real-world distributed datasets, showing markedly improved performance, especially when scaling up the number of machines.
