Interpretability has become an important issue as machine learning is increasingly used to inform consequential decisions.
We propose an approach for interpreting a blackbox model by extracting a decision tree that approximates the model.
Our model extraction algorithm avoids overfitting by leveraging blackbox model access to actively sample new training points.
We prove that as the number of samples goes to infinity, the decision tree learned using our algorithm converges to the exact greedy decision tree.
In our evaluation, we use our algorithm to interpret random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for three classical reinforcement learning problems.
We show that our algorithm improves over a baseline based on CART on every problem instance.
Furthermore, we show how an interpretation generated by our approach can be used to understand and debug these models.
