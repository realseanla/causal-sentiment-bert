We present the hierarchical Dirichlet scaling process (HDSP), a Bayesian nonparametric mixed membership model for multi-labeled data.
We construct the HDSP based on the gamma representation of the hierarchical Dirichlet process (HDP) which allows scaling the mixture components.
With such construction, HDSP allocates a latent location to each label and mixture component in a metric space, and uses the distance between them to guide membership probabilities.
We develop a variational Bayes algorithm for the approximate posterior inference of the HDSP.
Through experiments on synthetic datasets as well as datasets of newswire, medical journal articles, and Wikipedia, we show that the HDSP results in better predictive performance than HDP, labeled LDA and partially labeled LDA.
