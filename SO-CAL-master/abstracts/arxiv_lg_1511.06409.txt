Deep networks are increasingly being applied to problems involving image synthesis, e.g., generating images from textual descriptions, or generating reconstructions of an input image in an autoencoder architecture.
Supervised training of image-synthesis networks typically uses a pixel-wise squared error (SE) loss to indicate the mismatch between a generated image and its corresponding target image.
We propose to instead use a loss function that is better calibrated to human perceptual judgments of image quality: the structural-similarity (SSIM) score of Wang, Bovik, Sheikh, and Simoncelli (2004).
Because the SSIM score is differentiable, it is easily incorporated into gradient-descent learning.
We compare the consequences of using SSIM versus SE loss on representations formed in deep autoencoder and recurrent neural network architectures.
SSIM-optimized representations yield a superior basis for image classification compared to SE-optimized representations.
Further, human observers prefer images generated by the SSIM-optimized networks by nearly a 7:1 ratio.
Just as computer vision has advanced through the use of convolutional architectures that mimic the structure of the mammalian visual system, we argue that significant additional advances can be made in modeling images through the use of training objectives that are well aligned to characteristics of human perception.
