Despite the success of convolutional neural networks, selecting the optimal architecture for a given task remains an open problem.
Instead of aiming to select a single optimal architecture, we propose a $"$fabric$"$ that embeds an exponentially large number of CNN architectures.
The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern.
The only hyper-parameters of the model (nr.
of channels and layers) are not critical for performance.
While individual CNN architectures can be recovered as paths in the trellis, the trellis can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap.
The trellis parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size.
We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.
