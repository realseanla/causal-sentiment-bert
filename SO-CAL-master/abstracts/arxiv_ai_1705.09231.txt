Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients.
Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar.
