We present a general theoretical analysis of structured prediction.
By introducing a new complexity measure that explicitly factors in the structure of the output space and the loss function, we are able to derive new data-dependent learning guarantees for a broad family of losses and for hypothesis sets with an arbitrary factor graph decomposition.
We extend this theory by leveraging the principle of Voted Risk Minimization (VRM) and showing that learning is possible with complex factor graphs.
We both present new learning bounds in this advanced setting as well as derive two new families of algorithms, \emph{Voted Conditional Random Fields} and \emph{Voted Structured Boosting}, which can make use of very complex features and factor graphs without overfitting.
Finally, we also validate our theory through experiments on several datasets.
