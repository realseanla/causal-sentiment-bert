We show Correspondence Analysis (CA) is equivalent to defining Gini-index with appropriate scaled one-hot encoding.
Using this relation, we introduce non-linear kernel extension of CA.
The extended CA gives well-known analysis for categorical data (CD) and natural language processing by specializing kernels.
For example, our formulation can give G-test, skip-gram with negative-sampling (SGNS), and GloVe as a special case.
We introduce two kernels for natural language processing based on our formulation.
First is a stop word(SW) kernel.
Second is word similarity(WS) kernel.
The SW kernel is the system introducing appropriate weights for SW.
The WS kernel enables to use WS test data as training data for vector space representations of words.
We show these kernels enhances accuracy when training data is not sufficiently large.
