We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses.
The generator is based on recurrent neural networks and the sequence-to-sequence approach.
It is fully trainable from data which include preceding context along with responses to be generated.
We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.
