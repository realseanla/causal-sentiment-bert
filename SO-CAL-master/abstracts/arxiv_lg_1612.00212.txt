Fully convolutional neural networks give accurate, per-pixel prediction for input images and have applications like semantic segmentation.
However, a typical FCN usually requires lots of floating point computation and large run-time memory, which effectively limits its usability.
We propose a method to train Bit Fully Convolution Network (BFCN), a fully convolutional neural network that has low bit-width weights and activations.
Because most of its computation-intensive convolutions are accomplished between low bit-width numbers, a BFCN can be accelerated by an efficient bit-convolution implementation.
On CPU, the dot product operation between two bit vectors can be reduced to bitwise operations and popcounts, which can offer much higher throughput than 32-bit multiplications and additions.
