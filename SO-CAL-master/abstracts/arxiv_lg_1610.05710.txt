In the area of data classification, one of the prominent algorithms is the large margin nearest neighbor (LMNN) approach which is a metric learning to enhance the performance of the popular k-nearest neighbor classifier.
In principles, LMNN learns a more efficient metric in the input space by using a linear mapping as the outcome of a convex optimization problem.
However, one of the greatest weak points of LMNN is the strong reliance of its optimization paradigm on how the neighboring points are chosen.
In this paper, it is mathematically proved for the first time that the regular way of choosing the target points can lead to non-feasible optimization conditions regardless of the number of chosen neighboring points.
We present a mathematical approach to categorize the target points into feasible and infeasible exemplars, an also we provide a feasibility measure for preference of the target candidates.
In our proposed Feasibility Based-LMNN algorithm, we use the above clue to construct the optimization problem based on the most promising general mapping directions in the input space.
Our empirical results shows that via using the proposed FB-LMNN approach the optimization problem will converge in a better optimum point, and therefor leads to better classification on the well-known benchmark datasets.
