Assessing the degree of semantic relatedness between words is an important task with a variety of semantic applications, such as ontology learning for the Semantic Web, semantic search or query expansion.
To accomplish this in an automated fashion, many relatedness measures have been proposed.
However, most of these metrics only encode information contained in the underlying corpus and thus do not directly model human intuition.
To solve this, we propose to utilize a metric learning approach to improve existing semantic relatedness measures by learning from additional information, such as explicit human feedback.
For this, we argue to use word embeddings instead of traditional high-dimensional vector representations in order to leverage their semantic density and to reduce computational cost.
We rigorously test our approach on several domains including tagging data as well as publicly available embeddings based on Wikipedia texts and navigation.
Human feedback about semantic relatedness for learning and evaluation is extracted from publicly available datasets such as MEN or WS-353.
We find that our method can significantly improve semantic relatedness measures by learning from additional information, such as explicit human feedback.
For tagging data, we are the first to generate and study embeddings.
Our results are of special interest for ontology and recommendation engineers, but also for any other researchers and practitioners of Semantic Web techniques.
