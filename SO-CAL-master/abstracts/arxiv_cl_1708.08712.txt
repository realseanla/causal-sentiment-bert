In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario.
We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and weighted ensemble.
We evaluate these methods based on three criteria: i) translation quality, ii) training time, and iii) robustness towards out-of-domain tests.
Our findings on Arabic-English and German-English language pairs show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data.
Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on.
Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality.
A weighted ensemble of different individual models performed better than data selection.
It is beneficial in a scenario when there is no time for fine-tuning.
