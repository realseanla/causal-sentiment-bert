We study the complexity of functions computable by deep feedforward neural networks with piece-wise linear activations in terms of the number of regions of linearity that they have.
Deep networks are able to sequentially map portions of each layer's input space to the same output.
In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth.
This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.
