We propose to train bi-directional neural network language model(NNLM) with noise contrastive estimation(NCE).
Experiments are conducted on a rescore task on the PTB data set.
It is shown that NCE-trained bi-directional NNLM outperformed the one trained by conventional maximum likelihood training.
But still(regretfully), it did not out-perform the baseline uni-directional NNLM.
