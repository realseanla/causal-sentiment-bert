In scaffolding teaching, students are gradually asked questions to build background knowledge, clear up confusions, learn to be attentive, and improve comprehension.
Inspired by this approach, we explore methods for teaching machines to learn to reason over text documents through asking questions about the past information.
We address three key challenges in teaching and learning to reason: 1) the need for an effective architecture that learns from the information in text and keeps it in memory; 2) the difficulty of self-assessing what is learned at any given point and what is left to be learned; 3) the difficulty of teaching reasoning in a scalable way.
To address the first challenge, we present the Scaffolding Network, an attention-based neural network agent that can reason over a dynamic memory.
It learns a policy using reinforcement learning to incrementally register new information about concepts and their relations.
For the second challenge, we describe a question simulator as part of the scaffolding network that learns to continuously question the agent about the information processed so far.
Through questioning, the agent learns to correctly answer as many questions as possible.
For the last challenge, we explore training with reduced annotated data.
We evaluate on synthetic and real datasets, demonstrating that our model competes well with the state-of-the-art methods, especially when less supervision is used.
