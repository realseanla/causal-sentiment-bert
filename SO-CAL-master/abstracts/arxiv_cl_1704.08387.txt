We introduce a neural semantic parser which is interpretable and scalable.
Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains.
The semantic parser is trained end-to-end using annotated logical forms or their denotations.
We obtain competitive results on various datasets.
The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.
