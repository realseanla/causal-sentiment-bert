We study the problem of joint question answering (QA) and question generation (QG) in this paper.
Our intuition is that QA and QG have intrinsic connections and these two tasks could improve each other.
On one side, the QA model judges whether the generated question of a QG model is relevant to the answer.
On the other side, the QG model provides the probability of generating a question given the answer, which is a useful evidence that in turn facilitates QA.
In this paper we regard QA and QG as dual tasks.
We propose a novel training framework that trains the models of QA and QG simultaneously, and explicitly leverages their probabilistic correlation to guide the training process.
We implement a QG model based on sequence-to-sequence learning, and a QA model based on recurrent neural network.
All the parameters involved in these two tasks are jointly learned with back propagation.
Experimental results on two datasets show that our approach improves both QA and QG tasks.
