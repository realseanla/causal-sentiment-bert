Transfer learning has been developed to improve the performances of different but related tasks in machine learning.
However, such processes become less efficient with the increase of the size of training data and the number of tasks.
Moreover, privacy can be violated as some tasks may contain sensitive and private data, which are communicated between nodes and tasks.
We propose a consensus-based distributed transfer learning framework, where several tasks aim to find the best linear support vector machine (SVM) classifiers in a distributed network.
With alternating direction method of multipliers, tasks can achieve better classification accuracies more efficiently and privately, as each node and each task train with their own data, and only decision variables are transferred between different tasks and nodes.
Numerical experiments on MNIST datasets show that the knowledge transferred from the source tasks can be used to decrease the risks of the target tasks that lack training data or have unbalanced training labels.
We show that the risks of the target tasks in the nodes without the data of the source tasks can also be reduced using the information transferred from the nodes who contain the data of the source tasks.
We also show that the target tasks can enter and leave in real-time without rerunning the whole algorithm.
