In this paper, we extend a symbolic association framework to being able to handle missing elements in multimodal sequences.
The general scope of the work is the symbolic associations of object-word mappings as it happens in language development on infants.
This scenario has been long interested by Artificial Intelligence, Psychology and Neuroscience.
In this work, we extend a recent approach for multimodal sequences (visual and audio) to also cope with missing elements in one or both modalities.
Our approach uses two parallel Long Short-Term Memory (LSTM) networks with a learning rule based on EM-algorithm.
It aligns both LSTM outputs via Dynamic Time Warping (DTW).
We propose to include an extra step for the combination with max and mean operations for handling missing elements in the sequences.
The intuition behind is that the combination acts as a condition selector for choosing the best representation from both LSTMs.
We evaluated the proposed extension in three different scenarios: audio sequences with missing elements, visual sequences with missing elements, and sequences with missing elements in both modalities.
The performance of our extension reaches better results than the original model and similar results to a unique LSTM trained in one modality, i.e., where the learning problem is less difficult.
