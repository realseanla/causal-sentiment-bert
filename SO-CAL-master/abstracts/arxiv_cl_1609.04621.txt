We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network.
This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words.
By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size).
In addition, we can produce new words that are not in the vocabulary.
We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number).
We have extended the NMT approach with attention mechanism in order to have two different outputs, one for the lemmas and the other for the rest of the factors.
The final translation is built using some \textit{a priori} linguistic information.
We compare our extension with a word-based NMT system.
The experiments, performed on the IWSLT'15 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate.
We observe up to 2 percent BLEU point improvement in a simulated out of domain translation setup.
