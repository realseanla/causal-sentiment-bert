This paper considers an optimal task allocation problem for human robot collaboration in human robot systems with persistent tasks.
Such human robot systems consist of human operators and intelligent robots collaborating with each other to accomplish complex tasks that cannot be done by either part alone.
The system objective is to maximize the probability of successfully executing persistent tasks that are formulated as linear temporal logic specifications and minimize the average cost between consecutive visits of a particular proposition.
This paper proposes to model the human robot collaboration under a framework with the composition of multiple Markov Decision Process (MDP) with possibly unknown transition probabilities, which characterizes how human cognitive states, such as human trust and fatigue, stochastically change with the robot performance.
Under the unknown MDP models, an algorithm is developed to learn the model and obtain an optimal task allocation policy that minimizes the expected average cost for each task cycle and maximizes the probability of satisfying linear temporal logic constraints.
Moreover, this paper shows that the difference between the optimal policy based on the learned model and that based on the underlying ground truth model can be bounded by arbitrarily small constant and large confidence level with sufficient samples.
The case study of an assembly process demonstrates the effectiveness and benefits of our proposed learning based human robot collaboration.
