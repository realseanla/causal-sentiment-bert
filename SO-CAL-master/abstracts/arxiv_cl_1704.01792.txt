Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub-spans of the given passage.
Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions.
In this work, we propose to apply the neural encoder-decoder model to generate meaningful and diverse questions from natural language sentences.
The encoder reads the input text and the answer position, to produce an answer-aware input representation, which is fed to the decoder to generate an answer focused question.
We conduct a preliminary study on neural question generation from text with the SQuAD dataset, and the experiment results show that our method can produce fluent and diverse questions.
