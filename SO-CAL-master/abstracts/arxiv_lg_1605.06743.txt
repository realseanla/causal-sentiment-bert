Our formal understanding of the inductive bias that drives the success of convolutional networks on computer vision tasks is limited.
In particular, it is unclear what makes hypotheses spaces born from convolution and pooling operations so suitable for natural images.
In this paper we study the ability of convolutional arithmetic circuits to model correlations among regions of their input.
Correlations are formalized through the notion of separation rank, which for a given input partition, measures how far a function is from being separable.
We show that a polynomially sized deep network supports exponentially high separation ranks for certain input partitions, while being limited to polynomial separation ranks for others.
The network's pooling geometry effectively determines which input partitions are favored, thus serves as a means for controlling the inductive bias.
Contiguous pooling windows as commonly employed in practice favor interleaved partitions over coarse ones, orienting the inductive bias towards the statistics of natural images.
In addition to analyzing deep networks, we show that shallow ones support only linear separation ranks, and by this gain insight into the benefit of functions brought forth by depth - they are able to efficiently model strong correlation under favored partitions of the input.
