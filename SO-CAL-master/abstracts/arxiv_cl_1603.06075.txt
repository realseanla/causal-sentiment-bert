Most of the existing neural machine translation (NMT) models focus on the conversion of sequential data and do not directly take syntax into consideration.
We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure.
Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence.
Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system.
