This work targets people identification in video based on the way they walk (i.e.
gait).
While classical methods typically derive gait signatures from sequences of binary silhouettes, in this work we explore the use of convolutional neural networks (CNN) for learning high-level descriptors from low-level motion features (i.e.
optical flow components).
We carry out a thorough experimental evaluation of the proposed CNN architecture on the challenging TUM-GAID dataset.
The experimental results indicate that using spatio-temporal cuboids of optical flow as input data for CNN allows to obtain state-of-the-art results on the gait task with an image resolution eight times lower than the previously reported results (i.e.
80x60 pixels).
