Natural language generation plays a critical role in any spoken dialogue system.
We present a new approach to natural language generation using recurrent neural networks in an encoder-decoder framework.
In contrast with previous work, our model uses both lexicalized and delexicalized versions of slot-value pairs for each dialogue act.
This allows our model to learn from all available data, rather than being restricted to learning only from delexicalized slot-value pairs.
We show that this helps our model generate more natural sentences with better grammar.
We further improve our model's performance by initializing its weights from a pretrained language model.
Human evaluation of our best-performing model indicates that it generates sentences which users find more natural and appealing.
