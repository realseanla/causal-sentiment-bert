In visual recognition tasks, supervised learning shows excellent performance.
On the other hand, unsupervised learning exploits cheap unlabeled data and can help to solve the same tasks more efficiently.
We show that the recursive autoconvolutional operator, adopted from physics, boosts existing unsupervised methods to learn more powerful filters.
We use a well established multilayer convolutional network and train filters layer-wise.
To build a stronger classifier, we design a very light committee of SVM models.
The total number of trainable parameters is also greatly reduced by using shared filters in higher layers.
We evaluate our networks on the MNIST, CIFAR-10 and STL-10 benchmarks and report several state of the art results among other unsupervised methods.
