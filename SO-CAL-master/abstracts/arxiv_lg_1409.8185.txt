We develop a sequential low-complexity inference procedure for the Infinite Gaussian Mixture Model (IGMM) for the general case of an unknown mean and covariance.
The observations are sequentially allocated to classes based on a sequential maximum a-posterior (MAP) criterion.
We present an easily computed, closed form for the conditional likelihood, in which the parameters can be recursively updated as a function of the streaming data.
We propose a novel adaptive design for the Dirichlet process concentration parameter at each iteration, and prove, under a simplified model, that the sequence of concentration parameters is asymptotically well-behaved.
We sketch an equivalence between the steady-state performance of the algorithm and Gaussian classification.
The methodology is applied to the problem of adaptive modulation recognition and obviates the need for storing a large modulation library required for traditional modulation recognition.
We also numerically evaluate the bit error rate performance (BER) of the DPMM-trained classifier when used as a demodulator and show that there is critical signal-to-noise ratio (SNR) that characterizes whether successful decoding is possible.
