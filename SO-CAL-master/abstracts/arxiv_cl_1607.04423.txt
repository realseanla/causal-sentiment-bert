Cloze-style queries are representative problems in reading comprehension.
Over the past few months, we have seen much progress that utilizing neural network approach to solve Cloze-style questions.
In this work, we present a novel model for Cloze-style reading comprehension tasks, called attention-over-attention reader.
Our model aims to place another attention mechanism over the document-level attention, and induces "attended attention" for final predictions.
Unlike the previous works, our neural network model requires less pre-defined hyper-parameters and uses an elegant architecture for modeling.
Experimental results show that the proposed attention-over-attention model significantly outperforms various state-of-the-art systems by a large margin in public datasets, such as CNN and Children's Book Test datasets.
