Word embeddings have been demonstrated to benefit NLP tasks impressively.
Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise.
We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings.
The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter.
Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings.
