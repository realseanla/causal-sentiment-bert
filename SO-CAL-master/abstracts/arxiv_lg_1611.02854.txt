Recent work has demonstrated the effectiveness of employing explicit external memory structures in conjunction with deep neural models for algorithmic learning (Graves et al.
2014; Weston et al.
2014).
These models utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the variable-length storage necessary for computational tasks.
In this work, we propose an alternative model, Lie-access memory, that is explicitly designed for the neural setting.
In this paradigm, memory is accessed using a continuous head in a key-space manifold.
The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and soft memory access is performed by considering the distance to keys associated with each memory.
We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintain differentiability.
To experiment with this approach, we implement several simplified Lie-access neural Turing machine (LANTM) with different Lie groups.
We find that this approach is able to perform well on a range of algorithmic tasks.
