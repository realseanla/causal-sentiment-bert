Monte Carlo Tree Search (MCTS) has improved the performance of game playing engines in domains such as Go, Hex, and general-game playing.
MCTS has been shown to outperform outperform classic alpha-beta search in games where good heuristic evaluations are difficult to obtain.
In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough.
In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately.
Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during its MCTS simulations.
These learned evaluation values are then used to guide future simulations.
Compared to current techniques, we show that using implicit minimax backups leads to stronger play performance in Breakthrough, Lines of Action, and Kalah.
