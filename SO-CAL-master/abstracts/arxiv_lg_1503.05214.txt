Web sites, social networks, sensors, and scientific experiments currently generate massive amounts of data.
Owners of this data strive to obtain insights from it, often by applying machine learning algorithms.
Many machine learning algorithms, however, do not scale well to cope with the ever increasing volumes of data.
To address this problem, we identify several optimizations that are crucial for scaling various machine learning algorithms in distributed settings.
We apply these optimizations to the popular Principal Component Analysis (PCA) algorithm.
PCA is an important tool in many areas including image processing, data visualization, information retrieval, and dimensionality reduction.
We refer to the proposed optimized PCA algorithm as scalable PCA, or sPCA.
sPCA achieves scalability via employing efficient large matrix operations, effectively leveraging matrix sparsity, and minimizing intermediate data.
We implement sPCA on the widely-used MapReduce platform and on the memory-based Spark platform.
We compare sPCA against the closest PCA implementations, which are the ones in Mahout/MapReduce and MLlib/Spark.
Our experiments show that sPCA outperforms both Mahout-PCA and MLlib-PCA by wide margins in terms of accuracy, running time, and volume of intermediate data generated during the computation.
