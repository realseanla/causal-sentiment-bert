We combine supervised learning with unsupervised learning in deep neural networks.
The proposed model is trained to simultaneously minimize the sum of supervised and unsupervised cost functions by backpropagation, avoiding the need for layer-wise pretraining.
Our work builds on top of the Ladder network proposed by Valpola (2015) which we extend by combining the model with supervision.
We show that the resulting model reaches state-of-the-art performance in various tasks: MNIST and CIFAR-10 classification in a semi-supervised setting and permutation invariant MNIST in both semi-supervised and full-labels setting.
