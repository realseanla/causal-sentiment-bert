Automatic construction of large knowledge graphs (KG) by mining web-scale text datasets has received considerable attention over the last few years, resulting in the construction of several KGs, such as NELL, Google Knowledge Vault, etc.
These KGs consist of thousands of predicate-relations (e.g., isPerson, isMayorOf ) and millions of their instances (e.g., (Bill de Blasio, isMayorOf, New York City)).
Estimating accuracy of such automatically constructed KGs is a challenging problem due to their size and diversity.
Even though crowdsourcing is an obvious choice for such evaluation, the standard single-task crowdsourcing, where each predicate in the KG is evaluated independently, is very expensive and especially problematic if the budget available is limited.
We show that such approaches are sub-optimal as they ignore dependencies among various predicates and their instances.
To overcome this challenge, we propose Relational Crowdsourcing (RelCrowd), where the tasks are created while taking dependencies among predicates and instances into account.
We apply this framework in the context of evaluation of large-scale KGs and demonstrate its effectiveness through extensive experiments on real-world datasets.
