Lipreading is the task of decoding text from the movement of a speaker's mouth.
Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction.
More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung &amp; Zisserman, 2016a).
All existing works, however, perform only word classification, not sentence-level sequence prediction.
Studies have shown that human lipreading performance increases for longer words (Easton &amp; Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel.
Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, an LSTM recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end.
To the best of our knowledge, LipNet is the first lipreading model to operate at sentence-level, using a single end-to-end speaker-independent deep model to simultaneously learn spatiotemporal visual features and a sequence model.
On the GRID corpus, LipNet achieves 93.4 percent accuracy, outperforming experienced human lipreaders and the previous 79.6 percent state-of-the-art accuracy.
