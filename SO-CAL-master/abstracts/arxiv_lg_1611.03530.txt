Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance.
Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.
