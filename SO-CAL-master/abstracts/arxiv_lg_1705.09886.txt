In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks.
However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing.
