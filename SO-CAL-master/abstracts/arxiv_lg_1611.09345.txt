Multi-domain learning aims to benefit from simultaneously learning across several different but related domains.
In this chapter, we propose a single framework that unifies multi-domain learning (MDL) and the related but better studied area of multi-task learning (MTL).
By exploiting the concept of a \emph{semantic descriptor} we show how our framework encompasses various classic and recent MDL/MTL algorithms as special cases with different semantic descriptor encodings.
As a second contribution, we present a higher order generalisation of this framework, capable of simultaneous multi-task-multi-domain learning.
This generalisation has two mathematically equivalent views in multi-linear algebra and gated neural networks respectively.
Moreover, by exploiting the semantic descriptor, it provides neural networks the capability of zero-shot learning (ZSL), where a classifier is generated for an unseen class without any training data; as well as zero-shot domain adaptation (ZSDA), where a model is generated for an unseen domain without any training data.
In practice, this framework provides a powerful yet easy to implement method that can be flexibly applied to MTL, MDL, ZSL and ZSDA.
