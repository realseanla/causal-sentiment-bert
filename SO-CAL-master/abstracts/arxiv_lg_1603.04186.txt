Convolutional neural networks have been shown to develop internal representations, which correspond closely to semantically meaningful objects and parts, although trained solely on class labels.
Class Activation Mapping (CAM) is a recent method that makes it possible to easily highlight the image regions contributing to a network's classification decision.
We build upon these two developments to enable a network to re-examine informative image regions, which we term introspection.
We propose a weakly-supervised iterative scheme, which shifts its center of attention to increasingly discriminative regions as it progresses, by alternating stages of classification and introspection.
We evaluate our method and show its effectiveness over a range of several datasets, obtaining a top-1 accuracy 84.48 percent CUB-200-2011, which is the highest to-date without using external data or stronger supervision.
On Stanford-40 Actions, we set a new state-of the art of 87.89 percent, and on FGVC-Aircraft and the Stanford Dogs dataset, we show consistent improvements over baselines, some of which include significantly more supervision.
