Acoustic event detection for content analysis in most cases relies on lots of labeled data.
However, manually annotating data is a time-consuming task, which thus makes few annotated resources available so far.
Unlike audio event detection, automatic audio tagging, a multi-label acoustic event classification task, only relies on weakly labeled data.
This is highly desirable to some practical applications using audio analysis.
In this paper we propose to use a fully deep neural network (DNN) framework to handle the multi-label classification task in a regression way.
Considering that only chunk-level rather than frame-level labels are available, the whole or almost whole frames of the chunk were fed into the DNN to perform a multi-label regression for the expected tags.
The fully DNN, which is regarded as an encoding function, can well map the audio features sequence to a multi-tag vector.
A deep pyramid structure was also designed to extract more robust high-level features related to the target tags.
Further improved methods were adopted, such as the Dropout and background noise aware training, to enhance its generalization capability for new audio recordings in mismatched environments.
Compared with the conventional Gaussian Mixture Model (GMM) and support vector machine (SVM) methods, the proposed fully DNN-based method could well utilize the long-term temporal information with the whole chunk as the input.
The results show that our approach obtained a 15 percent relative improvement compared with the official GMM-based method of DCASE 2016 challenge.
