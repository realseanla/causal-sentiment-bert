Deep convolutional neural network (CNN) inference requires significant amount of memory and computation, which limits its deployment on embedded devices.
To alleviate these problems to some extent, prior research utilize low precision fixed-point numbers to represent the CNN weights and activations.
However, the minimum required data precision of fixed-point weights varies across different networks and also across different layers of the same network.
In this work, we propose using floating-point numbers for representing the weights and fixed-point numbers for representing the activations.
We show that using floating-point representation for weights is more efficient than fixed-point representation for the same bit-width and demonstrate it on popular large-scale CNNs such as AlexNet, SqueezeNet, GoogLeNet and VGG-16.
We also show that such a representation scheme enables compact hardware multiply-and-accumulate (MAC) unit design.
Experimental results show that the proposed scheme reduces the weight storage by up to 36 percent and power consumption of the hardware multiplier by up to 50 percent.
