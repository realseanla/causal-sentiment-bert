Object reconstruction is an important task in many fields of application as it allows to generate digital representations of our physical world used as base for analysis, planning, construction, visualization or other aims.
A reconstruction itself normally is based on reliable data (images, 3D point clouds for example) expressing the object in his complete extent.
This data then has to be compiled and analyzed in order to extract all necessary geometrical elements, which represent the object and form a digital copy of it.
Traditional strategies are largely based on manual interaction and interpretation, because with increasing complexity of objects human understanding is inevitable to achieve acceptable and reliable results.
But human interaction is time consuming and expensive, why many researches has already been invested to use algorithmic support, what allows to speed up the process and to reduce manual work load.
Presently most of such supporting algorithms are data-driven and concentate on specific features of the objects, being accessible to numerical models.
By means of these models, which normally will represent geometrical (flatness, roughness, for example) or physical features (color, texture), the data is classified and analyzed.
This is successful for objects with low complexity, but gets to its limits with increasing complexness of objects.
Then purely numerical strategies are not able to sufficiently model the reality.
Therefore, the intention of our approach is to take human cognitive strategy as an example, and to simulate extraction processes based on available human defined knowledge for the objects of interest.
Such processes will introduce a semantic structure for the objects and guide the algorithms used to detect and recognize objects, which will yield a higher effectiveness.
Hence, our research proposes an approach using knowledge to guide the algorithms in 3D point cloud and image processing.
