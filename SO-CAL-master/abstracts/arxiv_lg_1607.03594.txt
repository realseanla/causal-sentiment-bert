Assessing uncertainty within machine learning systems is an important step towards ensuring their safety and reliability.
Existing uncertainty estimation techniques may fail when their modeling assumptions are not met, e.g.
when the data distribution differs from the one seen at training time.
Here, we propose techniques that assess a classification algorithm's uncertainty via calibrated probabilities (i.e.
probabilities that match empirical outcome frequencies in the long run) and which are guaranteed to be reliable (i.e.
accurate and calibrated) on out-of-distribution input, including input generated by an adversary.
Our methods admit formal bounds that can serve as confidence intervals, and process data in an online manner, which obviates the need for a separate calibration dataset.
We establish theoretical guarantees on our methods' accuracies and convergence rates, and we validate them on two real-world problems: question answering and medical diagnosis from genomic data.
