We consider multi-label prediction problems with large output spaces under the assumption of output sparsity -- that the target vectors have small support.
We develop a general theory for a variant of the popular ECOC (error correcting output code) scheme, based on ideas from compressed sensing for exploiting this sparsity.
The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems.
It is shown that the number of subproblems need only be logarithmic in the total number of label values, making this approach radically more efficient than others.
We also state and prove performance guarantees for this method, and test it empirically.
