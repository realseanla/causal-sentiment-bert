This paper presents an empirical study of two widely-used sequence prediction models, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks (LSTMs), on two fundamental tasks for Vietnamese text processing, including part-of-speech tagging and named entity recognition.
We show that a strong lower bound for labeling accuracy can be obtained by relying only on simple word-based features with minimal hand-crafted feature engineering, of 90.65\ percent and 86.03\ percent performance scores on the standard test sets for the two tasks respectively.
In particular, we demonstrate empirically the surprising efficiency of word embeddings in both of the two tasks, with both of the two models.
We point out that the state-of-the-art LSTMs model does not always outperform significantly the traditional CRFs model, especially on moderate-sized data sets.
Finally, we give some suggestions and discussions for efficient use of sequence labeling models in practical applications.
