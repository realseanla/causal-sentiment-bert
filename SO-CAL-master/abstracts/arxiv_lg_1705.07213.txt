Deep Neural Networks (DNNs) are presently the state-of-the-art for image classification tasks.
However, recent works have shown that these systems can be easily fooled to misidentify images by modifying the image in a particular way.
Moreover, defense mechanisms proposed in the literature so far are mostly attack-specific and prove to be ineffective against new attacks.
Indeed, recent work on universal perturbations can generate a single modification for all test images that is able to make existing networks misclassify 90 percent of the time.
Presently, to our knowledge, no defense mechanisms are effective in preventing this.
As such, the design of a general defense strategy against a wide range of attacks for Neural Networks becomes a challenging problem.
In this paper, we derive inspiration from recent advances in the field of cybersecurity and multi-agent systems and propose to use the concept of Moving Target Defense (MTD) for increasing the robustness of well-known deep networks trained on the ImageNet dataset towards such adversarial attacks.
In using this technique, we formalize and exploit the notion of differential immunity of different networks to specific attacks.
To classify a single test image, we pick one of the trained networks each time and then use its classification output.
To ensure maximum robustness, we generate an effective strategy by formulating this interaction as a Repeated Bayesian Stackelberg Game with a Defender and the Users.
As a network switching strategy, we compute a Strong Stackelberg Equilibrium that optimizes the accuracy of prediction while at the same time reduces the misclassification rate on adversarial modification of test images.
We show that while our approach produces an accuracy of 92.79 percent for the legitimate users, attackers can only misclassify images 58 percent (instead of 93.7 percent) of the time even when they select the best attack available to them.
