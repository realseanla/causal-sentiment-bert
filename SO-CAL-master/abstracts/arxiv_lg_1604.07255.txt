The ability to reuse or transfer knowledge from one task to another in lifelong learning problems, such as Minecraft, is one of the major challenges faced in AI.
Reusing knowledge across tasks is crucial to solving tasks efficiently with lower sample complexity.
We provide a Reinforcement Learning agent with the ability to transfer knowledge by learning reusable skills, a type of temporally extended action (also know as Options (Sutton et.
al.
1999)).
The agent learns reusable skills using Deep Q Networks (Mnih et.
al.
2015) to solve tasks in Minecraft, a popular video game which is an unsolved and high-dimensional lifelong learning problem.
These reusable skills, which we refer to as Deep Skill Networks (DSNs), are then incorporated into our novel Hierarchical Deep Reinforcement Learning Network (H-DRLN) architecture.
The H-DRLN is a hierarchical version of Deep QNetworks and learns to efficiently solve tasks by reusing knowledge from previously learned DSNs.
The H-DRLN exhibits superior performance and lower learning sample complexity (by taking advantage of temporal extension) compared to the regular Deep Q Network (Mnih et.
al.
2015) in subdomains of Minecraft.
We also show the potential to transfer knowledge between related Minecraft tasks without any additional learning.
