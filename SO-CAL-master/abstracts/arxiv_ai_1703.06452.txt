A semantic segmentation algorithm must assign a label to every pixel in an image.
Recently, semantic segmentation of RGB imagery has advanced significantly due to deep learning.
Because creating datasets for semantic segmentation is laborious, these datasets tend to be significantly smaller than object recognition datasets.
This makes it difficult to directly train a deep neural network for semantic segmentation, because it will be prone to overfitting.
To cope with this, deep learning models typically use convolutional neural networks pre-trained on large-scale image classification datasets, which are then fine-tuned for semantic segmentation.
For non-RGB imagery, this is currently not possible because large-scale labeled non-RGB datasets do not exist.
In this paper, we developed two deep neural networks for semantic segmentation of multispectral remote sensing imagery.
Prior to training on the target dataset, we initialize the networks with large amounts of synthetic multispectral imagery.
We show that this significantly improves results on real-world remote sensing imagery, and we establish a new state-of-the-art result on the challenging Hamlin Beach State Park Dataset.
