A common and effective way to train translation systems between related languages is to consider sub-word level basic units.
However, this increases the length of the sentences resulting in increased decoding time.
The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords.
In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy.
We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy.
