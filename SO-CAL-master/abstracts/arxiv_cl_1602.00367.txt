Document classification tasks were primarily tackled at word level.
Recent research that works with character-level inputs shows several benefits over word-level approaches such as natural incorporation of morphemes and better handling of rare words.
We propose a neural network architecture that utilizes both convolution and recurrent layers to efficiently encode character inputs.
We validate the proposed model on eight large scale document classification tasks and compare with character-level convolution-only models.
It achieves comparable performances with much less parameters.
