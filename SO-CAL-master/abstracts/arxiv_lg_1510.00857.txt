The bag-of-words (BoW) model treats images as sets of local descriptors and represents them by visual word histograms.
The Fisher vector (FV) representation extends BoW, by considering the first and second order statistics of local descriptors.
In both representations local descriptors are assumed to be identically and independently distributed (iid), which is a poor assumption from a modeling perspective.
It has been experimentally observed that the performance of BoW and FV representations can be improved by employing discounting transformations such as power normalization.
In this paper, we introduce non-iid models by treating the model parameters as latent variables which are integrated out, rendering all local regions dependent.
Using the Fisher kernel principle we encode an image by the gradient of the data log-likelihood w.r.t.
the model hyper-parameters.
Our models naturally generate discounting effects in the representations; suggesting that such transformations have proven successful because they closely correspond to the representations obtained for non-iid models.
To enable tractable computation, we rely on variational free-energy bounds to learn the hyper-parameters and to compute approximate Fisher kernels.
Our experimental evaluation results validate that our models lead to performance improvements comparable to using power normalization, as employed in state-of-the-art feature aggregation methods.
