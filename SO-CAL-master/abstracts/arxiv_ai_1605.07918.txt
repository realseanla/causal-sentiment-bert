Previous studies in Open Information Extraction (Open IE) are mainly based on extraction patterns.
They manually define patterns or automatically learn them from a large corpus.
However, these approaches are limited when grasping the context of a sentence, and they fail to capture implicit relations.
In this paper, we address this problem with the following methods.
First, we exploit long short-term memory (LSTM) networks to extract higher-level features along the shortest dependency paths, connecting headwords of relations and arguments.
The path-level features from LSTM networks provide useful clues regarding contextual information and the validity of arguments.
Second, we constructed samples to train LSTM networks without the need for manual labeling.
In particular, feedback negative sampling picks highly negative samples among non-positive samples through a model trained with positive samples.
The experimental results show that our approach produces more precise and abundant extractions than state-of-the-art open IE systems.
To the best of our knowledge, this is the first work to apply deep learning to Open IE.
