A central unsolved problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable.
Previous approaches to this problem are subject to tradeoffs between flexibility and tractability.
We develop a promising approach that simultaneously achieves both.
The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process.
We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data.
This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps.
