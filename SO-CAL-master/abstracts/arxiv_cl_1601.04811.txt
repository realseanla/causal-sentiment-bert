However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems.
In response to this problem, we maintain a coverage vector to keep track of the attention history.
The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words.
Experiments show that coverage-based NMT significantly improves both alignment and translation quality over NMT without coverage.
