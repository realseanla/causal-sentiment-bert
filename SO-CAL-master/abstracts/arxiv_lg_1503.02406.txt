Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle.
We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables.
Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds.
The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity.
We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer.
The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve.
We believe that this new insight can lead to new optimality bounds and deep learning algorithms.
