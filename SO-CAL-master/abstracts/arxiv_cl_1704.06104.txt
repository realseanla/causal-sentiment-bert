We investigate neural techniques for end-to-end computational argumentation mining.
We frame the problem as a token-based dependency parsing as well as a token-based sequence tagging model, including a multi-task learning setup.
Contrary to models that operate on the argument component level, we find that framing the problem as dependency parsing leads to subpar performance results.
In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the argumentation mining problem.
Moreover, we find that jointly learning 'natural' subtasks, in a multi-task learning setup, improves performance.
