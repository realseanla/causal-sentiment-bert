Online reinforcement learning (RL) is increasingly popular for the personalized mobile health (mHealth) intervention.
It is able to personalize the type and dose of interventions according to user's ongoing statuses and changing needs.
However, at the beginning of online learning, there are usually too few samples to support the RL updating, which leads to poor performances.
A delay in good performance of the online learning algorithms can be especially detrimental in the mHealth, where users tend to quickly disengage with apps.
To address this problem, we propose a new online RL methodology that focuses on an effective warm start.
The main idea is to make full use of the data accumulated and the decision rule achieved in a former study.
As a result, we can greatly enrich the data size at the beginning of online learning in our method.
Such case accelerates the online learning process for new users to achieve good performances not only at the beginning of online learning but also through the whole online learning process.
Besides, we use the decision rules achieved previously to initialize the parameter in our online RL model for new users.
It provides a good initialization for the proposed online RL algorithm.
Experiment results show that promising improvements have been achieved by our method compared with the state-of-the-art method.
