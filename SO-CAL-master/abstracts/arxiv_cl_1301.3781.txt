We propose two novel model architectures for computing continuous vector representations of words from very large data sets.
The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.
We observe large improvements in accuracy at much lower computational cost, i.e.
it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set.
Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities.
We intend to publish this test set to be used by the research community.
