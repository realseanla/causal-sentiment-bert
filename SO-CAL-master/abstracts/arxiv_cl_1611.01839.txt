Reading an article and answering questions about its content is a fundamental task for natural language understanding.
While most successful neural approaches to this problem rely on recurrent neural networks (RNNs), training RNNs over long documents can be prohibitively slow.
We present a novel framework for question answering that can efficiently scale to longer documents while maintaining or even improving performance.
Our approach combines a coarse, inexpensive model for selecting one or more relevant sentences and a more expensive RNN that produces the answer from those sentences.
A central challenge is the lack of intermediate supervision for the coarse model, which we address using reinforcement learning.
Experiments demonstrate state-of-the-art performance on a challenging subset of the WIKIREADING dataset(Hewlett et al., 2016) and on a newly-gathered dataset, while reducing the number of sequential RNN steps by 88 percent against a standard sequence to sequence model.
