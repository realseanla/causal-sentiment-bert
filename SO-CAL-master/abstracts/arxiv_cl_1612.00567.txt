Transition-based models can be fast and accurate for constituent parsing.
Compared with chart-based models, they leverage richer features by extracting history information from a parser stack, which spans over non-local constituents.
On the other hand, during incremental parsing, constituent information on the right hand side of the current word is not utilized, which is a relative weakness of shift-reduce parsing.
To address this limitation, we leverage a fast neural model to extract lookahead features.
In particular, we build a bidirectional LSTM model, which leverages the full sentence information to predict the hierarchy of constituents that each word starts and ends.
The results are then passed to a strong transition-based constituent parser as lookahead features.
The resulting parser gives 1.3 percent absolute improvement in WSJ and 2.3 percent in CTB compared to the baseline, given the highest reported accuracies for fully-supervised parsing.
