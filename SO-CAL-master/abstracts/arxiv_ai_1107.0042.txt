Value iteration is a popular algorithm for finding near optimal policies for POMDPs.
It is inefficient due to the need to account for the entire belief space, which necessitates the solution of large numbers of linear programs.
In this paper, we study value iteration restricted to belief subsets.
We show that, together with properly chosen belief subsets, restricted value iteration yields near-optimal policies and we give a condition for determining whether a given belief subset would bring about savings in space and time.
We also apply restricted value iteration to two interesting classes of POMDPs, namely informative POMDPs and near-discernible POMDPs.
