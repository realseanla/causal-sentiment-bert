We show that an encoder-decoder framework can be successfully be applied to question-answering with a structured knowledge base.
In addition, we propose a new character-level modeling approach for this task, which we use to make our model robust to unseen entities and predicates.
We use our model for single-relation question answering, and demonstrate the effectiveness of our novel approach on the SimpleQuestions dataset, where we improve state-of-the-art accuracy by 2 percent for both Freebase2M and Freebase5M subsets proposed.
Importantly, we achieve these results even though our character-level model has 16x less parameters than an equivalent word-embedding model, uses significantly less training data than previous work which relies on data augmentation, and encounters only 1.18 percent of the entities seen during training when testing.
