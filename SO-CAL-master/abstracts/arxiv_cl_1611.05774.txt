Recurrent neural network grammars (RNNG) are a recently proposed probabilistic generative modeling family for natural language.
They show state-of-the-art language modeling and parsing performance.
We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection.
We find that explicit modeling of composition is crucial for achieving the best performance.
Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model's latent attention largely agreeing with predictions made by hand-crafted rules, albeit with some important differences).
By training grammars without non-terminal labels, we find that phrasal representations depend minimally on non-terminals, providing support for the endocentricity hypothesis.
