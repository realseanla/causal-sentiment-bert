Word embeddings have been found to capture a surprisingly rich amount of syntactic and semantic knowledge.
However, it is not yet sufficiently well-understood how the relational knowledge that is implicitly encoded in word embeddings can be extracted in a reliable way.
In this paper, we propose two probabilistic models to address this issue.
The first model is based on the common relations-as-translations view, but is cast in a probabilistic setting.
Our second model is based on the much weaker assumption that there is a linear relationship between the vector representations of related words.
Compared to existing approaches, our models lead to more accurate predictions, and they are more explicit about what can and cannot be extracted from the word embedding.
