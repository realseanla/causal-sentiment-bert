Deep learning has become the state-of-art tool in many applications, but the evaluation and training of deep models can be time-consuming and computationally expensive.
Dropout has been shown to be an effective strategy to sparsify computations (by not involving all units), as well as to regularize models.
In typical dropout, nodes are dropped uniformly at random.
Our goal is to use reinforcement learning in order to design better, more informed dropout policies, which are data-dependent.
We cast the problem of learning activation-dependent dropout policies for blocks of units as a reinforcement learning problem.
We propose a learning scheme motivated by computation speed, capturing the idea of wanting to have parsimonious activations while maintaining prediction accuracy.
We apply a policy gradient algorithm for learning policies that optimize this loss function and propose a regularization mechanism that encourages diversification of the dropout policy.
We present encouraging empirical results showing that this approach improves the speed of computation without impacting the quality of the approximation.
