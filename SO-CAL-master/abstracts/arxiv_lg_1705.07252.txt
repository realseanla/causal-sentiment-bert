Support Vector Machine is one of the most classical approaches for classification and regression.
Despite being studied for decades, obtaining practical algorithms for SVM is still an active research problem in machine learning.
In this paper, we propose a new perspective for SVM via saddle point optimization.
We provide an algorithm which achieves $(1-\epsilon)$-approximations with running time $\tilde{O}(nd+n\sqrt{d / \epsilon})$ for both separable (hard margin SVM) and non-separable cases ($\nu$-SVM ), where $n$ is the number of points and $d$ is the dimensionality.
To the best of our knowledge, the current best algorithm for hard margin SVM achieved by Gilbert algorithm~ requires $O(nd / \epsilon )$ time.
Our algorithm improves the running time by a factor of $\sqrt{d}/\sqrt{\epsilon}$.
For $\nu$-SVM, besides the well known quadratic programming approach which requires $\Omega(n^2 d)$ time~, no better algorithm is known.
In the paper, we provide the first nearly linear time algorithm for $\nu$-SVM.
We also consider the distributed settings and provide distributed algorithms with low communication cost via saddle point optimization.
Our algorithms require $\tilde{O}(k(d +\sqrt{d/\epsilon}))$ communication cost where $k$ is the number of clients, almost matching the theoretical lower bound.
