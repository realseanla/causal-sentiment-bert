In the paradigm of multi-task learning, mul- tiple related prediction tasks are learned jointly, sharing information across the tasks.
We propose a framework for multi-task learn- ing that enables one to selectively share the information across the tasks.
We assume that each task parameter vector is a linear combi- nation of a finite number of underlying basis tasks.
The coefficients of the linear combina- tion are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these.
Our model is based on on the assumption that task pa- rameters within a group lie in a low dimen- sional subspace but allows the tasks in differ- ent groups to overlap with each other in one or more bases.
Experimental results on four datasets show that our approach outperforms competing methods.
