In this paper, we propose a new deep learning approach, called neural association model (NAM), for probabilistic reasoning in artificial intelligence.
We propose to use neural networks to model association between any two events in a domain.
Neural networks take one event as input and compute a conditional probability of the other event to model how likely these two events are associated.
The actual meaning of the conditional probabilities varies between applications and depends on how the models are trained.
In this work, as two case studies, we have investigated two NAM structures, namely deep neural networks (DNNs) and relation modulated neural nets (RMNNs), on several probabilistic reasoning tasks in AI, including recognizing textual entailment, triple classification in multirelational knowledge bases and common-sense reasoning.
Experimental results on several popular data sets derived from WordNet, FreeBase and ConceptNet have all demonstrated that both DNNs and RMNNs perform equally well and they can significantly outperform the conventional methods available for these reasoning tasks.
Moreover, comparing with DNNs, RMNNs are superior in knowledge transfer, where a pre-trained model can be quickly extended to an unseen relation after observing only a few training samples.
