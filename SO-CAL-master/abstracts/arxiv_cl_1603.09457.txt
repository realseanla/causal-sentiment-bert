In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations.
Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model.
The conversational model can function as a language model or a language generation model.
Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants.
The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking.
Generated responses show characteristic differences between the two participant roles.
