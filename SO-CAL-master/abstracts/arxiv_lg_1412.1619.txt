In this work we consider the learning setting where in addition to the training set, the learner receives a collection of auxiliary hypotheses originating from other tasks.
This paradigm, known as Hypothesis Transfer Learning (HTL), has been successfully exploited in empirical works, but only recently has received a theoretical attention.
Here, we try to understand when HTL facilitates accelerated generalization -- the goal of the transfer learning paradigm.
Thus, we study a broad class of algorithms, a Hypothesis Transfer Learning through Regularized ERM, that can be instantiated with any non-negative smooth loss function and any strongly convex regularizer.
We establish generalization and excess risk bounds, showing that if the algorithm is fed with a good source hypotheses combination, generalization happens at the fast rate $\mathcal{O}(1/m)$ instead of usual $\mathcal{O}(1/\sqrt{m})$.
We also observe that if the combination is perfect, our theory formally backs up the intuition that learning is not necessary.
On the other hand, if the source hypotheses combination is a misfit for the target task, we recover the usual learning rate.
As a byproduct of our study, we also prove a new bound on the Rademacher complexity of the smooth loss class under weaker assumptions compared to previous works.
