Technical documents contain a fair amount of unnatural language, such as tables, formulas, pseudo-codes, etc.
Unnatural language can be an important factor of confusing existing NLP tools.
This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering.
We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories.
First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories.
We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text.
Our experiments show that removing unnatural language components gives an absolute improvement in document clustering up to 15 percent.
Our corpus and tool are publicly available.
