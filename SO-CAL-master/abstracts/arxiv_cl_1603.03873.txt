Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts.
Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion.
We refer to this recognizer as SeMDER.
Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse.
A semantic encoder with attention to the semantic memory matrix is further established over surface representations.
It is able to retrieve a deep semantic meaning representation for the discourse from the memory.
Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer.
Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56\ percent on average over current state-of-the-art baselines in terms of F1-score.
