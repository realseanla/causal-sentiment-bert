Unsupervised structure learning in high-dimensional time series data has attracted a lot of research interests.
For example, segmenting and labelling high dimensional time series can be helpful in behavior understanding and medical diagnosis.
Recent advances in generative sequential modeling have suggested to combine recurrent neural networks with state space models (e.g., Hidden Markov Models).
This combination can model not only the long term dependency in sequential data, but also the uncertainty included in the hidden states.
Inheriting these advantages of stochastic neural sequential models, we propose a structured and stochastic sequential neural network, which models both the long-term dependencies via recurrent neural networks and the uncertainty in the segmentation and labels via discrete random variables.
For accurate and efficient inference, we present a bi-directional inference network by reparamterizing the categorical segmentation and labels with the recent proposed Gumbel-Softmax approximation and resort to the Stochastic Gradient Variational Bayes.
We evaluate the proposed model in a number of tasks, including speech modeling, automatic segmentation and labeling in behavior understanding, and sequential multi-objects recognition.
Experimental results have demonstrated that our proposed model can achieve significant improvement over the state-of-the-art methods.
